{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0000", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nConcepts\n1: Overview\n1.1: Kubernetes Components\n1.2: Objects In Kubernetes\n1.2.1: Kubernetes Object Management\n1.2.2: Object Names and IDs\n1.2.3: Labels and Selectors\n1.2.4: Namespaces\n1.2.5: Annotations\n1.2.6: Field Selectors\n1.2.7: Finalizers\n1.2.8: Owners and Dependents\n1.2.9: Recommended Labels\n1.3: The Kubernetes API\n2: Cluster Architecture\n2.1: Nodes\n2.2: Communication between Nodes and the Control Plane\n2.3: Controllers\n2.4: Leases\n2.5: Cloud Controller Manager\n2.6: About cgroup v2\n2.7: Kubernetes Self-Healing\n2.8: Garbage Collection\n2.9: Mixed Version Proxy\n3: Containers\n3.1: Images\n3.2: Container Environment\n3.3: Runtime Class\n3.4: Container Lifecycle Hooks\n3.5: Container Runtime Interface (CRI)\n4: Workloads\n4.1: Pods\n4.1.1: Pod Lifecycle\n4.1.2: Init Containers\n4.1.3: Sidecar Containers\n4.1.4: Ephemeral Containers\n4.1.5: Disruptions\n4.1.6: Pod Hostname\n4.1.7: Pod Quality of Service Classes\n4.1.8: User Namespaces\n4.1.9: Downward API\n4.2: Workload Management\n4.2.1: Deployments\n4.2.2: ReplicaSet\n4.2.3: StatefulSets\n4.2.4: DaemonSet\n4.2.5: Jobs\n4.2.6: Automatic Cleanup for Finished Jobs\n4.2.7: CronJob\n4.2.8: ReplicationController\n4.3: Autoscaling Workloads\n4.4: Managing Workloads\nhttps://kubernetes.io/docs/concepts/_print/\n\n1/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0001", "text": "5: Services, Load Balancing, and Networking\n5.1: Service\n5.2: Ingress\n5.3: Ingress Controllers\n5.4: Gateway API\n5.5: EndpointSlices\n5.6: Network Policies\n5.7: DNS for Services and Pods\n5.8: IPv4/IPv6 dual-stack\n5.9: Topology Aware Routing\n5.10: Networking on Windows\n5.11: Service ClusterIP allocation\n5.12: Service Internal Traffic Policy\n6: Storage\n6.1: Volumes\n6.2: Persistent Volumes\n6.3: Projected Volumes\n6.4: Ephemeral Volumes\n6.5: Storage Classes\n6.6: Volume Attributes Classes\n6.7: Dynamic Volume Provisioning\n6.8: Volume Snapshots\n6.9: Volume Snapshot Classes\n6.10: CSI Volume Cloning\n6.11: Storage Capacity\n6.12: Node-specific Volume Limits\n6.13: Volume Health Monitoring\n6.14: Windows Storage\n7: Configuration\n7.1: Configuration Best Practices\n7.2: ConfigMaps\n7.3: Secrets\n7.4: Liveness, Readiness, and Startup Probes\n7.5: Resource Management for Pods and Containers\n7.6: Organizing Cluster Access Using kubeconfig Files\n7.7: Resource Management for Windows nodes\n8: Security\n8.1: Cloud Native Security and Kubernetes\n8.2: Pod Security Standards\n8.3: Pod Security Admission\n8.4: Service Accounts\n8.5: Pod Security Policies\n8.6: Security For Linux Nodes\n8.7: Security For Windows Nodes\n8.8: Controlling Access to the Kubernetes API\n8.9: Role Based Access Control Good Practices\n8.10: Good practices for Kubernetes Secrets\n8.11: Multi-tenancy\n8.12: Hardening Guide - Authentication Mechanisms\n8.13: Hardening Guide - Scheduler Configuration\n8.14: Kubernetes API Server Bypass Risks\n8.15: Linux kernel security constraints for Pods and containers\n8.16: Security Checklist\n8.17: Application Security Checklist\nhttps://kubernetes.io/docs/concepts/_print/\n\n2/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0002", "text": "9: Policies\n9.1: Limit Ranges\n9.2: Resource Quotas\n9.3: Process ID Limits And Reservations\n9.4: Node Resource Managers\n10: Scheduling, Preemption and Eviction\n10.1: Kubernetes Scheduler\n10.2: Assigning Pods to Nodes\n10.3: Pod Overhead\n10.4: Pod Scheduling Readiness\n10.5: Pod Topology Spread Constraints\n10.6: Taints and Tolerations\n10.7: Scheduling Framework\n10.8: Dynamic Resource Allocation\n10.9: Scheduler Performance Tuning\n10.10: Resource Bin Packing\n10.11: Pod Priority and Preemption\n10.12: Node-pressure Eviction\n10.13: API-initiated Eviction\n11: Cluster Administration\n11.1: Node Shutdowns\n11.2: Swap memory management\n11.3: Node Autoscaling\n11.4: Certificates\n11.5: Cluster Networking\n11.6: Observability\n11.7: Admission Webhook Good Practices\n11.8: Good practices for Dynamic Resource Allocation as a Cluster Admin\n11.9: Logging Architecture\n11.10: Compatibility Version For Kubernetes Control Plane Components\n11.11: Metrics For Kubernetes System Components\n11.12: Metrics for Kubernetes Object States\n11.13: System Logs\n11.14: Traces For Kubernetes System Components\n11.15: Proxies in Kubernetes\n11.16: API Priority and Fairness\n11.17: Installing Addons\n11.18: Coordinated Leader Election\n12: Windows in Kubernetes\n12.1: Windows containers in Kubernetes\n12.2: Guide for Running Windows Containers in Kubernetes\n13: Extending Kubernetes\n13.1: Compute, Storage, and Networking Extensions\n13.1.1: Network Plugins\n13.1.2: Device Plugins\n13.2: Extending the Kubernetes API\n13.2.1: Custom Resources\n13.2.2: Kubernetes API Aggregation Layer\n13.3: Operator pattern\nThe Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent\nyour cluster, and helps you obtain a deeper understanding of how Kubernetes works.\n\n1 - Overview\nhttps://kubernetes.io/docs/concepts/_print/\n\n3/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and\nservices, that facilitates both declarative configuration and automation. It has a large, rapidly growing\necosystem. Kubernetes services, support, and tools are widely available.\nThis page is an overview of Kubernetes.\nThe name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight\nletters between the \"K\" and the \"s\". Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of\nGoogle's experience running production workloads at scale with best-of-breed ideas and practices from the community."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0003", "text": "Why you need Kubernetes and what it can do\nContainers are a good way to bundle and run your applications. In a production environment, you need to manage the containers\nthat run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to\nstart. Wouldn't it be easier if this behavior was handled by a system?\nThat's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It\ntakes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily\nmanage a canary deployment for your system.\nKubernetes provides you with:\nService discovery and load balancing Kubernetes can expose a container using the DNS name or using their own IP address.\nIf traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is\nstable.\nStorage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storages,\npublic cloud providers, and more.\nAutomated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it\ncan change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new\ncontainers for your deployment, remove existing containers and adopt all their resources to the new container.\nAutomatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell\nKubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make\nthe best use of your resources.\nSelf-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your userdefined health check, and doesn't advertise them to clients until they are ready to serve.\nSecret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords,\nOAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your\ncontainer images, and without exposing secrets in your stack configuration.\nBatch execution In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if\ndesired.\nHorizontal scaling Scale your application up and down with a simple command, with a UI, or automatically based on CPU\nusage.\nIPv4/IPv6 dual-stack Allocation of IPv4 and IPv6 addresses to Pods and Services\nDesigned for extensibility Add features to your Kubernetes cluster without changing upstream source code."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0004", "text": "What Kubernetes is not\nKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level\nrather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment,\nscaling, load balancing, and lets users integrate their logging, monitoring, and alerting solutions. However, Kubernetes is not\nmonolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer\nplatforms, but preserves user choice and flexibility where it is important.\nKubernetes:\nDoes not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads,\nincluding stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on\nKubernetes.\nDoes not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD)\nworkflows are determined by organization cultures and preferences as well as technical requirements.\nhttps://kubernetes.io/docs/concepts/_print/\n\n4/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDoes not provide application-level services, such as middleware (for example, message buses), data-processing frameworks\n(for example, Spark), databases (for example, MySQL), caches, nor cluster storage systems (for example, Ceph) as built-in\nservices. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through\nportable mechanisms, such as the Open Service Broker.\nDoes not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms\nto collect and export metrics.\nDoes not provide nor mandate a configuration language/system (for example, Jsonnet). It provides a declarative API that may\nbe targeted by arbitrary forms of declarative specifications.\nDoes not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems.\nAdditionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical\ndefinition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set\nof independent, composable control processes that continuously drive the current state towards the provided desired state. It\nshouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use\nand more powerful, robust, resilient, and extensible.\n\nHistorical context for Kubernetes\nLet's take a look at why Kubernetes is so useful by going back in time."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0005", "text": "Traditional deployment era:\nEarly on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a\nphysical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can\nbe instances where one application would take up most of the resources, and as a result, the other applications would\nunderperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources\nwere underutilized, and it was expensive for organizations to maintain many physical servers.\nVirtualized deployment era:\nAs a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU.\nVirtualization allows applications to be isolated between VMs and provides a level of security as the information of one application\ncannot be freely accessed by another application.\nVirtualization allows better utilization of resources in a physical server and allows better scalability because an application can be\nadded or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as\na cluster of disposable virtual machines.\nEach VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.\nContainer deployment era:\nContainers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications.\nTherefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process\nspace, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.\nContainers have become popular because they provide extra benefits, such as:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n5/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0006", "text": "Agile application creation and deployment: increased ease and efficiency of container image creation compared to VM image\nuse.\nContinuous development, integration, and deployment: provides for reliable and frequent container image build and\ndeployment with quick and efficient rollbacks (due to image immutability).\nDev and Ops separation of concerns: create application container images at build/release time rather than deployment time,\nthereby decoupling applications from infrastructure.\nObservability: not only surfaces OS-level information and metrics, but also application health and other signals.\nEnvironmental consistency across development, testing, and production: runs the same on a laptop as it does in the cloud.\nCloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on major public clouds, and anywhere else.\nApplication-centric management: raises the level of abstraction from running an OS on virtual hardware to running an\napplication on an OS using logical resources.\nLoosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can\nbe deployed and managed dynamically â€“ not a monolithic stack running on one big single-purpose machine.\nResource isolation: predictable application performance.\nResource utilization: high efficiency and density.\n\nWhat's next\nTake a look at the Kubernetes Components\nTake a look at the The Kubernetes API\nTake a look at the Cluster Architecture\nReady to Get Started?\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n6/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n1.1 - Kubernetes Components\n\nAn overview of the key components that make up a Kubernetes cluster.\nThis page provides a high-level overview of the essential components that make up a Kubernetes cluster.\nAPI Server\napi\n\nCloud Controller Manager (optional)\nc-c-m\n\nKubernetes cluster\nController Manager\nc-m\n\nControl Plane\netcd (persistence store)\nc-m\n\nCloud provider\nAPI\n\netcd\n\nNode\nkubelet\n\nc-c-m\n\nkubelet\n\nk-proxy\nkubelet\n\napi\n\nkube-proxy\nk-proxy\n\nNode\n\netcd\n\nkubelet\n\nk-proxy\n\nScheduler\nsched\n\nsched\n\nNode\n\nControl Plane\n\nThe components of a Kubernetes cluster\n\nCore Components\nA Kubernetes cluster consists of a control plane and one or more worker nodes. Here's a brief overview of the main components:\n\nControl Plane Components\nManage the overall state of the cluster:\nkube-apiserver\nThe core component server that exposes the Kubernetes HTTP API.\netcd\nConsistent and highly-available key value store for all API server data.\nkube-scheduler\nLooks for Pods not yet bound to a node, and assigns each Pod to a suitable node.\nkube-controller-manager\nRuns controllers to implement Kubernetes API behavior.\ncloud-controller-manager (optional)\nIntegrates with underlying cloud provider(s)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0007", "text": "Node Components\nRun on every node, maintaining running pods and providing the Kubernetes runtime environment:\nkubelet\nEnsures that Pods are running, including their containers.\nhttps://kubernetes.io/docs/concepts/_print/\n\n7/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkube-proxy (optional)\nMaintains network rules on nodes to implement Services.\nContainer runtime\nSoftware responsible for running containers. Read Container Runtimes to learn more.\nðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. More information\n\nYour cluster may require additional software on each node; for example, you might also run systemd on a Linux node to supervise\nlocal components.\n\nAddons\nAddons extend the functionality of Kubernetes. A few important examples include:\nDNS\nFor cluster-wide DNS resolution.\nWeb UI (Dashboard)\nFor cluster management via a web interface.\nContainer Resource Monitoring\nFor collecting and storing container metrics.\nCluster-level Logging\nFor saving container logs to a central log store.\n\nFlexibility in Architecture\nKubernetes allows for flexibility in how these components are deployed and managed. The architecture can be adapted to various\nneeds, from small development environments to large-scale production deployments.\nFor more detailed information about each component and various ways to configure your cluster architecture, see the Cluster\nArchitecture page.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n8/684\n\n11/7/25, 4:37 PM\n\n1.2 - Objects In Kubernetes\n\nConcepts | Kubernetes\n\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to\nrepresent the state of your cluster. Learn about the Kubernetes object model and how to work with these\nobjects.\nThis page explains how Kubernetes objects are represented in the Kubernetes API, and how you can express them in .yaml format."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0008", "text": "Understanding Kubernetes objects\nKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your\ncluster. Specifically, they can describe:\nWhat containerized applications are running (and on which nodes)\nThe resources available to those applications\nThe policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance\nA Kubernetes object is a \"record of intent\"--once you create the object, the Kubernetes system will constantly work to ensure that\nthe object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to\nlook like; this is your cluster's desired state.\nTo work with Kubernetes objectsâ€”whether to create, modify, or delete themâ€”you'll need to use the Kubernetes API. When you use\nthe kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the\nKubernetes API directly in your own programs using one of the Client Libraries.\n\nObject spec and status\nAlmost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the\nobject status . For objects that have a spec , you have to set this when you create the object, providing a description of the\ncharacteristics you want the resource to have: its desired state.\nThe status describes the current state of the object, supplied and updated by the Kubernetes system and its components. The\nKubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied.\nFor example: in Kubernetes, a Deployment is an object that can represent an application running on your cluster. When you create\nthe Deployment, you might set the Deployment spec to specify that you want three replicas of the application to be running. The\nKubernetes system reads the Deployment spec and starts three instances of your desired application--updating the status to match\nyour spec. If any of those instances should fail (a status change), the Kubernetes system responds to the difference between spec\nand status by making a correction--in this case, starting a replacement instance.\nFor more information on the object spec, status, and metadata, see the Kubernetes API Conventions."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0009", "text": "Describing a Kubernetes object\nWhen you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic\ninformation about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via\nkubectl ), that API request must include that information as JSON in the request body. Most often, you provide the information to\nkubectl in a file known as a manifest. By convention, manifests are YAML (you could also use JSON format). Tools such as kubectl\nconvert the information from a manifest into JSON or another supported serialization format when making the API request over\nHTTP.\nHere's an example manifest that shows the required fields and object spec for a Kubernetes Deployment:\napplication/deployment.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n9/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nspec:\nselector:\nmatchLabels:\napp: nginx\nreplicas: 2 # tells deployment to run 2 pods matching the template\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nOne way to create a Deployment using a manifest file like the one above is to use the kubectl apply command in the kubectl\ncommand-line interface, passing the .yaml file as an argument. Here's an example:\n\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment created\n\nRequired fields\nIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for the following fields:\napiVersion\nkind\n\n- What kind of object you want to create\n\nmetadata\nspec\n\n- Which version of the Kubernetes API you're using to create this object\n\n- Data that helps uniquely identify the object, including a name string, UID , and optional namespace\n\n- What state you desire for the object"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0010", "text": "The precise format of the object spec is different for every Kubernetes object, and contains nested fields specific to that object. The\nKubernetes API Reference can help you find the spec format for all of the objects you can create using Kubernetes.\nFor example, see the spec field for the Pod API reference. For each Pod, the .spec field specifies the pod and its desired state\n(such as the container image name for each container within that pod). Another example of an object specification is the spec field\nfor the StatefulSet API. For StatefulSet, the .spec field specifies the StatefulSet and its desired state. Within the .spec of a\nStatefulSet is a template for Pod objects. That template describes Pods that the StatefulSet controller will create in order to satisfy\nthe StatefulSet specification. Different kinds of objects can also have different .status ; again, the API reference pages detail the\nstructure of that .status field, and its content for each different type of object.\nNote:\nSee Configuration Best Practices for additional information on writing YAML configuration files.\n\nServer side field validation\nStarting with Kubernetes v1.25, the API server offers server side field validation that detects unrecognized or duplicate fields in an\nobject. It provides all the functionality of kubectl --validate on the server side.\nhttps://kubernetes.io/docs/concepts/_print/\n\n10/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe kubectl tool uses the --validate flag to set the level of field validation. It accepts the values ignore , warn , and strict\nwhile also accepting the values true (equivalent to strict ) and false (equivalent to ignore ). The default validation setting for\nkubectl is --validate=true .\nStrict\n\nStrict field validation, errors on validation failure\nWarn\n\nField validation is performed, but errors are exposed as warnings rather than failing the request\nIgnore\n\nNo server side field validation is performed\nWhen kubectl cannot connect to an API server that supports field validation it will fall back to using client-side validation.\nKubernetes 1.27 and later versions always offer field validation; older Kubernetes releases might not. If your cluster is older than\nv1.27, check the documentation for your version of Kubernetes."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0011", "text": "What's next\nIf you're new to Kubernetes, read more about the following:\nPods which are the most important basic Kubernetes objects.\nDeployment objects.\nControllers in Kubernetes.\nkubectl and kubectl commands.\nKubernetes Object Management explains how to use kubectl to manage objects. You might need to install kubectl if you don't\nalready have it available.\nTo learn about the Kubernetes API in general, visit:\nKubernetes API overview\nTo learn about objects in Kubernetes in more depth, read other pages in this section:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n11/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n1.2.1 - Kubernetes Object Management\nThe kubectl command-line tool supports several different ways to create and manage Kubernetes objects. This document provides\nan overview of the different approaches. Read the Kubectl book for details of managing objects by Kubectl.\n\nManagement techniques\nWarning:\nA Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object\nresults in undefined behavior.\n\nManagement technique\n\nOperates on\n\nRecommended environment\n\nSupported writers\n\nLearning curve\n\nImperative commands\n\nLive objects\n\nDevelopment projects\n\n1+\n\nLowest\n\nImperative object configuration\n\nIndividual files\n\nProduction projects\n\n1\n\nModerate\n\nDeclarative object configuration\n\nDirectories of files\n\nProduction projects\n\n1+\n\nHighest\n\nImperative commands\nWhen using imperative commands, a user operates directly on live objects in a cluster. The user provides operations to the kubectl\ncommand as arguments or flags.\nThis is the recommended way to get started or to run a one-off task in a cluster. Because this technique operates directly on live\nobjects, it provides no history of previous configurations.\n\nExamples\nRun an instance of the nginx container by creating a Deployment object:\n\nkubectl create deployment nginx --image nginx\n\nTrade-offs\nAdvantages compared to object configuration:\nCommands are expressed as a single action word.\nCommands require only a single step to make changes to the cluster.\nDisadvantages compared to object configuration:\nCommands do not integrate with change review processes.\nCommands do not provide an audit trail associated with changes.\nCommands do not provide a source of records except for what is live.\nCommands do not provide a template for creating new objects.\n\nImperative object configuration\nIn imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one\nfile name. The file specified must contain a full definition of the object in YAML or JSON format.\nSee the API reference for more details on object definitions.\nhttps://kubernetes.io/docs/concepts/_print/\n\n12/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0012", "text": "Warning:\nThe imperative replace command replaces the existing spec with the newly provided one, dropping all changes to the object\nmissing from the configuration file. This approach should not be used with resource types whose specs are updated\nindependently of the configuration file. Services of type LoadBalancer, for example, have their externalIPs field updated\nindependently from the configuration by the cluster.\n\nExamples\nCreate the objects defined in a configuration file:\n\nkubectl create -f nginx.yaml\n\nDelete the objects defined in two configuration files:\n\nkubectl delete -f nginx.yaml -f redis.yaml\n\nUpdate the objects defined in a configuration file by overwriting the live configuration:\n\nkubectl replace -f nginx.yaml\n\nTrade-offs\nAdvantages compared to imperative commands:\nObject configuration can be stored in a source control system such as Git.\nObject configuration can integrate with processes such as reviewing changes before push and audit trails.\nObject configuration provides a template for creating new objects.\nDisadvantages compared to imperative commands:\nObject configuration requires basic understanding of the object schema.\nObject configuration requires the additional step of writing a YAML file.\nAdvantages compared to declarative object configuration:\nImperative object configuration behavior is simpler and easier to understand.\nAs of Kubernetes version 1.5, imperative object configuration is more mature.\nDisadvantages compared to declarative object configuration:\nImperative object configuration works best on files, not directories.\nUpdates to live objects must be reflected in configuration files, or they will be lost during the next replacement.\n\nDeclarative object configuration\nWhen using declarative object configuration, a user operates on object configuration files stored locally, however the user does not\ndefine the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by\nkubectl . This enables working on directories, where different operations might be needed for different objects.\nNote:\nDeclarative object configuration retains changes made by other writers, even if the changes are not merged back to the object\nconfiguration file. This is possible by using the patch API operation to write only observed differences, instead of using the\nreplace API operation to replace the entire object configuration.\nhttps://kubernetes.io/docs/concepts/_print/\n\n13/684\n\n11/7/25, 4:37 PM\n\nExamples\n\nConcepts | Kubernetes\n\nProcess all object configuration files in the configs directory, and create or patch the live objects. You can first diff to see what\nchanges are going to be made, and then apply:\n\nkubectl diff -f configs/\nkubectl apply -f configs/\n\nRecursively process directories:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0013", "text": "kubectl diff -R -f configs/\nkubectl apply -R -f configs/\n\nTrade-offs\nAdvantages compared to imperative object configuration:\nChanges made directly to live objects are retained, even if they are not merged back into the configuration files.\nDeclarative object configuration has better support for operating on directories and automatically detecting operation types\n(create, patch, delete) per-object.\nDisadvantages compared to imperative object configuration:\nDeclarative object configuration is harder to debug and understand results when they are unexpected.\nPartial updates using diffs create complex merge and patch operations.\n\nWhat's next\nManaging Kubernetes Objects Using Imperative Commands\nImperative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Configuration Files\nDeclarative Management of Kubernetes Objects Using Kustomize\nKubectl Command Reference\nKubectl Book\nKubernetes API Reference\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n14/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n1.2.2 - Object Names and IDs\n\nEach object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique\nacross your whole cluster.\nFor example, you can only have one Pod named myapp-1234 within the same namespace, but you can have one Pod and one\nDeployment that are each named myapp-1234 .\nFor non-unique user-provided attributes, Kubernetes provides labels and annotations."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0014", "text": "Names\nA client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name .\nOnly one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with\nthe same name.\nNames must be unique across all API versions of the same resource. API resources are distinguished by their API group,\nresource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.\nNote:\nIn cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under\nthe same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to\ninconsistencies.\nThe server may generate a name when generateName is provided instead of name in a resource create request. When\ngenerateName is used, the provided value is used as a name prefix, which server appends a generated suffix to. Even though the\nname is generated, it may conflict with existing names resulting in an HTTP 409 response. This became far less likely to happen in\nKubernetes v1.31 and later, since the server will make up to 8 attempts to generate a unique name before returning an HTTP 409\nresponse.\nBelow are four types of commonly used name constraints for resources.\n\nDNS Subdomain Names\nMost resource types require a name that can be used as a DNS subdomain name as defined in RFC 1123. This means the name\nmust:\ncontain no more than 253 characters\ncontain only lowercase alphanumeric characters, '-' or '.'\nstart with an alphanumeric character\nend with an alphanumeric character\n\nRFC 1123 Label Names\nSome resource types require their names to follow the DNS label standard as defined in RFC 1123. This means the name must:\ncontain at most 63 characters\ncontain only lowercase alphanumeric characters or '-'\nstart with an alphabetic character\nend with an alphanumeric character\nNote:\nWhen the RelaxedServiceNameValidation feature gate is enabled, Service object names are allowed to start with a digit.\n\nRFC 1035 Label Names\nSome resource types require their names to follow the DNS label standard as defined in RFC 1035. This means the name must:\nhttps://kubernetes.io/docs/concepts/_print/\n\n15/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0015", "text": "contain at most 63 characters\ncontain only lowercase alphanumeric characters or '-'\nstart with an alphabetic character\nend with an alphanumeric character\nNote:\nWhile RFC 1123 technically allows labels to start with digits, the current Kubernetes implementation requires both RFC 1035\nand RFC 1123 labels to start with an alphabetic character. The exception is when the RelaxedServiceNameValidation feature\ngate is enabled for Service objects, which allows Service names to start with digits.\n\nPath Segment Names\nSome resource types require their names to be able to be safely encoded as a path segment. In other words, the name may not be\n\".\" or \"..\" and the name may not contain \"/\" or \"%\".\nHere's an example manifest for a Pod named nginx-demo .\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx-demo\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nNote:\nSome resource types have additional restrictions on their names.\n\nUIDs\nA Kubernetes systems-generated string to uniquely identify objects.\nEvery object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between\nhistorical occurrences of similar entities.\nKubernetes UIDs are universally unique identifiers (also known as UUIDs). UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T\nX.667.\n\nWhat's next\nRead about labels and annotations in Kubernetes.\nSee the Identifiers and Names in Kubernetes design document.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n16/684\n\n11/7/25, 4:37 PM\n\n1.2.3 - Labels and Selectors\n\nConcepts | Kubernetes\n\nLabels are key/value pairs that are attached to objects such as Pods. Labels are intended to be used to specify identifying attributes\nof objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to\norganize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified\nat any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.\n\n\"metadata\": {\n\"labels\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n}\n\nLabels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-identifying information should be recorded\nusing annotations."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0016", "text": "Motivation\nLabels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring\nclients to store these mappings.\nService deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments,\nmultiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which\nbreaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather\nthan by users.\nExample labels:\n\"release\" : \"stable\" , \"release\" : \"canary\"\n\"environment\" : \"dev\" , \"environment\" : \"qa\" , \"environment\" : \"production\"\n\"tier\" : \"frontend\" , \"tier\" : \"backend\" , \"tier\" : \"cache\"\n\"partition\" : \"customerA\" , \"partition\" : \"customerB\"\n\"track\" : \"daily\" , \"track\" : \"weekly\"\n\nThese are examples of commonly used labels; you are free to develop your own conventions. Keep in mind that label Key must be\nunique for a given object.\n\nSyntax and character set\nLabels are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash ( / ). The name\nsegment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ( [a-z0-9A-Z] ) with\ndashes ( - ), underscores ( _ ), dots ( . ), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS\nsubdomain: a series of DNS labels separated by dots ( . ), not longer than 253 characters in total, followed by a slash ( / ).\nIf the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler ,\nkube-controller-manager , kube-apiserver , kubectl , or other third-party automation) which add labels to end-user objects must\nspecify a prefix.\nThe kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.\nValid label value:\nmust be 63 characters or less (can be empty),\nunless empty, must begin and end with an alphanumeric character ( [a-z0-9A-Z] ),\ncould contain dashes ( - ), underscores ( _ ), dots ( . ), and alphanumerics between.\nFor example, here's a manifest for a Pod that has two labels environment: production and app: nginx :\nhttps://kubernetes.io/docs/concepts/_print/\n\n17/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: label-demo\nlabels:\nenvironment: production\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0017", "text": "Label selectors\nUnlike names and UIDs, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).\nVia a label selector, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.\nThe API currently supports two types of selectors: equality-based and set-based. A label selector can be made of multiple requirements\nwhich are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical\nAND ( && ) operator.\nThe semantics of empty or non-specified selectors are dependent on the context, and API types that use selectors should document\nthe validity and meaning of them.\nNote:\nFor some API types, such as ReplicaSets, the label selectors of two instances must not overlap within a namespace, or the\ncontroller can see that as conflicting instructions and fail to determine how many replicas should be present.\n\nCaution:\nFor both equality-based and set-based conditions there is no logical OR (||) operator. Ensure your filter statements are\nstructured accordingly.\n\nEquality-based requirement\nEquality- or inequality-based requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified\nlabel constraints, though they may have additional labels as well. Three kinds of operators are admitted = , == , != . The first two\nrepresent equality (and are synonyms), while the latter represents inequality. For example:\nenvironment = production\ntier != frontend\n\nThe former selects all resources with key equal to environment and value equal to production . The latter selects all resources with\nkey equal to tier and value distinct from frontend , and all resources with no labels with the tier key. One could filter for\nresources in production excluding frontend using the comma operator: environment=production,tier!=frontend\nOne usage scenario for equality-based label requirement is for Pods to specify node selection criteria. For example, the sample Pod\nbelow selects nodes where the accelerator label exists and is set to nvidia-tesla-p100 .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n18/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: cuda-test\nspec:\ncontainers:\n- name: cuda-test\nimage: \"registry.k8s.io/cuda-vector-add:v0.1\"\nresources:\nlimits:\nnvidia.com/gpu: 1\nnodeSelector:\naccelerator: nvidia-tesla-p100"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0018", "text": "Set-based requirement\nSet-based label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: in , notin\nand exists (only the key identifier). For example:\nenvironment in (production, qa)\ntier notin (frontend, backend)\npartition\n!partition\n\nThe first example selects all resources with key equal to environment and value equal to production or qa .\nThe second example selects all resources with key equal to tier and values other than frontend and backend , and all\nresources with no labels with the tier key.\nThe third example selects all resources including a label with key partition ; no values are checked.\nThe fourth example selects all resources without a label with key partition ; no values are checked.\nSimilarly the comma separator acts as an AND operator. So filtering resources with a partition key (no matter the value) and with\nenvironment different than qa can be achieved using partition,environment notin (qa) . The set-based label selector is a general\nform of equality since environment=production is equivalent to environment in (production) ; similarly for != and notin .\nSet-based requirements can be mixed with equality-based requirements. For example: partition in (customerA,\ncustomerB),environment!=qa .\n\nAPI\nLIST and WATCH filtering\nFor list and watch operations, you can specify label selectors to filter the sets of objects returned; you specify the filter using a\nquery parameter. (To learn in detail about watches in Kubernetes, read efficient detection of changes). Both requirements are\npermitted (presented here as they would appear in a URL query string):\nequality-based requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontend\nset-based requirements: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29\nBoth label selector styles can be used to list or watch resources via a REST client. For example, targeting apiserver with kubectl\nand using equality-based one may write:\n\nkubectl get pods -l environment=production,tier=frontend\n\nor using set-based requirements:\n\nkubectl get pods -l 'environment in (production),tier in (frontend)'\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n19/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAs already mentioned set-based requirements are more expressive. For instance, they can implement the OR operator on values:\n\nkubectl get pods -l 'environment in (production, qa)'\n\nor restricting negative matching via notin operator:\n\nkubectl get pods -l 'environment,environment notin (frontend)'\n\nSet references in API objects\nSome Kubernetes objects, such as services and replicationcontrollers , also use label selectors to specify sets of other\nresources, such as pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0019", "text": "Service and ReplicationController\nThe set of pods that a service targets is defined with a label selector. Similarly, the population of pods that a\nreplicationcontroller should manage is also defined with a label selector.\nLabel selectors for both objects are defined in json or yaml files using maps, and only equality-based requirement selectors are\nsupported:\n\n\"selector\": {\n\"component\" : \"redis\",\n}\n\nor\n\nselector:\ncomponent: redis\n\nThis selector (respectively in json or yaml format) is equivalent to component=redis or component in (redis) .\n\nResources that support set-based requirements\nNewer resources, such as Job , Deployment , ReplicaSet , and DaemonSet , support set-based requirements as well.\n\nselector:\nmatchLabels:\ncomponent: redis\nmatchExpressions:\n- { key: tier, operator: In, values: [cache] }\n- { key: environment, operator: NotIn, values: [dev] }\n\nis a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of\nmatchExpressions , whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". matchExpressions\nis a list of pod selector requirements. Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty\nin the case of In and NotIn. All of the requirements, from both matchLabels and matchExpressions are ANDed together -- they\nmust all be satisfied in order to match.\nmatchLabels\n\nSelecting sets of nodes\nhttps://kubernetes.io/docs/concepts/_print/\n\n20/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOne use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule. See the documentation on\nnode selection for more information.\n\nUsing labels effectively\nYou can apply a single label to any resources, but this is not always the best practice. There are many scenarios where multiple\nlabels should be used to distinguish resource sets from one another.\nFor instance, different applications would use different values for the app label, but a multi-tier application, such as the guestbook\nexample, would additionally need to distinguish each tier. The frontend could carry the following labels:\n\nlabels:\napp: guestbook\ntier: frontend\n\nwhile the Redis master and replica would have different tier labels, and perhaps even an additional role label:\n\nlabels:\napp: guestbook\ntier: backend\nrole: master\n\nand\n\nlabels:\napp: guestbook\ntier: backend\nrole: replica\n\nThe labels allow for slicing and dicing the resources along any dimension specified by a label:\n\nkubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml\nkubectl get pods -Lapp -Ltier -Lrole\n\nNAME\nguestbook-fe-4nlpb\nguestbook-fe-ght6d"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0020", "text": "READY\n1/1\n1/1\n\nSTATUS\nRunning\nRunning\n\nRESTARTS\n0\n0\n\nAGE\n1m\n1m\n\nAPP\nguestbook\nguestbook\n\nTIER\nfrontend\nfrontend\n\nROLE\n<none>\n<none>\n\nguestbook-fe-jpy62\nguestbook-redis-master-5pg3b\n\n1/1\n1/1\n\nRunning\nRunning\n\n0\n0\n\n1m\n1m\n\nguestbook\nguestbook\n\nfrontend\nbackend\n\n<none>\nmaster\n\nguestbook-redis-replica-2q2yf\nguestbook-redis-replica-qgazl\nmy-nginx-divi2\n\n1/1\n1/1\n1/1\n\nRunning\nRunning\nRunning\n\n0\n0\n0\n\n1m\n1m\n29m\n\nguestbook\nguestbook\nnginx\n\nbackend\nbackend\n<none>\n\nreplica\nreplica\n<none>\n\nmy-nginx-o0ef1\n\n1/1\n\nRunning\n\n0\n\n29m\n\nnginx\n\n<none>\n\n<none>\n\nkubectl get pods -lapp=guestbook,role=replica\n\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nguestbook-redis-replica-2q2yf\nguestbook-redis-replica-qgazl\n\n1/1\n1/1\n\nRunning\nRunning\n\n0\n0\n\n3m\n3m\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n21/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nUpdating labels\n\nSometimes you may want to relabel existing pods and other resources before creating new resources. This can be done with\nkubectl label . For example, if you want to label all your NGINX Pods as frontend tier, run:\n\nkubectl label pods -l app=nginx tier=fe\n\npod/my-nginx-2035384211-j5fhi labeled\npod/my-nginx-2035384211-u2c7e labeled\npod/my-nginx-2035384211-u3t6x labeled\n\nThis first filters all pods with the label \"app=nginx\", and then labels them with the \"tier=fe\". To see the pods you labeled, run:\n\nkubectl get pods -l app=nginx -L tier\n\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nTIER\n\nmy-nginx-2035384211-j5fhi\nmy-nginx-2035384211-u2c7e\nmy-nginx-2035384211-u3t6x\n\n1/1\n1/1\n1/1\n\nRunning\nRunning\nRunning\n\n0\n0\n0\n\n23m\n23m\n23m\n\nfe\nfe\nfe\n\nThis outputs all \"app=nginx\" pods, with an additional label column of pods' tier (specified with -L or --label-columns ).\nFor more information, please see kubectl label.\n\nWhat's next\nLearn how to add a label to a node\nFind Well-known labels, Annotations and Taints\nSee Recommended labels\nEnforce Pod Security Standards with Namespace Labels\nRead a blog on Writing a Controller for Pod Labels\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n22/684\n\n11/7/25, 4:37 PM\n\n1.2.4 - Namespaces\n\nConcepts | Kubernetes\n\nIn Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need\nto be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects\n(e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0021", "text": "When to Use Multiple Namespaces\nNamespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a\nfew to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the\nfeatures they provide.\nNamespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces.\nNamespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.\nNamespaces are a way to divide cluster resources between multiple users (via resource quota).\nIt is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same\nsoftware: use labels to distinguish resources within the same namespace.\nNote:\nFor a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.\n\nInitial namespaces\nKubernetes starts with four initial namespaces:\ndefault\n\nKubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.\nkube-node-lease\n\nThis namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the\ncontrol plane can detect node failure.\nkube-public\n\nThis namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster\nusage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this\nnamespace is only a convention, not a requirement.\nkube-system\n\nThe namespace for objects created by the Kubernetes system.\n\nWorking with Namespaces\nCreation and deletion of namespaces are described in the Admin Guide documentation for namespaces.\nNote:\nAvoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.\n\nViewing namespaces\nYou can list the current namespaces in a cluster using:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n23/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get namespace\n\nNAME\n\nSTATUS\n\nAGE\n\ndefault\nkube-node-lease\nkube-public\n\nActive\nActive\nActive\n\n1d\n1d\n1d\n\nkube-system\n\nActive\n\n1d\n\nSetting the namespace for a request\nTo set the namespace for a current request, use the --namespace flag.\nFor example:\n\nkubectl run nginx --image=nginx --namespace=<insert-namespace-name-here>\nkubectl get pods --namespace=<insert-namespace-name-here>\n\nSetting the namespace preference\nYou can permanently save the namespace for all subsequent kubectl commands in that context.\n\nkubectl config set-context --current --namespace=<insert-namespace-name-here>\n# Validate it\nkubectl config view --minify | grep namespace:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0022", "text": "Namespaces and DNS\nWhen you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespacename>.svc.cluster.local , which means that if a container only uses <service-name> , it will resolve to the service which is local to a\nnamespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and\nProduction. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).\nAs a result, all namespace names must be valid RFC 1123 DNS labels.\n\nWarning:\nBy creating namespaces with the same name as public top-level domains, Services in these namespaces can have short DNS\nnames that overlap with public DNS records. Workloads from any namespace performing a DNS lookup without a trailing dot\nwill be redirected to those services, taking precedence over public DNS.\nTo mitigate this, limit privileges for creating namespaces to trusted users. If required, you could additionally configure thirdparty security controls, such as admission webhooks, to block creating any namespace with the name of public TLDs.\n\nNot all objects are in a namespace\nMost Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However namespace\nresources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes, are not in any\nnamespace.\nTo see which Kubernetes resources are and aren't in a namespace:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n24/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n# In a namespace\nkubectl api-resources --namespaced=true\n# Not in a namespace\nkubectl api-resources --namespaced=false\n\nAutomatic labelling\nâ“˜ FEATURE STATE: Kubernetes 1.22 [stable]\n\nThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces. The value of the label is\nthe namespace name.\n\nWhat's next\nLearn more about creating a new namespace.\nLearn more about deleting a namespace.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n25/684\n\n11/7/25, 4:37 PM\n\n1.2.5 - Annotations\n\nConcepts | Kubernetes\n\nYou can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can\nretrieve this metadata."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0023", "text": "Attaching metadata to objects\nYou can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find\ncollections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The\nmetadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. It is\npossible to use labels as well as annotations in the metadata of the same object.\nAnnotations, like labels, are key/value maps:\n\n\"metadata\": {\n\"annotations\": {\n\"key1\" : \"value1\",\n\"key2\" : \"value2\"\n}\n}\n\nNote:\nThe keys and the values in the map must be strings. In other words, you cannot use numeric, boolean, list or other types for\neither the keys or the values.\nHere are some examples of information that could be recorded in annotations:\nFields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default\nvalues set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.\nBuild, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.\nPointers to logging, monitoring, analytics, or audit repositories.\nClient library or tool information that can be used for debugging purposes: for example, name, version, and build information.\nUser or tool/system provenance information, such as URLs of related objects from other ecosystem components.\nLightweight rollout tool metadata: for example, config or checkpoints.\nPhone or pager numbers of persons responsible, or directory entries that specify where that information can be found, such as\na team web site.\nDirectives from the end-user to the implementations to modify behavior or engage non-standard features.\nInstead of using annotations, you could store this type of information in an external database or directory, but that would make it\nmuch harder to produce shared client libraries and tools for deployment, management, introspection, and the like."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0024", "text": "Syntax and character set\nAnnotations are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash ( / ).\nThe name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ( [a-z0-9AZ] ) with dashes ( - ), underscores ( _ ), dots ( . ), and alphanumerics between. The prefix is optional. If specified, the prefix must be\na DNS subdomain: a series of DNS labels separated by dots ( . ), not longer than 253 characters in total, followed by a slash ( / ).\nIf the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. kubescheduler , kube-controller-manager , kube-apiserver , kubectl , or other third-party automation) which add annotations to enduser objects must specify a prefix.\nThe kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.\nhttps://kubernetes.io/docs/concepts/_print/\n\n26/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, here's a manifest for a Pod that has the annotation imageregistry: https://hub.docker.com/ :\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: annotations-demo\nannotations:\nimageregistry: \"https://hub.docker.com/\"\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nWhat's next\nLearn more about Labels and Selectors.\nFind Well-known labels, Annotations and Taints\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n27/684\n\n11/7/25, 4:37 PM\n\n1.2.6 - Field Selectors\n\nConcepts | Kubernetes\n\nField selectors let you select Kubernetes objects based on the value of one or more resource fields. Here are some examples of field\nselector queries:\nmetadata.name=my-service\nmetadata.namespace!=default\nstatus.phase=Pending\n\nThis kubectl command selects all Pods for which the value of the status.phase field is Running :\n\nkubectl get pods --field-selector status.phase=Running\n\nNote:\nField selectors are essentially resource filters. By default, no selectors/filters are applied, meaning that all resources of the\nspecified type are selected. This makes the kubectl queries kubectl get pods and kubectl get pods --field-selector \"\"\nequivalent.\n\nSupported fields\nSupported field selectors vary by Kubernetes resource type. All resource types support the metadata.name and\nmetadata.namespace fields. Using unsupported field selectors produces an error. For example:\n\nkubectl get ingress --field-selector foo.bar=baz\n\nError from server (BadRequest): Unable to find \"ingresses\" that match label selector \"\", field selector \"foo.bar=baz\n\nList of supported fields\nKind\n\nFields\n\nPod\n\nspec.nodeName\nspec.restartPolicy\nspec.schedulerName\nspec.serviceAccountName\nspec.hostNetwork\nstatus.phase\nstatus.podIP\nstatus.nominatedNodeName\n\nEvent"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0025", "text": "involvedObject.kind\ninvolvedObject.namespace\ninvolvedObject.name\ninvolvedObject.uid\ninvolvedObject.apiVersion\ninvolvedObject.resourceVersion\ninvolvedObject.fieldPath\nreason\nreportingComponent\nsource\ntype\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n28/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nKind\n\nFields\n\nSecret\n\ntype\n\nNamespace\n\nstatus.phase\n\nReplicaSet\n\nstatus.replicas\n\nReplicationController\n\nstatus.replicas\n\nJob\n\nstatus.successful\n\nNode\n\nspec.unschedulable\n\nCertificateSigningRequest\n\nspec.signerName\n\nCustom resources fields\nAll custom resource types support the metadata.name and metadata.namespace fields.\nAdditionally, the spec.versions[*].selectableFields field of a CustomResourceDefinition declares which other fields in a custom\nresource may be used in field selectors. See selectable fields for custom resources for more information about how to use field\nselectors with CustomResourceDefinitions.\n\nSupported operators\nYou can use the = , == , and != operators with field selectors ( = and == mean the same thing). This kubectl command, for\nexample, selects all Kubernetes Services that aren't in the default namespace:\n\nkubectl get services\n\n--all-namespaces --field-selector metadata.namespace!=default\n\nNote:\nSet-based operators (in, notin, exists) are not supported for field selectors.\n\nChained selectors\nAs with label and other selectors, field selectors can be chained together as a comma-separated list. This kubectl command selects\nall Pods for which the status.phase does not equal Running and the spec.restartPolicy field equals Always :\n\nkubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always\n\nMultiple resource types\nYou can use field selectors across multiple resource types. This kubectl command selects all Statefulsets and Services that are not\nin the default namespace:\n\nkubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n29/684\n\n11/7/25, 4:37 PM\n\n1.2.7 - Finalizers\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0026", "text": "Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources that are\nmarked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.\nWhen you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by\npopulating .metadata.deletionTimestamp , and returns a 202 status code (HTTP \"Accepted\"). The target object remains in a\nterminating state while the control plane, or other components, take the actions defined by the finalizers. After these actions are\ncomplete, the controller removes the relevant finalizers from the target object. When the metadata.finalizers field is empty,\nKubernetes considers the deletion complete and deletes the object.\nYou can use finalizers to control garbage collection of resources. For example, you can define a finalizer to clean up related\nAPI resources or infrastructure before the controller deletes the object being finalized.\nYou can use finalizers to control garbage collection of objects by alerting controllers to perform specific cleanup tasks before\ndeleting the target resource.\nFinalizers don't usually specify the code to execute. Instead, they are typically lists of keys on a specific resource similar to\nannotations. Kubernetes specifies some finalizers automatically, but you can also specify your own."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0027", "text": "How finalizers work\nWhen you create a resource using a manifest file, you can specify finalizers in the metadata.finalizers field. When you attempt to\ndelete the resource, the API server handling the delete request notices the values in the finalizers field and does the following:\nModifies the object to add a metadata.deletionTimestamp field with the time you started the deletion.\nPrevents the object from being removed until all items are removed from its metadata.finalizers field\nReturns a 202 status code (HTTP \"Accepted\")\nThe controller managing that finalizer notices the update to the object setting the metadata.deletionTimestamp , indicating deletion\nof the object has been requested. The controller then attempts to satisfy the requirements of the finalizers specified for that\nresource. Each time a finalizer condition is satisfied, the controller removes that key from the resource's finalizers field. When the\nfinalizers field is emptied, an object with a deletionTimestamp field set is automatically deleted. You can also use finalizers to\nprevent deletion of unmanaged resources.\nA common example of a finalizer is kubernetes.io/pv-protection , which prevents accidental deletion of PersistentVolume\nobjects. When a PersistentVolume object is in use by a Pod, Kubernetes adds the pv-protection finalizer. If you try to delete the\nPersistentVolume , it enters a Terminating status, but the controller can't delete it because the finalizer exists. When the Pod stops\nusing the PersistentVolume , Kubernetes clears the pv-protection finalizer, and the controller deletes the volume.\nNote:\nWhen you DELETE an object, Kubernetes adds the deletion timestamp for that object and then immediately starts to\nrestrict changes to the .metadata.finalizers field for the object that is now pending deletion. You can remove existing\nfinalizers (deleting an entry from the finalizers list) but you cannot add a new finalizer. You also cannot modify the\ndeletionTimestamp for an object once it is set.\nAfter the deletion is requested, you can not resurrect this object. The only way is to delete it and make a new similar\nobject.\n\nNote:\nCustom finalizer names must be publicly qualified finalizer names, such as example.com/finalizer-name. Kubernetes enforces\nthis format; the API server rejects writes to objects where the change does not use qualified finalizer names for any custom\nfinalizer.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n30/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOwner references, labels, and finalizers"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0028", "text": "Like labels, owner references describe the relationships between objects in Kubernetes, but are used for a different purpose. When\na controller manages objects like Pods, it uses labels to track changes to groups of related objects. For example, when a Job creates\none or more Pods, the Job controller applies labels to those pods and tracks changes to any Pods in the cluster with the same label.\nThe Job controller also adds owner references to those Pods, pointing at the Job that created the Pods. If you delete the Job while\nthese Pods are running, Kubernetes uses the owner references (not labels) to determine which Pods in the cluster need cleanup.\nKubernetes also processes finalizers when it identifies owner references on a resource targeted for deletion.\nIn some situations, finalizers can block the deletion of dependent objects, which can cause the targeted owner object to remain for\nlonger than expected without being fully deleted. In these situations, you should check finalizers and owner references on the target\nowner and dependent objects to troubleshoot the cause.\nNote:\nIn cases where objects are stuck in a deleting state, avoid manually removing finalizers to allow deletion to continue. Finalizers\nare usually added to resources for a reason, so forcefully removing them can lead to issues in your cluster. This should only be\ndone when the purpose of the finalizer is understood and is accomplished in another way (for example, manually cleaning up\nsome dependent object).\n\nWhat's next\nRead Using Finalizers to Control Deletion on the Kubernetes blog.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n31/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n1.2.8 - Owners and Dependents\n\nIn Kubernetes, some objects are owners of other objects. For example, a ReplicaSet is the owner of a set of Pods. These owned\nobjects are dependents of their owner.\nOwnership is different from the labels and selectors mechanism that some resources also use. For example, consider a Service that\ncreates EndpointSlice objects. The Service uses labels to allow the control plane to determine which EndpointSlice objects are\nused for that Service. In addition to the labels, each EndpointSlice that is managed on behalf of a Service has an owner reference.\nOwner references help different parts of Kubernetes avoid interfering with objects they donâ€™t control."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0029", "text": "Owner references in object specifications\nDependent objects have a metadata.ownerReferences field that references their owner object. A valid owner reference consists of\nthe object name and a UID within the same namespace as the dependent object. Kubernetes sets the value of this field\nautomatically for objects that are dependents of other objects like ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and\nReplicationControllers. You can also configure these relationships manually by changing the value of this field. However, you usually\ndon't need to and can allow Kubernetes to automatically manage the relationships.\nDependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether\nspecific dependents can block garbage collection from deleting their owner object. Kubernetes automatically sets this field to true\nif a controller (for example, the Deployment controller) sets the value of the metadata.ownerReferences field. You can also set the\nvalue of the blockOwnerDeletion field manually to control which dependents block garbage collection.\nA Kubernetes admission controller controls user access to change this field for dependent resources, based on the delete\npermissions of the owner. This control prevents unauthorized users from delaying owner object deletion.\nNote:\nCross-namespace owner references are disallowed by design. Namespaced dependents can specify cluster-scoped or\nnamespaced owners. A namespaced owner must exist in the same namespace as the dependent. If it does not, the owner\nreference is treated as absent, and the dependent is subject to deletion once all owners are verified absent.\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-scoped dependent specifies a\nnamespaced kind as an owner, it is treated as having an unresolvable owner reference, and is not able to be garbage collected.\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a cluster-scoped dependent with an\nownerReference referencing a namespaced kind, a warning Event with a reason of OwnerRefInvalidNamespace and an\ninvolvedObject of the invalid dependent is reported. You can check for that kind of Event by running kubectl get events -A\n--field-selector=reason=OwnerRefInvalidNamespace ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0030", "text": "Ownership and finalizers\nWhen you tell Kubernetes to delete a resource, the API server allows the managing controller to process any finalizer rules for the\nresource. Finalizers prevent accidental deletion of resources your cluster may still need to function correctly. For example, if you try\nto delete a PersistentVolume that is still in use by a Pod, the deletion does not happen immediately because the PersistentVolume\nhas the kubernetes.io/pv-protection finalizer on it. Instead, the volume remains in the Terminating status until Kubernetes\nclears the finalizer, which only happens after the PersistentVolume is no longer bound to a Pod.\nKubernetes also adds finalizers to an owner resource when you use either foreground or orphan cascading deletion. In foreground\ndeletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have\nownerReferences.blockOwnerDeletion=true before it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds\nthe orphan finalizer so that the controller ignores dependent resources after it deletes the owner object.\n\nWhat's next\nLearn more about Kubernetes finalizers.\nLearn about garbage collection.\nRead the API reference for object metadata.\nhttps://kubernetes.io/docs/concepts/_print/\n\n32/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n1.2.9 - Recommended Labels\n\nYou can visualize and manage Kubernetes objects with more tools than kubectl and the dashboard. A common set of labels allows\ntools to work interoperably, describing objects in a common manner that all tools can understand.\nIn addition to supporting tooling, the recommended labels describe applications in a way that can be queried.\nThe metadata is organized around the concept of an application. Kubernetes is not a platform as a service (PaaS) and doesn't have or\nenforce a formal notion of an application. Instead, applications are informal and described with metadata. The definition of what an\napplication contains is loose.\nNote:\nThese are recommended labels. They make it easier to manage applications but aren't required for any core tooling.\nShared labels and annotations share a common prefix: app.kubernetes.io . Labels without a prefix are private to users. The shared\nprefix ensures that shared labels do not interfere with custom user labels.\n\nLabels\nIn order to take full advantage of using these labels, they should be applied on every resource object.\nKey\n\nDescription\n\nExample\n\nType\n\napp.kubernetes.io/name\n\nThe name of the application\n\nmysql\n\nstring\n\napp.kubernetes.io/instance\n\nA unique name identifying the instance of an application\n\nmysql-\n\nstring\n\nabcxyz\napp.kubernetes.io/version"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0031", "text": "The current version of the application (e.g., a SemVer 1.0,\nrevision hash, etc.)\n\n5.7.21\n\nstring\n\napp.kubernetes.io/component\n\nThe component within the architecture\n\ndatabase\n\nstring\n\napp.kubernetes.io/part-of\n\nThe name of a higher level application this one is part of\n\nwordpress\n\nstring\n\napp.kubernetes.io/managed-\n\nThe tool being used to manage the operation of an\napplication\n\nHelm\n\nstring\n\nby\n\nTo illustrate these labels in action, consider the following StatefulSet object:\n\n# This is an excerpt\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nlabels:\napp.kubernetes.io/name: mysql\napp.kubernetes.io/instance: mysql-abcxyz\napp.kubernetes.io/version: \"5.7.21\"\napp.kubernetes.io/component: database\napp.kubernetes.io/part-of: wordpress\napp.kubernetes.io/managed-by: Helm\n\nApplications And Instances Of Applications\nAn application can be installed one or more times into a Kubernetes cluster and, in some cases, the same namespace. For example,\nWordPress can be installed more than once where different websites are different installations of WordPress.\nhttps://kubernetes.io/docs/concepts/_print/\n\n33/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe name of an application and the instance name are recorded separately. For example, WordPress has a app.kubernetes.io/name\nof wordpress while it has an instance name, represented as app.kubernetes.io/instance with a value of wordpress-abcxyz . This\nenables the application and instance of the application to be identifiable. Every instance of an application must have a unique name.\n\nExamples\nTo illustrate different ways to use these labels the following examples have varying complexity.\n\nA Simple Stateless Service\nConsider the case for a simple stateless service deployed using Deployment and Service objects. The following two snippets\nrepresent how the labels could be used in their simplest form.\nThe Deployment is used to oversee the pods running the application itself.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: myservice\napp.kubernetes.io/instance: myservice-abcxyz\n...\n\nThe Service is used to expose the application.\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp.kubernetes.io/name: myservice\napp.kubernetes.io/instance: myservice-abcxyz\n...\n\nWeb Application With A Database\nConsider a slightly more complicated application: a web application (WordPress) using a database (MySQL), installed using Helm. The\nfollowing snippets illustrate the start of objects used to deploy this application.\nThe start to the following Deployment is used for WordPress:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: wordpress\napp.kubernetes.io/instance: wordpress-abcxyz\napp.kubernetes.io/version: \"4.9.4\"\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/component: server\napp.kubernetes.io/part-of: wordpress\n..."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0032", "text": "The Service is used to expose WordPress:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n34/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp.kubernetes.io/name: wordpress\napp.kubernetes.io/instance: wordpress-abcxyz\napp.kubernetes.io/version: \"4.9.4\"\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/component: server\napp.kubernetes.io/part-of: wordpress\n...\n\nMySQL is exposed as a StatefulSet with metadata for both it and the larger application it belongs to:\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nlabels:\napp.kubernetes.io/name: mysql\napp.kubernetes.io/instance: mysql-abcxyz\napp.kubernetes.io/version: \"5.7.21\"\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/component: database\napp.kubernetes.io/part-of: wordpress\n...\n\nThe Service is used to expose MySQL as part of WordPress:\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp.kubernetes.io/name: mysql\napp.kubernetes.io/instance: mysql-abcxyz\napp.kubernetes.io/version: \"5.7.21\"\napp.kubernetes.io/managed-by: Helm\napp.kubernetes.io/component: database\napp.kubernetes.io/part-of: wordpress\n...\n\nWith the MySQL StatefulSet and Service you'll notice information about both MySQL and WordPress, the broader application,\nare included.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n35/684\n\n11/7/25, 4:37 PM\n\n1.3 - The Kubernetes API\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0033", "text": "The Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core of Kubernetes'\ncontrol plane is the API server and the HTTP API that it exposes. Users, the different parts of your cluster, and\nexternal components all communicate with one another through the API server.\nThe core of Kubernetes' control plane is the API server. The API server exposes an HTTP API that lets end users, different parts of\nyour cluster, and external components communicate with one another.\nThe Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for example: Pods, Namespaces,\nConfigMaps, and Events).\nMost operations can be performed through the kubectl command-line interface or other command-line tools, such as kubeadm,\nwhich in turn use the API. However, you can also access the API directly using REST calls. Kubernetes provides a set of client libraries\nfor those looking to write applications using the Kubernetes API.\nEach Kubernetes cluster publishes the specification of the APIs that the cluster serves. There are two mechanisms that Kubernetes\nuses to publish these API specifications; both are useful to enable automatic interoperability. For example, the kubectl tool fetches\nand caches the API specification for enabling command-line completion and other features. The two supported mechanisms are as\nfollows:\nThe Discovery API provides information about the Kubernetes APIs: API names, resources, versions, and supported operations.\nThis is a Kubernetes specific term as it is a separate API from the Kubernetes OpenAPI. It is intended to be a brief summary of\nthe available resources and it does not detail specific schema for the resources. For reference about resource schemas, please\nrefer to the OpenAPI document.\nThe Kubernetes OpenAPI Document provides (full) OpenAPI v2.0 and 3.0 schemas for all Kubernetes API endpoints. The\nOpenAPI v3 is the preferred method for accessing OpenAPI as it provides a more comprehensive and accurate view of the API.\nIt includes all the available API paths, as well as all resources consumed and produced for every operations on every endpoints.\nIt also includes any extensibility components that a cluster supports. The data is a complete specification and is significantly\nlarger than that from the Discovery API."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0034", "text": "Discovery API\nKubernetes publishes a list of all group versions and resources supported via the Discovery API. This includes the following for each\nresource:\nName\nCluster or namespaced scope\nEndpoint URL and supported verbs\nAlternative names\nGroup, version, kind\nThe API is available in both aggregated and unaggregated form. The aggregated discovery serves two endpoints, while the\nunaggregated discovery serves a separate endpoint for each group version.\n\nAggregated discovery\nâ“˜ FEATURE STATE: Kubernetes v1.30 [stable] (enabled by default: true)\n\nKubernetes offers stable support for aggregated discovery, publishing all resources supported by a cluster through two endpoints\n( /api and /apis ). Requesting this endpoint drastically reduces the number of requests sent to fetch the discovery data from the\ncluster. You can access the data by requesting the respective endpoints with an Accept header indicating the aggregated discovery\nresource: Accept: application/json;v=v2;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList .\nWithout indicating the resource type using the Accept header, the default response for the /api and /apis endpoint is an\nunaggregated discovery document.\nThe discovery document for the built-in resources can be found in the Kubernetes GitHub repository. This Github document can be\nused as a reference of the base set of the available resources if a Kubernetes cluster is not available to query.\nhttps://kubernetes.io/docs/concepts/_print/\n\n36/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe endpoint also supports ETag and protobuf encoding.\n\nUnaggregated discovery\nWithout discovery aggregation, discovery is published in levels, with the root endpoints publishing discovery information for\ndownstream documents.\nA list of all group versions supported by a cluster is published at the /api and /apis endpoints. Example:\n{\n\"kind\": \"APIGroupList\",\n\"apiVersion\": \"v1\",\n\"groups\": [\n{\n\"name\": \"apiregistration.k8s.io\",\n\"versions\": [\n{\n\"groupVersion\": \"apiregistration.k8s.io/v1\",\n\"version\": \"v1\"\n}\n],\n\"preferredVersion\": {\n\"groupVersion\": \"apiregistration.k8s.io/v1\",\n\"version\": \"v1\"\n}\n},\n{\n\"name\": \"apps\",\n\"versions\": [\n{\n\"groupVersion\": \"apps/v1\",\n\"version\": \"v1\"\n}\n],\n\"preferredVersion\": {\n\"groupVersion\": \"apps/v1\",\n\"version\": \"v1\"\n}\n},\n...\n}\n\nAdditional requests are needed to obtain the discovery document for each group version at /apis/<group>/<version> (for example:\n/apis/rbac.authorization.k8s.io/v1alpha1 ), which advertises the list of resources served under a particular group version. These\nendpoints are used by kubectl to fetch the list of resources supported by a cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0035", "text": "OpenAPI interface definition\nFor details about the OpenAPI specifications, see the OpenAPI documentation.\nKubernetes serves both OpenAPI v2.0 and OpenAPI v3.0. OpenAPI v3 is the preferred method of accessing the OpenAPI because it\noffers a more comprehensive (lossless) representation of Kubernetes resources. Due to limitations of OpenAPI version 2, certain\nfields are dropped from the published OpenAPI including but not limited to default , nullable , oneOf .\n\nOpenAPI V2\nThe Kubernetes API server serves an aggregated OpenAPI v2 spec via the /openapi/v2 endpoint. You can request the response\nformat using request headers as follows:\nHeader\n\nPossible values\n\nNotes\n\nAccept-\n\ngzip\n\nnot supplying this header is also\nacceptable\n\nEncoding\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n37/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nHeader\n\nPossible values\n\nNotes\n\nAccept\n\napplication/com.github.proto-\n\nmainly for intra-cluster use\n\nopenapi.spec.v2@v1.0+protobuf\napplication/json\n\ndefault\n\n*\n\nserves application/json\n\nWarning:\nThe validation rules published as part of OpenAPI schemas may not be complete, and usually aren't. Additional validation\noccurs within the API server. If you want precise and complete verification, a kubectl apply --dry-run=server runs all the\napplicable validation (and also activates admission-time checks).\n\nOpenAPI V3\nâ“˜ FEATURE STATE: Kubernetes v1.27 [stable] (enabled by default: true)\n\nKubernetes supports publishing a description of its APIs as OpenAPI v3.\nA discovery endpoint /openapi/v3 is provided to see a list of all group/versions available. This endpoint only returns JSON. These\ngroup/versions are provided in the following format:\n\n{\n\"paths\": {\n...,\n\"api/v1\": {\n\"serverRelativeURL\": \"/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF\n},\n\"apis/admissionregistration.k8s.io/v1\": {\n\"serverRelativeURL\": \"/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590\n},\n....\n}\n}\n\nThe relative URLs are pointing to immutable OpenAPI descriptions, in order to improve client-side caching. The proper HTTP caching\nheaders are also set by the API server for that purpose ( Expires to 1 year in the future, and Cache-Control to immutable ). When\nan obsolete URL is used, the API server returns a redirect to the newest URL.\nThe Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes group version at the\n/openapi/v3/apis/<group>/<version>?hash=<hash> endpoint.\nRefer to the table below for accepted request headers.\nHeader\n\nPossible values\n\nNotes\n\nAccept-\n\ngzip\n\nnot supplying this header is also\nacceptable\n\napplication/com.github.proto-\n\nmainly for intra-cluster use\n\nEncoding\nAccept\n\nopenapi.spec.v3@v1.0+protobuf\napplication/json\n\ndefault\n\n*\n\nserves application/json\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n38/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0036", "text": "A Golang implementation to fetch the OpenAPI V3 is provided in the package k8s.io/client-go/openapi3 .\nKubernetes 1.34 publishes OpenAPI v2.0 and v3.0; there are no plans to support 3.1 in the near future.\n\nProtobuf serialization\nKubernetes implements an alternative Protobuf based serialization format that is primarily intended for intra-cluster\ncommunication. For more information about this format, see the Kubernetes Protobuf serialization design proposal and the\nInterface Definition Language (IDL) files for each schema located in the Go packages that define the API objects.\n\nPersistence\nKubernetes stores the serialized state of objects by writing them into etcd.\n\nAPI groups and versioning\nTo make it easier to eliminate fields or restructure resource representations, Kubernetes supports multiple API versions, each at a\ndifferent API path, such as /api/v1 or /apis/rbac.authorization.k8s.io/v1alpha1 .\nVersioning is done at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of\nsystem resources and behavior, and to enable controlling access to end-of-life and/or experimental APIs.\nTo make it easier to evolve and to extend its API, Kubernetes implements API groups that can be enabled or disabled.\nAPI resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. The API server\nhandles the conversion between API versions transparently: all the different versions are actually representations of the same\npersisted data. The API server may serve the same underlying data through multiple API versions.\nFor example, suppose there are two API versions, v1 and v1beta1 , for the same resource. If you originally created an object using\nthe v1beta1 version of its API, you can later read, update, or delete that object using either the v1beta1 or the v1 API version,\nuntil the v1beta1 version is deprecated and removed. At that point you can continue accessing and modifying the object using the\nv1 API."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0037", "text": "API changes\nAny system that is successful needs to grow and change as new use cases emerge or existing ones change. Therefore, Kubernetes\nhas designed the Kubernetes API to continuously change and grow. The Kubernetes project aims to not break compatibility with\nexisting clients, and to maintain that compatibility for a length of time so that other projects have an opportunity to adapt.\nIn general, new API resources and new resource fields can be added often and frequently. Elimination of resources or fields requires\nfollowing the API deprecation policy.\nKubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs once they reach general availability\n(GA), typically at API version v1 . Additionally, Kubernetes maintains compatibility with data persisted via beta API versions of official\nKubernetes APIs, and ensures that data can be converted and accessed via GA API versions when the feature goes stable.\nIf you adopt a beta API version, you will need to transition to a subsequent beta or stable API version once the API graduates. The\nbest time to do this is while the beta API is in its deprecation period, since objects are simultaneously accessible via both API\nversions. Once the beta API completes its deprecation period and is no longer served, the replacement API version must be used.\nNote:\nAlthough Kubernetes also aims to maintain compatibility for alpha APIs versions, in some circumstances this is not possible. If\nyou use any alpha API versions, check the release notes for Kubernetes when upgrading your cluster, in case the API did\nchange in incompatible ways that require deleting all existing alpha objects prior to upgrade.\nRefer to API versions reference for more details on the API version level definitions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n39/684\n\n11/7/25, 4:37 PM\n\nAPI Extension\n\nConcepts | Kubernetes\n\nThe Kubernetes API can be extended in one of two ways:\n1. Custom resources let you declaratively define how the API server should provide your chosen resource API.\n2. You can also extend the Kubernetes API by implementing an aggregation layer.\n\nWhat's next\nLearn how to extend the Kubernetes API by adding your own CustomResourceDefinition.\nControlling Access To The Kubernetes API describes how the cluster manages authentication and authorization for API access.\nLearn about API endpoints, resource types and samples by reading API Reference.\nLearn about what constitutes a compatible change, and how to change the API, from API changes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n40/684\n\n11/7/25, 4:37 PM"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0038", "text": "Concepts | Kubernetes\n\n2 - Cluster Architecture\nThe architectural concepts behind Kubernetes.\n\nA Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes, that run containerized applications.\nEvery cluster needs at least one worker node in order to run Pods.\nThe worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker\nnodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a\ncluster usually runs multiple nodes, providing fault-tolerance and high availability.\nThis document outlines the various components you need to have for a complete and working Kubernetes cluster.\n\nCLUSTER\nCONTROL PLANE\n\ncloud-controller-manager\n\netcd\n\nCLOUD PROVIDER API\n\nNode 1\n\nkube-api-server\n\nkubelet\n\nscheduler\n\ncontroller manager\n\nkube-scheduler\n\nkube-controller-manager\n\nNode 2\n\nkube-proxy\n\nkube-proxy\n\nkubelet\n\npod\n\npod\n\npod\npod\nCRI\n\nCRI\n\nFigure 1. Kubernetes cluster components.\nAbout this architectureâ€¦\n\nControl plane components\nThe control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and\nresponding to cluster events (for example, starting up a new pod when a Deployment's replicas field is unsatisfied).\nControl plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control\nplane components on the same machine, and do not run user containers on this machine. See Creating Highly Available clusters\nwith kubeadm for an example control plane setup that runs across multiple machines.\n\nkube-apiserver\nThe API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for\nthe Kubernetes control plane.\nThe main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontallyâ€”that is, it\nscales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.\nhttps://kubernetes.io/docs/concepts/_print/\n\n41/684\n\n11/7/25, 4:37 PM\n\netcd\n\nConcepts | Kubernetes\n\nConsistent and highly-available key value store used as Kubernetes' backing store for all cluster data.\nIf your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.\nYou can find in-depth information about etcd in the official documentation."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0039", "text": "kube-scheduler\nControl plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.\nFactors taken into account for scheduling decisions include: individual and collective resource requirements,\nhardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and\ndeadlines.\n\nkube-controller-manager\nControl plane component that runs controller processes.\nLogically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a\nsingle process.\nThere are many different types of controllers. Some examples of them are:\nNode controller: Responsible for noticing and responding when nodes go down.\nJob controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.\nEndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).\nServiceAccount controller: Create default ServiceAccounts for new namespaces.\nThe above is not an exhaustive list.\n\ncloud-controller-manager\nA Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your\ncluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components\nthat only interact with your cluster.\nThe cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your\nown premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.\nAs with the kube-controller-manager, the cloud-controller-manager combines several logically independent control loops into a\nsingle binary that you run as a single process. You can scale horizontally (run more than one copy) to improve performance or to\nhelp tolerate failures.\nThe following controllers can have cloud provider dependencies:\nNode controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding\nRoute controller: For setting up routes in the underlying cloud infrastructure\nService controller: For creating, updating and deleting cloud provider load balancers\n\nNode components\nNode components run on every node, maintaining running pods and providing the Kubernetes runtime environment."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0040", "text": "kubelet\nAn agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.\nThe kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in\nthose PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n42/684\n\n11/7/25, 4:37 PM\n\nkube-proxy (optional)\n\nConcepts | Kubernetes\n\nkube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.\nkube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network\nsessions inside or outside of your cluster.\nkube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the\ntraffic itself.\nIf you use a network plugin that implements packet forwarding for Services by itself, and providing equivalent behavior to kubeproxy, then you do not need to run kube-proxy on the nodes in your cluster.\n\nContainer runtime\nA fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and\nlifecycle of containers within the Kubernetes environment.\nKubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container\nRuntime Interface).\n\nAddons\nAddons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing\ncluster-level features, namespaced resources for addons belong within the kube-system namespace.\nSelected addons are described below; for an extended list of available addons, please see Addons.\n\nDNS\nWhile the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it.\nCluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes\nservices.\nContainers started by Kubernetes automatically include this DNS server in their DNS searches.\n\nWeb UI (Dashboard)\nDashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications\nrunning in the cluster, as well as the cluster itself.\n\nContainer resource monitoring\nContainer Resource Monitoring records generic time-series metrics about containers in a central database, and provides a UI for\nbrowsing that data.\n\nCluster-level Logging\nA cluster-level logging mechanism is responsible for saving container logs to a central log store with a search/browsing interface."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0041", "text": "Network plugins\nNetwork plugins are software components that implement the container network interface (CNI) specification. They are responsible\nfor allocating IP addresses to pods and enabling them to communicate with each other within the cluster.\n\nArchitecture variations\nWhile the core components of Kubernetes remain consistent, the way they are deployed and managed can vary. Understanding\nthese variations is crucial for designing and maintaining Kubernetes clusters that meet specific operational needs.\nhttps://kubernetes.io/docs/concepts/_print/\n\n43/684\n\n11/7/25, 4:37 PM\n\nControl plane deployment options\n\nConcepts | Kubernetes\n\nThe control plane components can be deployed in several ways:\nTraditional deployment\nControl plane components run directly on dedicated machines or VMs, often managed as systemd services.\nStatic Pods\nControl plane components are deployed as static Pods, managed by the kubelet on specific nodes. This is a common approach\nused by tools like kubeadm.\nSelf-hosted\nThe control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments and StatefulSets or other\nKubernetes primitives.\nManaged Kubernetes services\nCloud providers often abstract away the control plane, managing its components as part of their service offering.\n\nWorkload placement considerations\nThe placement of workloads, including the control plane components, can vary based on cluster size, performance requirements,\nand operational policies:\nIn smaller or development clusters, control plane components and user workloads might run on the same nodes.\nLarger production clusters often dedicate specific nodes to control plane components, separating them from user workloads.\nSome organizations run critical add-ons or monitoring tools on control plane nodes.\n\nCluster management tools\nTools like kubeadm, kops, and Kubespray offer different approaches to deploying and managing clusters, each with its own method\nof component layout and management.\nThe flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as\noperational complexity, performance, and management overhead.\n\nCustomization and extensibility\nKubernetes architecture allows for significant customization:\nCustom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely.\nAPI servers can be extended with CustomResourceDefinitions and API Aggregation.\nCloud providers can integrate deeply with Kubernetes using the cloud-controller-manager.\nThe flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as\noperational complexity, performance, and management overhead."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0042", "text": "What's next\nLearn more about the following:\nNodes and their communication with the control plane.\nKubernetes controllers.\nkube-scheduler which is the default scheduler for Kubernetes.\nEtcd's official documentation.\nSeveral container runtimes in Kubernetes.\nIntegrating with cloud providers using cloud-controller-manager.\nkubectl commands.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n44/684\n\n11/7/25, 4:37 PM\n\n2.1 - Nodes\n\nConcepts | Kubernetes\n\nKubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine,\ndepending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.\nTypically you have several nodes in a cluster; in a learning or resource-limited environment, you might have only one node.\nThe components on a node include the kubelet, a container runtime, and the kube-proxy.\n\nManagement\nThere are two main ways to have Nodes added to the API server:\n1. The kubelet on a node self-registers to the control plane\n2. You (or another human user) manually add a Node object\nAfter you create a Node object, or the kubelet on a node self-registers, the control plane checks whether the new Node object is\nvalid. For example, if you try to create a Node from the following JSON manifest:\n\n{\n\"kind\": \"Node\",\n\"apiVersion\": \"v1\",\n\"metadata\": {\n\"name\": \"10.240.79.157\",\n\"labels\": {\n\"name\": \"my-first-k8s-node\"\n}\n}\n}\n\nKubernetes creates a Node object internally (the representation). Kubernetes checks that a kubelet has registered to the API server\nthat matches the metadata.name field of the Node. If the node is healthy (i.e. all necessary services are running), then it is eligible to\nrun a Pod. Otherwise, that node is ignored for any cluster activity until it becomes healthy.\nNote:\nKubernetes keeps the object for the invalid Node and continues checking to see whether it becomes healthy.\nYou, or a controller, must explicitly delete the Node object to stop that health checking.\n\nThe name of a Node object must be a valid DNS subdomain name."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0043", "text": "Node name uniqueness\nThe name identifies a Node. Two Nodes cannot have the same name at the same time. Kubernetes also assumes that a resource\nwith the same name is the same object. In case of a Node, it is implicitly assumed that an instance using the same name will have\nthe same state (e.g. network settings, root disk contents) and attributes like node labels. This may lead to inconsistencies if an\ninstance was modified without changing its name. If the Node needs to be replaced or updated significantly, the existing Node object\nneeds to be removed from API server first and re-added after the update.\n\nSelf-registration of Nodes\nWhen the kubelet flag --register-node is true (the default), the kubelet will attempt to register itself with the API server. This is the\npreferred pattern, used by most distros.\nFor self-registration, the kubelet is started with the following options:\n--kubeconfig\n\n- Path to credentials to authenticate itself to the API server.\n\n--cloud-provider\nhttps://kubernetes.io/docs/concepts/_print/\n\n- How to talk to a cloud provider to read metadata about itself.\n45/684\n\n11/7/25, 4:37 PM\n\n--register-node\n\nConcepts | Kubernetes\n\n- Automatically register with the API server.\n\n--register-with-taints\n\n- Register the node with the given list of taints (comma separated <key>=<value>:<effect> ).\n\nNo-op if register-node is false.\n- Optional comma-separated list of the IP addresses for the node. You can only specify a single address for each\naddress family. For example, in a single-stack IPv4 cluster, you set this value to be the IPv4 address that the kubelet should use\nfor the node. See configure IPv4/IPv6 dual stack for details of running a dual-stack cluster.\n--node-ip\n\nIf you don't provide this argument, the kubelet uses the node's default IPv4 address, if any; if the node has no IPv4 addresses\nthen the kubelet uses the node's default IPv6 address.\n- Labels to add when registering the node in the cluster (see label restrictions enforced by the NodeRestriction\nadmission plugin).\n--node-labels\n\n--node-status-update-frequency\n\n- Specifies how often kubelet posts its node status to the API server."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0044", "text": "When the Node authorization mode and NodeRestriction admission plugin are enabled, kubelets are only authorized to\ncreate/modify their own Node resource.\nNote:\nAs mentioned in the Node name uniqueness section, when Node configuration needs to be updated, it is a good practice to reregister the node with the API server. For example, if the kubelet is being restarted with a new set of --node-labels , but the\nsame Node name is used, the change will not take effect, as labels are only set (or modified) upon Node registration with the\nAPI server.\nPods already scheduled on the Node may misbehave or cause issues if the Node configuration will be changed on kubelet\nrestart. For example, already running Pod may be tainted against the new labels assigned to the Node, while other Pods, that\nare incompatible with that Pod will be scheduled based on this new label. Node re-registration ensures all Pods will be drained\nand properly re-scheduled.\n\nManual Node administration\nYou can create and modify Node objects using kubectl.\nWhen you want to create Node objects manually, set the kubelet flag --register-node=false .\nYou can modify Node objects regardless of the setting of --register-node . For example, you can set labels on an existing Node or\nmark it unschedulable.\nYou can set optional node role(s) for nodes by adding one or more node-role.kubernetes.io/<role>: <role> labels to the node\nwhere characters of <role> are limited by the syntax rules for labels.\nKubernetes ignores the label value for node roles; by convention, you can set it to the same string you used for the node role in the\nlabel key.\nYou can use labels on Nodes in conjunction with node selectors on Pods to control scheduling. For example, you can constrain a Pod\nto only be eligible to run on a subset of the available nodes.\nMarking a node as unschedulable prevents the scheduler from placing new pods onto that Node but does not affect existing Pods\non the Node. This is useful as a preparatory step before a node reboot or other maintenance.\nTo mark a Node unschedulable, run:\n\nkubectl cordon $NODENAME\n\nSee Safely Drain a Node for more details.\nNote:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n46/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0045", "text": "Pods that are part of a DaemonSet tolerate being run on an unschedulable Node. DaemonSets typically provide node-local\nservices that should run on the Node even if it is being drained of workload applications.\n\nNode status\nA Node's status contains the following information:\nAddresses\nConditions\nCapacity and Allocatable\nInfo\nYou can use kubectl to view a Node's status and other details:\n\nkubectl describe node <insert-node-name-here>\n\nSee Node Status for more details.\n\nNode heartbeats\nHeartbeats, sent by Kubernetes nodes, help your cluster determine the availability of each node, and to take action when failures\nare detected.\nFor nodes there are two forms of heartbeats:\nUpdates to the .status of a Node.\nLease objects within the kube-node-lease namespace. Each Node has an associated Lease object.\n\nNode controller\nThe node controller is a Kubernetes control plane component that manages various aspects of nodes.\nThe node controller has multiple roles in a node's life. The first is assigning a CIDR block to the node when it is registered (if CIDR\nassignment is turned on).\nThe second is keeping the node controller's internal list of nodes up to date with the cloud provider's list of available machines.\nWhen running in a cloud environment and whenever a node is unhealthy, the node controller asks the cloud provider if the VM for\nthat node is still available. If not, the node controller deletes the node from its list of nodes.\nThe third is monitoring the nodes' health. The node controller is responsible for:\nIn the case that a node becomes unreachable, updating the Ready condition in the Node's .status field. In this case the node\ncontroller sets the Ready condition to Unknown .\nIf a node remains unreachable: triggering API-initiated eviction for all of the Pods on the unreachable node. By default, the\nnode controller waits 5 minutes between marking the node as Unknown and submitting the first eviction request.\nBy default, the node controller checks the state of each node every 5 seconds. This period can be configured using the --nodemonitor-period flag on the kube-controller-manager component."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0046", "text": "Rate limits on eviction\nIn most cases, the node controller limits the eviction rate to --node-eviction-rate (default 0.1) per second, meaning it won't evict\npods from more than 1 node per 10 seconds.\nThe node eviction behavior changes when a node in a given availability zone becomes unhealthy. The node controller checks what\npercentage of nodes in the zone are unhealthy (the Ready condition is Unknown or False ) at the same time:\nIf the fraction of unhealthy nodes is at least --unhealthy-zone-threshold (default 0.55), then the eviction rate is reduced.\nhttps://kubernetes.io/docs/concepts/_print/\n\n47/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the cluster is small (i.e. has less than or equal to --large-cluster-size-threshold nodes - default 50), then evictions are\nstopped.\nOtherwise, the eviction rate is reduced to --secondary-node-eviction-rate (default 0.01) per second.\nThe reason these policies are implemented per availability zone is because one availability zone might become partitioned from the\ncontrol plane while the others remain connected. If your cluster does not span multiple cloud provider availability zones, then the\neviction mechanism does not take per-zone unavailability into account.\nA key reason for spreading your nodes across availability zones is so that the workload can be shifted to healthy zones when one\nentire zone goes down. Therefore, if all nodes in a zone are unhealthy, then the node controller evicts at the normal rate of --nodeeviction-rate . The corner case is when all zones are completely unhealthy (none of the nodes in the cluster are healthy). In such a\ncase, the node controller assumes that there is some problem with connectivity between the control plane and the nodes, and\ndoesn't perform any evictions. (If there has been an outage and some nodes reappear, the node controller does evict pods from the\nremaining nodes that are unhealthy or unreachable).\nThe node controller is also responsible for evicting pods running on nodes with NoExecute taints, unless those pods tolerate that\ntaint. The node controller also adds taints corresponding to node problems like node unreachable or not ready. This means that the\nscheduler won't place Pods onto unhealthy nodes."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0047", "text": "Resource capacity tracking\nNode objects track information about the Node's resource capacity: for example, the amount of memory available and the number\nof CPUs. Nodes that self register report their capacity during registration. If you manually add a Node, then you need to set the\nnode's capacity information when you add it.\nThe Kubernetes scheduler ensures that there are enough resources for all the Pods on a Node. The scheduler checks that the sum of\nthe requests of containers on the node is no greater than the node's capacity. That sum of requests includes all containers managed\nby the kubelet, but excludes any containers started directly by the container runtime, and also excludes any processes running\noutside of the kubelet's control.\nNote:\nIf you want to explicitly reserve resources for non-Pod processes, see reserve resources for system daemons.\n\nNode topology\nâ“˜ FEATURE STATE: Kubernetes v1.27 [stable] (enabled by default: true)\n\nIf you have enabled the TopologyManager feature gate, then the kubelet can use topology hints when making resource assignment\ndecisions. See Control Topology Management Policies on a Node for more information.\n\nWhat's next\nLearn more about the following:\nComponents that make up a node.\nAPI definition for Node.\nNode section of the architecture design document.\nGraceful/non-graceful node shutdown.\nNode autoscaling to manage the number and size of nodes in your cluster.\nTaints and Tolerations.\nNode Resource Managers.\nResource Management for Windows nodes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n48/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n2.2 - Communication between Nodes and the Control\nPlane\nThis document catalogs the communication paths between the API server and the Kubernetes cluster. The intent is to allow users to\ncustomize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on\nfully public IPs on a cloud provider)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0048", "text": "Node to Control Plane\nKubernetes has a \"hub-and-spoke\" API pattern. All API usage from nodes (or the pods they run) terminates at the API server. None of\nthe other control plane components are designed to expose remote services. The API server is configured to listen for remote\nconnections on a secure HTTPS port (typically 443) with one or more forms of client authentication enabled. One or more forms of\nauthorization should be enabled, especially if anonymous requests or service account tokens are allowed.\nNodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the API server\nalong with valid client credentials. A good approach is that the client credentials provided to the kubelet are in the form of a client\ncertificate. See kubelet TLS bootstrapping for automated provisioning of kubelet client certificates.\nPods that wish to connect to the API server can do so securely by leveraging a service account so that Kubernetes will automatically\ninject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in default\nnamespace) is configured with a virtual IP address that is redirected (via kube-proxy ) to the HTTPS endpoint on the API server.\nThe control plane components also communicate with the API server over the secure port.\nAs a result, the default operating mode for connections from the nodes and pod running on the nodes to the control plane is\nsecured by default and can run over untrusted and/or public networks.\n\nControl plane to node\nThere are two primary communication paths from the control plane (the API server) to the nodes. The first is from the API server to\nthe kubelet process which runs on each node in the cluster. The second is from the API server to any node, pod, or service through\nthe API server's proxy functionality."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0049", "text": "API server to kubelet\nThe connections from the API server to the kubelet are used for:\nFetching logs for pods.\nAttaching (usually through kubectl ) to running pods.\nProviding the kubelet's port-forwarding functionality.\nThese connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not verify the kubelet's serving\ncertificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public\nnetworks.\nTo verify this connection, use the --kubelet-certificate-authority flag to provide the API server with a root certificate bundle to\nuse to verify the kubelet's serving certificate.\nIf that is not possible, use SSH tunneling between the API server and kubelet if required to avoid connecting over an untrusted or\npublic network.\nFinally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.\n\nAPI server to nodes, pods, and services\nThe connections from the API server to a node, pod, or service default to plain HTTP connections and are therefore neither\nauthenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service\nname in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So while\nthe connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run\nover untrusted or public networks.\nhttps://kubernetes.io/docs/concepts/_print/\n\n49/684\n\n11/7/25, 4:37 PM\n\nSSH tunnels\n\nConcepts | Kubernetes\n\nKubernetes supports SSH tunnels to protect the control plane to nodes communication paths. In this configuration, the API server\ninitiates an SSH tunnel to each node in the cluster (connecting to the SSH server listening on port 22) and passes all traffic destined\nfor a kubelet, node, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed outside of the network in\nwhich the nodes are running.\nNote:\nSSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what you are doing. The Konnectivity\nservice is a replacement for this communication channel.\n\nKonnectivity service\nâ“˜ FEATURE STATE: Kubernetes v1.18 [beta]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0050", "text": "As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the control plane to cluster\ncommunication. The Konnectivity service consists of two parts: the Konnectivity server in the control plane network and the\nKonnectivity agents in the nodes network. The Konnectivity agents initiate connections to the Konnectivity server and maintain the\nnetwork connections. After enabling the Konnectivity service, all control plane to nodes traffic goes through these connections.\nFollow the Konnectivity service task to set up the Konnectivity service in your cluster.\n\nWhat's next\nRead about the Kubernetes control plane components\nLearn more about Hubs and Spoke model\nLearn how to Secure a Cluster\nLearn more about the Kubernetes API\nSet up Konnectivity service\nUse Port Forwarding to Access Applications in a Cluster\nLearn how to Fetch logs for Pods, use kubectl port-forward\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n50/684\n\n11/7/25, 4:37 PM\n\n2.3 - Controllers\n\nConcepts | Kubernetes\n\nIn robotics and automation, a control loop is a non-terminating loop that regulates the state of a system.\nHere is one example of a control loop: a thermostat in a room.\nWhen you set the temperature, that's telling the thermostat about your desired state. The actual room temperature is the current\nstate. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off.\nIn Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each\ncontroller tries to move the current cluster state closer to the desired state.\n\nController pattern\nA controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state. The\ncontroller(s) for that resource are responsible for making the current state come closer to that desired state.\nThe controller might carry the action out itself; more commonly, in Kubernetes, a controller will send messages to the API server that\nhave useful side effects. You'll see examples of this below."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0051", "text": "Control via API server\nThe Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage state by interacting with the cluster\nAPI server.\nJob is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and then stop.\n(Once scheduled, Pod objects become part of the desired state for a kubelet).\nWhen the Job controller sees a new task it makes sure that, somewhere in your cluster, the kubelets on a set of Nodes are running\nthe right number of Pods to get the work done. The Job controller does not run any Pods or containers itself. Instead, the Job\ncontroller tells the API server to create or remove Pods. Other components in the control plane act on the new information (there\nare new Pods to schedule and run), and eventually the work is done.\nAfter you create a new Job, the desired state is for that Job to be completed. The Job controller makes the current state for that Job\nbe nearer to your desired state: creating Pods that do the work you wanted for that Job, so that the Job is closer to completion.\nControllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates\nthat Job object to mark it Finished .\n(This is a bit like how some thermostats turn a light off to indicate that your room is now at the temperature you set)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0052", "text": "Direct control\nIn contrast with Job, some controllers need to make changes to things outside of your cluster.\nFor example, if you use a control loop to make sure there are enough Nodes in your cluster, then that controller needs something\noutside the current cluster to set up new Nodes when needed.\nControllers that interact with external state find their desired state from the API server, then communicate directly with an external\nsystem to bring the current state closer in line.\n(There actually is a controller that horizontally scales the nodes in your cluster.)\nThe important point here is that the controller makes some changes to bring about your desired state, and then reports the current\nstate back to your cluster's API server. Other control loops can observe that reported data and take their own actions.\nIn the thermostat example, if the room is very cold then a different controller might also turn on a frost protection heater. With\nKubernetes clusters, the control plane indirectly works with IP address management tools, storage services, cloud provider APIs, and\nother services by extending Kubernetes to implement that.\n\nDesired versus current state\nKubernetes takes a cloud-native view of systems, and is able to handle constant change.\nhttps://kubernetes.io/docs/concepts/_print/\n\n51/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYour cluster could be changing at any point as work happens and control loops automatically fix failures. This means that,\npotentially, your cluster never reaches a stable state.\nAs long as the controllers for your cluster are running and able to make useful changes, it doesn't matter if the overall state is stable\nor not."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0053", "text": "Design\nAs a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect of cluster state. Most commonly, a\nparticular control loop (controller) uses one kind of resource as its desired state, and has a different kind of resource that it manages\nto make that desired state happen. For example, a controller for Jobs tracks Job objects (to discover new work) and Pod objects (to\nrun the Jobs, and then to see when the work is finished). In this case something else creates the Jobs, whereas the Job controller\ncreates Pods.\nIt's useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so\nKubernetes is designed to allow for that.\nNote:\nThere can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers\nmake sure that they only pay attention to the resources linked to their controlling resource.\nFor example, you can have Deployments and Jobs; these both create Pods. The Job controller does not delete the Pods that\nyour Deployment created, because there is information (labels) the controllers can use to tell those Pods apart.\n\nWays of running controllers\nKubernetes comes with a set of built-in controllers that run inside the kube-controller-manager. These built-in controllers provide\nimportant core behaviors.\nThe Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself (\"built-in\"\ncontrollers). Kubernetes lets you run a resilient control plane, so that if any of the built-in controllers were to fail, another part of the\ncontrol plane will take over the work.\nYou can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller\nyourself. You can run your own controller as a set of Pods, or externally to Kubernetes. What fits best will depend on what that\nparticular controller does.\n\nWhat's next\nRead about the Kubernetes control plane\nDiscover some of the basic Kubernetes objects\nLearn more about the Kubernetes API\nIf you want to write your own controller, see Kubernetes extension patterns and the sample-controller repository.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n52/684\n\n11/7/25, 4:37 PM\n\n2.4 - Leases\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0054", "text": "Distributed systems often have a need for leases, which provide a mechanism to lock shared resources and coordinate activity\nbetween members of a set. In Kubernetes, the lease concept is represented by Lease objects in the coordination.k8s.io API Group\n, which are used for system-critical capabilities such as node heartbeats and component-level leader election.\n\nNode heartbeats\nKubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server. For every Node , there is a\nLease object with a matching name in the kube-node-lease namespace. Under the hood, every kubelet heartbeat is an update\nrequest to this Lease object, updating the spec.renewTime field for the Lease. The Kubernetes control plane uses the time stamp\nof this field to determine the availability of this Node .\nSee Node Lease objects for more details.\n\nLeader election\nKubernetes also uses Leases to ensure only one instance of a component is running at any given time. This is used by control plane\ncomponents like kube-controller-manager and kube-scheduler in HA configurations, where only one instance of the component\nshould be actively running while the other instances are on stand-by.\nRead coordinated leader election to learn about how Kubernetes builds on the Lease API to select which component instance acts as\nleader.\n\nAPI server identity\nâ“˜ FEATURE STATE: Kubernetes v1.26 [beta] (enabled by default: true)\n\nStarting in Kubernetes v1.26, each kube-apiserver uses the Lease API to publish its identity to the rest of the system. While not\nparticularly useful on its own, this provides a mechanism for clients to discover how many instances of kube-apiserver are\noperating the Kubernetes control plane. Existence of kube-apiserver leases enables future capabilities that may require coordination\nbetween each kube-apiserver.\nYou can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-system namespace with the name\napiserver-<sha256-hash> . Alternatively you can use the label selector apiserver.kubernetes.io/identity=kube-apiserver :\n\nkubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver\n\nNAME\napiserver-07a5ea9b9b072c4a5f3d1c3702\napiserver-7be9e061c59d368b3ddaf1376e\napiserver-1dfef752bcb36637d2763d1868\n\nHOLDER\napiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a0\napiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896\napiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0055", "text": "The SHA256 hash used in the lease name is based on the OS hostname as seen by that API server. Each kube-apiserver should be\nconfigured to use a hostname that is unique within the cluster. New instances of kube-apiserver that use the same hostname will\ntake over existing Leases using a new holder identity, as opposed to instantiating new Lease objects. You can check the hostname\nused by kube-apiserver by checking the value of the kubernetes.io/hostname label:\n\nkubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n53/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: coordination.k8s.io/v1\nkind: Lease\nmetadata:\ncreationTimestamp: \"2023-07-02T13:16:48Z\"\nlabels:\napiserver.kubernetes.io/identity: kube-apiserver\nkubernetes.io/hostname: master-1\nname: apiserver-07a5ea9b9b072c4a5f3d1c3702\nnamespace: kube-system\nresourceVersion: \"334899\"\nuid: 90870ab5-1ba9-4523-b215-e4d4e662acb1\nspec:\nholderIdentity: apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05\nleaseDurationSeconds: 3600\nrenewTime: \"2023-07-04T21:58:48.065888Z\"\n\nExpired leases from kube-apiservers that no longer exist are garbage collected by new kube-apiservers after 1 hour.\nYou can disable API server identity leases by disabling the APIServerIdentity feature gate.\n\nWorkloads\nYour own workload can define its own use of Leases. For example, you might run a custom controller where a primary or leader\nmember performs operations that its peers do not. You define a Lease so that the controller replicas can select or elect a leader,\nusing the Kubernetes API for coordination. If you do use a Lease, it's a good practice to define a name for the Lease that is obviously\nlinked to the product or component. For example, if you have a component named Example Foo, use a Lease named example-foo .\nIf a cluster operator or another end user could deploy multiple instances of a component, select a name prefix and pick a\nmechanism (such as hash of the name of the Deployment) to avoid name collisions for the Leases.\nYou can use another approach so long as it achieves the same outcome: different software products do not conflict with one\nanother.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n54/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n2.5 - Cloud Controller Manager\nâ“˜ FEATURE STATE: Kubernetes v1.11 [beta]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0056", "text": "Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds. Kubernetes believes in automated,\nAPI-driven infrastructure without tight coupling between components.\nThe cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud\ncontroller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that\ncloud platform from components that only interact with your cluster.\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager\ncomponent enables cloud providers to release features at a different pace compared to the main Kubernetes project.\nThe cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their\nplatforms with Kubernetes.\n\nDesign\nAPI Server\napi\n\nCloud Controller Manager (optional)\nc-c-m\n\nKubernetes cluster\nController Manager\nc-m\n\nControl Plane\netcd (persistence store)\nc-m\n\nCloud provider\nAPI\n\netcd\n\nNode\nkubelet\n\nc-c-m\n\nkubelet\n\nk-proxy\nkubelet\n\napi\n\nkube-proxy\nk-proxy\n\nNode\n\netcd\n\nkubelet\n\nk-proxy\n\nScheduler\nsched\n\nsched\n\nNode\n\nControl Plane\n\nThe cloud controller manager runs in the control plane as a replicated set of processes (usually, these are containers in Pods). Each\ncloud-controller-manager implements multiple controllers in a single process.\nNote:\nYou can also run the cloud controller manager as a Kubernetes addon rather than as part of the control plane.\n\nCloud controller manager functions\nThe controllers inside the cloud controller manager include:\n\nNode controller\nThe node controller is responsible for updating Node objects when new servers are created in your cloud infrastructure. The node\ncontroller obtains information about the hosts running inside your tenancy with the cloud provider. The node controller performs\nthe following functions:\n1. Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.\nhttps://kubernetes.io/docs/concepts/_print/\n\n55/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n2. Annotating and labelling the Node object with cloud-specific information, such as the region the node is deployed into and the\nresources (CPU, memory, etc) that it has available.\n3. Obtain the node's hostname and network addresses.\n4. Verifying the node's health. In case a node becomes unresponsive, this controller checks with your cloud provider's API to see if\nthe server has been deactivated / deleted / terminated. If the node has been deleted from the cloud, the controller deletes the\nNode object from your Kubernetes cluster.\nSome cloud provider implementations split this into a node controller and a separate node lifecycle controller."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0057", "text": "Route controller\nThe route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in your\nKubernetes cluster can communicate with each other.\nDepending on the cloud provider, the route controller might also allocate blocks of IP addresses for the Pod network.\n\nService controller\nServices integrate with cloud infrastructure components such as managed load balancers, IP addresses, network packet filtering,\nand target health checking. The service controller interacts with your cloud provider's APIs to set up load balancers and other\ninfrastructure components when you declare a Service resource that requires them.\n\nAuthorization\nThis section breaks down the access that the cloud controller manager requires on various API objects, in order to perform its\noperations.\n\nNode controller\nThe Node controller only works with Node objects. It requires full access to read and modify Node objects.\nv1/Node :\n\nget\nlist\ncreate\nupdate\npatch\nwatch\ndelete\n\nRoute controller\nThe route controller listens to Node object creation and configures routes appropriately. It requires Get access to Node objects.\nv1/Node :\n\nget\n\nService controller\nThe service controller watches for Service object create, update and delete events and then configures load balancers for those\nServices appropriately.\nTo access Services, it requires list, and watch access. To update Services, it requires patch and update access to the status\nsubresource.\nv1/Service :\n\nlist\nget\nhttps://kubernetes.io/docs/concepts/_print/\n\n56/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nwatch\npatch\nupdate\n\nOthers\nThe implementation of the core of the cloud controller manager requires access to create Event objects, and to ensure secure\noperation, it requires access to create ServiceAccounts.\nv1/Event :\n\ncreate\npatch\nupdate\nv1/ServiceAccount :\n\ncreate\nThe RBAC ClusterRole for the cloud controller manager looks like:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n57/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: cloud-controller-manager\nrules:\n- apiGroups:\n- \"\"\nresources:\n- events\nverbs:\n- create\n- patch\n- update\n- apiGroups:\n- \"\"\nresources:\n- nodes\nverbs:\n- '*'\n- apiGroups:\n- \"\"\nresources:\n- nodes/status\nverbs:\n- patch\n- apiGroups:\n- \"\"\nresources:\n- services\nverbs:\n- list\n- watch\n- apiGroups:\n- \"\"\nresources:\n- services/status\nverbs:\n- patch\n- update\n- apiGroups:\n- \"\"\nresources:\n- serviceaccounts\nverbs:\n- create\n- apiGroups:\n- \"\"\nresources:\n- persistentvolumes\nverbs:\n- get\n- list\n- update\n- watch"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0058", "text": "What's next\nCloud Controller Manager Administration has instructions on running and managing the cloud controller manager.\nTo upgrade a HA control plane to use the cloud controller manager, see Migrate Replicated Control Plane To Use Cloud\nController Manager.\nWant to know how to implement your own cloud controller manager, or extend an existing project?\nhttps://kubernetes.io/docs/concepts/_print/\n\n58/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe cloud controller manager uses Go interfaces, specifically, CloudProvider interface defined in cloud.go from\nkubernetes/cloud-provider to allow implementations from any cloud to be plugged in.\nThe implementation of the shared controllers highlighted in this document (Node, Route, and Service), and some\nscaffolding along with the shared cloudprovider interface, is part of the Kubernetes core. Implementations specific to\ncloud providers are outside the core of Kubernetes and implement the CloudProvider interface.\nFor more information about developing plugins, see Developing Cloud Controller Manager.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n59/684\n\n11/7/25, 4:37 PM\n\n2.6 - About cgroup v2\n\nConcepts | Kubernetes\n\nOn Linux, control groups constrain resources that are allocated to processes.\nThe kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and\ncontainers which includes cpu/memory requests and limits for containerized workloads.\nThere are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new generation of the cgroup API.\n\nWhat is cgroup v2?\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\ncgroup v2 is the next version of the Linux cgroup API. cgroup v2 provides a unified control system with enhanced resource\nmanagement capabilities.\ncgroup v2 offers several improvements over cgroup v1, such as the following:\nSingle unified hierarchy design in API\nSafer sub-tree delegation to containers\nNewer features like Pressure Stall Information\nEnhanced resource allocation management and isolation across multiple resources\nUnified accounting for different types of memory allocations (network memory, kernel memory, etc)\nAccounting for non-immediate resource changes such as page cache write backs\nSome Kubernetes features exclusively use cgroup v2 for enhanced resource management and isolation. For example, the\nMemoryQoS feature improves memory QoS and relies on cgroup v2 primitives.\n\nUsing cgroup v2\nThe recommended way to use cgroup v2 is to use a Linux distribution that enables and uses cgroup v2 by default.\nTo check if your distribution uses cgroup v2, refer to Identify cgroup version on Linux nodes."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0059", "text": "Requirements\ncgroup v2 has the following requirements:\nOS distribution enables cgroup v2\nLinux Kernel version is 5.8 or later\nContainer runtime supports cgroup v2. For example:\ncontainerd v1.4 and later\ncri-o v1.20 and later\nThe kubelet and the container runtime are configured to use the systemd cgroup driver\n\nLinux Distribution cgroup v2 support\nFor a list of Linux distributions that use cgroup v2, refer to the cgroup v2 documentation\nContainer Optimized OS (since M97)\nUbuntu (since 21.10, 22.04+ recommended)\nDebian GNU/Linux (since Debian 11 bullseye)\nFedora (since 31)\nArch Linux (since April 2021)\nRHEL and RHEL-like distributions (since 9)\nTo check if your distribution is using cgroup v2, refer to your distribution's documentation or follow the instructions in Identify the\ncgroup version on Linux nodes.\nhttps://kubernetes.io/docs/concepts/_print/\n\n60/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can also enable cgroup v2 manually on your Linux distribution by modifying the kernel cmdline boot arguments. If your\ndistribution uses GRUB, systemd.unified_cgroup_hierarchy=1 should be added in GRUB_CMDLINE_LINUX under /etc/default/grub ,\nfollowed by sudo update-grub . However, the recommended approach is to use a distribution that already enables cgroup v2 by\ndefault.\n\nMigrating to cgroup v2\nTo migrate to cgroup v2, ensure that you meet the requirements, then upgrade to a kernel version that enables cgroup v2 by\ndefault.\nThe kubelet automatically detects that the OS is running on cgroup v2 and performs accordingly with no additional configuration\nrequired.\nThere should not be any noticeable difference in the user experience when switching to cgroup v2, unless users are accessing the\ncgroup file system directly, either on the node or from within the containers.\ncgroup v2 uses a different API than cgroup v1, so if there are any applications that directly access the cgroup file system, they need\nto be updated to newer versions that support cgroup v2. For example:\nSome third-party monitoring and security agents may depend on the cgroup filesystem. Update these agents to versions that\nsupport cgroup v2.\nIf you run cAdvisor as a stand-alone DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.\nIf you deploy Java applications, prefer to use versions which fully support cgroup v2:\nOpenJDK / HotSpot: jdk8u372, 11.0.16, 15 and later\nIBM Semeru Runtimes: 8.0.382.0, 11.0.20.0, 17.0.8.0, and later\nIBM Java: 8.0.8.6 and later\nIf you are using the uber-go/automaxprocs package, make sure the version you use is v1.5.1 or higher."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0060", "text": "Identify the cgroup version on Linux Nodes\nThe cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check\nwhich cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node:\n\nstat -fc %T /sys/fs/cgroup/\n\nFor cgroup v2, the output is cgroup2fs .\nFor cgroup v1, the output is tmpfs.\n\nWhat's next\nLearn more about cgroups\nLearn more about container runtime\nLearn more about cgroup drivers\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n61/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n2.7 - Kubernetes Self-Healing\n\nKubernetes is designed with self-healing capabilities that help maintain the health and availability of workloads. It automatically\nreplaces failed containers, reschedules workloads when nodes become unavailable, and ensures that the desired state of the system\nis maintained.\n\nSelf-Healing capabilities\nContainer-level restarts: If a container inside a Pod fails, Kubernetes restarts it based on the restartPolicy .\nReplica replacement: If a Pod in a Deployment or StatefulSet fails, Kubernetes creates a replacement Pod to maintain the\nspecified number of replicas. If a Pod fails that is part of a DaemonSet fails, the control plane creates a replacement Pod to run\non the same node.\nPersistent storage recovery: If a node is running a Pod with a PersistentVolume (PV) attached, and the node fails, Kubernetes\ncan reattach the volume to a new Pod on a different node.\nLoad balancing for Services: If a Pod behind a Service fails, Kubernetes automatically removes it from the Service's endpoints\nto route traffic only to healthy Pods.\nHere are some of the key components that provide Kubernetes self-healing:\nkubelet: Ensures that containers are running, and restarts those that fail.\nReplicaSet, StatefulSet and DaemonSet controller: Maintains the desired number of Pod replicas.\nPersistentVolume controller: Manages volume attachment and detachment for stateful workloads.\n\nConsiderations\nStorage Failures: If a persistent volume becomes unavailable, recovery steps may be required.\nApplication Errors: Kubernetes can restart containers, but underlying application issues must be addressed separately.\n\nWhat's next\nRead more about Pods\nLearn about Kubernetes Controllers\nExplore PersistentVolumes\nRead about node autoscaling. Node autoscaling also provides automatic healing if or when nodes fail in your cluster.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n62/684\n\n11/7/25, 4:37 PM\n\n2.8 - Garbage Collection\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0061", "text": "Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources. This allows the\nclean up of resources like the following:\nTerminated pods\nCompleted Jobs\nObjects without owner references\nUnused containers and container images\nDynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete\nStale or expired CertificateSigningRequests (CSRs)\nNodes deleted in the following scenarios:\nOn a cloud when the cluster uses a cloud controller manager\nOn-premises when the cluster uses an addon similar to a cloud controller manager\nNode Lease objects\n\nOwners and dependents\nMany objects in Kubernetes link to each other through owner references. Owner references tell the control plane which objects are\ndependent on others. Kubernetes uses owner references to give the control plane, and other API clients, the opportunity to clean up\nrelated resources before deleting an object. In most cases, Kubernetes manages owner references automatically.\nOwnership is different from the labels and selectors mechanism that some resources also use. For example, consider a Service that\ncreates EndpointSlice objects. The Service uses labels to allow the control plane to determine which EndpointSlice objects are\nused for that Service. In addition to the labels, each EndpointSlice that is managed on behalf of a Service has an owner reference.\nOwner references help different parts of Kubernetes avoid interfering with objects they donâ€™t control.\nNote:\nCross-namespace owner references are disallowed by design. Namespaced dependents can specify cluster-scoped or\nnamespaced owners. A namespaced owner must exist in the same namespace as the dependent. If it does not, the owner\nreference is treated as absent, and the dependent is subject to deletion once all owners are verified absent.\nCluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-scoped dependent specifies a\nnamespaced kind as an owner, it is treated as having an unresolvable owner reference, and is not able to be garbage collected.\nIn v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference , or a cluster-scoped dependent with an\nownerReference referencing a namespaced kind, a warning Event with a reason of OwnerRefInvalidNamespace and an\ninvolvedObject of the invalid dependent is reported. You can check for that kind of Event by running kubectl get events -A\n--field-selector=reason=OwnerRefInvalidNamespace ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0062", "text": "Cascading deletion\nKubernetes checks for and deletes objects that no longer have owner references, like the pods left behind when you delete a\nReplicaSet. When you delete an object, you can control whether Kubernetes deletes the object's dependents automatically, in a\nprocess called cascading deletion. There are two types of cascading deletion, as follows:\nForeground cascading deletion\nBackground cascading deletion\nYou can also control how and when garbage collection deletes resources that have owner references using Kubernetes finalizers.\n\nForeground cascading deletion\nIn foreground cascading deletion, the owner object you're deleting first enters a deletion in progress state. In this state, the following\nhappens to the owner object:\nThe Kubernetes API server sets the object's metadata.deletionTimestamp field to the time the object was marked for deletion.\nhttps://kubernetes.io/docs/concepts/_print/\n\n63/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe Kubernetes API server also sets the metadata.finalizers field to foregroundDeletion .\nThe object remains visible through the Kubernetes API until the deletion process is complete.\nAfter the owner object enters the deletion in progress state, the controller deletes dependents it knows about. After deleting all the\ndependent objects it knows about, the controller deletes the owner object. At this point, the object is no longer visible in the\nKubernetes API.\nDuring foreground cascading deletion, the only dependents that block owner deletion are those that have the\nownerReference.blockOwnerDeletion=true field and are in the garbage collection controller cache. The garbage collection controller\ncache may not contain objects whose resource type cannot be listed / watched successfully, or objects that are created concurrent\nwith deletion of an owner object. See Use foreground cascading deletion to learn more.\n\nBackground cascading deletion\nIn background cascading deletion, the Kubernetes API server deletes the owner object immediately and the garbage collector\ncontroller (custom or default) cleans up the dependent objects in the background. If a finalizer exists, it ensures that objects are not\ndeleted until all necessary clean-up tasks are completed. By default, Kubernetes uses background cascading deletion unless you\nmanually use foreground deletion or choose to orphan the dependent objects.\nSee Use background cascading deletion to learn more.\n\nOrphaned dependents\nWhen Kubernetes deletes an owner object, the dependents left behind are called orphan objects. By default, Kubernetes deletes\ndependent objects. To learn how to override this behaviour, see Delete owner objects and orphan dependents."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0063", "text": "Garbage collection of unused containers and images\nThe kubelet performs garbage collection on unused images every five minutes and on unused containers every minute. You should\navoid using external garbage collection tools, as these can break the kubelet behavior and remove containers that should exist.\nTo configure options for unused container and image garbage collection, tune the kubelet using a configuration file and change the\nparameters related to garbage collection using the KubeletConfiguration resource type.\n\nContainer image lifecycle\nKubernetes manages the lifecycle of all images through its image manager, which is part of the kubelet, with the cooperation of\ncadvisor. The kubelet considers the following disk usage limits when making garbage collection decisions:\nHighThresholdPercent\nLowThresholdPercent\n\nDisk usage above the configured HighThresholdPercent value triggers garbage collection, which deletes images in order based on\nthe last time they were used, starting with the oldest first. The kubelet deletes images until disk usage reaches the\nLowThresholdPercent value.\n\nGarbage collection for unused container images\nâ“˜ FEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: true)\n\nAs a beta feature, you can specify the maximum time a local image can be unused for, regardless of disk usage. This is a kubelet\nsetting that you configure for each node.\nTo configure the setting, you need to set a value for the imageMaximumGCAge field in the kubelet configuration file.\nThe value is specified as a Kubernetes duration. See duration in the glossary for more details.\nFor example, you can set the configuration field to 12h45m , which means 12 hours and 45 minutes.\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n64/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThis feature does not track image usage across kubelet restarts. If the kubelet is restarted, the tracked image age is reset,\ncausing the kubelet to wait the full imageMaximumGCAge duration before qualifying images for garbage collection based on\nimage age.\n\nContainer garbage collection\nThe kubelet garbage collects unused containers based on the following variables, which you can define:\nMinAge : the minimum age at which the kubelet can garbage collect a container. Disable by setting to 0 .\nMaxPerPodContainer : the maximum number of dead containers each Pod can have. Disable by setting to less than 0 .\nMaxContainers : the maximum number of dead containers the cluster can have. Disable by setting to less than 0 ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0064", "text": "In addition to these variables, the kubelet garbage collects unidentified and deleted containers, typically starting with the oldest first.\nand MaxContainers may potentially conflict with each other in situations where retaining the maximum\nnumber of containers per Pod ( MaxPerPodContainer ) would go outside the allowable total of global dead containers\n( MaxContainers ). In this situation, the kubelet adjusts MaxPerPodContainer to address the conflict. A worst-case scenario would be\nto downgrade MaxPerPodContainer to 1 and evict the oldest containers. Additionally, containers owned by pods that have been\ndeleted are removed once they are older than MinAge .\nMaxPerPodContainer\n\nNote:\nThe kubelet only garbage collects the containers it manages.\n\nConfiguring garbage collection\nYou can tune garbage collection of resources by configuring options specific to the controllers managing those resources. The\nfollowing pages show you how to configure garbage collection:\nConfiguring cascading deletion of Kubernetes objects\nConfiguring cleanup of finished Jobs\n\nWhat's next\nLearn more about ownership of Kubernetes objects.\nLearn more about Kubernetes finalizers.\nLearn about the TTL controller that cleans up finished Jobs.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n65/684\n\n11/7/25, 4:37 PM\n\n2.9 - Mixed Version Proxy\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.28 [alpha] (enabled by default: false)\n\nKubernetes 1.34 includes an alpha feature that lets an API Server proxy a resource requests to other peer API servers. This is useful\nwhen there are multiple API servers running different versions of Kubernetes in one cluster (for example, during a long-lived rollout\nto a new release of Kubernetes).\nThis enables cluster administrators to configure highly available clusters that can be upgraded more safely, by directing resource\nrequests (made during the upgrade) to the correct kube-apiserver. That proxying prevents users from seeing unexpected 404 Not\nFound errors that stem from the upgrade process.\nThis mechanism is called the Mixed Version Proxy.\n\nEnabling the Mixed Version Proxy\nEnsure that UnknownVersionInteroperabilityProxy feature gate is enabled when you start the API Server:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0065", "text": "kube-apiserver \\\n--feature-gates=UnknownVersionInteroperabilityProxy=true \\\n# required command line arguments for this feature\n--peer-ca-file=<path to kube-apiserver CA cert>\n--proxy-client-cert-file=<path to aggregator proxy cert>,\n--proxy-client-key-file=<path to aggregator proxy key>,\n--requestheader-client-ca-file=<path to aggregator CA cert>,\n# requestheader-allowed-names can be set to blank to allow any Common Name\n--requestheader-allowed-names=<valid Common Names to verify proxy client cert against>,\n# optional flags for this feature\n--peer-advertise-ip=`IP of this kube-apiserver that should be used by peers to proxy requests`\n--peer-advertise-port=`port of this kube-apiserver that should be used by peers to proxy requests`\n# â€¦and other flags as usual\n\nProxy transport and authentication between API servers\nThe source kube-apiserver reuses the existing APIserver client authentication flags --proxy-client-cert-file and --proxyclient-key-file to present its identity that will be verified by its peer (the destination kube-apiserver). The destination API\nserver verifies that peer connection based on the configuration you specify using the --requestheader-client-ca-file\ncommand line argument.\nTo authenticate the destination server's serving certs, you must configure a certificate authority bundle by specifying the -peer-ca-file command line argument to the source API server.\n\nConfiguration for peer API server connectivity\nTo set the network location of a kube-apiserver that peers will use to proxy requests, use the --peer-advertise-ip and --peeradvertise-port command line arguments to kube-apiserver or specify these fields in the API server configuration file. If these flags\nare unspecified, peers will use the value from either --advertise-address or --bind-address command line argument to the kubeapiserver. If those too, are unset, the host's default interface is used.\n\nMixed version proxying\nWhen you enable mixed version proxying, the aggregation layer loads a special filter that does the following:\nWhen a resource request reaches an API server that cannot serve that API (either because it is at a version pre-dating the\nintroduction of the API or the API is turned off on the API server) the API server attempts to send the request to a peer API\nserver that can serve the requested API. It does so by identifying API groups / versions / resources that the local server doesn't\nrecognise, and tries to proxy those requests to a peer API server that is capable of handling the request.\nhttps://kubernetes.io/docs/concepts/_print/\n\n66/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the peer API server fails to respond, the source API server responds with 503 (\"Service Unavailable\") error."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0066", "text": "How it works under the hood\nWhen an API Server receives a resource request, it first checks which API servers can serve the requested resource. This check\nhappens using the internal StorageVersion API.\nIf the resource is known to the API server that received the request (for example, GET /api/v1/pods/some-pod ), the request is\nhandled locally.\nIf there is no internal StorageVersion object found for the requested resource (for example, GET /my-api/v1/my-resource )\nand the configured APIService specifies proxying to an extension API server, that proxying happens following the usual flow for\nextension APIs.\nIf a valid internal StorageVersion object is found for the requested resource (for example, GET /batch/v1/jobs ) and the API\nserver trying to handle the request (the handling API server) has the batch API disabled, then the handling API server fetches the\npeer API servers that do serve the relevant API group / version / resource ( api/v1/batch in this case) using the information in\nthe fetched StorageVersion object. The handling API server then proxies the request to one of the matching peer kubeapiservers that are aware of the requested resource.\nIf there is no peer known for that API group / version / resource, the handling API server passes the request to its own\nhandler chain which should eventually return a 404 (\"Not Found\") response.\nIf the handling API server has identified and selected a peer API server, but that peer fails to respond (for reasons such as\nnetwork connectivity issues, or a data race between the request being received and a controller registering the peer's info\ninto the control plane), then the handling API server responds with a 503 (\"Service Unavailable\") error.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n67/684\n\n11/7/25, 4:37 PM\n\n3 - Containers\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0067", "text": "Technology for packaging an application along with its runtime dependencies.\nThis page will discuss containers and container images, as well as their use in operations and solution development.\nThe word container is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.\nEach container that you run is repeatable; the standardization from having dependencies included means that you get the same\nbehavior wherever you run it.\nContainers decouple applications from the underlying host infrastructure. This makes deployment easier in different cloud or OS\nenvironments.\nEach node in a Kubernetes cluster runs the containers that form the Pods assigned to that node. Containers in a Pod are co-located\nand co-scheduled to run on the same node.\n\nContainer images\nA container image is a ready-to-run software package containing everything needed to run an application: the code and any runtime\nit requires, application and system libraries, and default values for any essential settings.\nContainers are intended to be stateless and immutable: you should not change the code of a container that is already running. If you\nhave a containerized application and want to make changes, the correct process is to build a new image that includes the change,\nthen recreate the container to start from the updated image.\n\nContainer runtimes\nA fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and\nlifecycle of containers within the Kubernetes environment.\nKubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container\nRuntime Interface).\nUsually, you can allow your cluster to pick the default container runtime for a Pod. If you need to use more than one container\nruntime in your cluster, you can specify the RuntimeClass for a Pod to make sure that Kubernetes runs those containers using a\nparticular container runtime.\nYou can also use RuntimeClass to run different Pods with the same container runtime but with different settings.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n68/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3.1 - Images"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0068", "text": "A container image represents binary data that encapsulates an application and all its software dependencies. Container images are\nexecutable software bundles that can run standalone and that make very well-defined assumptions about their runtime\nenvironment.\nYou typically create a container image of your application and push it to a registry before referring to it in a Pod.\nThis page provides an outline of the container image concept.\nNote:\nIf you are looking for the container images for a Kubernetes release (such as v1.34, the latest minor release), visit Download\nKubernetes.\n\nImage names\nContainer images are usually given a name such as pause , example/mycontainer , or kube-apiserver . Images can also include a\nregistry hostname; for example: fictional.registry.example/imagename , and possibly a port number as well; for example:\nfictional.registry.example:10443/imagename .\nIf you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry. You can change this\nbehavior by setting a default image registry in the container runtime configuration.\nAfter the image name part you can add a tag or digest (in the same way you would when using with commands like docker or\npodman ). Tags let you identify different versions of the same series of images. Digests are a unique identifier for a specific version of\nan image. Digests are hashes of the image's content, and are immutable. Tags can be moved to point to different images, but digests\nare fixed.\nImage tags consist of lowercase and uppercase letters, digits, underscores ( _ ), periods ( . ), and dashes ( - ). A tag can be up to 128\ncharacters long, and must conform to the following regex pattern: [a-zA-Z0-9_][a-zA-Z0-9._-]{0,127} . You can read more about it\nand find the validation regex in the OCI Distribution Specification. If you don't specify a tag, Kubernetes assumes you mean the tag\nlatest .\nImage digests consists of a hash algorithm (such as sha256 ) and a hash value. For example:\nsha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 . You can find more information about the digest\nformat in the OCI Image Specification.\nSome image name examples that Kubernetes can use are:\nâ€” Image name only, no tag or digest. Kubernetes will use the Docker public registry and latest tag. Equivalent to\ndocker.io/library/busybox:latest .\nbusybox\n\nâ€” Image name with tag. Kubernetes will use the Docker public registry. Equivalent to\ndocker.io/library/busybox:1.32.0 .\nbusybox:1.32.0\n\nregistry.k8s.io/pause:latest\nregistry.k8s.io/pause:3.5"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0069", "text": "â€” Image name with a custom registry and latest tag.\n\nâ€” Image name with a custom registry and non-latest tag.\n\nregistry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07\n\nâ€” Image name with\n\ndigest.\nregistry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07\n\nâ€” Image name\n\nwith tag and digest. Only the digest will be used for pulling.\n\nUpdating images\nWhen you first create a Deployment, StatefulSet, Pod, or other object that includes a PodTemplate, and a pull policy was not\nexplicitly specified, then by default the pull policy of all containers in that Pod will be set to IfNotPresent . This policy causes the\nkubelet to skip pulling an image if it already exists.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n69/684\n\n11/7/25, 4:37 PM\n\nImage pull policy\n\nConcepts | Kubernetes\n\nThe imagePullPolicy for a container and the tag of the image both affect when the kubelet attempts to pull (download) the\nspecified image.\nHere's a list of the values you can set for imagePullPolicy and the effects these values have:\nIfNotPresent\n\nthe image is pulled only if it is not already present locally.\nAlways\n\nevery time the kubelet launches a container, the kubelet queries the container image registry to resolve the name to an image\ndigest. If the kubelet has a container image with that exact digest cached locally, the kubelet uses its cached image; otherwise,\nthe kubelet pulls the image with the resolved digest, and uses that image to launch the container.\nNever\n\nthe kubelet does not try fetching the image. If the image is somehow already present locally, the kubelet attempts to start the\ncontainer; otherwise, startup fails. See pre-pulled images for more details.\nThe caching semantics of the underlying image provider make even imagePullPolicy: Always efficient, as long as the registry is\nreliably accessible. Your container runtime can notice that the image layers already exist on the node so that they don't need to be\ndownloaded again.\nNote:\nYou should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the\nimage is running and more difficult to roll back properly.\nInstead, specify a meaningful tag such as v1.42.0 and/or a digest."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0070", "text": "To make sure the Pod always uses the same version of a container image, you can specify the image's digest; replace <image-name>:\n<tag> with <image-name>@<digest> (for example,\nimage@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 ).\nWhen using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a\nmix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs\nthe same code every time it starts a container with that image name and digest specified. Specifying an image by digest pins the\ncode that you run so that a change at the registry cannot lead to that mix of versions.\nThere are third-party admission controllers that mutate Pods (and PodTemplates) when they are created, so that the running\nworkload is defined based on an image digest rather than a tag. That might be useful if you want to make sure that your entire\nworkload is running the same code no matter what tag changes happen at the registry.\n\nDefault image pull policy\nWhen you (or a controller) submit a new Pod to the API server, your cluster sets the imagePullPolicy field when specific conditions\nare met:\nif you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is\nautomatically set to IfNotPresent .\nif you omit the imagePullPolicy field, and the tag for the container image is :latest , imagePullPolicy is automatically set\nto Always .\nif you omit the imagePullPolicy field, and you don't specify the tag for the container image, imagePullPolicy is automatically\nset to Always .\nif you omit the imagePullPolicy field, and you specify a tag for the container image that isn't :latest , the imagePullPolicy\nis automatically set to IfNotPresent .\nNote:\nThe value of imagePullPolicy of the container is always set when the object is first created, and is not updated if the image's\ntag or digest later changes.\nhttps://kubernetes.io/docs/concepts/_print/\n\n70/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, if you create a Deployment with an image whose tag is not :latest , and later update that Deployment's image\nto a :latest tag, the imagePullPolicy field will not change to Always . You must manually change the pull policy of any\nobject after its initial creation."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0071", "text": "Required image pull\nIf you would like to always force a pull, you can do one of the following:\nSet the imagePullPolicy of the container to Always .\nOmit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when\nyou submit the Pod.\nOmit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the\nPod.\nEnable the AlwaysPullImages admission controller.\n\nImagePullBackOff\nWhen a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state\nbecause of ImagePullBackOff .\nThe status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image (for\nreasons such as invalid image name, or pulling from a private registry without imagePullSecret ). The BackOff part indicates that\nKubernetes will keep trying to pull the image, with an increasing back-off delay.\nKubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).\n\nImage pull per runtime class\nâ“˜ FEATURE STATE: Kubernetes v1.29 [alpha] (enabled by default: false)\n\nKubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.\nIf you enable the RuntimeClassInImageCriApi feature gate, the kubelet references container images by a tuple of image name and\nruntime handler rather than just the image name or digest. Your container runtime may adapt its behavior based on the selected\nruntime handler. Pulling images based on runtime class is useful for VM-based containers, such as Windows Hyper-V containers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0072", "text": "Serial and parallel image pulls\nBy default, the kubelet pulls images serially. In other words, the kubelet sends only one image pull request to the image service at a\ntime. Other image pull requests have to wait until the one being processed is complete.\nNodes make image pull decisions in isolation. Even when you use serialized image pulls, two different nodes can pull the same\nimage in parallel.\nIf you would like to enable parallel image pulls, you can set the field serializeImagePulls to false in the kubelet configuration. With\nserializeImagePulls set to false, image pull requests will be sent to the image service immediately, and multiple images will be\npulled at the same time.\nWhen enabling parallel image pulls, ensure that the image service of your container runtime can handle parallel image pulls.\nThe kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you have a Pod that has an init container\nand an application container, the image pulls for the two containers will not be parallelized. However, if you have two Pods that use\ndifferent images, and the parallel image pull feature is enabled, the kubelet will pull the images in parallel on behalf of the two\ndifferent Pods.\n\nMaximum parallel image pulls\nâ“˜ FEATURE STATE: Kubernetes v1.32 [beta]\nhttps://kubernetes.io/docs/concepts/_print/\n\n71/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen serializeImagePulls is set to false, the kubelet defaults to no limit on the maximum number of images being pulled at the\nsame time. If you would like to limit the number of parallel image pulls, you can set the field maxParallelImagePulls in the kubelet\nconfiguration. With maxParallelImagePulls set to n, only n images can be pulled at the same time, and any image pull beyond n will\nhave to wait until at least one ongoing image pull is complete.\nLimiting the number of parallel image pulls prevents image pulling from consuming too much network bandwidth or disk I/O, when\nparallel image pulling is enabled.\nYou can set maxParallelImagePulls to a positive number that is greater than or equal to 1. If you set maxParallelImagePulls to be\ngreater than or equal to 2, you must set serializeImagePulls to false. The kubelet will fail to start with an invalid\nmaxParallelImagePulls setting."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0073", "text": "Multi-architecture images with image indexes\nAs well as providing binary images, a container registry can also serve a container image index. An image index can point to multiple\nimage manifests for architecture-specific versions of a container. The idea is that you can have a name for an image (for example:\npause , example/mycontainer , kube-apiserver ) and allow different systems to fetch the right binary image for the machine\narchitecture they are using.\nThe Kubernetes project typically creates container images for its releases with names that include the suffix -$(ARCH) . For\nbackward compatibility, generate older images with suffixes. For instance, an image named as pause would be a multi-architecture\nimage containing manifests for all supported architectures, while pause-amd64 would be a backward-compatible version for older\nconfigurations, or for YAML files with hardcoded image names containing suffixes.\n\nUsing a private registry\nPrivate registries may require authentication to be able to discover and/or pull images from them. Credentials can be provided in\nseveral ways:\nSpecifying imagePullSecrets when you define a Pod\nOnly Pods which provide their own keys can access the private registry.\nConfiguring Nodes to Authenticate to a Private Registry\nAll Pods can read any configured private registries.\nRequires node configuration by cluster administrator.\nUsing a kubelet credential provider plugin to dynamically fetch credentials for private registries\nThe kubelet can be configured to use credential provider exec plugin for the respective private registry.\nPre-pulled Images\nAll Pods can use any images cached on a node.\nRequires root access to all nodes to set up.\nVendor-specific or local extensions\nIf you're using a custom node configuration, you (or your cloud provider) can implement your mechanism for authenticating\nthe node to the container registry.\nThese options are explained in more detail below.\n\nSpecifying imagePullSecrets on a Pod\nNote:\nThis is the recommended approach to run containers based on images in private registries.\nKubernetes supports specifying container image registry keys on a Pod. All imagePullSecrets must be Secrets that exist in the same\nNamespace as the Pod. These Secrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson .\nhttps://kubernetes.io/docs/concepts/_print/\n\n72/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nConfiguring nodes to authenticate to a private registry"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0074", "text": "Specific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your\nsolution's documentation for the most accurate information.\nFor an example of configuring a private container image registry, see the Pull an Image from a Private Registry task. That example\nuses a private registry in Docker Hub.\n\nKubelet credential provider for authenticated image pulls\nYou can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image. This is the\nmost robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable.\nThis technique can be especially useful for running static Pods that require container images hosted in a private registry. Using a\nServiceAccount or a Secret to provide private registry credentials is not possible in the specification of a static Pod, because it cannot\nhave references to other API resources in its specification.\nSee Configure a kubelet image credential provider for more details.\n\nInterpretation of config.json\nThe interpretation of config.json varies between the original Docker implementation and the Kubernetes interpretation. In\nDocker, the auths keys can only specify root URLs, whereas Kubernetes allows glob URLs as well as prefix-matched paths. The only\nlimitation is that glob patterns ( * ) have to include the dot ( . ) for each subdomain. The amount of matched subdomains has to be\nequal to the amount of glob patterns ( *. ), for example:\n*.kubernetes.io\n\nwill not match kubernetes.io , but will match abc.kubernetes.io .\n\n*.*.kubernetes.io\nprefix.*.io\n\nwill not match abc.kubernetes.io , but will match abc.def.kubernetes.io .\n\nwill match prefix.kubernetes.io .\n\n*-good.kubernetes.io\n\nwill match prefix-good.kubernetes.io .\n\nThis means that a config.json like this is valid:\n\n{\n\"auths\": {\n\"my-registry.example/images\": { \"auth\": \"â€¦\" },\n\"*.my-registry.example/images\": { \"auth\": \"â€¦\" }\n}\n}\n\nImage pull operations pass the credentials to the CRI container runtime for every valid pattern. For example, the following container\nimage names would match successfully:\nmy-registry.example/images\nmy-registry.example/images/my-image\nmy-registry.example/images/another-image\nsub.my-registry.example/images/my-image\n\nHowever, these container image names would not match:\na.sub.my-registry.example/images/my-image\na.b.sub.my-registry.example/images/my-image\n\nThe kubelet performs image pulls sequentially for every found credential. This means that multiple entries in config.json for\ndifferent paths are possible, too:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n73/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0075", "text": "{\n\"auths\": {\n\"my-registry.example/images\": {\n\"auth\": \"â€¦\"\n},\n\"my-registry.example/images/subpath\": {\n\"auth\": \"â€¦\"\n}\n}\n}\n\nIf now a container specifies an image my-registry.example/images/subpath/my-image to be pulled, then the kubelet will try to\ndownload it using both authentication sources if one of them fails.\n\nPre-pulled images\nNote:\nThis approach is suitable if you can control node configuration. It will not work reliably if your cloud provider manages nodes\nand replaces them automatically.\nBy default, the kubelet tries to pull each image from the specified registry. However, if the imagePullPolicy property of the\ncontainer is set to IfNotPresent or Never , then a local image is used (preferentially or exclusively, respectively).\nIf you want to rely on pre-pulled images as a substitute for registry authentication, you must ensure all nodes in the cluster have the\nsame pre-pulled images.\nThis can be used to preload certain images for speed or as an alternative to authenticating to a private registry.\nSimilar to the usage of the kubelet credential provider, pre-pulled images are also suitable for launching static Pods that depend on\nimages hosted in a private registry.\nNote:\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nAccess to pre-pulled images may be authorized according to image pull credential verification.\n\nEnsure image pull credential verification\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nIf the KubeletEnsureSecretPulledImages feature gate is enabled for your cluster, Kubernetes will validate image credentials for\nevery image that requires credentials to be pulled, even if that image is already present on the node. This validation ensures that\nimages in a Pod request which have not been successfully pulled with the provided credentials must re-pull the images from the\nregistry. Additionally, image pulls that re-use the same credentials which previously resulted in a successful image pull will not need\nto re-pull from the registry and are instead validated locally without accessing the registry (provided the image is available locally).\nThis is controlled by the imagePullCredentialsVerificationPolicy field in the Kubelet configuration.\nThis configuration controls when image pull credentials must be verified if the image is already present on the node:\nNeverVerify : Mimics the behavior of having this feature gate disabled. If the image is present locally, image pull credentials\n\nare not verified.\nNeverVerifyPreloadedImages : Images pulled outside the kubelet are not verified, but all other images will have their"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0076", "text": "credentials verified. This is the default behavior.\nNeverVerifyAllowListedImages : Images pulled outside the kubelet and mentioned within the\npreloadedImagesVerificationAllowlist specified in the kubelet config are not verified.\nAlwaysVerify : All images will have their credentials verified before they can be used.\nhttps://kubernetes.io/docs/concepts/_print/\n\n74/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThis verification applies to pre-pulled images, images pulled using node-wide secrets, and images pulled using Pod-level secrets.\nNote:\nIn the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to\naccess the registry. New or rotated credentials will require the image to be re-pulled from the registry.\n\nCreating a Secret with a Docker config\nYou need to know the username, registry password and client email address for authenticating to the registry, as well as its\nhostname. Run the following command, substituting placeholders with the appropriate values:\n\nkubectl create secret docker-registry <name> \\\n--docker-server=<docker-registry-server> \\\n--docker-username=<docker-user> \\\n--docker-password=<docker-password> \\\n--docker-email=<docker-email>\n\nIf you already have a Docker credentials file then, rather than using the above command, you can import the credentials file as a\nKubernetes Secret. Create a Secret based on existing Docker credentials explains how to set this up.\nThis is particularly useful if you are using multiple private container registries, as kubectl create secret docker-registry creates a\nSecret that only works with a single private registry.\nNote:\nPods can only reference image pull secrets in their own namespace, so this process needs to be done one time per\nnamespace.\n\nReferring to imagePullSecrets on a Pod\nNow, you can create pods which reference that secret by adding the imagePullSecrets section to a Pod definition. Each item in the\nimagePullSecrets array can only reference one Secret in the same namespace.\nFor example:\n\ncat <<EOF > pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: foo\nnamespace: awesomeapps\nspec:\ncontainers:\n- name: foo\nimage: janedoe/awesomeapp:v1\nimagePullSecrets:\n- name: myregistrykey\nEOF\ncat <<EOF >> ./kustomization.yaml\nresources:\n- pod.yaml\nEOF\n\nThis needs to be done for each Pod that is using a private registry.\nHowever, you can automate this process by specifying the imagePullSecrets section in a ServiceAccount resource. See Add\nImagePullSecrets to a Service Account for detailed instructions.\nYou can use this in conjunction with a per-node .docker/config.json . The credentials will be merged.\nhttps://kubernetes.io/docs/concepts/_print/\n\n75/684\n\n11/7/25, 4:37 PM\n\nUse cases\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0077", "text": "There are a number of solutions for configuring private registries. Here are some common use cases and suggested solutions.\n1. Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.\nUse public images from a public registry\nNo configuration required.\nSome cloud providers automatically cache or mirror public images, which improves availability and reduces the time\nto pull images.\n2. Cluster running some proprietary images which should be hidden to those outside the company, but visible to all cluster users.\nUse a hosted private registry\nManual configuration may be required on the nodes that need to access to private registry.\nOr, run an internal private registry behind your firewall with open read access.\nNo Kubernetes configuration is required.\nUse a hosted container image registry service that controls image access\nIt will work better with Node autoscaling than manual node configuration.\nOr, on a cluster where changing the node configuration is inconvenient, use imagePullSecrets .\n3. Cluster with proprietary images, a few of which require stricter access control.\nEnsure AlwaysPullImages admission controller is active. Otherwise, all Pods potentially have access to all images.\nMove sensitive data into a Secret resource, instead of packaging it in an image.\n4. A multi-tenant cluster where each tenant needs own private registry.\nEnsure AlwaysPullImages admission controller is active. Otherwise, all Pods of all tenants potentially have access to all\nimages.\nRun a private registry with authorization required.\nGenerate registry credentials for each tenant, store into a Secret, and propagate the Secret to every tenant namespace.\nThe tenant then adds that Secret to imagePullSecrets of each namespace.\nIf you need access to multiple registries, you can create one Secret per registry.\n\nLegacy built-in kubelet credential provider\nIn older versions of Kubernetes, the kubelet had a direct integration with cloud provider credentials. This provided the ability to\ndynamically fetch credentials for image registries.\nThere were three built-in implementations of the kubelet credential provider integration: ACR (Azure Container Registry), ECR (Elastic\nContainer Registry), and GCR (Google Container Registry).\nStarting with version 1.26 of Kubernetes, the legacy mechanism has been removed, so you would need to either:\nconfigure a kubelet image credential provider on each node; or\nspecify image pull credentials using imagePullSecrets and at least one Secret.\n\nWhat's next\nRead the OCI Image Manifest Specification.\nLearn about container image garbage collection.\nLearn more about pulling an Image from a Private Registry.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n76/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0078", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3.2 - Container Environment\n\nThis page describes the resources available to Containers in the Container environment.\n\nContainer environment\nThe Kubernetes Container environment provides several important resources to Containers:\nA filesystem, which is a combination of an image and one or more volumes.\nInformation about the Container itself.\nInformation about other objects in the cluster.\n\nContainer information\nThe hostname of a Container is the name of the Pod in which the Container is running. It is available through the hostname\ncommand or the gethostname function call in libc.\nThe Pod name and namespace are available as environment variables through the downward API.\nUser defined environment variables from the Pod definition are also available to the Container, as are any environment variables\nspecified statically in the container image.\n\nCluster information\nA list of all services that were running when a Container was created is available to that Container as environment variables. This list\nis limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.\nFor a service named foo that maps to a Container named bar, the following variables are defined:\n\nFOO_SERVICE_HOST=<the host the service is running on>\nFOO_SERVICE_PORT=<the port the service is running on>\n\nServices have dedicated IP addresses and are available to the Container via DNS, if DNS addon is enabled.\n\nWhat's next\nLearn more about Container lifecycle hooks.\nGet hands-on experience attaching handlers to Container lifecycle events.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n77/684\n\n11/7/25, 4:37 PM\n\n3.3 - Runtime Class\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.20 [stable]\n\nThis page describes the RuntimeClass resource and runtime selection mechanism.\nRuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a\nPod's containers.\n\nMotivation\nYou can set a different RuntimeClass between different Pods to provide a balance of performance versus security. For example, if\npart of your workload deserves a high level of information security assurance, you might choose to schedule those Pods so that they\nrun in a container runtime that uses hardware virtualization. You'd then benefit from the extra isolation of the alternative runtime,\nat the expense of some additional overhead.\nYou can also use RuntimeClass to run different Pods with the same container runtime but with different settings.\n\nSetup\n1. Configure the CRI implementation on nodes (runtime dependent)\n2. Create the corresponding RuntimeClass resources"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0079", "text": "1. Configure the CRI implementation on nodes\nThe configurations available through RuntimeClass are Container Runtime Interface (CRI) implementation dependent. See the\ncorresponding documentation (below) for your CRI implementation for how to configure.\nNote:\nRuntimeClass assumes a homogeneous node configuration across the cluster by default (which means that all nodes are\nconfigured the same way with respect to container runtimes). To support heterogeneous node configurations, see Scheduling\nbelow.\nThe configurations have a corresponding handler name, referenced by the RuntimeClass. The handler must be a valid DNS label\nname.\n\n2. Create the corresponding RuntimeClass resources\nThe configurations setup in step 1 should each have an associated handler name, which identifies the configuration. For each\nhandler, create a corresponding RuntimeClass object.\nThe RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name ( metadata.name ) and the handler\n( handler ). The object definition looks like this:\n\n# RuntimeClass is defined in the node.k8s.io API group\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n# The name the RuntimeClass will be referenced by.\n# RuntimeClass is a non-namespaced resource.\nname: myclass\n# The name of the corresponding CRI configuration\nhandler: myconfiguration\n\nThe name of a RuntimeClass object must be a valid DNS subdomain name.\nhttps://kubernetes.io/docs/concepts/_print/\n\n78/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nIt is recommended that RuntimeClass write operations (create/update/patch/delete) be restricted to the cluster administrator.\nThis is typically the default. See Authorization Overview for more details.\n\nUsage\nOnce RuntimeClasses are configured for the cluster, you can specify a runtimeClassName in the Pod spec to use it. For example:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: mypod\nspec:\nruntimeClassName: myclass\n# ...\n\nThis will instruct the kubelet to use the named RuntimeClass to run this pod. If the named RuntimeClass does not exist, or the CRI\ncannot run the corresponding handler, the pod will enter the Failed terminal phase. Look for a corresponding event for an error\nmessage.\nIf no runtimeClassName is specified, the default RuntimeHandler will be used, which is equivalent to the behavior when the\nRuntimeClass feature is disabled.\n\nCRI Configuration\nFor more details on setting up CRI runtimes, see CRI installation.\n\ncontainerd\nRuntime handlers are configured through containerd's configuration at /etc/containerd/config.toml . Valid handlers are\nconfigured under the runtimes section:\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.${HANDLER_NAME}]\n\nSee containerd's config documentation for more details:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0080", "text": "CRI-O\nRuntime handlers are configured through CRI-O's configuration at /etc/crio/crio.conf . Valid handlers are configured under the\ncrio.runtime table:\n[crio.runtime.runtimes.${HANDLER_NAME}]\nruntime_path = \"${PATH_TO_BINARY}\"\n\nSee CRI-O's config documentation for more details.\n\nScheduling\nâ“˜ FEATURE STATE: Kubernetes v1.16 [beta]\n\nBy specifying the scheduling field for a RuntimeClass, you can set constraints to ensure that Pods running with this RuntimeClass\nare scheduled to nodes that support it. If scheduling is not set, this RuntimeClass is assumed to be supported by all nodes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n79/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTo ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a common label which is then\nselected by the runtimeclass.scheduling.nodeSelector field. The RuntimeClass's nodeSelector is merged with the pod's\nnodeSelector in admission, effectively taking the intersection of the set of nodes selected by each. If there is a conflict, the pod will\nbe rejected.\nIf the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you can add tolerations to the\nRuntimeClass. As with the nodeSelector , the tolerations are merged with the pod's tolerations in admission, effectively taking the\nunion of the set of nodes tolerated by each.\nTo learn more about configuring the node selector and tolerations, see Assigning Pods to Nodes.\n\nPod Overhead\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nYou can specify overhead resources that are associated with running a Pod. Declaring overhead allows the cluster (including the\nscheduler) to account for it when making decisions about Pods and resources.\nPod overhead is defined in RuntimeClass through the overhead field. Through the use of this field, you can specify the overhead of\nrunning pods utilizing this RuntimeClass and ensure these overheads are accounted for in Kubernetes.\n\nWhat's next\nRuntimeClass Design\nRuntimeClass Scheduling Design\nRead about the Pod Overhead concept\nPodOverhead Feature Design\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n80/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3.4 - Container Lifecycle Hooks\n\nThis page describes how kubelet managed Containers can use the Container lifecycle hook framework to run code triggered by\nevents during their management lifecycle.\n\nOverview\nAnalogous to many programming language frameworks that have component lifecycle hooks, such as Angular, Kubernetes provides\nContainers with lifecycle hooks. The hooks enable Containers to be aware of events in their management lifecycle and run code\nimplemented in a handler when the corresponding lifecycle hook is executed."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0081", "text": "Container hooks\nThere are two hooks that are exposed to Containers:\nPostStart\n\nThis hook is executed immediately after a container is created. However, there is no guarantee that the hook will execute before the\ncontainer ENTRYPOINT. No parameters are passed to the handler.\nPreStop\n\nThis hook is called immediately before a container is terminated due to an API request or management event such as a\nliveness/startup probe failure, preemption, resource contention and others. A call to the PreStop hook fails if the container is\nalready in a terminated or completed state and the hook must complete before the TERM signal to stop the container can be sent.\nThe Pod's termination grace period countdown begins before the PreStop hook is executed, so regardless of the outcome of the\nhandler, the container will eventually terminate within the Pod's termination grace period. No parameters are passed to the handler.\nA more detailed description of the termination behavior can be found in Termination of Pods.\nStopSignal\n\nThe StopSignal lifecycle can be used to define a stop signal which would be sent to the container when it is stopped. If you set this, it\noverrides any STOPSIGNAL instruction defined within the container image.\nA more detailed description of termination behaviour with custom stop signals can be found in Stop Signals.\n\nHook handler implementations\nContainers can access a hook by implementing and registering a handler for that hook. There are three types of hook handlers that\ncan be implemented for Containers:\nExec - Executes a specific command, such as pre-stop.sh , inside the cgroups and namespaces of the Container. Resources\nconsumed by the command are counted against the Container.\nHTTP - Executes an HTTP request against a specific endpoint on the Container.\nSleep - Pauses the container for a specified duration."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0082", "text": "Hook handler execution\nWhen a Container lifecycle management hook is called, the Kubernetes management system executes the handler according to the\nhook action, httpGet , tcpSocket (deprecated) and sleep are executed by the kubelet process, and exec is executed in the\ncontainer.\nThe PostStart hook handler call is initiated when a container is created, meaning the container ENTRYPOINT and the PostStart\nhook are triggered simultaneously. However, if the PostStart hook takes too long to execute or if it hangs, it can prevent the\ncontainer from transitioning to a running state.\nhooks are not executed asynchronously from the signal to stop the Container; the hook must complete its execution\nbefore the TERM signal can be sent. If a PreStop hook hangs during execution, the Pod's phase will be Terminating and remain\nthere until the Pod is killed after its terminationGracePeriodSeconds expires. This grace period applies to the total time it takes for\nboth the PreStop hook to execute and for the Container to stop normally. If, for example, terminationGracePeriodSeconds is 60,\nPreStop\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n81/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nand the hook takes 55 seconds to complete, and the Container takes 10 seconds to stop normally after receiving the signal, then the\nContainer will be killed before it can stop normally, since terminationGracePeriodSeconds is less than the total time (55+10) it takes\nfor these two things to happen.\nIf either a PostStart or PreStop hook fails, it kills the Container.\nUsers should make their hook handlers as lightweight as possible. There are cases, however, when long running commands make\nsense, such as when saving state prior to stopping a Container.\n\nHook delivery guarantees\nHook delivery is intended to be at least once, which means that a hook may be called multiple times for any given event, such as for\nPostStart or PreStop . It is up to the hook implementation to handle this correctly.\nGenerally, only single deliveries are made. If, for example, an HTTP hook receiver is down and is unable to take traffic, there is no\nattempt to resend. In some rare cases, however, double delivery may occur. For instance, if a kubelet restarts in the middle of\nsending a hook, the hook might be resent after the kubelet comes back up."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0083", "text": "Debugging Hook handlers\nThe logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it broadcasts an event. For PostStart ,\nthis is the FailedPostStartHook event, and for PreStop , this is the FailedPreStopHook event. To generate a failed\nFailedPostStartHook event yourself, modify the lifecycle-events.yaml file to change the postStart command to \"badcommand\" and\napply it. Here is some example output of the resulting events you see from running kubectl describe pod lifecycle-demo :\nEvents:\nType\n---Normal\nNormal\nNormal\nNormal\nNormal\nWarning\nNormal\nNormal\nWarning\n\nReason\n-----Scheduled\nPulled\nPulling\nCreated\nStarted\nFailedPostStartHook\nKilling\nPulled\nBackOff\n\nAge\n---7s\n6s\n4s (x2 over 6s)\n4s (x2 over 5s)\n4s (x2 over 5s)\n4s (x2 over 5s)\n4s (x2 over 5s)\n4s\n2s (x2 over 3s)\n\nFrom\n---default-scheduler\nkubelet\nkubelet\nkubelet\nkubelet\nkubelet\nkubelet\nkubelet\nkubelet\n\nMessage\n------Successfully assigned default/lifecycle-demo to\nSuccessfully pulled image \"nginx\" in 229.604315m\nPulling image \"nginx\"\nCreated container lifecycle-demo-container\nStarted container lifecycle-demo-container\nExec lifecycle hook ([badcommand]) for Container\nFailedPostStartHook\nSuccessfully pulled image \"nginx\" in 215.66395ms\nBack-off restarting failed container\n\nWhat's next\nLearn more about the Container environment.\nGet hands-on experience attaching handlers to Container lifecycle events.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n82/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3.5 - Container Runtime Interface (CRI)\nThe CRI is a plugin interface which enables the kubelet to use a wide variety of container runtimes, without having a need to\nrecompile the cluster components.\nYou need a working container runtime on each Node in your cluster, so that the kubelet can launch Pods and their containers.\nThe Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime.\nThe Kubernetes Container Runtime Interface (CRI) defines the main gRPC protocol for the communication between the node\ncomponents kubelet and container runtime.\n\nThe API\nâ“˜ FEATURE STATE: Kubernetes v1.23 [stable]\n\nThe kubelet acts as a client when connecting to the container runtime via gRPC. The runtime and image service endpoints have to be\navailable in the container runtime, which can be configured separately within the kubelet by using the --container-runtimeendpoint command line flag.\nFor Kubernetes v1.26 and later, the kubelet requires that the container runtime supports the v1 CRI API. If a container runtime\ndoes not support the v1 API, the kubelet will not register the node."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0084", "text": "Upgrading\nWhen upgrading the Kubernetes version on a node, the kubelet restarts. If the container runtime does not support the v1 CRI API,\nthe kubelet will fail to register and report an error. If a gRPC re-dial is required because the container runtime has been upgraded,\nthe runtime must support the v1 CRI API for the connection to succeed. This might require a restart of the kubelet after the\ncontainer runtime is correctly configured.\n\nWhat's next\nLearn more about the CRI protocol definition\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n83/684\n\n11/7/25, 4:37 PM\n\n4 - Workloads\n\nConcepts | Kubernetes\n\nUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions\nthat help you to run them.\nA workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on\nKubernetes you run it inside a set of pods. In Kubernetes, a Pod represents a set of running containers on your cluster.\nKubernetes pods have a defined lifecycle. For example, once a pod is running in your cluster then a critical fault on the node where\nthat pod is running means that all the pods on that node fail. Kubernetes treats that level of failure as final: you would need to\ncreate a new Pod to recover, even if the node later becomes healthy.\nHowever, to make life considerably easier, you don't need to manage each Pod directly. Instead, you can use workload resources that\nmanage a set of pods on your behalf. These resources configure controllers that make sure the right number of the right kind of pod\nare running, to match the state you specified.\nKubernetes provides several built-in workload resources:\nDeployment and ReplicaSet (replacing the legacy resource ReplicationController). Deployment is a good fit for managing a\nstateless application workload on your cluster, where any Pod in the Deployment is interchangeable and can be replaced if\nneeded.\nStatefulSet lets you run one or more related Pods that do track state somehow. For example, if your workload records data\npersistently, you can run a StatefulSet that matches each Pod with a PersistentVolume. Your code, running in the Pods for that\nStatefulSet, can replicate data to other Pods in the same StatefulSet to improve overall resilience.\nDaemonSet defines Pods that provide facilities that are local to nodes. Every time you add a node to your cluster that matches\nthe specification in a DaemonSet, the control plane schedules a Pod for that DaemonSet onto the new node. Each pod in a\nDaemonSet performs a job similar to a system daemon on a classic Unix / POSIX server. A DaemonSet might be fundamental to\nthe operation of your cluster, such as a plugin to run cluster networking, it might help you to manage the node, or it could\nprovide optional behavior that enhances the container platform you are running.\nJob and CronJob provide different ways to define tasks that run to completion and then stop. You can use a Job to define a task\nthat runs to completion, just once. You can use a CronJob to run the same Job multiple times according a schedule.\nIn the wider Kubernetes ecosystem, you can find third-party workload resources that provide additional behaviors. Using a custom\nresource definition, you can add in a third-party workload resource if you want a specific behavior that's not part of Kubernetes'\ncore. For example, if you wanted to run a group of Pods for your application but stop work unless all the Pods are available (perhaps\nfor some high-throughput distributed task), then you can implement or install an extension that does provide that feature."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0085", "text": "What's next\nAs well as reading about each API kind for workload management, you can read how to do specific tasks:\nRun a stateless application using a Deployment\nRun a stateful application either as a single instance or as a replicated set\nRun automated tasks with a CronJob\nTo learn about Kubernetes' mechanisms for separating code from configuration, visit Configuration.\nThere are two supporting concepts that provide backgrounds about how Kubernetes manages pods for applications:\nGarbage collection tidies up objects from your cluster after their owning resource has been removed.\nThe time-to-live after finished controller removes Jobs once a defined time has passed since they completed.\nOnce your application is running, you might want to make it available on the internet as a Service or, for web application only, using\nan Ingress.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n84/684\n\n11/7/25, 4:37 PM\n\n4.1 - Pods\n\nConcepts | Kubernetes\n\nPods are the smallest deployable units of computing that you can create and manage in Kubernetes.\nA Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a\nspecification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A\nPod models an application-specific \"logical host\": it contains one or more application containers which are relatively tightly coupled.\nIn non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed\non the same logical host.\nAs well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject\nephemeral containers for debugging a running Pod."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0086", "text": "What is a Pod?\nNote:\nYou need to install a container runtime into each node in the cluster so that Pods can run there.\nThe shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that\nisolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.\nA Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.\nPods in a Kubernetes cluster are used in two main ways:\nPods that run a single container. The \"one-container-per-Pod\" model is the most common Kubernetes use case; in this case,\nyou can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers\ndirectly.\nPods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple\nco-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive\nunit.\nGrouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this\npattern only in specific instances in which your containers are tightly coupled.\nYou don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see\nWorkload management.\n\nUsing Pods\nThe following is an example of a Pod which consists of a container running the image nginx:1.14.2 .\npods/simple-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nTo create the Pod shown above, run the following command:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n85/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml\n\nPods are generally not created directly and are created using workload resources. See Working with Pods for more information on\nhow Pods are used with workload resources."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0087", "text": "Workload resources for managing pods\nUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as\nDeployment or Job. If your Pods need to track state, consider the StatefulSet resource.\nEach Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more\noverall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically\nreferred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.\nSee Pods and controllers for more information on how Kubernetes uses workload resources, and their controllers, to implement\napplication scaling and auto-healing.\nPods natively provide two kinds of shared resources for their constituent containers: networking and storage.\n\nWorking with Pods\nYou'll rarely create individual Pods directly in Kubernetesâ€”even singleton Pods. This is because Pods are designed as relatively\nephemeral, disposable entities. When a Pod gets created (directly by you, or indirectly by a controller), the new Pod is scheduled to\nrun on a Node in your cluster. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is\nevicted for lack of resources, or the node fails.\nNote:\nRestarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for\nrunning container(s). A Pod persists until it is deleted.\nThe name of a Pod must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostname. For best\ncompatibility, the name should follow the more restrictive rules for a DNS label.\n\nPod OS\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0088", "text": "You should set the .spec.os.name field to either windows or linux to indicate the OS on which you want the pod to run. These\ntwo are the only operating systems supported for now by Kubernetes. In the future, this list may be expanded.\nIn Kubernetes v1.34, the value of .spec.os.name does not affect how the kube-scheduler picks a node for the Pod to run on. In any\ncluster where there is more than one operating system for running nodes, you should set the kubernetes.io/os label correctly on\neach node, and define pods with a nodeSelector based on the operating system label. The kube-scheduler assigns your pod to a\nnode based on other criteria and may or may not succeed in picking a suitable node placement where the node OS is right for the\ncontainers in that Pod. The Pod security standards also use this field to avoid enforcing policies that aren't relevant to the operating\nsystem.\n\nPods and controllers\nYou can use workload resources to create and manage multiple Pods for you. A controller for the resource handles replication and\nrollout and automatic healing in case of Pod failure. For example, if a Node fails, a controller notices that Pods on that Node have\nstopped working and creates a replacement Pod. The scheduler places the replacement Pod onto a healthy Node.\nHere are some examples of workload resources that manage one or more Pods:\nDeployment\nStatefulSet\nhttps://kubernetes.io/docs/concepts/_print/\n\n86/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDaemonSet\n\nPod templates\nControllers for workload resources create Pods from a pod template and manage those Pods on your behalf.\nPodTemplates are specifications for creating Pods, and are included in workload resources such as Deployments, Jobs, and\nDaemonSets.\nEach controller for a workload resource uses the PodTemplate inside the workload object to make actual Pods. The PodTemplate is\npart of the desired state of whatever workload resource you used to run your app.\nWhen you create a Pod, you can include environment variables in the Pod template for the containers that run in the Pod.\nThe sample below is a manifest for a simple Job with a template that starts one container. The container in that Pod prints a\nmessage then pauses."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0089", "text": "apiVersion: batch/v1\nkind: Job\nmetadata:\nname: hello\nspec:\ntemplate:\n# This is the pod template\nspec:\ncontainers:\n- name: hello\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo \"Hello, Kubernetes!\" && sleep 3600']\nrestartPolicy: OnFailure\n# The pod template ends here\n\nModifying the pod template or switching to a new pod template has no direct effect on the Pods that already exist. If you change the\npod template for a workload resource, that resource needs to create replacement Pods that use the updated template.\nFor example, the StatefulSet controller ensures that the running Pods match the current pod template for each StatefulSet object. If\nyou edit the StatefulSet to change its pod template, the StatefulSet starts to create new Pods based on the updated template.\nEventually, all of the old Pods are replaced with new Pods, and the update is complete.\nEach workload resource implements its own rules for handling changes to the Pod template. If you want to read more about\nStatefulSet specifically, read Update strategy in the StatefulSet Basics tutorial.\nOn Nodes, the kubelet does not directly observe or manage any of the details around pod templates and updates; those details are\nabstracted away. That abstraction and separation of concerns simplifies system semantics, and makes it feasible to extend the\ncluster's behavior without changing existing code.\n\nPod update and replacement\nAs mentioned in the previous section, when the Pod template for a workload resource is changed, the controller creates new Pods\nbased on the updated template instead of updating or patching the existing Pods.\nKubernetes doesn't prevent you from managing Pods directly. It is possible to update some fields of a running Pod, in place.\nHowever, Pod update operations like patch , and replace have some limitations:\nMost of the metadata about a Pod is immutable. For example, you cannot change the namespace , name , uid , or\ncreationTimestamp fields.\nIf the metadata.deletionTimestamp is set, no new entry can be added to the metadata.finalizers list.\nPod updates may not change fields other than spec.containers[*].image , spec.initContainers[*].image ,\nspec.activeDeadlineSeconds , spec.terminationGracePeriodSeconds , spec.tolerations or spec.schedulingGates . For\nspec.tolerations , you can only add new entries.\nhttps://kubernetes.io/docs/concepts/_print/\n\n87/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen updating the spec.activeDeadlineSeconds field, two types of updates are allowed:\n1. setting the unassigned field to a positive number;\n2. updating the field from a positive number to a smaller, non-negative number."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0090", "text": "Pod subresources\nThe above update rules apply to regular pod updates, but other pod fields can be updated through subresources.\nResize: The resize subresource allows container resources ( spec.containers[*].resources ) to be updated. See Resize\nContainer Resources for more details.\nEphemeral Containers: The ephemeralContainers subresource allows ephemeral containers to be added to a Pod. See\nEphemeral Containers for more details.\nStatus: The status subresource allows the pod status to be updated. This is typically only used by the Kubelet and other\nsystem controllers.\nBinding: The binding subresource allows setting the pod's spec.nodeName via a Binding request. This is typically only used\nby the scheduler.\n\nPod generation\nThe metadata.generation field is unique. It will be automatically set by the system such that new pods have a\nmetadata.generation of 1, and every update to mutable fields in the pod's spec will increment the metadata.generation by 1.\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)\n\nis a field that is captured in the status section of the Pod object. If the feature gate\nPodObservedGenerationTracking is set, the Kubelet will set status.observedGeneration to track the pod state to the current\npod status. The pod's status.observedGeneration will reflect the metadata.generation of the pod at the point that the pod\nstatus is being reported.\nobservedGeneration\n\nNote:\nThe status.observedGeneration field is managed by the kubelet and external controllers should not modify this field.\nDifferent status fields may either be associated with the metadata.generation of the current sync loop, or with the\nmetadata.generation of the previous sync loop. The key distinction is whether a change in the spec is reflected directly in the\nstatus or is an indirect result of a running process.\n\nDirect Status Updates\nFor status fields where the allocated spec is directly reflected, the observedGeneration will be associated with the current\nmetadata.generation (Generation N).\nThis behavior applies to:\nResize Status: The status of a resource resize operation.\nAllocated Resources: The resources allocated to the Pod after a resize.\nEphemeral Containers: When a new ephemeral container is added, and it is in Waiting state."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0091", "text": "Indirect Status Updates\nFor status fields that are an indirect result of running the spec, the observedGeneration will be associated with the\nmetadata.generation of the previous sync loop (Generation N-1).\nThis behavior applies to:\nContainer Image: The ContainerStatus.ImageID reflects the image from the previous generation until the new image is\npulled and the container is updated.\nActual Resources: During an in-progress resize, the actual resources in use still belong to the previous generation's request.\nContainer state: During an in-progress resize, with require restart policy reflects the previous generation's request.\nhttps://kubernetes.io/docs/concepts/_print/\n\n88/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nactiveDeadlineSeconds & terminationGracePeriodSeconds & deletionTimestamp: The effects of these fields on the Pod's\nstatus are a result of the previously observed specification.\n\nResource sharing and communication\nPods enable data sharing and communication among their constituent containers.\n\nStorage in Pods\nA Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those\ncontainers to share data. Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be\nrestarted. See Storage for more information on how Kubernetes implements shared storage and makes it available to Pods.\n\nPod networking\nEach Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including\nthe IP address and network ports. Inside a Pod (and only then), the containers that belong to the Pod can communicate with one\nanother using localhost . When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use\nthe shared network resources (such as ports). Within a Pod, containers share an IP address and port space, and can find each other\nvia localhost . The containers in a Pod can also communicate with each other using standard inter-process communications like\nSystemV semaphores or POSIX shared memory. Containers in different Pods have distinct IP addresses and can not communicate by\nOS-level IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP\nnetworking to communicate.\nContainers within the Pod see the system hostname as being the same as the configured name for the Pod. There's more about this\nin the networking section."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0092", "text": "Pod security settings\nTo set security constraints on Pods and containers, you use the securityContext field in the Pod specification. This field gives you\ngranular control over what a Pod or individual containers can do. For example:\nDrop specific Linux capabilities to avoid the impact of a CVE.\nForce all processes in the Pod to run as a non-root user or as a specific user or group ID.\nSet a specific seccomp profile.\nSet Windows security options, such as whether containers run as HostProcess.\nCaution:\nYou can also use the Pod securityContext to enable privileged mode in Linux containers. Privileged mode overrides many of the\nother security settings in the securityContext. Avoid using this setting unless you can't grant the equivalent permissions by\nusing other fields in the securityContext. In Kubernetes 1.26 and later, you can run Windows containers in a similarly privileged\nmode by setting the windowsOptions.hostProcess flag on the security context of the Pod spec. For details and instructions, see\nCreate a Windows HostProcess Pod.\nTo learn about kernel-level security constraints that you can use, see Linux kernel security constraints for Pods and containers.\nTo learn more about the Pod security context, see Configure a Security Context for a Pod or Container.\n\nStatic Pods\nStatic Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Whereas most\nPods are managed by the control plane (for example, a Deployment), for static Pods, the kubelet directly supervises each static Pod\n(and restarts it if it fails).\nStatic Pods are always bound to one Kubelet on a specific node. The main use for static Pods is to run a self-hosted control plane: in\nother words, using the kubelet to supervise the individual control plane components.\nhttps://kubernetes.io/docs/concepts/_print/\n\n89/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods\nrunning on a node are visible on the API server, but cannot be controlled from there. See the guide Create static Pods for more\ninformation.\nNote:\nThe spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount, ConfigMap, Secret, etc)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0093", "text": "Pods with multiple containers\nPods are designed to support multiple cooperating processes (as containers) that form a cohesive unit of service. The containers in a\nPod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share\nresources and dependencies, communicate with one another, and coordinate when and how they are terminated.\nPods in a Kubernetes cluster are used in two main ways:\nPods that run a single container. The \"one-container-per-Pod\" model is the most common Kubernetes use case; in this case,\nyou can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers\ndirectly.\nPods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple\nco-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive\nunit of serviceâ€”for example, one container serving data stored in a shared volume to the public, while a separate\nsidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral\nnetwork identity together as a single unit.\nFor example, you might have a container that acts as a web server for files in a shared volume, and a separate sidecar container that\nupdates those files from a remote source, as in the following diagram:\n\nSome Pods have init containers as well as app containers. By default, init containers run and complete before the app containers are\nstarted.\nYou can also have sidecar containers that provide auxiliary services to the main application Pod (for example: a service mesh).\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\n\nEnabled by default, the SidecarContainers feature gate allows you to specify restartPolicy: Always for init containers. Setting\nthe Always restart policy ensures that the containers where you set it are treated as sidecars that are kept running during the entire\nlifetime of the Pod. Containers that you explicitly define as sidecar containers start up before the main application Pod and remain\nrunning until the Pod is shut down.\n\nContainer probes\nA probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke\ndifferent actions:\nExecAction\n\n(performed with the help of the container runtime)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n90/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTCPSocketAction\nHTTPGetAction\n\n(checked directly by the kubelet)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0094", "text": "(checked directly by the kubelet)\n\nYou can read more about probes in the Pod Lifecycle documentation.\n\nWhat's next\nLearn about the lifecycle of a Pod.\nLearn about RuntimeClass and how you can use it to configure different Pods with different container runtime configurations.\nRead about PodDisruptionBudget and how you can use it to manage application availability during disruptions.\nPod is a top-level resource in the Kubernetes REST API. The Pod object definition describes the object in detail.\nThe Distributed System Toolkit: Patterns for Composite Containers explains common layouts for Pods with more than one\ncontainer.\nRead about Pod topology spread constraints\nTo understand the context for why Kubernetes wraps a common Pod API in other resources (such as StatefulSets or Deployments),\nyou can read about the prior art, including:\nAurora\nBorg\nMarathon\nOmega\nTupperware.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n91/684\n\n11/7/25, 4:37 PM\n\n4.1.1 - Pod Lifecycle\n\nConcepts | Kubernetes\n\nThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting in the Pending phase, moving through Running if\nat least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether\nany container in the Pod terminated in failure.\nLike individual application containers, Pods are considered to be relatively ephemeral (rather than durable) entities. Pods are\ncreated, assigned a unique ID (UID), and scheduled to run on nodes where they remain until termination (according to restart policy)\nor deletion. If a Node dies, the Pods running on (or scheduled to run on) that node are marked for deletion. The control plane marks\nthe Pods for removal after a timeout period."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0095", "text": "Pod lifetime\nWhilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults. Within a Pod, Kubernetes tracks\ndifferent container states and determines what action to take to make the Pod healthy again.\nIn the Kubernetes API, Pods have both a specification and an actual status. The status for a Pod object consists of a set of Pod\nconditions. You can also inject custom readiness information into the condition data for a Pod, if that is useful to your application.\nPods are only scheduled once in their lifetime; assigning a Pod to a specific node is called binding, and the process of selecting which\nnode to use is called scheduling. Once a Pod has been scheduled and is bound to a node, Kubernetes tries to run that Pod on the\nnode. The Pod runs on that node until it stops, or until the Pod is terminated; if Kubernetes isn't able to start the Pod on the selected\nnode (for example, if the node crashes before the Pod starts), then that particular Pod never starts.\nYou can use Pod Scheduling Readiness to delay scheduling for a Pod until all its scheduling gates are removed. For example, you\nmight want to define a set of Pods but only trigger scheduling once all the Pods have been created."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0096", "text": "Pods and fault recovery\nIf one of the containers in the Pod fails, then Kubernetes may try to restart that specific container. Read How Pods handle problems\nwith containers to learn more.\nPods can however fail in a way that the cluster cannot recover from, and in that case Kubernetes does not attempt to heal the Pod\nfurther; instead, Kubernetes deletes the Pod and relies on other components to provide automatic healing.\nIf a Pod is scheduled to a node and that node then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod.\nA Pod won't survive an eviction due to a lack of resources or Node maintenance.\nKubernetes uses a higher-level abstraction, called a controller, that handles the work of managing the relatively disposable Pod\ninstances.\nA given Pod (as defined by a UID) is never \"rescheduled\" to a different node; instead, that Pod can be replaced by a new, nearidentical Pod. If you make a replacement Pod, it can even have same name (as in .metadata.name ) that the old Pod had, but the\nreplacement would have a different .metadata.uid from the old Pod.\nKubernetes does not guarantee that a replacement for an existing Pod would be scheduled to the same node as the old Pod that\nwas being replaced.\n\nAssociated lifetimes\nWhen something is said to have the same lifetime as a Pod, such as a volume, that means that the thing exists as long as that\nspecific Pod (with that exact UID) exists. If that Pod is deleted for any reason, and even if an identical replacement is created, the\nrelated thing (a volume, in this example) is also destroyed and created anew.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n92/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFigure 1.\nA multi-container Pod that contains a file puller sidecar and a web server. The Pod uses an ephemeral emptyDir volume for shared\nstorage between the containers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0097", "text": "Pod phase\nA Pod's status field is a PodStatus object, which has a phase field.\nThe phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. The phase is not intended to be a\ncomprehensive rollup of observations of container or Pod state, nor is it intended to be a comprehensive state machine.\nThe number and meanings of Pod phase values are tightly guarded. Other than what is documented here, nothing should be\nassumed about Pods that have a given phase value.\nHere are the possible values for phase :\nValue\n\nDescription\n\nPending\n\nThe Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up\nand made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent\ndownloading container images over the network.\n\nRunning\n\nThe Pod has been bound to a node, and all of the containers have been created. At least one container is still\nrunning, or is in the process of starting or restarting.\n\nSucceeded\n\nAll containers in the Pod have terminated in success, and will not be restarted.\n\nFailed\n\nAll containers in the Pod have terminated, and at least one container has terminated in failure. That is, the\ncontainer either exited with non-zero status or was terminated by the system, and is not set for automatic\nrestarting.\n\nUnknown\n\nFor some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in\ncommunicating with the node where the Pod should be running.\n\nNote:\nWhen a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands.\nSimilarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.\nMake sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase . Pod phase is an explicit part of\nthe Kubernetes data model and of the Pod API.\nNAMESPACE\n\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nalessandras-namespace\n\nalessandras-pod\n\n0/1\n\nCrashLoopBackOff\n\n200\n\n2d9h\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n93/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a\nPod by force."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0098", "text": "Since Kubernetes 1.27, the kubelet transitions deleted Pods, except for static Pods and force-deleted Pods without a finalizer, to a\nterminal phase ( Failed or Succeeded depending on the exit statuses of the pod containers) before their deletion from the API\nserver.\nIf a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for setting the phase of all Pods on the lost\nnode to Failed.\n\nContainer states\nAs well as the phase of the Pod overall, Kubernetes tracks the state of each container inside a Pod. You can use container lifecycle\nhooks to trigger events to run at certain points in a container's lifecycle.\nOnce the scheduler assigns a Pod to a Node, the kubelet starts creating containers for that Pod using a container runtime. There are\nthree possible container states: Waiting , Running , and Terminated .\nTo check the state of a Pod's containers, you can use kubectl describe pod <name-of-pod> . The output shows the state for each\ncontainer within that Pod.\nEach state has a specific meaning:\n\nWaiting\nIf a container is not in either the Running or Terminated state, it is Waiting . A container in the Waiting state is still running the\noperations it requires in order to complete start up: for example, pulling the container image from a container image registry, or\napplying Secret data. When you use kubectl to query a Pod with a container that is Waiting , you also see a Reason field to\nsummarize why the container is in that state.\n\nRunning\nThe Running status indicates that a container is executing without issues. If there was a postStart hook configured, it has already\nexecuted and finished. When you use kubectl to query a Pod with a container that is Running , you also see information about\nwhen the container entered the Running state.\n\nTerminated\nA container in the Terminated state began execution and then either ran to completion or failed for some reason. When you use\nkubectl to query a Pod with a container that is Terminated , you see a reason, an exit code, and the start and finish time for that\ncontainer's period of execution.\nIf a container has a preStop hook configured, this hook runs before the container enters the Terminated state."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0099", "text": "How Pods handle problems with containers\nKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec . This policy determines how\nKubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:\n1. Initial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy .\n2. Repeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described\nin restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.\n3. CrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in\na crash loop, failing and restarting repeatedly.\n4. Backoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay,\ntreating any new crash as the first one.\nIn practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing\nor listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.\nhttps://kubernetes.io/docs/concepts/_print/\n\n94/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0100", "text": "In other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the\nContainer restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start\nattempts.\nThe CrashLoopBackOff can be caused by issues like the following:\nApplication errors that cause the container to exit.\nConfiguration errors, such as incorrect environment variables or missing configuration files.\nResource constraints, where the container might not have enough memory or CPU to start properly.\nHealth checks failing if the application doesn't start serving within the expected time.\nContainer liveness probes or startup probes returning a Failure result as mentioned in the probes section.\nTo investigate the root cause of a CrashLoopBackOff issue, a user can:\n1. Check logs: Use kubectl logs <name-of-pod> to check the logs of the container. This is often the most direct way to diagnose\nthe issue causing the crashes.\n2. Inspect events: Use kubectl describe pod <name-of-pod> to see events for the Pod, which can provide hints about\nconfiguration or resource issues.\n3. Review configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct\nand that all required external resources are available.\n4. Check resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the\nresources in the Pod definition can resolve the issue.\n5. Debug application: There might exist bugs or misconfigurations in the application code. Running this container image locally\nor in a development environment can help diagnose application specific issues.\n\nContainer restarts\nWhen a container in your Pod stops, or experiences failure, Kubernetes can restart it. A restart isn't always appropriate; for example,\ninit containers run only once, during Pod startup.\nYou can configure restarts as a policy that applies to all Pods, or using container-level configuration (for example: when you define a\nsidecar container).\n\nContainer restarts and resilience\nThe Kubernetes project recommends following cloud-native principles, including resilient design that accounts for unannounced or\narbitrary restarts. You can achieve this either by failing the Pod and relying on automatic replacement, or you can design for\ncontainer-level resilience. Either approach helps to ensure that your overall workload remains available despite partial failure."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0101", "text": "Pod-level container restart policy\nThe spec of a Pod has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always.\nThe restartPolicy for a Pod applies to app containers in the Pod and to regular init containers. Sidecar containers ignore the Podlevel restartPolicy field: in Kubernetes, a sidecar is defined as an entry inside initContainers that has its container-level\nrestartPolicy set to Always . For init containers that exit with an error, the kubelet restarts the init container if the Pod level\nrestartPolicy is either OnFailure or Always :\nAlways : Automatically restarts the container after any termination.\nOnFailure : Only restarts the container if it exits with an error (non-zero exit status).\nNever : Does not automatically restart the terminated container.\n\nWhen the kubelet is handling container restarts according to the configured restart policy, that only applies to restarts that make\nreplacement containers inside the same Pod and running on the same node. After containers in a Pod exit, the kubelet restarts them\nwith an exponential backoff delay (10s, 20s, 40s, â€¦), that is capped at 300 seconds (5 minutes). Once a container has executed for 10\nminutes without any problems, the kubelet resets the restart backoff timer for that container. Sidecar containers and Pod lifecycle\nexplains the behaviour of init containers when specify restartpolicy field on it.\n\nIndividual container restart policy and rules\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n95/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\nIf your cluster has the feature gate ContainerRestartRules enabled, you can specify restartPolicy and restartPolicyRules on\nindividual containers to override the Pod restart policy. Container restart policy and rules applies to app containers in the Pod and to\nregular init containers.\nA Kubernetes-native sidecar container has its container-level restartPolicy set to Always , and does not support\nrestartPolicyRules .\nThe container restarts will follow the same exponential backoff as pod restart policy described above. Supported container restart\npolicies:\nAlways : Automatically restarts the container after any termination.\nOnFailure : Only restarts the container if it exits with an error (non-zero exit status).\nNever : Does not automatically restart the terminated container."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0102", "text": "Additionally, individual containers can specify restartPolicyRules . If the restartPolicyRules field is specified, then container\nrestartPolicy must also be specified. The restartPolicyRules define a list of rules to apply on container exit. Each rule will\nconsist of a condition and an action. The supported condition is exitCodes , which compares the exit code of the container with a\nlist of given values. The supported action is Restart , which means the container will be restarted. The rules will be evaluated in\norder. On the first match, the action will be applied. If none of the rulesâ€™ conditions matched, Kubernetes fallback to containerâ€™s\nconfigured restartPolicy .\nFor example, a Pod with OnFailure restart policy that have a try-once container. This allows Pod to only restart certain containers:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: on-failure-pod\nspec:\nrestartPolicy: OnFailure\ncontainers:\n- name: try-once-container\n\n# This container will run only once because the restartPolicy is Never.\n\nimage: docker.io/library/busybox:1.28\ncommand: ['sh', '-c', 'echo \"Only running once\" && sleep 10 && exit 1']\nrestartPolicy: Never\n- name: on-failure-container\n\n# This container will be restarted on failure.\n\nimage: docker.io/library/busybox:1.28\ncommand: ['sh', '-c', 'echo \"Keep restarting\" && sleep 1800 && exit 1']\n\nA Pod with Always restart policy with an init container that only execute once. If the init container fails, the Pod fails. This allows the\nPod to fail if the initialization failed, but also keep running once the initialization succeeds:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: fail-pod-if-init-fails\nspec:\nrestartPolicy: Always\ninitContainers:\n- name: init-once\n\n# This init container will only try once. If it fails, the pod will fail.\n\nimage: docker.io/library/busybox:1.28\ncommand: ['sh', '-c', 'echo \"Failing initialization\" && sleep 10 && exit 1']\nrestartPolicy: Never\ncontainers:\n- name: main-container # This container will always be restarted once initialization succeeds.\nimage: docker.io/library/busybox:1.28\ncommand: ['sh', '-c', 'sleep 1800 && exit 0']\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n96/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA Pod with Never restart policy with a container that ignores and restarts on specific exit codes. This is useful to differentiate\nbetween restartable errors and non-restartable errors:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: restart-on-exit-codes\nspec:\nrestartPolicy: Never\ncontainers:\n- name: restart-on-exit-codes\nimage: docker.io/library/busybox:1.28\ncommand: ['sh', '-c', 'sleep 60 && exit 0']\nrestartPolicy: Never\n\n# Container restart policy must be specified if rules are specified\n\nrestartPolicyRules:\n\n# Only restart the container if it exits with code 42\n\n- action: Restart\nexitCodes:\noperator: In\nvalues: [42]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0103", "text": "Restart rules can be used for many more advanced lifecycle management scenarios. Note, restart rules are affected by the same\ninconsistencies as the regular restart policy. Kubelet restarts, container runtime garbage collection, intermitted connectivity issues\nwith the control plane may cause the state loss and containers may be re-run even when you expect a container not to be restarted.\n\nReduced container restart delay\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nWith the alpha feature gate ReduceDefaultCrashLoopBackOffDecay enabled, container start retries across your cluster will be\nreduced to begin at 1s (instead of 10s) and increase exponentially by 2x each restart until a maximum delay of 60s (instead of 300s\nwhich is 5 minutes).\nIf you use this feature along with the alpha feature KubeletCrashLoopBackOffMax (described below), individual nodes may have\ndifferent maximum delays.\n\nConfigurable container restart delay\nâ“˜ FEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)\n\nWith the alpha feature gate KubeletCrashLoopBackOffMax enabled, you can reconfigure the maximum delay between container start\nretries from the default of 300s (5 minutes). This configuration is set per node using kubelet configuration. In your kubelet\nconfiguration, under crashLoopBackOff set the maxContainerRestartPeriod field between \"1s\" and \"300s\" . As described above\nin Container restart policy, delays on that node will still start at 10s and increase exponentially by 2x each restart, but will now be\ncapped at your configured maximum. If the maxContainerRestartPeriod you configure is less than the default initial value of 10s,\nthe initial delay will instead be set to the configured maximum.\nSee the following kubelet configuration examples:\n\n# container restart delays will start at 10s, increasing\n# 2x each time they are restarted, to a maximum of 100s\nkind: KubeletConfiguration\ncrashLoopBackOff:\nmaxContainerRestartPeriod: \"100s\"\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n97/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n# delays between container restarts will always be 2s\nkind: KubeletConfiguration\ncrashLoopBackOff:\nmaxContainerRestartPeriod: \"2s\"\n\nIf you use this feature along with the alpha feature ReduceDefaultCrashLoopBackOffDecay (described above), your cluster defaults\nfor initial backoff and maximum backoff will no longer be 10s and 300s, but 1s and 60s. Per node configuration takes precedence\nover the defaults set by ReduceDefaultCrashLoopBackOffDecay , even if this would result in a node having a longer maximum backoff\nthan other nodes in the cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0104", "text": "Pod conditions\nA Pod has a PodStatus, which has an array of PodConditions through which the Pod has or has not passed. Kubelet manages the\nfollowing PodConditions:\nPodScheduled : the Pod has been scheduled to a node.\nPodReadyToStartContainers : (beta feature; enabled by default) the Pod sandbox has been successfully created and networking\n\nconfigured.\nContainersReady : all containers in the Pod are ready.\nInitialized : all init containers have completed successfully.\nReady : the Pod is able to serve requests and should be added to the load balancing pools of all matching Services.\nDisruptionTarget : the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).\nPodResizePending : a pod resize was requested but cannot be applied. See Pod resize status.\nPodResizeInProgress : the pod is in the process of resizing. See Pod resize status.\n\nField name\n\nDescription\n\ntype\n\nName of this Pod condition.\n\nstatus\n\nIndicates whether that condition is applicable, with possible values \" True \", \" False \", or\n\" Unknown \".\n\nlastProbeTime\n\nTimestamp of when the Pod condition was last probed.\n\nlastTransitionTime\n\nTimestamp for when the Pod last transitioned from one status to another.\n\nreason\n\nMachine-readable, UpperCamelCase text indicating the reason for the condition's last transition.\n\nmessage\n\nHuman-readable message indicating details about the last status transition.\n\nPod readiness\nâ“˜ FEATURE STATE: Kubernetes v1.14 [stable]\n\nYour application can inject extra feedback or signals into PodStatus: Pod readiness. To use this, set readinessGates in the Pod's\nspec to specify a list of additional conditions that the kubelet evaluates for Pod readiness.\nReadiness gates are determined by the current state of status.condition fields for the Pod. If Kubernetes cannot find such a\ncondition in the status.conditions field of a Pod, the status of the condition is defaulted to \" False \".\nHere is an example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n98/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkind: Pod\n...\nspec:\nreadinessGates:\n- conditionType: \"www.example.com/feature-1\"\nstatus:\nconditions:\n- type: Ready\n\n# a built-in PodCondition\n\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\n- type: \"www.example.com/feature-1\"\n\n# an extra PodCondition\n\nstatus: \"False\"\nlastProbeTime: null\nlastTransitionTime: 2018-01-01T00:00:00Z\ncontainerStatuses:\n- containerID: docker://abcd...\nready: true\n...\n\nThe Pod conditions you add must have names that meet the Kubernetes label key format."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0105", "text": "Status for Pod readiness\nThe kubectl patch command does not support patching object status. To set these status.conditions for the Pod, applications\nand operators should use the PATCH action. You can use a Kubernetes client library to write code that sets custom Pod conditions\nfor Pod readiness.\nFor a Pod that uses custom conditions, that Pod is evaluated to be ready only when both the following statements apply:\nAll containers in the Pod are ready.\nAll conditions specified in readinessGates are True .\nWhen a Pod's containers are Ready but at least one custom condition is missing or False , the kubelet sets the Pod's condition to\nContainersReady .\n\nPod network readiness\nâ“˜ FEATURE STATE: Kubernetes v1.29 [beta]\n\nNote:\nDuring its early development, this condition was named PodHasNetwork.\nAfter a Pod gets scheduled on a node, it needs to be admitted by the kubelet and to have any required storage volumes mounted.\nOnce these phases are complete, the kubelet works with a container runtime (using Container Runtime Interface (CRI)) to set up a\nruntime sandbox and configure networking for the Pod. If the PodReadyToStartContainersCondition feature gate is enabled (it is\nenabled by default for Kubernetes 1.34), the PodReadyToStartContainers condition will be added to the status.conditions field of\na Pod.\nThe PodReadyToStartContainers condition is set to False by the Kubelet when it detects a Pod does not have a runtime sandbox\nwith networking configured. This occurs in the following scenarios:\nEarly in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for the Pod using the container\nruntime.\nLater in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:\nthe node rebooting, without the Pod getting evicted\nfor container runtimes that use virtual machines for isolation, the Pod sandbox virtual machine rebooting, which then\nrequires creating a new sandbox and fresh container network configuration.\nhttps://kubernetes.io/docs/concepts/_print/\n\n99/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0106", "text": "The PodReadyToStartContainers condition is set to True by the kubelet after the successful completion of sandbox creation and\nnetwork configuration for the Pod by the runtime plugin. The kubelet can start pulling container images and create containers after\nPodReadyToStartContainers condition has been set to True .\nFor a Pod with init containers, the kubelet sets the Initialized condition to True after the init containers have successfully\ncompleted (which happens after successful sandbox creation and network configuration by the runtime plugin). For a Pod without\ninit containers, the kubelet sets the Initialized condition to True before sandbox creation and network configuration starts.\n\nContainer probes\nA probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet either executes\ncode within the container, or makes a network request.\n\nCheck mechanisms\nThere are four different ways to check a container using a probe. Each probe must define exactly one of these four mechanisms:\nexec\n\nExecutes a specified command inside the container. The diagnostic is considered successful if the command exits with a status\ncode of 0.\ngrpc\n\nPerforms a remote procedure call using gRPC. The target should implement gRPC health checks. The diagnostic is considered\nsuccessful if the status of the response is SERVING.\nhttpGet\n\nPerforms an HTTP GET request against the Pod's IP address on a specified port and path. The diagnostic is considered successful\nif the response has a status code greater than or equal to 200 and less than 400.\ntcpSocket\n\nPerforms a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open.\nIf the remote system (the container) closes the connection immediately after it opens, this counts as healthy.\nCaution:\nUnlike the other mechanisms, exec probe's implementation involves the creation/forking of multiple processes each time\nwhen executed. As a result, in case of the clusters having higher pod densities, lower intervals of initialDelaySeconds,\nperiodSeconds, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node. In\nsuch scenarios, consider using the alternative probe mechanisms to avoid the overhead.\n\nProbe outcome\nEach probe has one of three results:\nSuccess\n\nThe container passed the diagnostic.\nFailure\n\nThe container failed the diagnostic.\nUnknown\n\nThe diagnostic failed (no action should be taken, and the kubelet will make further checks)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0107", "text": "Types of probe\nThe kubelet can optionally perform and react to three kinds of probes on running containers:\nhttps://kubernetes.io/docs/concepts/_print/\n\n100/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nlivenessProbe\n\nIndicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is\nsubjected to its restart policy. If a container does not provide a liveness probe, the default state is Success.\nreadinessProbe\n\nIndicates whether the container is ready to respond to requests. If the readiness probe fails, the EndpointSlice controller removes\nthe Pod's IP address from the EndpointSlices of all Services that match the Pod. The default state of readiness before the initial\ndelay is Failure. If a container does not provide a readiness probe, the default state is Success.\nstartupProbe\n\nIndicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until\nit succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a\ncontainer does not provide a startup probe, the default state is Success.\nFor more information about how to set up a liveness, readiness, or startup probe, see Configure Liveness, Readiness and Startup\nProbes.\n\nWhen should you use a liveness probe?\nIf the process in your container is able to crash on its own whenever it encounters an issue or becomes unhealthy, you do not\nnecessarily need a liveness probe; the kubelet will automatically perform the correct action in accordance with the Pod's\nrestartPolicy .\nIf you'd like your container to be killed and restarted if a probe fails, then specify a liveness probe, and specify a restartPolicy of\nAlways or OnFailure."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0108", "text": "When should you use a readiness probe?\nIf you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe. In this case, the readiness probe\nmight be the same as the liveness probe, but the existence of the readiness probe in the spec means that the Pod will start without\nreceiving any traffic and only start receiving traffic after the probe starts succeeding.\nIf you want your container to be able to take itself down for maintenance, you can specify a readiness probe that checks an endpoint\nspecific to readiness that is different from the liveness probe.\nIf your app has a strict dependency on back-end services, you can implement both a liveness and a readiness probe. The liveness\nprobe passes when the app itself is healthy, but the readiness probe additionally checks that each required back-end service is\navailable. This helps you avoid directing traffic to Pods that can only respond with error messages.\nIf your container needs to work on loading large data, configuration files, or migrations during startup, you can use a startup probe.\nHowever, if you want to detect the difference between an app that has failed and an app that is still processing its startup data, you\nmight prefer a readiness probe.\nNote:\nIf you want to be able to drain requests when the Pod is deleted, you do not necessarily need a readiness probe; when the Pod\nis deleted, the corresponding endpoint in the EndpointSlice will update its conditions: the endpoint ready condition will be set\nto false, so load balancers will not use the Pod for regular traffic. See Pod termination for more information about how the\nkubelet handles Pod deletion."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0109", "text": "When should you use a startup probe?\nStartup probes are useful for Pods that have containers that take a long time to come into service. Rather than set a long liveness\ninterval, you can configure a separate configuration for probing the container as it starts up, allowing a time longer than the liveness\ninterval would allow.\nIf your container usually starts in more than \\( initialDelaySeconds + failureThreshold \\times periodSeconds \\), you should specify a\nstartup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its\nfailureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps\nto protect against deadlocks.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n101/684\n\n11/7/25, 4:37 PM\n\nTermination of Pods\n\nConcepts | Kubernetes\n\nBecause Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate\nwhen they are no longer needed (rather than being abruptly stopped with a KILL signal and having no chance to clean up).\nThe design aim is for you to be able to request deletion and know when processes terminate, but also be able to ensure that deletes\neventually complete. When you request deletion of a Pod, the cluster records and tracks the intended grace period before the Pod is\nallowed to be forcefully killed. With that forceful shutdown tracking in place, the kubelet attempts graceful shutdown.\nTypically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the\ncontainers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each\ncontainer. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the\norder of processing for these requests. Many container runtimes respect the STOPSIGNAL value defined in the container image and,\nif different, send the container image configured STOPSIGNAL instead of TERM. Once the grace period has expired, the KILL signal is\nsent to any remaining processes, and the Pod is then deleted from the API Server. If the kubelet or the container runtime's\nmanagement service is restarted while waiting for processes to terminate, the cluster retries from the start including the full original\ngrace period."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0110", "text": "Stop Signals\nThe stop signal used to kill the container can be defined in the container image with the STOPSIGNAL instruction. If no stop signal is\ndefined in the image, the default signal of the container runtime (SIGTERM for both containerd and CRI-O) would be used to kill the\ncontainer.\n\nDefining custom stop signals\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nIf the ContainerStopSignals feature gate is enabled, you can configure a custom stop signal for your containers from the container\nLifecycle. We require the Pod's spec.os.name field to be present as a requirement for defining stop signals in the container lifecycle.\nThe list of signals that are valid depends on the OS the Pod is scheduled to. For Pods scheduled to Windows nodes, we only support\nSIGTERM and SIGKILL as valid signals.\nHere is an example Pod spec defining a custom stop signal:\n\nspec:\nos:\nname: linux\ncontainers:\n- name: my-container\nimage: container-image:latest\nlifecycle:\nstopSignal: SIGUSR1\n\nIf a stop signal is defined in the lifecycle, this will override the signal defined in the container image. If no stop signal is defined in the\ncontainer spec, the container would fall back to the default behavior.\n\nPod Termination Flow\nPod termination flow, illustrated with an example:\n1. You use the kubectl tool to manually delete a specific Pod, with the default grace period (30 seconds).\n2. The Pod in the API server is updated with the time beyond which the Pod is considered \"dead\" along with the grace period. If\nyou use kubectl describe to check the Pod you're deleting, that Pod shows up as \"Terminating\". On the node where the Pod\nis running: as soon as the kubelet sees that a Pod has been marked as terminating (a graceful shutdown duration has been\nset), the kubelet begins the local Pod shutdown process.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n102/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0111", "text": "1. If one of the Pod's containers has defined a preStop hook and the terminationGracePeriodSeconds in the Pod spec is\nnot set to 0, the kubelet runs that hook inside of the container. The default terminationGracePeriodSeconds setting is 30\nseconds.\nIf the preStop hook is still running after the grace period expires, the kubelet requests a small, one-off grace period\nextension of 2 seconds.\nNote:\nIf the preStop hook needs longer to complete than the default grace period allows, you must modify\nterminationGracePeriodSeconds to suit this.\n2. The kubelet triggers the container runtime to send a TERM signal to process 1 inside each container.\nThere is special ordering if the Pod has any sidecar containers defined. Otherwise, the containers in the Pod receive the\nTERM signal at different times and in an arbitrary order. If the order of shutdowns matters, consider using a preStop\nhook to synchronize (or switch to using sidecar containers).\n3. At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane evaluates whether to remove that\nshutting-down Pod from EndpointSlice objects, where those objects represent a Service with a configured selector. ReplicaSets\nand other workload resources no longer treat the shutting-down Pod as a valid, in-service replica.\nPods that shut down slowly should not continue to serve regular traffic and should start terminating and finish processing\nopen connections. Some applications need to go beyond finishing open connections and need more graceful termination, for\nexample, session draining and completion.\nAny endpoints that represent the terminating Pods are not immediately removed from EndpointSlices, and a status indicating\nterminating state is exposed from the EndpointSlice API. Terminating endpoints always have their ready status as false (for\nbackward compatibility with versions before 1.26), so load balancers will not use it for regular traffic.\nIf traffic draining on terminating Pod is needed, the actual readiness can be checked as a condition serving . You can find\nmore details on how to implement connections draining in the tutorial Pods And Endpoints Termination Flow\n4. The kubelet ensures the Pod is shut down and terminated\n1. When the grace period expires, if there is still any container running in the Pod, the kubelet triggers forcible shutdown.\nThe container runtime sends SIGKILL to any processes still running in any container in the Pod. The kubelet also cleans up\na hidden pause container if that container runtime uses one.\n2. The kubelet transitions the Pod into a terminal phase (Failed or Succeeded depending on the end state of its containers).\n3. The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period to 0 (immediate\ndeletion).\n4. The API server deletes the Pod's API object, which is then no longer visible from any client."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0112", "text": "Forced Pod termination\nCaution:\nForced deletions can be potentially disruptive for some workloads and their Pods.\nBy default, all deletes are graceful within 30 seconds. The kubectl delete command supports the --grace-period=<seconds>\noption which allows you to override the default and specify your own value.\nSetting the grace period to 0 forcibly and immediately deletes the Pod from the API server. If the Pod was still running on a node,\nthat forcible deletion triggers the kubelet to begin immediate cleanup.\nUsing kubectl, You must specify an additional flag --force along with --grace-period=0 in order to perform force deletions.\nWhen a force deletion is performed, the API server does not wait for confirmation from the kubelet that the Pod has been\nterminated on the node it was running on. It removes the Pod in the API immediately so a new Pod can be created with the same\nname. On the node, Pods that are set to terminate immediately will still be given a small grace period before being force killed.\nCaution:\nhttps://kubernetes.io/docs/concepts/_print/\n\n103/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nImmediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue\nto run on the cluster indefinitely.\nIf you need to force-delete Pods that are part of a StatefulSet, refer to the task documentation for deleting Pods from a StatefulSet."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0113", "text": "Pod shutdown and sidecar containers\nIf your Pod includes one or more sidecar containers (init containers with an Always restart policy), the kubelet will delay sending the\nTERM signal to these sidecar containers until the last main container has fully terminated. The sidecar containers will be terminated\nin the reverse order they are defined in the Pod spec. This ensures that sidecar containers continue serving the other containers in\nthe Pod until they are no longer needed.\nThis means that slow termination of a main container will also delay the termination of the sidecar containers. If the grace period\nexpires before the termination process is complete, the Pod may enter forced termination. In this case, all remaining containers in\nthe Pod will be terminated simultaneously with a short grace period.\nSimilarly, if the Pod has a preStop hook that exceeds the termination grace period, emergency termination may occur. In general, if\nyou have used preStop hooks to control the termination order without sidecar containers, you can now remove them and allow the\nkubelet to manage sidecar termination automatically.\n\nGarbage collection of Pods\nFor failed Pods, the API objects remain in the cluster's API until a human or controller process explicitly removes them.\nThe Pod garbage collector (PodGC), which is a controller in the control plane, cleans up terminated Pods (with a phase of Succeeded\nor Failed ), when the number of Pods exceeds the configured threshold (determined by terminated-pod-gc-threshold in the kubecontroller-manager). This avoids a resource leak as Pods are created and terminated over time.\nAdditionally, PodGC cleans up any Pods which satisfy any of the following conditions:\n1. are orphan Pods - bound to a node which no longer exists,\n2. are unscheduled terminating Pods,\n3. are terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-service.\nAlong with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal phase. Also, PodGC adds a Pod\ndisruption condition when cleaning up an orphan Pod. See Pod disruption conditions for more details.\n\nWhat's next\nGet hands-on experience attaching handlers to container lifecycle events.\nGet hands-on experience configuring Liveness, Readiness and Startup Probes.\nLearn more about container lifecycle hooks.\nLearn more about sidecar containers.\nFor detailed information about Pod and container status in the API, see the API reference documentation covering status for\nPod.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n104/684\n\n11/7/25, 4:37 PM\n\n4.1.2 - Init Containers"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0114", "text": "Concepts | Kubernetes\n\nThis page provides an overview of init containers: specialized containers that run before app containers in a Pod. Init containers can\ncontain utilities or setup scripts not present in an app image.\nYou can specify init containers in the Pod specification alongside the containers array (which describes app containers).\nIn Kubernetes, a sidecar container is a container that starts before the main application container and continues to run. This\ndocument is about init containers: containers that run to completion during Pod initialization.\n\nUnderstanding init containers\nA Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the\napp containers are started.\nInit containers are exactly like regular containers, except:\nInit containers always run to completion.\nEach init container must complete successfully before the next one starts.\nIf a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a\nrestartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.\nTo specify an init container for a Pod, add the initContainers field into the Pod specification, as an array of container items\n(similar to the app containers field and its contents). See Container in the API reference for more details.\nThe status of the init containers is returned in .status.initContainerStatuses field as an array of the container statuses (similar to\nthe .status.containerStatuses field).\n\nDifferences from regular containers\nInit containers support all the fields and features of app containers, including resource limits, volumes, and security settings.\nHowever, the resource requests and limits for an init container are handled differently, as documented in Resource sharing within\ncontainers.\nRegular init containers (in other words: excluding sidecar containers) do not support the lifecycle , livenessProbe ,\nreadinessProbe , or startupProbe fields. Init containers must run to completion before the Pod can be ready; sidecar containers\ncontinue running during a Pod's lifetime, and do support some probes. See sidecar container for further details about sidecar\ncontainers.\nIf you specify multiple init containers for a Pod, kubelet runs each init container sequentially. Each init container must succeed\nbefore the next can run. When all of the init containers have run to completion, kubelet initializes the application containers for the\nPod and runs them as usual."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0115", "text": "Differences from sidecar containers\nInit containers run and complete their tasks before the main application container starts. Unlike sidecar containers, init containers\nare not continuously running alongside the main containers.\nInit containers run to completion sequentially, and the main container does not start until all the init containers have successfully\ncompleted.\ninit containers do not support lifecycle , livenessProbe , readinessProbe , or startupProbe whereas sidecar containers support\nall these probes to control their lifecycle.\nInit containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly\nwith them. They can, however, use shared volumes for data exchange.\n\nUsing init containers\nBecause init containers have separate images from app containers, they have some advantages for start-up related code:\nhttps://kubernetes.io/docs/concepts/_print/\n\n105/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nInit containers can contain utilities or custom code for setup that are not present in an app image. For example, there is no\nneed to make an image FROM another image just to use a tool like sed , awk , python , or dig during setup.\nThe application image builder and deployer roles can work independently without the need to jointly build a single app image.\nInit containers can run with a different view of the filesystem than app containers in the same Pod. Consequently, they can be\ngiven access to Secrets that app containers cannot access.\nBecause init containers run to completion before any app containers start, init containers offer a mechanism to block or delay\napp container startup until a set of preconditions are met. Once preconditions are met, all of the app containers in a Pod can\nstart in parallel.\nInit containers can securely run utilities or custom code that would otherwise make an app container image less secure. By\nkeeping unnecessary tools separate you can limit the attack surface of your app container image.\n\nExamples\nHere are some ideas for how to use init containers:\nWait for a Service to be created, using a shell one-line command like:\n\nfor i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1\n\nRegister this Pod with a remote server from the downward API with a command like:\n\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$(<POD_NAME>)&ip=$(\n\nWait for some time before starting the app container with a command like\n\nsleep 60"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0116", "text": "Clone a Git repository into a Volume\nPlace values into a configuration file and run a template tool to dynamically generate a configuration file for the main app\ncontainer. For example, place the POD_IP value in a configuration and generate the main app configuration file using Jinja.\n\nInit containers in use\nThis example defines a simple Pod that has two init containers. The first waits for myservice , and the second waits for mydb . Once\nboth init containers complete, the Pod runs the app container from its spec section.\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp-pod\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\ncontainers:\n- name: myapp-container\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo The app is running! && sleep 3600']\ninitContainers:\n- name: init-myservice\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).s\n- name: init-mydb\nimage: busybox:1.28\ncommand: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.c\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n106/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can start this Pod by running:\n\nkubectl apply -f myapp.yaml\n\nThe output is similar to this:\npod/myapp-pod created\n\nAnd check on its status with:\n\nkubectl get -f myapp.yaml\n\nThe output is similar to this:\nNAME\nmyapp-pod\n\nREADY\n0/1\n\nSTATUS\nInit:0/2\n\nRESTARTS\n0\n\nAGE\n6m\n\nor for more details:\n\nkubectl describe -f myapp.yaml\n\nThe output is similar to this:\nName:\nmyapp-pod\nNamespace:\ndefault\n[...]\nLabels:\napp.kubernetes.io/name=MyApp\nStatus:\nPending\n[...]\nInit Containers:\ninit-myservice:\n[...]\nState:\nRunning\n[...]\ninit-mydb:\n[...]\nState:\nWaiting\nReason:\nPodInitializing\nReady:\nFalse\n[...]\nContainers:\nmyapp-container:\n[...]\nState:\nWaiting\nReason:\nPodInitializing\nReady:\nFalse\n[...]\nEvents:\nFirstSeen\nLastSeen\nCount\nFrom\n----------------------16s\n16s\n1\n{default-scheduler }\n16s\n16s\n1\n{kubelet 172.17.4.201}\n13s\n13s\n1\n{kubelet 172.17.4.201}\n13s\n13s\n1\n{kubelet 172.17.4.201}\n13s\n13s\n1\n{kubelet 172.17.4.201}\n\nSubObjectPath\n------------spec.initContainers{init-myservice}\nspec.initContainers{init-myservice}\nspec.initContainers{init-myservice}\nspec.initContainers{init-myservice}\n\nType\n-------Normal\nNormal\nNormal\nNormal\nNormal\n\nTo see logs for the init containers in this Pod, run:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n107/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl logs myapp-pod -c init-myservice # Inspect the first init container\nkubectl logs myapp-pod -c init-mydb\n# Inspect the second init container\n\nAt this point, those init containers will be waiting to discover Services named mydb and myservice .\nHere's a configuration you can use to make those Services appear:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0117", "text": "--apiVersion: v1\nkind: Service\nmetadata:\nname: myservice\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\n--apiVersion: v1\nkind: Service\nmetadata:\nname: mydb\nspec:\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9377\n\nTo create the mydb and myservice services:\n\nkubectl apply -f services.yaml\n\nThe output is similar to this:\nservice/myservice created\nservice/mydb created\n\nYou'll then see that those init containers complete, and that the myapp-pod Pod moves into the Running state:\n\nkubectl get -f myapp.yaml\n\nThe output is similar to this:\nNAME\nmyapp-pod\n\nREADY\n1/1\n\nSTATUS\nRunning\n\nRESTARTS\n0\n\nAGE\n9m\n\nThis simple example should provide some inspiration for you to create your own init containers. What's next contains a link to a\nmore detailed example.\n\nDetailed behavior\nDuring Pod startup, the kubelet delays running init containers until the networking and storage are ready. Then the kubelet runs the\nPod's init containers in the order they appear in the Pod's spec.\nhttps://kubernetes.io/docs/concepts/_print/\n\n108/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0118", "text": "Each init container must exit successfully before the next container starts. If a container fails to start due to the runtime or exits with\nfailure, it is retried according to the Pod restartPolicy . However, if the Pod restartPolicy is set to Always, the init containers use\nrestartPolicy OnFailure.\nA Pod cannot be Ready until all init containers have succeeded. The ports on an init container are not aggregated under a Service. A\nPod that is initializing is in the Pending state but should have a condition Initialized set to false.\nIf the Pod restarts, or is restarted, all init containers must execute again.\nChanges to the init container spec are limited to the container image field. Directly altering the image field of an init container does\nnot restart the Pod or trigger its recreation. If the Pod has yet to start, that change may have an effect on how the Pod boots up.\nFor a pod template you can typically change any field for an init container; the impact of making that change depends on where the\npod template is used.\nBecause init containers can be restarted, retried, or re-executed, init container code should be idempotent. In particular, code that\nwrites into any emptyDir volume should be prepared for the possibility that an output file already exists.\nInit containers have all of the fields of an app container. However, Kubernetes prohibits readinessProbe from being used because\ninit containers cannot define readiness distinct from completion. This is enforced during validation.\nUse activeDeadlineSeconds on the Pod to prevent init containers from failing forever. The active deadline includes init containers.\nHowever it is recommended to use activeDeadlineSeconds only if teams deploy their application as a Job, because\nactiveDeadlineSeconds has an effect even after initContainer finished. The Pod which is already running correctly would be killed\nby activeDeadlineSeconds if you set.\nThe name of each app and init container in a Pod must be unique; a validation error is thrown for any container sharing a name with\nanother."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0119", "text": "Resource sharing within containers\nGiven the order of execution for init, sidecar and app containers, the following rules for resource usage apply:\nThe highest of any particular resource request or limit defined on all init containers is the effective init request/limit. If any\nresource has no resource limit specified this is considered as the highest limit.\nThe Pod's effective request/limit for a resource is the higher of:\nthe sum of all app containers request/limit for a resource\nthe effective init request/limit for a resource\nScheduling is done based on effective requests/limits, which means init containers can reserve resources for initialization that\nare not used during the life of the Pod.\nThe QoS (quality of service) tier of the Pod's effective QoS tier is the QoS tier for init containers and app containers alike.\nQuota and limits are applied based on the effective Pod request and limit.\n\nInit containers and Linux cgroups\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as\nthe scheduler.\n\nPod restart reasons\nA Pod can restart, causing re-execution of init containers, for the following reasons:\nThe Pod infrastructure container is restarted. This is uncommon and would have to be done by someone with root access to\nnodes.\nAll containers in a Pod are terminated while restartPolicy is set to Always, forcing a restart, and the init container completion\nrecord has been lost due to garbage collection.\nThe Pod will not be restarted when the init container image is changed, or the init container completion record has been lost due to\ngarbage collection. This applies for Kubernetes v1.20 and later. If you are using an earlier version of Kubernetes, consult the\ndocumentation for the version you are using.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n109/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nLearn more about the following:\nCreating a Pod that has an init container.\nDebug init containers.\nOverview of kubelet and kubectl.\nTypes of probes: liveness, readiness, startup probe.\nSidecar containers.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n110/684\n\n11/7/25, 4:37 PM\n\n4.1.3 - Sidecar Containers\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0120", "text": "Sidecar containers are the secondary containers that run along with the main application container within the same Pod. These\ncontainers are used to enhance or to extend the functionality of the primary app container by providing additional services, or\nfunctionality such as logging, monitoring, security, or data synchronization, without directly altering the primary application code.\nTypically, you only have one app container in a Pod. For example, if you have a web application that requires a local webserver, the\nlocal webserver is a sidecar and the web application itself is the app container.\n\nSidecar containers in Kubernetes\nKubernetes implements sidecar containers as a special case of init containers; sidecar containers remain running after Pod startup.\nThis document uses the term regular init containers to clearly refer to containers that only run during Pod startup.\nProvided that your cluster has the SidecarContainers feature gate enabled (the feature is active by default since Kubernetes v1.29),\nyou can specify a restartPolicy for containers listed in a Pod's initContainers field. These restartable sidecar containers are\nindependent from other init containers and from the main application container(s) within the same pod. These can be started,\nstopped, or restarted without affecting the main application container and other init containers.\nYou can also run a Pod with multiple containers that are not marked as init or sidecar containers. This is appropriate if the\ncontainers within the Pod are required for the Pod to work overall, but you don't need to control which containers start or stop first.\nYou could also do this if you need to support older versions of Kubernetes that don't support a container-level restartPolicy field.\n\nExample application\nHere's an example of a Deployment with two containers, one of which is a sidecar:\napplication/deployment-sidecar.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n111/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: myapp\nlabels:\napp: myapp\nspec:\nreplicas: 1\nselector:\nmatchLabels:\napp: myapp\ntemplate:\nmetadata:\nlabels:\napp: myapp\nspec:\ncontainers:\n- name: myapp\nimage: alpine:latest\ncommand: ['sh', '-c', 'while true; do echo \"logging\" >> /opt/logs.txt; sleep 1; done']\nvolumeMounts:\n- name: data\nmountPath: /opt\ninitContainers:\n- name: logshipper\nimage: alpine:latest\nrestartPolicy: Always\ncommand: ['sh', '-c', 'tail -F /opt/logs.txt']\nvolumeMounts:\n- name: data\nmountPath: /opt\nvolumes:\n- name: data\nemptyDir: {}"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0121", "text": "Sidecar containers and Pod lifecycle\nIf an init container is created with its restartPolicy set to Always , it will start and remain running during the entire life of the Pod.\nThis can be helpful for running supporting services separated from the main application containers.\nIf a readinessProbe is specified for this init container, its result will be used to determine the ready state of the Pod.\nSince these containers are defined as init containers, they benefit from the same ordering and sequential guarantees as regular init\ncontainers, allowing you to mix sidecar containers with regular init containers for complex Pod initialization flows.\nCompared to regular init containers, sidecars defined within initContainers continue to run after they have started. This is\nimportant when there is more than one entry inside .spec.initContainers for a Pod. After a sidecar-style init container is running\n(the kubelet has set the started status for that init container to true), the kubelet then starts the next init container from the\nordered .spec.initContainers list. That status either becomes true because there is a process running in the container and no\nstartup probe defined, or as a result of its startupProbe succeeding.\nUpon Pod termination, the kubelet postpones terminating sidecar containers until the main application container has fully stopped.\nThe sidecar containers are then shut down in the opposite order of their appearance in the Pod specification. This approach ensures\nthat the sidecars remain operational, supporting other containers within the Pod, until their service is no longer required.\n\nJobs with sidecar containers\nIf you define a Job that uses sidecar using Kubernetes-style init containers, the sidecar container in each Pod does not prevent the\nJob from completing after the main container has finished.\nHere's an example of a Job with two containers, one of which is a sidecar:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n112/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napplication/job/job-sidecar.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: myjob\nspec:\ntemplate:\nspec:\ncontainers:\n- name: myjob\nimage: alpine:latest\ncommand: ['sh', '-c', 'echo \"logging\" > /opt/logs.txt']\nvolumeMounts:\n- name: data\nmountPath: /opt\ninitContainers:\n- name: logshipper\nimage: alpine:latest\nrestartPolicy: Always\ncommand: ['sh', '-c', 'tail -F /opt/logs.txt']\nvolumeMounts:\n- name: data\nmountPath: /opt\nrestartPolicy: Never\nvolumes:\n- name: data\nemptyDir: {}"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0122", "text": "Differences from application containers\nSidecar containers run alongside app containers in the same pod. However, they do not execute the primary application logic;\ninstead, they provide supporting functionality to the main application.\nSidecar containers have their own independent lifecycles. They can be started, stopped, and restarted independently of app\ncontainers. This means you can update, scale, or maintain sidecar containers without affecting the primary application.\nSidecar containers share the same network and storage namespaces with the primary container. This co-location allows them to\ninteract closely and share resources.\nFrom a Kubernetes perspective, the sidecar container's graceful termination is less important. When other containers take all\nallotted graceful termination time, the sidecar containers will receive the SIGTERM signal, followed by the SIGKILL signal, before\nthey have time to terminate gracefully. So exit codes different from 0 ( 0 indicates successful exit), for sidecar containers are\nnormal on Pod termination and should be generally ignored by the external tooling.\n\nDifferences from init containers\nSidecar containers work alongside the main container, extending its functionality and providing additional services.\nSidecar containers run concurrently with the main application container. They are active throughout the lifecycle of the pod and can\nbe started and stopped independently of the main container. Unlike init containers, sidecar containers support probes to control\ntheir lifecycle.\nSidecar containers can interact directly with the main application containers, because like init containers they always share the same\nnetwork, and can optionally also share volumes (filesystems).\nInit containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a\nPod. Any data passing is one-way (for example, an init container can put information inside an emptyDir volume).\nChanging the image of a sidecar container will not cause the Pod to restart, but will trigger a container restart.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n113/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nResource sharing within containers"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0123", "text": "Given the order of execution for init, sidecar and app containers, the following rules for resource usage apply:\nThe highest of any particular resource request or limit defined on all init containers is the effective init request/limit. If any\nresource has no resource limit specified this is considered as the highest limit.\nThe Pod's effective request/limit for a resource is the sum of pod overhead and the higher of:\nthe sum of all non-init containers(app and sidecar containers) request/limit for a resource\nthe effective init request/limit for a resource\nScheduling is done based on effective requests/limits, which means init containers can reserve resources for initialization that\nare not used during the life of the Pod.\nThe QoS (quality of service) tier of the Pod's effective QoS tier is the QoS tier for all init, sidecar and app containers alike.\nQuota and limits are applied based on the effective Pod request and limit.\n\nSidecar containers and Linux cgroups\nOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as\nthe scheduler.\n\nWhat's next\nLearn how to Adopt Sidecar Containers\nRead a blog post on native sidecar containers.\nRead about creating a Pod that has an init container.\nLearn about the types of probes: liveness, readiness, startup probe.\nLearn about pod overhead.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n114/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.1.4 - Ephemeral Containers\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nThis page provides an overview of ephemeral containers: a special type of container that runs temporarily in an existing Pod to\naccomplish user-initiated actions such as troubleshooting. You use ephemeral containers to inspect services rather than to build\napplications.\n\nUnderstanding ephemeral containers\nPods are the fundamental building block of Kubernetes applications. Since Pods are intended to be disposable and replaceable, you\ncannot add a container to a Pod once it has been created. Instead, you usually delete and replace Pods in a controlled fashion using\ndeployments.\nSometimes it's necessary to inspect the state of an existing Pod, however, for example to troubleshoot a hard-to-reproduce bug. In\nthese cases you can run an ephemeral container in an existing Pod to inspect its state and run arbitrary commands."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0124", "text": "What is an ephemeral container?\nEphemeral containers differ from other containers in that they lack guarantees for resources or execution, and they will never be\nautomatically restarted, so they are not appropriate for building applications. Ephemeral containers are described using the same\nContainerSpec as regular containers, but many fields are incompatible and disallowed for ephemeral containers.\nEphemeral containers may not have ports, so fields such as ports , livenessProbe , readinessProbe are disallowed.\nPod resource allocations are immutable, so setting resources is disallowed.\nFor a complete list of allowed fields, see the EphemeralContainer reference documentation.\nEphemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to\npod.spec , so it's not possible to add an ephemeral container using kubectl edit .\nLike regular containers, you may not change or remove an ephemeral container after you have added it to a Pod.\nNote:\nEphemeral containers are not supported by static pods.\n\nUses for ephemeral containers\nEphemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed\nor a container image doesn't include debugging utilities.\nIn particular, distroless images enable you to deploy minimal container images that reduce attack surface and exposure to bugs and\nvulnerabilities. Since distroless images do not include a shell or any debugging utilities, it's difficult to troubleshoot distroless images\nusing kubectl exec alone.\nWhen using ephemeral containers, it's helpful to enable process namespace sharing so you can view processes in other containers.\n\nWhat's next\nLearn how to debug pods using ephemeral containers.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n115/684\n\n11/7/25, 4:37 PM\n\n4.1.5 - Disruptions\n\nConcepts | Kubernetes\n\nThis guide is for application owners who want to build highly available applications, and thus need to understand what types of\ndisruptions can happen to Pods.\nIt is also for cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0125", "text": "Voluntary and involuntary disruptions\nPods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system\nsoftware error.\nWe call these unavoidable cases involuntary disruptions to an application. Examples are:\na hardware failure of the physical machine backing the node\ncluster administrator deletes VM (instance) by mistake\ncloud provider or hypervisor failure makes VM disappear\na kernel panic\nthe node disappears from the cluster due to cluster network partition\neviction of a pod due to the node being out-of-resources.\nExcept for the out-of-resources condition, all these conditions should be familiar to most users; they are not specific to Kubernetes.\nWe call other cases voluntary disruptions. These include both actions initiated by the application owner and those initiated by a\nCluster Administrator. Typical application owner actions include:\ndeleting the deployment or other controller that manages the pod\nupdating a deployment's pod template causing a restart\ndirectly deleting a pod (e.g. by accident)\nCluster administrator actions include:\nDraining a node for repair or upgrade.\nDraining a node from a cluster to scale the cluster down (learn about Node Autoscaling).\nRemoving a pod from a node to permit something else to fit on that node.\nThese actions might be taken directly by the cluster administrator, or by automation run by the cluster administrator, or by your\ncluster hosting provider.\nAsk your cluster administrator or consult your cloud provider or distribution documentation to determine if any sources of voluntary\ndisruptions are enabled for your cluster. If none are enabled, you can skip creating Pod Disruption Budgets.\nCaution:\nNot all voluntary disruptions are constrained by Pod Disruption Budgets. For example, deleting deployments or pods bypasses\nPod Disruption Budgets.\n\nDealing with disruptions\nHere are some ways to mitigate involuntary disruptions:\nEnsure your pod requests the resources it needs.\nReplicate your application if you need higher availability. (Learn about running replicated stateless and stateful applications.)\nFor even higher availability when running replicated applications, spread applications across racks (using anti-affinity) or across\nzones (if using a multi-zone cluster.)\nThe frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are no automated voluntary disruptions (only\nuser-triggered ones). However, your cluster administrator or hosting provider may run some additional services which cause\nvoluntary disruptions. For example, rolling out node software updates can cause voluntary disruptions. Also, some implementations\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n116/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0126", "text": "of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes. Your cluster administrator or\nhosting provider should have documented what level of voluntary disruptions, if any, to expect. Certain configuration options, such\nas using PriorityClasses in your pod spec can also cause voluntary (and involuntary) disruptions.\n\nPod disruption budgets\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]\n\nKubernetes offers features to help you run highly available applications even when you introduce frequent voluntary disruptions.\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a\nreplicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would\nlike to ensure that the number of replicas running is never brought below the number needed for a quorum. A web front end might\nwant to ensure that the number of replicas serving load never falls below a certain percentage of the total.\nCluster managers and hosting providers should use tools which respect PodDisruptionBudgets by calling the Eviction API instead of\ndirectly deleting pods or deployments.\nFor example, the kubectl drain subcommand lets you mark a node as going out of service. When you run kubectl drain , the tool\ntries to evict all of the Pods on the Node you're taking out of service. The eviction request that kubectl submits on your behalf may\nbe temporarily rejected, so the tool periodically retries all failed requests until all Pods on the target node are terminated, or until a\nconfigurable timeout is reached.\nA PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have. For\nexample, a Deployment which has a .spec.replicas: 5 is supposed to have 5 pods at any given time. If its PDB allows for there to\nbe 4 at a time, then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.\nThe group of pods that comprise the application is specified using a label selector, the same as the one used by the application's\ncontroller (deployment, stateful-set, etc).\nThe \"intended\" number of pods is computed from the .spec.replicas of the workload resource that is managing those pods. The\ncontrol plane discovers the owning workload resource by examining the .metadata.ownerReferences of the Pod.\nInvoluntary disruptions cannot be prevented by PDBs; however they do count against the budget.\nPods which are deleted or unavailable due to a rolling upgrade to an application do count against the disruption budget, but\nworkload resources (such as Deployment and StatefulSet) are not limited by PDBs when doing rolling upgrades. Instead, the\nhandling of failures during application updates is configured in the spec for the specific workload resource.\nIt is recommended to set AlwaysAllow Unhealthy Pod Eviction Policy to your PodDisruptionBudgets to support eviction of\nmisbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the\ndrain can proceed.\nWhen a pod is evicted using the eviction API, it is gracefully terminated, honoring the terminationGracePeriodSeconds setting in its\nPodSpec."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0127", "text": "PodDisruptionBudget example\nConsider a cluster with 3 nodes, node-1 through node-3 . The cluster is running several applications. One of them has 3 replicas\ninitially called pod-a , pod-b , and pod-c . Another, unrelated pod without a PDB, called pod-x , is also shown. Initially, the pods are\nlaid out as follows:\nnode-1\n\nnode-2\n\nnode-3\n\npod-a available\n\npod-b available\n\npod-c available\n\npod-x available\nAll 3 pods are part of a deployment, and they collectively have a PDB which requires there be at least 2 of the 3 pods to be available\nat all times.\nhttps://kubernetes.io/docs/concepts/_print/\n\n117/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel. The cluster\nadministrator first tries to drain node-1 using the kubectl drain command. That tool tries to evict pod-a and pod-x . This\nsucceeds immediately. Both pods go into the terminating state at the same time. This puts the cluster in this state:\nnode-1 draining\n\nnode-2\n\nnode-3\n\npod-a terminating\n\npod-b available\n\npod-c available\n\npod-x terminating\nThe deployment notices that one of the pods is terminating, so it creates a replacement called pod-d . Since node-1 is cordoned, it\nlands on another node. Something has also created pod-y as a replacement for pod-x .\n(Note: for a StatefulSet, pod-a , which would be called something like pod-0 , would need to terminate completely before its\nreplacement, which is also called pod-0 but has a different UID, could be created. Otherwise, the example applies to a StatefulSet as\nwell.)\nNow the cluster is in this state:\nnode-1 draining\n\nnode-2\n\nnode-3\n\npod-a terminating\n\npod-b available\n\npod-c available\n\npod-x terminating\n\npod-d starting\n\npod-y\n\nAt some point, the pods terminate, and the cluster looks like this:\nnode-1 drained\n\nnode-2\n\nnode-3\n\npod-b available\n\npod-c available\n\npod-d starting\n\npod-y\n\nAt this point, if an impatient cluster administrator tries to drain node-2 or node-3 , the drain command will block, because there are\nonly 2 available pods for the deployment, and its PDB requires at least 2. After some time passes, pod-d becomes available.\nThe cluster state now looks like this:\nnode-1 drained\n\nnode-2\n\nnode-3\n\npod-b available\n\npod-c available\n\npod-d available\n\npod-y"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0128", "text": "Now, the cluster administrator tries to drain node-2 . The drain command will try to evict the two pods in some order, say pod-b\nfirst and then pod-d . It will succeed at evicting pod-b . But, when it tries to evict pod-d , it will be refused because that would leave\nonly one pod available for the deployment.\nThe deployment creates a replacement for pod-b called pod-e . Because there are not enough resources in the cluster to schedule\npod-e the drain will again block. The cluster may end up in this state:\nnode-1 drained\n\nnode-2\n\nnode-3\n\nno node\n\npod-b terminating\n\npod-c available\n\npod-e pending\n\npod-d available\n\npod-y\n\nAt this point, the cluster administrator needs to add a node back to the cluster to proceed with the upgrade.\nYou can see how Kubernetes varies the rate at which disruptions can happen, according to:\nhttps://kubernetes.io/docs/concepts/_print/\n\n118/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nhow many replicas an application needs\nhow long it takes to gracefully shutdown an instance\nhow long it takes a new instance to start up\nthe type of controller\nthe cluster's resource capacity\n\nPod disruption conditions\nâ“˜ FEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)\n\nA dedicated Pod DisruptionTarget condition is added to indicate that the Pod is about to be deleted due to a disruption. The\nreason field of the condition additionally indicates one of the following reasons for the Pod termination:\nPreemptionByScheduler\n\nPod is due to be preempted by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see\nPod priority preemption.\nDeletionByTaintManager\n\nPod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within kube-controller-manager) due to\na NoExecute taint that the Pod does not tolerate; see taint-based evictions.\nEvictionByEvictionAPI\n\nPod has been marked for eviction using the Kubernetes API .\nDeletionByPodGC\n\nPod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage collection.\nTerminationByKubelet"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0129", "text": "Pod has been terminated by the kubelet, because of either node pressure eviction, the graceful node shutdown, or preemption\nfor system critical pods.\nIn all other disruption scenarios, like eviction due to exceeding Pod container limits, Pods don't receive the DisruptionTarget\ncondition because the disruptions were probably caused by the Pod and would reoccur on retry.\nNote:\nA Pod disruption might be interrupted. The control plane might re-attempt to continue the disruption of the same Pod, but it is\nnot guaranteed. As a result, the DisruptionTarget condition might be added to a Pod, but that Pod might then not actually be\ndeleted. In such a situation, after some time, the Pod disruption condition will be cleared.\nAlong with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal phase\n(see also Pod garbage collection).\nWhen using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's Pod failure policy.\n\nSeparating Cluster Owner and Application Owner Roles\nOften, it is useful to think of the Cluster Manager and Application Owner as separate roles with limited knowledge of each other. This\nseparation of responsibilities may make sense in these scenarios:\nwhen there are many application teams sharing a Kubernetes cluster, and there is natural specialization of roles\nwhen third-party tools or services are used to automate cluster management\nPod Disruption Budgets support this separation of roles by providing an interface between the roles.\nIf you do not have such a separation of responsibilities in your organization, you may not need to use Pod Disruption Budgets.\nhttps://kubernetes.io/docs/concepts/_print/\n\n119/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nHow to perform Disruptive Actions on your Cluster\nIf you are a Cluster Administrator, and you need to perform a disruptive action on all the nodes in your cluster, such as a node or\nsystem software upgrade, here are some options:\nAccept downtime during the upgrade.\nFailover to another complete replica cluster.\nNo downtime, but may be costly both for the duplicated nodes and for human effort to orchestrate the switchover.\nWrite disruption tolerant applications and use PDBs.\nNo downtime.\nMinimal resource duplication.\nAllows more automation of cluster administration.\nWriting disruption-tolerant applications is tricky, but the work to tolerate voluntary disruptions largely overlaps with work\nto support autoscaling and tolerating involuntary disruptions."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0130", "text": "What's next\nFollow steps to protect your application by configuring a Pod Disruption Budget.\nLearn more about draining nodes\nLearn about updating a deployment including steps to maintain its availability during the rollout.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n120/684\n\n11/7/25, 4:37 PM\n\n4.1.6 - Pod Hostname\n\nConcepts | Kubernetes\n\nThis page explains how to set a Pod's hostname, potential side effects after configuration, and the underlying mechanics.\n\nDefault Pod hostname\nWhen a Pod is created, its hostname (as observed from within the Pod) is derived from the Pod's metadata.name value. Both the\nhostname and its corresponding fully qualified domain name (FQDN) are set to the metadata.name value (from the Pod's\nperspective)\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox-1\nspec:\ncontainers:\n- image: busybox:1.28\ncommand:\n- sleep\n- \"3600\"\nname: busybox\n\nThe Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to busybox-1 .\n\nHostname with pod's hostname and subdomain fields\nThe Pod spec includes an optional hostname field. When set, this value takes precedence over the Pod's metadata.name as the\nhostname (observed from within the Pod). For example, a Pod with spec.hostname set to my-host will have its hostname set to myhost .\nThe Pod spec also includes an optional subdomain field, indicating the Pod belongs to a subdomain within its namespace. If a Pod\nhas spec.hostname set to \"foo\" and spec.subdomain set to \"bar\" in the namespace my-namespace , its hostname becomes foo and\nits fully qualified domain name (FQDN) becomes foo.bar.my-namespace.svc.cluster-domain.example (observed from within the\nPod).\nWhen both hostname and subdomain are set, the cluster's DNS server will create A and/or AAAA records based on these fields.\nRefer to: Pod's hostname and subdomain fields.\n\nHostname with pod's setHostnameAsFQDN fields\nâ“˜ FEATURE STATE: Kubernetes v1.22 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0131", "text": "When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have\na Pod with the fully qualified domain name busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example , then by\ndefault the hostname command inside that Pod returns busybox-1 and the hostname --fqdn command returns the FQDN.\nWhen both setHostnameAsFQDN: true and the subdomain field is set in the Pod spec, the kubelet writes the Pod's FQDN into the\nhostname for that Pod's namespace. In this case, both hostname and hostname --fqdn return the Pod's FQDN.\nThe Pod's FQDN is constructed in the same manner as previously defined. It is composed of the Pod's spec.hostname (if specified)\nor metadata.name field, the spec.subdomain , the namespace name, and the cluster domain suffix.\nNote:\nIn Linux, the hostname field of the kernel (the nodename field of struct utsname ) is limited to 64 characters.\nhttps://kubernetes.io/docs/concepts/_print/\n\n121/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status\n( ContainerCreating as seen by kubectl ) generating error events, such as \"Failed to construct FQDN from Pod hostname and\ncluster domain\".\nThis means that when using this field, you must ensure the combined length of the Pod's metadata.name (or spec.hostname )\nand spec.subdomain fields results in an FQDN that does not exceed 64 characters.\n\nHostname with pod's hostnameOverride\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\nSetting a value for hostnameOverride in the Pod spec causes the kubelet to unconditionally set both the Pod's hostname and fully\nqualified domain name (FQDN) to the hostnameOverride value.\nThe hostnameOverride field has a length limitation of 64 characters and must adhere to the DNS subdomain names standard\ndefined in RFC 1123.\nExample:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox-2-busybox-example-domain\nspec:\nhostnameOverride: busybox-2.busybox.example.domain\ncontainers:\n- image: busybox:1.28\ncommand:\n- sleep\n- \"3600\"\nname: busybox"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0132", "text": "Note:\nThis only affects the hostname within the Pod; it does not affect the Pod's A or AAAA records in the cluster DNS server.\nIf hostnameOverride is set alongside hostname and subdomain fields:\nThe hostname inside the Pod is overridden to the hostnameOverride value.\nThe Pod's A and/or AAAA records in the cluster DNS server are still generated based on the hostname and subdomain fields.\nNote: If hostnameOverride is set, you cannot simultaneously set the hostNetwork and setHostnameAsFQDN fields. The API server will\nexplicitly reject any create request attempting this combination.\nFor details on behavior when hostnameOverride is set in combination with other fields (hostname, subdomain,\nsetHostnameAsFQDN, hostNetwork), see the table in the KEP-4762 design details.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n122/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.1.7 - Pod Quality of Service Classes\nThis page introduces Quality of Service (QoS) classes in Kubernetes, and explains how Kubernetes assigns a QoS class to each Pod as a\nconsequence of the resource constraints that you specify for the containers in that Pod. Kubernetes relies on this classification to\nmake decisions about which Pods to evict when there are not enough available resources on a Node.\n\nQuality of Service classes\nKubernetes classifies the Pods that you run and allocates each Pod into a specific quality of service (QoS) class. Kubernetes uses that\nclassification to influence how different pods are handled. Kubernetes does this classification based on the resource requests of the\nContainers in that Pod, along with how those requests relate to resource limits. This is known as Quality of Service (QoS) class.\nKubernetes assigns every Pod a QoS class based on the resource requests and limits of its component Containers. QoS classes are\nused by Kubernetes to decide which Pods to evict from a Node experiencing Node Pressure. The possible QoS classes are\nGuaranteed , Burstable , and BestEffort . When a Node runs out of resources, Kubernetes will first evict BestEffort Pods running\non that Node, followed by Burstable and finally Guaranteed Pods. When this eviction is due to resource pressure, only Pods\nexceeding resource requests are candidates for eviction."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0133", "text": "Guaranteed\nPods that are Guaranteed have the strictest resource limits and are least likely to face eviction. They are guaranteed not to be killed\nuntil they exceed their limits or there are no lower-priority Pods that can be preempted from the Node. They may not acquire\nresources beyond their specified limits. These Pods can also make use of exclusive CPUs using the static CPU management policy.\n\nCriteria\nFor a Pod to be given a QoS class of Guaranteed :\nEvery Container in the Pod must have a memory limit and a memory request.\nFor every Container in the Pod, the memory limit must equal the memory request.\nEvery Container in the Pod must have a CPU limit and a CPU request.\nFor every Container in the Pod, the CPU limit must equal the CPU request.\n\nBurstable\nPods that are Burstable have some lower-bound resource guarantees based on the request, but do not require a specific limit. If a\nlimit is not specified, it defaults to a limit equivalent to the capacity of the Node, which allows the Pods to flexibly increase their\nresources if resources are available. In the event of Pod eviction due to Node resource pressure, these Pods are evicted only after all\nBestEffort Pods are evicted. Because a Burstable Pod can include a Container that has no resource limits or requests, a Pod that\nis Burstable can try to use any amount of node resources.\n\nCriteria\nA Pod is given a QoS class of Burstable if:\nThe Pod does not meet the criteria for QoS class Guaranteed .\nAt least one Container in the Pod has a memory or CPU request or limit.\n\nBestEffort\nPods in the BestEffort QoS class can use node resources that aren't specifically assigned to Pods in other QoS classes. For\nexample, if you have a node with 16 CPU cores available to the kubelet, and you assign 4 CPU cores to a Guaranteed Pod, then a\nPod in the BestEffort QoS class can try to use any amount of the remaining 12 CPU cores.\nThe kubelet prefers to evict BestEffort Pods if the node comes under resource pressure.\n\nCriteria\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n123/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0134", "text": "A Pod has a QoS class of BestEffort if it doesn't meet the criteria for either Guaranteed or Burstable . In other words, a Pod is\nBestEffort only if none of the Containers in the Pod have a memory limit or a memory request, and none of the Containers in the\nPod have a CPU limit or a CPU request. Containers in a Pod can request other resources (not CPU or memory) and still be classified\nas BestEffort .\n\nMemory QoS with cgroup v2\nâ“˜ FEATURE STATE: Kubernetes v1.22 [alpha] (enabled by default: false)\n\nMemory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits\nof containers in pod are used to set specific interfaces memory.min and memory.high provided by the memory controller. When\nmemory.min is set to memory requests, memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS\nensures memory availability for Kubernetes pods. And if memory limits are set in the container, this means that the system needs to\nlimit container memory usage; Memory QoS uses memory.high to throttle workload approaching its memory limit, ensuring that the\nsystem is not overwhelmed by instantaneous memory allocation.\nMemory QoS relies on QoS class to determine which settings to apply; however, these are different mechanisms that both provide\ncontrols over quality of service.\n\nSome behavior is independent of QoS class\nCertain behavior is independent of the QoS class assigned by Kubernetes. For example:\nAny Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that\nPod.\nIf a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a\ncandidate for eviction. If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a replacement Pod,\nusually on a different node.\nThe resource request of a Pod is equal to the sum of the resource requests of its component Containers, and the resource limit\nof a Pod is equal to the sum of the resource limits of its component Containers.\nThe kube-scheduler does not consider QoS class when selecting which Pods to preempt. Preemption can occur when a cluster\ndoes not have enough resources to run all the Pods you defined."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0135", "text": "What's next\nLearn about resource management for Pods and Containers.\nLearn about Node-pressure eviction.\nLearn about Pod priority and preemption.\nLearn about Pod disruptions.\nLearn how to assign memory resources to containers and pods.\nLearn how to assign CPU resources to containers and pods.\nLearn how to configure Quality of Service for Pods.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n124/684\n\n11/7/25, 4:37 PM\n\n4.1.8 - User Namespaces\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.30 [beta]\n\nThis page explains how user namespaces are used in Kubernetes pods. A user namespace isolates the user running inside the\ncontainer from the one in the host.\nA process running as root in a container can run as a different (non-root) user in the host; in other words, the process has full\nprivileges for operations inside the user namespace, but is unprivileged for operations outside the namespace.\nYou can use this feature to reduce the damage a compromised container can do to the host or other pods in the same node. There\nare several security vulnerabilities rated either HIGH or CRITICAL that were not exploitable when user namespaces is active. It is\nexpected user namespace will mitigate some future vulnerabilities too."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0136", "text": "Before you begin\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nThis is a Linux-only feature and support is needed in Linux for idmap mounts on the filesystems used. This means:\nOn the node, the filesystem you use for /var/lib/kubelet/pods/ , or the custom directory you configure for this, needs idmap\nmount support.\nAll the filesystems used in the pod's volumes must support idmap mounts.\nIn practice this means you need at least Linux 6.3, as tmpfs started supporting idmap mounts in that version. This is usually needed\nas several Kubernetes features use tmpfs (the service account token that is mounted by default uses a tmpfs, Secrets use a tmpfs,\netc.)\nSome popular filesystems that support idmap mounts in Linux 6.3 are: btrfs, ext4, xfs, fat, tmpfs, overlayfs.\nIn addition, the container runtime and its underlying OCI runtime must support user namespaces. The following OCI runtimes offer\nsupport:\ncrun version 1.9 or greater (it's recommend version 1.13+).\nrunc version 1.2 or greater\nNote:\nSome OCI runtimes do not include the support needed for using user namespaces in Linux pods. If you use a managed\nKubernetes, or have downloaded it from packages and set it up, it's possible that nodes in your cluster use a runtime that\ndoesn't include this support.\nTo use user namespaces with Kubernetes, you also need to use a CRI container runtime to use this feature with Kubernetes pods:\ncontainerd: version 2.0 (and later) supports user namespaces for containers.\nCRI-O: version 1.25 (and later) supports user namespaces for containers.\nYou can see the status of user namespaces support in cri-dockerd tracked in an issue on GitHub.\n\nIntroduction\nUser namespaces is a Linux feature that allows to map users in the container to different users in the host. Furthermore, the\ncapabilities granted to a pod in a user namespace are valid only in the namespace and void outside of it.\nA pod can opt-in to use user namespaces by setting the pod.spec.hostUsers field to false .\nhttps://kubernetes.io/docs/concepts/_print/\n\n125/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0137", "text": "The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee that no two pods on the same node\nuse the same mapping.\nThe runAsUser , runAsGroup , fsGroup , etc. fields in the pod.spec always refer to the user inside the container. These users will be\nused for volume mounts (specified in pod.spec.volumes ) and therefore the host UID/GID will not have any effect on writes/reads\nfrom volumes the pod can mount. In other words, the inodes created/read in volumes mounted by the pod will be the same as if the\npod wasn't using user namespaces.\nThis way, a pod can easily enable and disable user namespaces (without affecting its volume's file ownerships) and can also share\nvolumes with pods without user namespaces by just setting the appropriate users inside the container ( RunAsUser , RunAsGroup ,\nfsGroup , etc.). This applies to any volume the pod can mount, including hostPath (if the pod is allowed to mount hostPath\nvolumes).\nBy default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and processes ( runAsUser ,\nrunAsGroup , etc.).\nFiles using a UID/GID outside this range will be seen as belonging to the overflow ID, usually 65534 (configured in\n/proc/sys/kernel/overflowuid and /proc/sys/kernel/overflowgid ). However, it is not possible to modify those files, even by\nrunning as the 65534 user/group.\nIf the range 0-65535 is extended with a configuration knob, the aforementioned restrictions apply to the extended range.\nMost applications that need to run as root but don't access other host namespaces or resources, should continue to run fine without\nany changes needed if user namespaces is activated."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0138", "text": "Understanding user namespaces for pods\nSeveral container runtimes with their default configuration (like Docker Engine, containerd, CRI-O) use Linux namespaces for\nisolation. Other technologies exist and can be used with those runtimes too (e.g. Kata Containers uses VMs instead of Linux\nnamespaces). This page is applicable for container runtimes using Linux namespaces for isolation.\nWhen creating a pod, by default, several new namespaces are used for isolation: a network namespace to isolate the network of the\ncontainer, a PID namespace to isolate the view of processes, etc. If a user namespace is used, this will isolate the users in the\ncontainer from the users in the node.\nThis means containers can run as root and be mapped to a non-root user on the host. Inside the container the process will think it is\nrunning as root (and therefore tools like apt , yum , etc. work fine), while in reality the process doesn't have privileges on the host.\nYou can verify this, for example, if you check which user the container process is running by executing ps aux from the host. The\nuser ps shows is not the same as the user you see if you execute inside the container the command id .\nThis abstraction limits what can happen, for example, if the container manages to escape to the host. Given that the container is\nrunning as a non-privileged user on the host, it is limited what it can do to the host.\nFurthermore, as users on each pod will be mapped to different non-overlapping users in the host, it is limited what they can do to\nother pods too.\nCapabilities granted to a pod are also limited to the pod user namespace and mostly invalid out of it, some are even completely void.\nHere are two examples:\nCAP_SYS_MODULE\n\ndoes not have any effect if granted to a pod using user namespaces, the pod isn't able to load kernel\n\nmodules.\nCAP_SYS_ADMIN\n\nis limited to the pod's user namespace and invalid outside of it."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0139", "text": "Without using a user namespace a container running as root, in the case of a container breakout, has root privileges on the node.\nAnd if some capability were granted to the container, the capabilities are valid on the host too. None of this is true when we use user\nnamespaces.\nIf you want to know more details about what changes when user namespaces are in use, see man 7 user_namespaces .\n\nSet up a node to support user namespaces\nBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on the assumption that the host's files and\nprocesses use UIDs/GIDs within this range, which is standard for most Linux distributions. This approach prevents any overlap\nbetween the UIDs/GIDs of the host and those of the pods.\nhttps://kubernetes.io/docs/concepts/_print/\n\n126/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0140", "text": "Avoiding the overlap is important to mitigate the impact of vulnerabilities such as CVE-2021-25741, where a pod can potentially read\narbitrary files in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is limited what a pod would be able to do: the pod\nUID/GID won't match the host's file owner/group.\nThe kubelet can use a custom range for user IDs and group IDs for pods. To configure a custom range, the node needs to have:\nA user kubelet in the system (you cannot use any other username here)\nThe binary getsubids installed (part of shadow-utils) and in the PATH for the kubelet binary.\nA configuration of subordinate UIDs/GIDs for the kubelet user (see man 5 subuid and man 5 subgid).\nThis setting only gathers the UID/GID range configuration and does not change the user executing the kubelet .\nYou must follow some constraints for the subordinate ID range that you assign to the kubelet user:\nThe subordinate user ID, that starts the UID range for Pods, must be a multiple of 65536 and must also be greater than or\nequal to 65536. In other words, you cannot use any ID from the range 0-65535 for Pods; the kubelet imposes this restriction to\nmake it difficult to create an accidentally insecure configuration.\nThe subordinate ID count must be a multiple of 65536\nThe subordinate ID count must be at least 65536 x <maxPods> where <maxPods> is the maximum number of pods that can\nrun on the node.\nYou must assign the same range for both user IDs and for group IDs, It doesn't matter if other users have user ID ranges that\ndon't align with the group ID ranges.\nNone of the assigned ranges should overlap with any other assignment.\nThe subordinate configuration must be only one line. In other words, you can't have multiple ranges.\nFor example, you could define /etc/subuid and /etc/subgid to both have these entries for the kubelet user:\n# The format is\n#\nname:firstID:count of IDs\n# where\n# - firstID is 65536 (the minimum value possible)\n# - count of IDs is 110 * 65536\n#\n(110 is the default limit for number of pods on the node)\nkubelet:65536:7208960\n\nID count for each of Pods\nStarting with Kubernetes v1.33, the ID count for each of Pods can be set in KubeletConfiguration ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0141", "text": "apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nuserNamespaces:\nidsPerPod: 1048576\n\nThe value of idsPerPod (uint32) must be a multiple of 65536. The default value is 65536. This value only applies to containers\ncreated after the kubelet was started with this KubeletConfiguration . Running containers are not affected by this config.\nIn Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to 65536.\n\nIntegration with Pod security admission checks\nâ“˜ FEATURE STATE: Kubernetes v1.29 [alpha]\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n127/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor Linux Pods that enable user namespaces, Kubernetes relaxes the application of Pod Security Standards in a controlled way. This\nbehavior can be controlled by the feature gate UserNamespacesPodSecurityStandards , which allows an early opt-in for end users.\nAdmins have to ensure that user namespaces are enabled by all nodes within the cluster if using the feature gate.\nIf you enable the associated feature gate and create a Pod that uses user namespaces, the following fields won't be constrained\neven in contexts that enforce the Baseline or Restricted pod security standard. This behavior does not present a security concern\nbecause root inside a Pod with user namespaces actually refers to the user inside the container, that is never mapped to a\nprivileged user on the host. Here's the list of fields that are not checks for Pods in those circumstances:\nspec.securityContext.runAsNonRoot\nspec.containers[*].securityContext.runAsNonRoot\nspec.initContainers[*].securityContext.runAsNonRoot\nspec.ephemeralContainers[*].securityContext.runAsNonRoot\nspec.securityContext.runAsUser\nspec.containers[*].securityContext.runAsUser\nspec.initContainers[*].securityContext.runAsUser\nspec.ephemeralContainers[*].securityContext.runAsUser\n\nLimitations\nWhen using a user namespace for the pod, it is disallowed to use other host namespaces. In particular, if you set hostUsers: false\nthen you are not allowed to set any of:\nhostNetwork: true\nhostIPC: true\nhostPID: true\n\nNo container can use volumeDevices (raw block volumes, like /dev/sda) either. This includes all the container arrays in the pod spec:\ncontainers\ninitContainers\nephemeralContainers\n\nMetrics and observability\nThe kubelet exports two prometheus metrics specific to user-namespaces:\nstarted_user_namespaced_pods_total : a counter that tracks the number of user namespaced pods that are attempted to be\n\ncreated.\nstarted_user_namespaced_pods_errors_total : a counter that tracks the number of errors creating user namespaced pods.\n\nWhat's next\nTake a look at Use a User Namespace With a Pod\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n128/684\n\n11/7/25, 4:37 PM\n\n4.1.9 - Downward API\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0142", "text": "There are two ways to expose Pod and container fields to a running container: environment variables, and as\nfiles that are populated by a special volume type. Together, these two ways of exposing Pod and container\nfields are called the downward API.\nIt is sometimes useful for a container to have information about itself, without being overly coupled to Kubernetes. The downward\nAPI allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.\nAn example is an existing application that assumes a particular well-known environment variable holds a unique identifier. One\npossibility is to wrap the application, but that is tedious and error-prone, and it violates the goal of low coupling. A better option\nwould be to use the Pod's name as an identifier, and inject the Pod's name into the well-known environment variable.\nIn Kubernetes, there are two ways to expose Pod and container fields to a running container:\nas environment variables\nas files in a downwardAPI volume\nTogether, these two ways of exposing Pod and container fields are called the downward API.\n\nAvailable fields\nOnly some Kubernetes API fields are available through the downward API. This section lists which fields you can make available.\nYou can pass information from available Pod-level fields using fieldRef . At the API level, the spec for a Pod always defines at least\none Container. You can pass information from available Container-level fields using resourceFieldRef .\n\nInformation available via fieldRef\nFor some Pod-level fields, you can provide them to a container either as an environment variable or using a downwardAPI volume.\nThe fields available via either mechanism are:\nmetadata.name\n\nthe pod's name\nmetadata.namespace\n\nthe pod's namespace\nmetadata.uid\n\nthe pod's unique ID\nmetadata.annotations['<KEY>']\n\nthe value of the pod's annotation named <KEY> (for example, metadata.annotations['myannotation'])\nmetadata.labels['<KEY>']\n\nthe text value of the pod's label named <KEY> (for example, metadata.labels['mylabel'])\nThe following information is available through environment variables but not as a downwardAPI volume fieldRef:\nspec.serviceAccountName\n\nthe name of the pod's service account\nspec.nodeName\n\nthe name of the node where the Pod is executing\nstatus.hostIP\n\nthe primary IP address of the node to which the Pod is assigned\nhttps://kubernetes.io/docs/concepts/_print/\n\n129/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nstatus.hostIPs\n\nthe IP addresses is a dual-stack version of status.hostIP, the first is always the same as status.hostIP.\nstatus.podIP\n\nthe pod's primary IP address (usually, its IPv4 address)\nstatus.podIPs"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0143", "text": "the IP addresses is a dual-stack version of status.podIP, the first is always the same as status.podIP\nThe following information is available through a downwardAPI volume fieldRef , but not as environment variables:\nmetadata.labels\n\nall of the pod's labels, formatted as label-key=\"escaped-label-value\" with one label per line\nmetadata.annotations\n\nall of the pod's annotations, formatted as annotation-key=\"escaped-annotation-value\" with one annotation per line\n\nInformation available via resourceFieldRef\nThese container-level fields allow you to provide information about requests and limits for resources such as CPU and memory.\nNote:\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)\n\nContainer CPU and memory resources can be resized while the container is running. If this happens, a downward API volume\nwill be updated, but environment variables will not be updated unless the container restarts. See Resize CPU and Memory\nResources assigned to Containers for more details.\n\nresource: limits.cpu\n\nA container's CPU limit\nresource: requests.cpu\n\nA container's CPU request\nresource: limits.memory\n\nA container's memory limit\nresource: requests.memory\n\nA container's memory request\nresource: limits.hugepages-*\n\nA container's hugepages limit\nresource: requests.hugepages-*\n\nA container's hugepages request\nresource: limits.ephemeral-storage\n\nA container's ephemeral-storage limit\nresource: requests.ephemeral-storage\n\nA container's ephemeral-storage request\n\nFallback information for resource limits\nhttps://kubernetes.io/docs/concepts/_print/\n\n130/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf CPU and memory limits are not specified for a container, and you use the downward API to try to expose that information, then\nthe kubelet defaults to exposing the maximum allocatable value for CPU and memory based on the node allocatable calculation.\n\nWhat's next\nYou can read about downwardAPI volumes.\nYou can try using the downward API to expose container- or Pod-level information:\nas environment variables\nas files in downwardAPI volume\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n131/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.2 - Workload Management"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0144", "text": "Kubernetes provides several built-in APIs for declarative management of your workloads and the components of those workloads.\nUltimately, your applications run as containers inside Pods; however, managing individual Pods would be a lot of effort. For example,\nif a Pod fails, you probably want to run a new Pod to replace it. Kubernetes can do that for you.\nYou use the Kubernetes API to create a workload object that represents a higher abstraction level than a Pod, and then the\nKubernetes control plane automatically manages Pod objects on your behalf, based on the specification for the workload object you\ndefined.\nThe built-in APIs for managing workloads are:\nDeployment (and, indirectly, ReplicaSet), the most common way to run an application on your cluster. Deployment is a good fit for\nmanaging a stateless application workload on your cluster, where any Pod in the Deployment is interchangeable and can be\nreplaced if needed. (Deployments are a replacement for the legacy ReplicationController API).\nA StatefulSet lets you manage one or more Pods â€“ all running the same application code â€“ where the Pods rely on having a distinct\nidentity. This is different from a Deployment where the Pods are expected to be interchangeable. The most common use for a\nStatefulSet is to be able to make a link between its Pods and their persistent storage. For example, you can run a StatefulSet that\nassociates each Pod with a PersistentVolume. If one of the Pods in the StatefulSet fails, Kubernetes makes a replacement Pod that is\nconnected to the same PersistentVolume.\nA DaemonSet defines Pods that provide facilities that are local to a specific node; for example, a driver that lets containers on that\nnode access a storage system. You use a DaemonSet when the driver, or other node-level service, has to run on the node where it's\nuseful. Each Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX server. A DaemonSet might\nbe fundamental to the operation of your cluster, such as a plugin to let that node access cluster networking, it might help you to\nmanage the node, or it could provide less essential facilities that enhance the container platform you are running. You can run\nDaemonSets (and their pods) across every node in your cluster, or across just a subset (for example, only install the GPU accelerator\ndriver on nodes that have a GPU installed).\nYou can use a Job and / or a CronJob to define tasks that run to completion and then stop. A Job represents a one-off task, whereas\neach CronJob repeats according to a schedule.\nOther topics in this section:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0145", "text": "https://kubernetes.io/docs/concepts/_print/\n\n132/684\n\n11/7/25, 4:37 PM\n\n4.2.1 - Deployments\n\nConcepts | Kubernetes\n\nA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.\nA Deployment provides declarative updates for Pods and ReplicaSets.\nYou describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a\ncontrolled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their\nresources with new Deployments.\nNote:\nDo not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use\ncase is not covered below.\n\nUse Case\nThe following are typical use cases for Deployments:\nCreate a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to\nsee if it succeeds or not.\nDeclare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created, and the\nDeployment gradually scales it up while scaling down the old ReplicaSet, ensuring Pods are replaced at a controlled rate. Each\nnew ReplicaSet updates the revision of the Deployment.\nRollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the\nrevision of the Deployment.\nScale up the Deployment to facilitate more load.\nPause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.\nUse the status of the Deployment as an indicator that a rollout has stuck.\nClean up older ReplicaSets that you don't need anymore.\n\nCreating a Deployment\nThe following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods:\ncontrollers/nginx-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n133/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0146", "text": "In this example:\nA Deployment named nginx-deployment is created, indicated by the .metadata.name field. This name will become the basis\nfor the ReplicaSets and Pods which are created later. See Writing a Deployment Spec for more details.\nThe Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field.\nThe .spec.selector field defines how the created ReplicaSet finds which Pods to manage. In this case, you select a label that\nis defined in the Pod template ( app: nginx ). However, more sophisticated selection rules are possible, as long as the Pod\ntemplate itself satisfies the rule.\nNote:\nThe .spec.selector.matchLabels field is a map of {key,value} pairs. A single {key,value} in the matchLabels map is\nequivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array\ncontains only \"value\". All of the requirements, from both matchLabels and matchExpressions, must be satisfied in order\nto match.\nThe .spec.template field contains the following sub-fields:\nThe Pods are labeled app: nginx using the .metadata.labels field.\nThe Pod template's specification, or .spec field, indicates that the Pods run one container, nginx , which runs the\nnginx Docker Hub image at version 1.14.2.\nCreate one container and name it nginx using the .spec.containers[0].name field.\nBefore you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above\nDeployment:\n1. Create the Deployment by running the following command:\n\nkubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml\n\n2. Run kubectl get deployments to check if the Deployment was created.\nIf the Deployment is still being created, the output is similar to the following:\nNAME\n\nREADY\n\nUP-TO-DATE\n\nAVAILABLE\n\nAGE\n\nnginx-deployment\n\n0/3\n\n0\n\n0\n\n1s\n\nWhen you inspect the Deployments in your cluster, the following fields are displayed:\nNAME\n\nlists the names of the Deployments in the namespace.\n\nREADY\n\ndisplays how many replicas of the application are available to your users. It follows the pattern ready/desired.\n\nUP-TO-DATE\n\ndisplays the number of replicas that have been updated to achieve the desired state.\n\nAVAILABLE\n\ndisplays how many replicas of the application are available to your users.\n\nAGE\n\ndisplays the amount of time that the application has been running."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0147", "text": "Notice how the number of desired replicas is 3 according to .spec.replicas field.\n3. To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment .\nThe output is similar to:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\ndeployment \"nginx-deployment\" successfully rolled out\n\n4. Run the kubectl get deployments again a few seconds later. The output is similar to this:\nNAME\n\nREADY\n\nUP-TO-DATE\n\nAVAILABLE\n\nAGE\n\nnginx-deployment\n\n3/3\n\n3\n\n3\n\n18s\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n134/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNotice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template)\nand available.\n5. To see the ReplicaSet ( rs ) created by the Deployment, run kubectl get rs . The output is similar to this:\nNAME\nnginx-deployment-75675f5897\n\nDESIRED\n3\n\nCURRENT\n3\n\nREADY\n3\n\nAGE\n18s\n\nReplicaSet output shows the following fields:\nlists the names of the ReplicaSets in the namespace.\n\nNAME\n\ndisplays the desired number of replicas of the application, which you define when you create the Deployment.\nThis is the desired state.\nCURRENT displays how many replicas are currently running.\nDESIRED\n\nREADY\nAGE\n\ndisplays how many replicas of the application are available to your users.\n\ndisplays the amount of time that the application has been running.\n\nNotice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[HASH] . This name will become the basis for\nthe Pods which are created.\nThe HASH string is the same as the pod-template-hash label on the ReplicaSet.\n6. To see the labels automatically generated for each Pod, run kubectl get pods --show-labels . The output is similar to:\nNAME\nnginx-deployment-75675f5897-7ci7o\nnginx-deployment-75675f5897-kzszj\nnginx-deployment-75675f5897-qqcnn\n\nREADY\n1/1\n1/1\n1/1\n\nSTATUS\nRunning\nRunning\nRunning\n\nRESTARTS\n0\n0\n0\n\nAGE\n18s\n18s\n18s\n\nLABELS\napp=nginx,pod-template-hash=75675f\napp=nginx,pod-template-hash=75675f\napp=nginx,pod-template-hash=75675f\n\nThe created ReplicaSet ensures that there are three nginx Pods.\nNote:\nYou must specify an appropriate selector and Pod template labels in a Deployment (in this case, app: nginx ).\nDo not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't\nstop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave\nunexpectedly."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0148", "text": "Pod-template-hash label\nCaution:\nDo not change this label.\nThe pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.\nThis label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the\nReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any\nexisting Pods that the ReplicaSet might have.\n\nUpdating a Deployment\nNote:\nA Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed, for\nexample if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not\ntrigger a rollout.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n135/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFollow the steps given below to update your Deployment:\n1. Let's update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image.\n\nkubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1\n\nor use the following command:\n\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n\nwhere deployment/nginx-deployment indicates the Deployment, nginx indicates the Container the update will take place and\nnginx:1.16.1 indicates the new image and its tag.\nThe output is similar to:\ndeployment.apps/nginx-deployment image updated\n\nAlternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to\nnginx:1.16.1 :\n\nkubectl edit deployment/nginx-deployment\n\nThe output is similar to:\ndeployment.apps/nginx-deployment edited\n\n2. To see the rollout status, run:\n\nkubectl rollout status deployment/nginx-deployment\n\nThe output is similar to this:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n\nor\ndeployment \"nginx-deployment\" successfully rolled out\n\nGet more details on your updated Deployment:\nAfter the rollout succeeds, you can view the Deployment by running kubectl get deployments . The output is similar to this:\nNAME\n\nREADY\n\nUP-TO-DATE\n\nAVAILABLE\n\nAGE\n\nnginx-deployment\n\n3/3\n\n3\n\n3\n\n36s\n\nRun kubectl get rs to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas,\nas well as scaling down the old ReplicaSet to 0 replicas.\n\nkubectl get rs\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n136/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe output is similar to this:\nNAME\nnginx-deployment-1564180365\n\nDESIRED\n3\n\nCURRENT\n3\n\nREADY\n3\n\nAGE\n6s\n\nnginx-deployment-2035384211\n\n0\n\n0\n\n0\n\n36s\n\nRunning get pods should now show only the new Pods:\n\nkubectl get pods\n\nThe output is similar to this:\nNAME\nnginx-deployment-1564180365-khku8\n\nREADY\n1/1\n\nSTATUS\nRunning\n\nRESTARTS\n0\n\nAGE\n14s"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0149", "text": "nginx-deployment-1564180365-nacti\nnginx-deployment-1564180365-z9gth\n\n1/1\n1/1\n\nRunning\nRunning\n\n0\n0\n\n14s\n14s\n\nNext time you want to update these Pods, you only need to update the Deployment's Pod template again.\nDeployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at\nleast 75% of the desired number of Pods are up (25% max unavailable).\nDeployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it\nensures that at most 125% of the desired number of Pods are up (25% max surge).\nFor example, if you look at the above Deployment closely, you will see that it first creates a new Pod, then deletes an old Pod,\nand creates another new one. It does not kill old Pods until a sufficient number of new Pods have come up, and does not\ncreate new Pods until a sufficient number of old Pods have been killed. It makes sure that at least 3 Pods are available and that\nat max 4 Pods in total are available. In case of a Deployment with 4 replicas, the number of Pods would be between 3 and 5.\nGet details of your Deployment:\n\nkubectl describe deployments\n\nThe output is similar to this:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n137/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\nNamespace:\nCreationTimestamp:\nLabels:\nAnnotations:\nSelector:\nReplicas:\nStrategyType:\nMinReadySeconds:\nRollingUpdateStrategy:\nPod Template:\nLabels: app=nginx\nContainers:\n\nnginx-deployment\ndefault\nThu, 30 Nov 2017 10:56:25 +0000\napp=nginx\ndeployment.kubernetes.io/revision=2\napp=nginx\n3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nRollingUpdate\n0\n25% max unavailable, 25% max surge\n\nnginx:\nImage:\nnginx:1.16.1\nPort:\n80/TCP\nEnvironment: <none>\nMounts:\n<none>\nVolumes:\n<none>\nConditions:\nType\nStatus Reason\n--------- -----Available\nTrue\nMinimumReplicasAvailable\nProgressing\nTrue\nNewReplicaSetAvailable\nOldReplicaSets: <none>\nNewReplicaSet:\nnginx-deployment-1564180365 (3/3 replicas created)\nEvents:\nType\nReason\nAge\nFrom\nMessage\n------------ ---------Normal ScalingReplicaSet 2m\ndeployment-controller Scaled up replica set nginx-deployment-2035384211 t\nNormal ScalingReplicaSet 24s\ndeployment-controller Scaled up replica set nginx-deployment-1564180365 t\nNormal ScalingReplicaSet 22s\ndeployment-controller Scaled down replica set nginx-deployment-2035384211\nNormal\nNormal\nNormal\nNormal\n\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\n\n22s\n19s\n19s\n14s\n\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\n\nScaled up replica set nginx-deployment-1564180365 t\nScaled down replica set nginx-deployment-2035384211\nScaled up replica set nginx-deployment-1564180365 t\nScaled down replica set nginx-deployment-2035384211"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0150", "text": "Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211) and scaled it\nup to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet (nginx-deployment-1564180365) and\nscaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled up the new ReplicaSet to 2\nso that at least 3 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down the\nnew and the old ReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas in the new ReplicaSet,\nand the old ReplicaSet is scaled down to 0.\nNote:\nKubernetes doesn't count terminating Pods when calculating the number of availableReplicas, which must be between\nreplicas - maxUnavailable and replicas + maxSurge. As a result, you might notice that there are more Pods than expected\nduring a rollout, and that the total resources consumed by the Deployment is more than replicas + maxSurge until the\nterminationGracePeriodSeconds of the terminating Pods expires.\n\nRollover (aka multiple updates in-flight)\nEach time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the\nDeployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does\nnot match .spec.template is scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is\nscaled to 0.\nIf you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet as per the update and\nstart scaling that up, and rolls over the ReplicaSet that it was scaling up previously -- it will add it to its list of old ReplicaSets and start\nscaling it down.\nFor example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2 , but then update the Deployment to create 5\nreplicas of nginx:1.16.1 , when only 3 replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately starts\nkilling the 3 nginx:1.14.2 Pods that it had created, and starts creating nginx:1.16.1 Pods. It does not wait for the 5 replicas of\nnginx:1.14.2 to be created before changing course.\nhttps://kubernetes.io/docs/concepts/_print/\n\n138/684\n\n11/7/25, 4:37 PM\n\nLabel selector updates\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0151", "text": "It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need\nto perform a label selector update, exercise great caution and make sure you have grasped all of the implications.\nNote:\nIn API version apps/v1, a Deployment's label selector is immutable after it gets created.\nSelector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a\nvalidation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets\nand Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.\nSelector updates changes the existing value in a selector key -- result in the same behavior as additions.\nSelector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template\nlabels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in\nany existing Pods and ReplicaSets.\n\nRolling Back a Deployment\nSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By\ndefault, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that\nby modifying revision history limit).\nNote:\nA Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and\nonly if the Deployment's Pod template (.spec.template) is changed, for example if you update the labels or container images\nof the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate\nsimultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment's Pod\ntemplate part is rolled back.\nSuppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of\nnginx:1.16.1 :\n\nkubectl set image deployment/nginx-deployment nginx=nginx:1.161\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\n\nThe rollout gets stuck. You can verify it by checking the rollout status:\n\nkubectl rollout status deployment/nginx-deployment\n\nThe output is similar to this:\nWaiting for rollout to finish: 1 out of 3 new replicas have been updated..."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0152", "text": "Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts, read more here.\nYou see that the number of old replicas (adding the replica count from nginx-deployment-1564180365 and nginx-deployment2035384211 ) is 3, and the number of new replicas (from nginx-deployment-3066724191 ) is 1.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n139/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\nnginx-deployment-1564180365\n\nDESIRED\n3\n\nCURRENT\n3\n\nREADY\n3\n\nAGE\n25s\n\nnginx-deployment-2035384211\nnginx-deployment-3066724191\n\n0\n1\n\n0\n1\n\n0\n0\n\n36s\n6s\n\nLooking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.\n\nkubectl get pods\n\nThe output is similar to this:\nNAME\nnginx-deployment-1564180365-70iae\n\nREADY\n1/1\n\nSTATUS\nRunning\n\nRESTARTS\n0\n\nAGE\n25s\n\nnginx-deployment-1564180365-jbqqo\nnginx-deployment-1564180365-hysrc\n\n1/1\n1/1\n\nRunning\nRunning\n\n0\n0\n\n25s\n25s\n\nnginx-deployment-3066724191-08mng\n\n0/1\n\nImagePullBackOff\n\n0\n\n6s\n\nNote:\nThe Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on\nthe rollingUpdate parameters (maxUnavailable specifically) that you have specified. Kubernetes by default sets the value\nto 25%.\nGet the description of the Deployment:\n\nkubectl describe deployment\n\nThe output is similar to this:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n140/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\nnginx-deployment\nNamespace:\ndefault\nCreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700\nLabels:\napp=nginx\nSelector:\napp=nginx\nReplicas:\n3 desired | 1 updated | 4 total | 3 available | 1 unavailable\nStrategyType:\nRollingUpdate\nMinReadySeconds:\n0\nRollingUpdateStrategy: 25% max unavailable, 25% max surge\nPod Template:\nLabels: app=nginx\nContainers:\nnginx:\nImage:\nnginx:1.161\nPort:\n80/TCP\nHost Port:\n0/TCP\nEnvironment: <none>\nMounts:\n<none>\nVolumes:\n<none>\nConditions:\nType\nStatus Reason\n--------- -----Available\nTrue\nMinimumReplicasAvailable\nProgressing\nTrue\nReplicaSetUpdated\nOldReplicaSets:\nnginx-deployment-1564180365 (3/3 replicas created)\nNewReplicaSet:\nnginx-deployment-3066724191 (1/1 replicas created)\nEvents:\nFirstSeen LastSeen\nCount\nFrom\nSubObjectPath\n--------- --------------------------1m\n1m\n1\n{deployment-controller }\n22s\n22s\n1\n{deployment-controller }\n22s\n22s\n1\n{deployment-controller }\n22s\n21s\n21s\n13s\n13s\n\n22s\n21s\n21s\n13s\n13s\n\n1\n1\n1\n1\n1\n\n{deployment-controller }\n{deployment-controller }\n{deployment-controller }\n{deployment-controller }\n{deployment-controller }\n\nType\n-------Normal\nNormal\nNormal\n\nReason\n-----ScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\n\nMessage\n------Scaled\nScaled\nScaled\n\nNormal\nNormal\nNormal\nNormal\nNormal\n\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\n\nScaled\nScaled\nScaled\nScaled\nScaled"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0153", "text": "To fix this, you need to rollback to a previous revision of Deployment that is stable.\n\nChecking Rollout History of a Deployment\nFollow the steps given below to check the rollout history:\n1. First, check the revisions of this Deployment:\n\nkubectl rollout history deployment/nginx-deployment\n\nThe output is similar to this:\ndeployments \"nginx-deployment\"\nREVISION\nCHANGE-CAUSE\n1\n2\n3\n\n<none>\n<none>\n<none>\n\nis copied from the Deployment annotation kubernetes.io/change-cause to its revisions upon creation. You can\nspecify the CHANGE-CAUSE message by:\nCHANGE-CAUSE\n\nAnnotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause=\"image\nupdated to 1.16.1\"\n\nManually editing the manifest of the resource.\nUsing tooling that sets the annotation automatically.\nhttps://kubernetes.io/docs/concepts/_print/\n\n141/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nIn older versions of Kubernetes, you could use the --record flag with kubectl commands to automatically populate the\nCHANGE-CAUSE field. This flag is deprecated and will be removed in a future release.\n2. To see the details of each revision, run:\n\nkubectl rollout history deployment/nginx-deployment --revision=2\n\nThe output is similar to this:\ndeployments \"nginx-deployment\" revision 2\nLabels:\napp=nginx\npod-template-hash=1159050644\nContainers:\nnginx:\nImage:\nPort:\nQoS Tier:\ncpu:\nmemory:\n\nnginx:1.16.1\n80/TCP\nBestEffort\nBestEffort\n\nEnvironment Variables:\nNo volumes.\n\n<none>\n\nRolling Back to a Previous Revision\nFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.\n1. Now you've decided to undo the current rollout and rollback to the previous revision:\n\nkubectl rollout undo deployment/nginx-deployment\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment rolled back\n\nAlternatively, you can rollback to a specific revision by specifying it with --to-revision :\n\nkubectl rollout undo deployment/nginx-deployment --to-revision=2\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment rolled back\n\nFor more details about rollout related commands, read kubectl rollout .\nThe Deployment is now rolled back to a previous stable revision. As you can see, a DeploymentRollback event for rolling back\nto revision 2 is generated from Deployment controller.\n2. Check if the rollback was successful and the Deployment is running as expected, run:\n\nkubectl get deployment nginx-deployment\n\nThe output is similar to this:\nhttps://kubernetes.io/docs/concepts/_print/\n\n142/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNAME\nnginx-deployment\n\nREADY\n3/3\n\nUP-TO-DATE\n3\n\nAVAILABLE\n3\n\nAGE\n30m\n\n3. Get the description of the Deployment:\n\nkubectl describe deployment nginx-deployment"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0154", "text": "The output is similar to this:\nName:\nnginx-deployment\nNamespace:\ndefault\nCreationTimestamp:\nSun, 02 Sep 2018 18:17:55 -0500\nLabels:\napp=nginx\nAnnotations:\ndeployment.kubernetes.io/revision=4\nSelector:\napp=nginx\nReplicas:\n3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:\nRollingUpdate\nMinReadySeconds:\n0\nRollingUpdateStrategy: 25% max unavailable, 25% max surge\nPod Template:\nLabels: app=nginx\nContainers:\nnginx:\nImage:\nnginx:1.16.1\nPort:\n80/TCP\nHost Port:\n0/TCP\nEnvironment: <none>\nMounts:\n<none>\nVolumes:\n<none>\nConditions:\nType\nStatus Reason\n--------- -----Available\nTrue\nMinimumReplicasAvailable\nProgressing\nTrue\nNewReplicaSetAvailable\nOldReplicaSets: <none>\nNewReplicaSet:\nnginx-deployment-c4747d96c (3/3 replicas created)\nEvents:\nType\nReason\nAge\nFrom\nMessage\n------------ ---------Normal ScalingReplicaSet\n12m\ndeployment-controller Scaled up replica set nginx-deployment-75675f5897 to\nNormal ScalingReplicaSet\n11m\ndeployment-controller Scaled up replica set nginx-deployment-c4747d96c to\nNormal\nNormal\nNormal\nNormal\nNormal\nNormal\nNormal\nNormal\n\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nScalingReplicaSet\nDeploymentRollback\nScalingReplicaSet\n\n11m\n11m\n11m\n11m\n11m\n11m\n15s\n15s\n\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\ndeployment-controller\n\nScaled down replica set nginx-deployment-75675f5897\nScaled up replica set nginx-deployment-c4747d96c to\nScaled down replica set nginx-deployment-75675f5897\nScaled up replica set nginx-deployment-c4747d96c to\nScaled down replica set nginx-deployment-75675f5897\nScaled up replica set nginx-deployment-595696685f to\nRolled back deployment \"nginx-deployment\" to revisio\nScaled down replica set nginx-deployment-595696685f\n\nScaling a Deployment\nYou can scale a Deployment by using the following command:\n\nkubectl scale deployment/nginx-deployment --replicas=10\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment scaled\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n143/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAssuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the\nminimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods.\n\nkubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment scaled"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0155", "text": "Proportional scaling\nRollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales\na RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), the Deployment controller balances the\nadditional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling.\nFor example, you are running a Deployment with 10 replicas, maxSurge=3, and maxUnavailable=2.\nEnsure that the 10 replicas in your Deployment are running.\n\nkubectl get deploy\n\nThe output is similar to this:\nNAME\nnginx-deployment\n\nDESIRED\n10\n\nCURRENT\n10\n\nUP-TO-DATE\n10\n\nAVAILABLE\n10\n\nAGE\n50s\n\nYou update to a new image which happens to be unresolvable from inside the cluster.\n\nkubectl set image deployment/nginx-deployment nginx=nginx:sometag\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\n\nThe image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the\nmaxUnavailable requirement that you mentioned above. Check out the rollout status:\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\n\nDESIRED\n\nCURRENT\n\nREADY\n\nAGE\n\nnginx-deployment-1989198191\nnginx-deployment-618515232\n\n5\n8\n\n5\n8\n\n0\n8\n\n9s\n1m\n\nThen a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas to 15. The\nDeployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of\nthem would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all\nReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less\nreplicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.\nIn our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the new ReplicaSet. The rollout process\nshould eventually move all replicas to the new ReplicaSet, assuming the new replicas become healthy. To confirm this, run:\nhttps://kubernetes.io/docs/concepts/_print/\n\n144/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get deploy\n\nThe output is similar to this:\nNAME\nnginx-deployment\n\nDESIRED\n15\n\nCURRENT\n18\n\nUP-TO-DATE\n7\n\nAVAILABLE\n8\n\nAGE\n7m\n\nThe rollout status confirms how the replicas were added to each ReplicaSet.\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\n\nDESIRED\n\nCURRENT\n\nREADY\n\nAGE\n\nnginx-deployment-1989198191\nnginx-deployment-618515232\n\n7\n11\n\n7\n11\n\n0\n11\n\n7m\n7m"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0156", "text": "Pausing and Resuming a rollout of a Deployment\nWhen you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates.\nWhen you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple\nfixes in between pausing and resuming without triggering unnecessary rollouts.\nFor example, with a Deployment that was created:\nGet the Deployment details:\n\nkubectl get deploy\n\nThe output is similar to this:\nNAME\nnginx\n\nDESIRED\n3\n\nCURRENT\n3\n\nUP-TO-DATE\n3\n\nAVAILABLE\n3\n\nAGE\n1m\n\nGet the rollout status:\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\n\nDESIRED\n\nCURRENT\n\nREADY\n\nAGE\n\nnginx-2142116321\n\n3\n\n3\n\n3\n\n1m\n\nPause by running the following command:\n\nkubectl rollout pause deployment/nginx-deployment\n\nThe output is similar to this:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n145/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ndeployment.apps/nginx-deployment paused\n\nThen update the image of the Deployment:\n\nkubectl set image deployment/nginx-deployment nginx=nginx:1.16.1\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment image updated\n\nNotice that no new rollout started:\n\nkubectl rollout history deployment/nginx-deployment\n\nThe output is similar to this:\ndeployments \"nginx\"\nREVISION CHANGE-CAUSE\n1\n\n<none>\n\nGet the rollout status to verify that the existing ReplicaSet has not changed:\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\nnginx-2142116321\n\nDESIRED\n3\n\nCURRENT\n3\n\nREADY\n3\n\nAGE\n2m\n\nYou can make as many updates as you wish, for example, update the resources that will be used:\n\nkubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment resource requirements updated\n\nThe initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to the Deployment will\nnot have any effect as long as the Deployment rollout is paused.\nEventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:\n\nkubectl rollout resume deployment/nginx-deployment\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment resumed\n\nWatch the status of the rollout until it's done.\nhttps://kubernetes.io/docs/concepts/_print/\n\n146/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get rs --watch\n\nThe output is similar to this:\nNAME\nnginx-2142116321\nnginx-3926361531\n\nDESIRED\n2\n2\n\nCURRENT\n2\n2\n\nREADY\n2\n0\n\nAGE\n2m\n6s\n\nnginx-3926361531\nnginx-2142116321\n\n2\n1\n\n2\n2\n\n1\n2\n\n18s\n2m\n\nnginx-2142116321\nnginx-3926361531\n\n1\n3\n\n2\n2\n\n2\n1\n\n2m\n18s\n\nnginx-3926361531\nnginx-2142116321\nnginx-3926361531\n\n3\n1\n3\n\n2\n1\n3\n\n1\n1\n1\n\n18s\n2m\n18s\n\nnginx-3926361531\nnginx-2142116321\n\n3\n0\n\n3\n1\n\n2\n1"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0157", "text": "19s\n2m\n\nnginx-2142116321\nnginx-2142116321\n\n0\n0\n\n1\n0\n\n1\n0\n\n2m\n2m\n\nnginx-3926361531\n\n3\n\n3\n\n3\n\n20s\n\nCURRENT\n0\n3\n\nREADY\n0\n3\n\nAGE\n2m\n28s\n\nGet the status of the latest rollout:\n\nkubectl get rs\n\nThe output is similar to this:\nNAME\nnginx-2142116321\nnginx-3926361531\n\nDESIRED\n0\n3\n\nNote:\nYou cannot rollback a paused Deployment until you resume it.\n\nDeployment status\nA Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete,\nor it can fail to progress.\n\nProgressing Deployment\nKubernetes marks a Deployment as progressing when one of the following tasks is performed:\nThe Deployment creates a new ReplicaSet.\nThe Deployment is scaling up its newest ReplicaSet.\nThe Deployment is scaling down its older ReplicaSet(s).\nNew Pods become ready or available (ready for at least MinReadySeconds).\nWhen the rollout becomes â€œprogressingâ€, the Deployment controller adds a condition with the following attributes to the\nDeployment's .status.conditions :\ntype: Progressing\nstatus: \"True\"\nreason: NewReplicaSetCreated\n\n| reason: FoundNewReplicaSet | reason: ReplicaSetUpdated\n\nYou can monitor the progress for a Deployment by using kubectl rollout status .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n147/684\n\n11/7/25, 4:37 PM\n\nComplete Deployment\n\nConcepts | Kubernetes\n\nKubernetes marks a Deployment as complete when it has the following characteristics:\nAll of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any\nupdates you've requested have been completed.\nAll of the replicas associated with the Deployment are available.\nNo old replicas for the Deployment are running.\nWhen the rollout becomes â€œcompleteâ€, the Deployment controller sets a condition with the following attributes to the Deployment's\n.status.conditions :\ntype: Progressing\nstatus: \"True\"\nreason: NewReplicaSetAvailable\n\nThis Progressing condition will retain a status value of \"True\" until a new rollout is initiated. The condition holds even when\navailability of replicas changes (which does instead affect the Available condition).\nYou can check if a Deployment has completed by using kubectl rollout status . If the rollout completed successfully, kubectl\nrollout status returns a zero exit code.\n\nkubectl rollout status deployment/nginx-deployment\n\nThe output is similar to this:\nWaiting for rollout to finish: 2 of 3 updated replicas are available...\ndeployment \"nginx-deployment\" successfully rolled out\n\nand the exit status from kubectl rollout is 0 (success):\n\necho $?\n\n0"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0158", "text": "Failed Deployment\nYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur due to some of the\nfollowing factors:\nInsufficient quota\nReadiness probe failures\nImage pull errors\nInsufficient permissions\nLimit ranges\nApplication runtime misconfiguration\nOne way you can detect this condition is to specify a deadline parameter in your Deployment spec:\n( .spec.progressDeadlineSeconds ). .spec.progressDeadlineSeconds denotes the number of seconds the Deployment controller\nwaits before indicating (in the Deployment status) that the Deployment progress has stalled.\nThe following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a\nrollout for a Deployment after 10 minutes:\n\nkubectl patch deployment/nginx-deployment -p '{\"spec\":{\"progressDeadlineSeconds\":600}}'\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n148/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe output is similar to this:\ndeployment.apps/nginx-deployment patched\n\nOnce the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the\nDeployment's .status.conditions :\ntype: Progressing\nstatus: \"False\"\nreason: ProgressDeadlineExceeded\n\nThis condition can also fail early and is then set to status value of \"False\" due to reasons as ReplicaSetCreateError . Also, the\ndeadline is not taken into account anymore once the Deployment rollout completes.\nSee the Kubernetes API conventions for more information on status conditions.\nNote:\nKubernetes takes no action on a stalled Deployment other than to report a status condition with reason:\nProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the\nDeployment to its previous version.\n\nNote:\nIf you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can safely pause\na Deployment rollout in the middle of a rollout and resume without triggering the condition for exceeding the deadline.\nYou may experience transient errors with your Deployments, either due to a low timeout that you have set or due to any other kind\nof error that can be treated as transient. For example, let's suppose you have insufficient quota. If you describe the Deployment you\nwill notice the following section:\n\nkubectl describe deployment nginx-deployment\n\nThe output is similar to this:\n<...>\nConditions:\nType\n---Available\n\nStatus\n-----True\n\nReason\n-----MinimumReplicasAvailable\n\nProgressing\nReplicaFailure\n\nTrue\nTrue\n\nReplicaSetUpdated\nFailedCreate\n\n<...>\n\nIf you run kubectl get deployment nginx-deployment -o yaml , the Deployment status is similar to this:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n149/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0159", "text": "status:\navailableReplicas: 2\nconditions:\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: Replica set \"nginx-deployment-4262182780\" is progressing.\nreason: ReplicaSetUpdated\nstatus: \"True\"\ntype: Progressing\n- lastTransitionTime: 2016-10-04T12:25:42Z\nlastUpdateTime: 2016-10-04T12:25:42Z\nmessage: Deployment has minimum availability.\nreason: MinimumReplicasAvailable\nstatus: \"True\"\ntype: Available\n- lastTransitionTime: 2016-10-04T12:25:39Z\nlastUpdateTime: 2016-10-04T12:25:39Z\nmessage: 'Error creating: pods \"nginx-deployment-4262182780-\" is forbidden: exceeded quota:\nobject-counts, requested: pods=1, used: pods=3, limited: pods=2'\nreason: FailedCreate\nstatus: \"True\"\ntype: ReplicaFailure\nobservedGeneration: 3\nreplicas: 2\nunavailableReplicas: 2\n\nEventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the reason for the Progressing\ncondition:\nConditions:\nType\n----\n\nStatus\n------\n\nReason\n------\n\nAvailable\nProgressing\n\nTrue\nFalse\n\nMinimumReplicasAvailable\nProgressDeadlineExceeded\n\nReplicaFailure\n\nTrue\n\nFailedCreate\n\nYou can address an issue of insufficient quota by scaling down your Deployment, by scaling down other controllers you may be\nrunning, or by increasing quota in your namespace. If you satisfy the quota conditions and the Deployment controller then\ncompletes the Deployment rollout, you'll see the Deployment's status update with a successful condition ( status: \"True\" and\nreason: NewReplicaSetAvailable ).\nConditions:\nType\n---Available\nProgressing\n\nStatus\n------\n\nReason\n------\n\nTrue\nTrue\n\nMinimumReplicasAvailable\nNewReplicaSetAvailable\n\nwith status: \"True\" means that your Deployment has minimum availability. Minimum availability is dictated by\nthe parameters specified in the deployment strategy. type: Progressing with status: \"True\" means that your Deployment is\neither in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum required\nnew replicas are available (see the Reason of the condition for the particulars - in our case reason: NewReplicaSetAvailable means\nthat the Deployment is complete).\ntype: Available\n\nYou can check if a Deployment has failed to progress by using kubectl rollout status . kubectl rollout status returns a nonzero exit code if the Deployment has exceeded the progression deadline.\n\nkubectl rollout status deployment/nginx-deployment\n\nThe output is similar to this:\nWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\nerror: deployment \"nginx\" exceeded its progress deadline\nhttps://kubernetes.io/docs/concepts/_print/\n\n150/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nand the exit status from kubectl rollout is 1 (indicating an error):\n\necho $?\n\n1"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0160", "text": "Operating on a failed deployment\nAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back to a previous\nrevision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.\n\nClean up Policy\nYou can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want\nto retain. The rest will be garbage-collected in the background. By default, it is 10.\nNote:\nExplicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be\nable to roll back.\nThe cleanup only starts after a Deployment reaches a complete state. If you set .spec.revisionHistoryLimit to 0, any rollout\nnonetheless triggers creation of a new ReplicaSet before Kubernetes removes the old one.\nEven with a non-zero revision history limit, you can have more ReplicaSets than the limit you configure. For example, if pods are\ncrash looping, and there are multiple rolling updates events triggered over time, you might end up with more ReplicaSets than the\n.spec.revisionHistoryLimit because the Deployment never reaches a complete state.\n\nCanary Deployment\nIf you want to roll out releases to a subset of users or servers using the Deployment, you can create multiple Deployments, one for\neach release, following the canary pattern described in managing resources.\n\nWriting a Deployment Spec\nAs with all other Kubernetes configs, a Deployment needs .apiVersion , .kind , and .metadata fields. For general information\nabout working with config files, see deploying applications, configuring containers, and using kubectl to manage resources\ndocuments.\nWhen the control plane creates new Pods for a Deployment, the .metadata.name of the Deployment is part of the basis for naming\nthose Pods. The name of a Deployment must be a valid DNS subdomain value, but this can produce unexpected results for the Pod\nhostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.\nA Deployment also needs a .spec section."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0161", "text": "Pod Template\nThe .spec.template and .spec.selector are the only required fields of the .spec .\nThe .spec.template is a Pod template. It has exactly the same schema as a Pod, except it is nested and does not have an\napiVersion or kind .\nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate labels and an appropriate restart\npolicy. For labels, make sure not to overlap with other controllers. See selector.\nOnly a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.\nhttps://kubernetes.io/docs/concepts/_print/\n\n151/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nReplicas\n.spec.replicas\n\nis an optional field that specifies the number of desired Pods. It defaults to 1.\n\nShould you manually scale a Deployment, example via kubectl scale deployment deployment --replicas=X , and then you update\nthat Deployment based on a manifest (for example: by running kubectl apply -f deployment.yaml ), then applying that manifest\noverwrites the manual scaling that you previously did.\nIf a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Deployment, don't set\n.spec.replicas .\nInstead, allow the Kubernetes control plane to manage the .spec.replicas field automatically.\n\nSelector\n.spec.selector\n\nis a required field that specifies a label selector for the Pods targeted by this Deployment.\n\n.spec.selector\n\nmust match .spec.template.metadata.labels , or it will be rejected by the API.\n\nIn API version apps/v1 , .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set. So\nthey must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1 .\nA Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total\nnumber of such Pods exceeds .spec.replicas . It brings up new Pods with .spec.template if the number of Pods is less than the\ndesired number.\nNote:\nYou should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by\ncreating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it\ncreated these other Pods. Kubernetes does not stop you from doing this.\nIf you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0162", "text": "Strategy\nspecifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be \"Recreate\" or\n\"RollingUpdate\". \"RollingUpdate\" is the default value.\n.spec.strategy\n\nRecreate Deployment\nAll existing Pods are killed before new ones are created when .spec.strategy.type==Recreate .\nNote:\nThis will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old\nrevision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you\nmanually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the\nold Pod is still in a Terminating state). If you need an \"at most\" guarantee for your Pods, you should consider using a\nStatefulSet.\n\nRolling Update Deployment\nThe Deployment updates Pods in a rolling update fashion (gradually scale down the old ReplicaSets and scale up the new one) when\n.spec.strategy.type==RollingUpdate . You can specify maxUnavailable and maxSurge to control the rolling update process.\n\nMax Unavailable\nis an optional field that specifies the maximum number of Pods that can be\nunavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for\nexample, 10%). The absolute number is calculated from percentage by rounding down. The value cannot be 0 if\n.spec.strategy.rollingUpdate.maxSurge is 0. The default value is 25%.\n.spec.strategy.rollingUpdate.maxUnavailable\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n152/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the\nrolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new\nReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.\n\nMax Surge\nis an optional field that specifies the maximum number of Pods that can be created over\nthe desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example,\n10%). The value cannot be 0 if maxUnavailable is 0. The absolute number is calculated from the percentage by rounding up. The\ndefault value is 25%.\n.spec.strategy.rollingUpdate.maxSurge"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0163", "text": "For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such\nthat the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new\nReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130%\nof desired Pods.\nHere are some Rolling Update Deployment examples that use the maxUnavailable and maxSurge :\nMax Unavailable\n\nMax Surge\n\nHybrid\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: nginx-deployment\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\nstrategy:\ntype: RollingUpdate\nrollingUpdate:\nmaxUnavailable: 1\n\nProgress Deadline Seconds\nis an optional field that specifies the number of seconds you want to wait for your Deployment to\nprogress before the system reports back that the Deployment has failed progressing - surfaced as a condition with type:\nProgressing , status: \"False\" . and reason: ProgressDeadlineExceeded in the status of the resource. The Deployment controller\nwill keep retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment\ncontroller will roll back a Deployment as soon as it observes such a condition.\n.spec.progressDeadlineSeconds\n\nIf specified, this field needs to be greater than .spec.minReadySeconds .\n\nMin Ready Seconds\nis an optional field that specifies the minimum number of seconds for which a newly created Pod should be\nready without any of its containers crashing, for it to be considered available. This defaults to 0 (the Pod will be considered available\nas soon as it is ready). To learn more about when a Pod is considered ready, see Container Probes.\n.spec.minReadySeconds\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n153/684\n\n11/7/25, 4:37 PM\n\nTerminating Pods\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nYou can enable this feature by setting the DeploymentReplicaSetTerminatingReplicas feature gate on the API server and on the\nkube-controller-manager\nPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional\nresources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas . Terminating pods\ncan be tracked using the .status.terminatingReplicas field of the Deployment."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0164", "text": "Revision History Limit\nA Deployment's revision history is stored in the ReplicaSets it controls.\nis an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old\nReplicaSets consume resources in etcd and crowd the output of kubectl get rs . The configuration of each Deployment revision is\nstored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment.\nBy default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.\n.spec.revisionHistoryLimit\n\nMore specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up. In this case, a new\nDeployment rollout cannot be undone, since its revision history is cleaned up.\n\nPaused\nis an optional boolean field for pausing and resuming a Deployment. The only difference between a paused\nDeployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger\nnew rollouts as long as it is paused. A Deployment is not paused by default when it is created.\n.spec.paused\n\nWhat's next\nLearn more about Pods.\nRun a stateless application using a Deployment.\nRead the Deployment to understand the Deployment API.\nRead about PodDisruptionBudget and how you can use it to manage application availability during disruptions.\nUse kubectl to create a Deployment.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n154/684\n\n11/7/25, 4:37 PM\n\n4.2.2 - ReplicaSet\n\nConcepts | Kubernetes\n\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define\na Deployment and let that Deployment manage ReplicaSets automatically.\nA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the\navailability of a specified number of identical Pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0165", "text": "How a ReplicaSet works\nA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number of replicas\nindicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods it should create to meet the\nnumber of replicas criteria. A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired\nnumber. When a ReplicaSet needs to create new Pods, it uses its Pod template.\nA ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences field, which specifies what resource the current object is\nowned by. All Pods acquired by a ReplicaSet have their owning ReplicaSet's identifying information within their ownerReferences\nfield. It's through this link that the ReplicaSet knows of the state of the Pods it is maintaining and plans accordingly.\nA ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the\nOwnerReference is not a Controller and it matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.\n\nWhen to use a ReplicaSet\nA ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level\nconcept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, we\nrecommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't\nrequire updates at all.\nThis actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your\napplication in the spec section.\n\nExample\ncontrollers/frontend.yaml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\nname: frontend\nlabels:\napp: guestbook\ntier: frontend\nspec:\n# modify replicas according to your case\nreplicas: 3\nselector:\nmatchLabels:\ntier: frontend\ntemplate:\nmetadata:\nlabels:\ntier: frontend\nspec:\ncontainers:\n- name: php-redis\nimage: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n155/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nSaving this manifest into frontend.yaml and submitting it to a Kubernetes cluster will create the defined ReplicaSet and the Pods\nthat it manages.\n\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n\nYou can then get the current ReplicaSets deployed:\n\nkubectl get rs\n\nAnd see the frontend one you created:\nNAME\n\nDESIRED\n\nCURRENT\n\nREADY\n\nAGE\n\nfrontend\n\n3\n\n3\n\n3\n\n6s\n\nYou can also check on the state of the ReplicaSet:\n\nkubectl describe rs/frontend"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0166", "text": "And you will see output similar to:\nName:\nNamespace:\n\nfrontend\ndefault\n\nSelector:\nLabels:\n\ntier=frontend\napp=guestbook\n\nAnnotations:\nReplicas:\n\ntier=frontend\n<none>\n3 current / 3 desired\n\nPods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels: tier=frontend\nContainers:\nphp-redis:\nImage:\nPort:\n\nus-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5\n<none>\n\nHost Port:\nEnvironment:\n\n<none>\n<none>\n\nMounts:\nVolumes:\nEvents:\n\n<none>\n<none>\n\nType\n----\n\nReason\n------\n\nAge\n----\n\nFrom\n----\n\nMessage\n-------\n\nNormal\nNormal\n\nSuccessfulCreate\nSuccessfulCreate\n\n13s\n13s\n\nreplicaset-controller\nreplicaset-controller\n\nCreated pod: frontend-gbgfx\nCreated pod: frontend-rwz57\n\nNormal\n\nSuccessfulCreate\n\n13s\n\nreplicaset-controller\n\nCreated pod: frontend-wkl7w\n\nAnd lastly you can check for the Pods brought up:\n\nkubectl get pods\n\nYou should see Pod information similar to:\nNAME\nfrontend-gbgfx\nfrontend-rwz57\n\nREADY\n1/1\n1/1\n\nSTATUS\nRunning\nRunning\n\nRESTARTS\n0\n0\n\nAGE\n10m\n10m\n\nfrontend-wkl7w\n\n1/1\n\nRunning\n\n0\n\n10m\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n156/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can also verify that the owner reference of these pods is set to the frontend ReplicaSet. To do this, get the yaml of one of the\nPods running:\n\nkubectl get pods frontend-gbgfx -o yaml\n\nThe output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:\n\napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: \"2024-02-28T22:30:44Z\"\ngenerateName: frontendlabels:\ntier: frontend\nname: frontend-gbgfx\nnamespace: default\nownerReferences:\n- apiVersion: apps/v1\nblockOwnerDeletion: true\ncontroller: true\nkind: ReplicaSet\nname: frontend\nuid: e129deca-f864-481b-bb16-b27abfd92292\n...\n\nNon-Template Pod acquisitions\nWhile you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have labels\nwhich match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited to owning Pods\nspecified by its template-- it can acquire other Pods in the manner specified in the previous sections.\nTake the previous frontend ReplicaSet example, and the Pods specified in the following manifest:\npods/pod-rs.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod1\nlabels:\ntier: frontend\nspec:\ncontainers:\n- name: hello1\nimage: gcr.io/google-samples/hello-app:2.0\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: pod2\nlabels:\ntier: frontend\nspec:\ncontainers:\n- name: hello2\nimage: gcr.io/google-samples/hello-app:1.0\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n157/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0167", "text": "As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend ReplicaSet,\nthey will immediately be acquired by it.\nSuppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to fulfill its\nreplica count requirement:\n\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n\nThe new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over its desired\ncount.\nFetching the Pods:\n\nkubectl get pods\n\nThe output shows that the new Pods are either already terminated, or in the process of being terminated:\nNAME\nfrontend-b2zdv\n\nREADY\n1/1\n\nSTATUS\nRunning\n\nRESTARTS\n0\n\nAGE\n10m\n\nfrontend-vcmts\nfrontend-wtsmm\n\n1/1\n1/1\n\nRunning\nRunning\n\n0\n0\n\n10m\n10m\n\npod1\npod2\n\n0/1\n0/1\n\nTerminating\nTerminating\n\n0\n0\n\n1s\n1s\n\nIf you create the Pods first:\n\nkubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml\n\nAnd then create the ReplicaSet however:\n\nkubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml\n\nYou shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the number of its\nnew Pods and the original matches its desired count. As fetching the Pods:\n\nkubectl get pods\n\nWill reveal in its output:\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nfrontend-hmmj2\npod1\npod2\n\n1/1\n1/1\n1/1\n\nRunning\nRunning\nRunning\n\n0\n0\n0\n\n9s\n36s\n36s\n\nIn this manner, a ReplicaSet can own a non-homogeneous set of Pods\n\nWriting a ReplicaSet manifest\nAs with all other Kubernetes API objects, a ReplicaSet needs the apiVersion , kind , and metadata fields. For ReplicaSets, the kind\nis always a ReplicaSet.\nhttps://kubernetes.io/docs/concepts/_print/\n\n158/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen the control plane creates new Pods for a ReplicaSet, the .metadata.name of the ReplicaSet is part of the basis for naming\nthose Pods. The name of a ReplicaSet must be a valid DNS subdomain value, but this can produce unexpected results for the Pod\nhostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.\nA ReplicaSet also needs a .spec section."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0168", "text": "Pod Template\nThe .spec.template is a pod template which is also required to have labels in place. In our frontend.yaml example we had one\nlabel: tier: frontend . Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.\nFor the template's restart policy field, .spec.template.spec.restartPolicy , the only allowed value is Always , which is the default.\n\nPod Selector\nThe .spec.selector field is a label selector. As discussed earlier these are the labels used to identify potential Pods to acquire. In\nour frontend.yaml example, the selector was:\n\nmatchLabels:\ntier: frontend\n\nIn the ReplicaSet, .spec.template.metadata.labels must match spec.selector , or it will be rejected by the API.\nNote:\nFor 2 ReplicaSets specifying the same .spec.selector but different .spec.template.metadata.labels and\n.spec.template.spec fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.\n\nReplicas\nYou can specify how many Pods should run concurrently by setting .spec.replicas . The ReplicaSet will create/delete its Pods to\nmatch this number.\nIf you do not specify .spec.replicas , then it defaults to 1.\n\nWorking with ReplicaSets\nDeleting a ReplicaSet and its Pods\nTo delete a ReplicaSet and all of its Pods, use kubectl delete . The Garbage collector automatically deletes all of the dependent\nPods by default.\nWhen using the REST API or the client-go library, you must set propagationPolicy to Background or Foreground in the -d\noption. For example:\n\nkubectl proxy --port=8080\ncurl -X DELETE 'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Foreground\"}' \\\n-H \"Content-Type: application/json\"\n\nDeleting just a ReplicaSet\nYou can delete a ReplicaSet without affecting any of its Pods using kubectl delete with the --cascade=orphan option. When using\nthe REST API or the client-go library, you must set propagationPolicy to Orphan . For example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n159/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl proxy --port=8080\ncurl -X DELETE 'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \\\n-d '{\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Orphan\"}' \\\n-H \"Content-Type: application/json\"\n\nOnce the original is deleted, you can create a new ReplicaSet to replace it. As long as the old and new .spec.selector are the same,\nthen the new one will adopt the old Pods. However, it will not make any effort to make existing Pods match a new, different pod\ntemplate. To update Pods to a new spec in a controlled way, use a Deployment, as ReplicaSets do not support a rolling update\ndirectly.\n\nTerminating Pods\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0169", "text": "You can enable this feature by setting the DeploymentReplicaSetTerminatingReplicas feature gate on the API server and on the\nkube-controller-manager\nPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional\nresources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas . Terminating pods\ncan be tracked using the .status.terminatingReplicas field of the ReplicaSet.\n\nIsolating Pods from a ReplicaSet\nYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods from service for\ndebugging, data recovery, etc. Pods that are removed in this way will be replaced automatically ( assuming that the number of\nreplicas is not also changed).\n\nScaling a ReplicaSet\nA ReplicaSet can be easily scaled up or down by simply updating the .spec.replicas field. The ReplicaSet controller ensures that a\ndesired number of Pods with a matching label selector are available and operational.\nWhen scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to prioritize scaling down\npods based on the following general algorithm:\n1. Pending (and unschedulable) pods are scaled down first\n2. If controller.kubernetes.io/pod-deletion-cost annotation is set, then the pod with the lower value will come first.\n3. Pods on nodes with more replicas come before pods on nodes with fewer replicas.\n4. If the pods' creation times differ, the pod that was created more recently comes before the older pod (the creation times are\nbucketed on an integer log scale).\nIf all of the above match, then selection is random.\n\nPod deletion cost\nâ“˜ FEATURE STATE: Kubernetes v1.22 [beta]\n\nUsing the controller.kubernetes.io/pod-deletion-cost annotation, users can set a preference regarding which pods to remove\nfirst when downscaling a ReplicaSet.\nThe annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of deleting a pod compared\nto other pods belonging to the same ReplicaSet. Pods with lower deletion cost are preferred to be deleted before pods with higher\ndeletion cost.\nThe implicit value for this annotation for pods that don't set it is 0; negative values are permitted. Invalid values will be rejected by\nthe API server.\nThis feature is beta and enabled by default. You can disable it using the feature gate PodDeletionCost in both kube-apiserver and\nkube-controller-manager.\nhttps://kubernetes.io/docs/concepts/_print/\n\n160/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0170", "text": "Note:\nThis is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.\nUsers should avoid updating the annotation frequently, such as updating it based on a metric value, because doing so\nwill generate a significant number of pod updates on the apiserver.\n\nExample Use Case\nThe different pods of an application could have different utilization levels. On scale down, the application may prefer to remove the\npods with lower utilization. To avoid frequently updating the pods, the application should update controller.kubernetes.io/poddeletion-cost once before issuing a scale down (setting the annotation to a value proportional to pod utilization level). This works if\nthe application itself controls the down scaling; for example, the driver pod of a Spark deployment.\n\nReplicaSet as a Horizontal Pod Autoscaler Target\nA ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA). That is, a ReplicaSet can be auto-scaled by an HPA. Here is an\nexample HPA targeting the ReplicaSet we created in the previous example.\ncontrollers/hpa-rs.yaml\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\nname: frontend-scaler\nspec:\nscaleTargetRef:\napiVersion: apps/v1\nkind: ReplicaSet\nname: frontend\nminReplicas: 3\nmaxReplicas: 10\ntargetCPUUtilizationPercentage: 50\n\nSaving this manifest into hpa-rs.yaml and submitting it to a Kubernetes cluster should create the defined HPA that autoscales the\ntarget ReplicaSet depending on the CPU usage of the replicated Pods.\n\nkubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml\n\nAlternatively, you can use the kubectl autoscale command to accomplish the same (and it's easier!)\n\nkubectl autoscale rs frontend --max=10 --min=3 --cpu=50%\n\nAlternatives to ReplicaSet\nDeployment (recommended)\nis an object which can own ReplicaSets and update them and their Pods via declarative, server-side rolling updates.\nWhile ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod\ncreation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that they\ncreate. Deployments own and manage their ReplicaSets. As such, it is recommended to use Deployments when you want\nReplicaSets.\nDeployment\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n161/684\n\n11/7/25, 4:37 PM\n\nBare Pods\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0171", "text": "Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as\nin the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a\nReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple\nPods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some\nagent on the node such as Kubelet.\n\nJob\nUse a Job instead of a ReplicaSet for Pods that are expected to terminate on their own (that is, batch jobs).\n\nDaemonSet\nUse a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level function, such as machine monitoring or machine\nlogging. These Pods have a lifetime that is tied to a machine lifetime: the Pod needs to be running on the machine before other Pods\nstart, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n\nReplicationController\nReplicaSets are the successors to ReplicationControllers. The two serve the same purpose, and behave similarly, except that a\nReplicationController does not support set-based selector requirements as described in the labels user guide. As such, ReplicaSets\nare preferred over ReplicationControllers\n\nWhat's next\nLearn about Pods.\nLearn about Deployments.\nRun a Stateless Application Using a Deployment, which relies on ReplicaSets to work.\nReplicaSet is a top-level resource in the Kubernetes REST API. Read the ReplicaSet object definition to understand the API for\nreplica sets.\nRead about PodDisruptionBudget and how you can use it to manage application availability during disruptions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n162/684\n\n11/7/25, 4:37 PM\n\n4.2.3 - StatefulSets\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0172", "text": "A StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for\nmanaging applications that need persistent storage or a stable, unique network identity.\nStatefulSet is the workload API object used to manage stateful applications.\nManages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.\nLike a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet\nmaintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a\npersistent identifier that it maintains across any rescheduling.\nIf you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution.\nAlthough individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing\nvolumes to the new Pods that replace any that have failed.\n\nUsing StatefulSets\nStatefulSets are valuable for applications that require one or more of the following:\nStable, unique network identifiers.\nStable, persistent storage.\nOrdered, graceful deployment and scaling.\nOrdered, automated rolling updates.\nIn the above, stable is synonymous with persistence across Pod (re)scheduling. If an application doesn't require any stable identifiers\nor ordered deployment, deletion, or scaling, you should deploy your application using a workload object that provides a set of\nstateless replicas. Deployment or ReplicaSet may be better suited to your stateless needs."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0173", "text": "Limitations\nThe storage for a given Pod must either be provisioned by a PersistentVolume Provisioner based on the requested storage\nclass, or pre-provisioned by an admin.\nDeleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure\ndata safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.\nStatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for\ncreating this Service.\nStatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and\ngraceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.\nWhen using Rolling Updates with the default Pod Management Policy ( OrderedReady ), it's possible to get into a broken state\nthat requires manual intervention to repair.\n\nComponents\nThe example below demonstrates the components of a StatefulSet.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n163/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: nginx\nlabels:\napp: nginx\nspec:\nports:\n- port: 80\nname: web\nclusterIP: None\nselector:\napp: nginx\n--apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: web\nspec:\nselector:\nmatchLabels:\napp: nginx # has to match .spec.template.metadata.labels\nserviceName: \"nginx\"\nreplicas: 3 # by default is 1\nminReadySeconds: 10 # by default is 0\ntemplate:\nmetadata:\nlabels:\napp: nginx # has to match .spec.selector.matchLabels\nspec:\nterminationGracePeriodSeconds: 10\ncontainers:\n- name: nginx\nimage: registry.k8s.io/nginx-slim:0.24\nports:\n- containerPort: 80\nname: web\nvolumeMounts:\n- name: www\nmountPath: /usr/share/nginx/html\nvolumeClaimTemplates:\n- metadata:\nname: www\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nstorageClassName: \"my-storage-class\"\nresources:\nrequests:\nstorage: 1Gi\n\nNote:\nThis example uses the ReadWriteOnce access mode, for simplicity. For production use, the Kubernetes project recommends\nusing the ReadWriteOncePod access mode instead.\nIn the above example:\nA Headless Service, named nginx , is used to control the network domain.\nThe StatefulSet, named web , has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.\nThe volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.\nThe name of a StatefulSet object must be a valid DNS label.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n164/684\n\n11/7/25, 4:37 PM\n\nPod Selector\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0174", "text": "You must set the .spec.selector field of a StatefulSet to match the labels of its .spec.template.metadata.labels . Failing to specify\na matching Pod Selector will result in a validation error during StatefulSet creation.\n\nVolume Claim Templates\nYou can set the .spec.volumeClaimTemplates field to create a PersistentVolumeClaim. This will provide stable storage to the\nStatefulSet if either:\nThe StorageClass specified for the volume claim is set up to use dynamic provisioning.\nThe cluster already contains a PersistentVolume with the correct StorageClass and sufficient available storage space.\n\nMinimum ready seconds\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nis an optional field that specifies the minimum number of seconds for which a newly created Pod should be\nrunning and ready without any of its containers crashing, for it to be considered available. This is used to check progression of a\nrollout when using a Rolling Update strategy. This field defaults to 0 (the Pod will be considered available as soon as it is ready). To\nlearn more about when a Pod is considered ready, see Container Probes.\n.spec.minReadySeconds\n\nPod Identity\nStatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity, and stable storage. The identity sticks to\nthe Pod, regardless of which node it's (re)scheduled on.\n\nOrdinal Index\nFor a StatefulSet with N replicas, each Pod in the StatefulSet will be assigned an integer ordinal, that is unique over the Set. By\ndefault, pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller will also add a pod label with this index:\napps.kubernetes.io/pod-index .\n\nStart ordinal\nâ“˜ FEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)\n\nis an optional field that allows you to configure the integer ordinals assigned to each Pod. It defaults to nil. Within\nthe field, you can configure the following options:\n.spec.ordinals\n\n.spec.ordinals.start : If the .spec.ordinals.start\n\nfield is set, Pods will be assigned ordinals from .spec.ordinals.start\nup through .spec.ordinals.start + .spec.replicas - 1 ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0175", "text": "Stable Network ID\nEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod. The pattern for the\nconstructed hostname is $(statefulset name)-$(ordinal) . The example above will create three Pods named web-0,web-1,web-2 .\nA StatefulSet can use a Headless Service to control the domain of its Pods. The domain managed by this Service takes the form:\n$(service name).$(namespace).svc.cluster.local , where \"cluster.local\" is the cluster domain. As each Pod is created, it gets a\nmatching DNS subdomain, taking the form: $(podname).$(governing service domain) , where the governing service is defined by\nthe serviceName field on the StatefulSet.\nDepending on how DNS is configured in your cluster, you may not be able to look up the DNS name for a newly-run Pod\nimmediately. This behavior can occur when other clients in the cluster have already sent queries for the hostname of the Pod before\nit was created. Negative caching (normal in DNS) means that the results of previous failed lookups are remembered and reused,\neven after the Pod is running, for at least a few seconds.\nIf you need to discover Pods promptly after they are created, you have a few options:\nhttps://kubernetes.io/docs/concepts/_print/\n\n165/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nQuery the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.\nDecrease the time of caching in your Kubernetes DNS provider (typically this means editing the config map for CoreDNS, which\ncurrently caches for 30 seconds).\nAs mentioned in the limitations section, you are responsible for creating the Headless Service responsible for the network identity of\nthe pods.\nHere are some examples of choices for Cluster Domain, Service name, StatefulSet name, and how that affects the DNS names for\nthe StatefulSet's Pods.\nCluster\nDomain\n\nService\n(ns/name)\n\nStatefulSet\n(ns/name)\n\nStatefulSet Domain\n\nPod DNS\n\nPod\nHostname\n\ncluster.local\n\ndefault/nginx\n\ndefault/web\n\nnginx.default.svc.cluster.local\n\nweb-{0..N1}.nginx.default.svc.cluster.local\n\nweb-{0..N1}\n\ncluster.local\n\nfoo/nginx\n\nfoo/web\n\nnginx.foo.svc.cluster.local\n\nweb-{0..N1}.nginx.foo.svc.cluster.local\n\nweb-{0..N1}\n\nkube.local\n\nfoo/nginx\n\nfoo/web\n\nnginx.foo.svc.kube.local\n\nweb-{0..N1}.nginx.foo.svc.kube.local\n\nweb-{0..N1}\n\nNote:\nCluster Domain will be set to cluster.local unless otherwise configured."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0176", "text": "Stable Storage\nFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one PersistentVolumeClaim. In the nginx example\nabove, each Pod receives a single PersistentVolume with a StorageClass of my-storage-class and 1 GiB of provisioned storage. If no\nStorageClass is specified, then the default StorageClass will be used. When a Pod is (re)scheduled onto a node, its volumeMounts\nmount the PersistentVolumes associated with its PersistentVolume Claims. Note that, the PersistentVolumes associated with the\nPods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.\n\nPod Name Label\nWhen the StatefulSet controller creates a Pod, it adds a label, statefulset.kubernetes.io/pod-name , that is set to the name of the\nPod. This label allows you to attach a Service to a specific Pod in the StatefulSet.\n\nPod index label\nâ“˜ FEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)\n\nWhen the StatefulSet controller creates a Pod, the new Pod is labelled with apps.kubernetes.io/pod-index . The value of this label is\nthe ordinal index of the Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics using the pod index\nlabel, and more. Note the feature gate PodIndexLabel is enabled and locked by default for this feature, in order to disable it, users\nwill have to use server emulated version v1.31.\n\nDeployment and Scaling Guarantees\nFor a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.\nWhen Pods are being deleted, they are terminated in reverse order, from {N-1..0}.\nBefore a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.\nBefore a Pod is terminated, all of its successors must be completely shutdown.\nThe StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly\ndiscouraged. For further explanation, please refer to force deleting StatefulSet Pods.\nhttps://kubernetes.io/docs/concepts/_print/\n\n166/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0177", "text": "When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed\nbefore web-0 is Running and Ready, and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after\nweb-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and\nbecomes Running and Ready.\nIf a user were to scale the deployed example by patching the StatefulSet such that replicas=1 , web-2 would be terminated first.\nweb-1 would not be terminated until web-2 is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and\nis completely shutdown, but prior to web-1's termination, web-1 would not be terminated until web-0 is Running and Ready.\n\nPod Management Policies\nStatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and identity guarantees via its\n.spec.podManagementPolicy field.\n\nOrderedReady Pod Management\nOrderedReady\n\npod management is the default for StatefulSets. It implements the behavior described in Deployment and Scaling\n\nGuarantees.\n\nParallel Pod Management\npod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and to not wait for Pods to\nbecome Running and Ready or completely terminated prior to launching or terminating another Pod. This option only affects the\nbehavior for scaling operations. Updates are not affected.\nParallel\n\nUpdate strategies\nA StatefulSet's .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels,\nresource request/limits, and annotations for the Pods in a StatefulSet. There are two possible values:\nOnDelete\n\nWhen a StatefulSet's .spec.updateStrategy.type is set to OnDelete, the StatefulSet controller will not automatically update the\nPods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications\nmade to a StatefulSet's .spec.template.\nRollingUpdate\n\nThe RollingUpdate update strategy implements automated, rolling updates for the Pods in a StatefulSet. This is the default\nupdate strategy."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0178", "text": "Rolling Updates\nWhen a StatefulSet's .spec.updateStrategy.type is set to RollingUpdate , the StatefulSet controller will delete and recreate each\nPod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each\nPod one at a time.\nThe Kubernetes control plane waits until an updated Pod is Running and Ready prior to updating its predecessor. If you have set\n.spec.minReadySeconds (see Minimum Ready Seconds), the control plane additionally waits that amount of time after the Pod turns\nready, before moving on.\n\nPartitioned rolling updates\nThe RollingUpdate update strategy can be partitioned, by specifying a .spec.updateStrategy.rollingUpdate.partition . If a\npartition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet's\n.spec.template is updated. All Pods with an ordinal that is less than the partition will not be updated, and, even if they are deleted,\nthey will be recreated at the previous version. If a StatefulSet's .spec.updateStrategy.rollingUpdate.partition is greater than its\n.spec.replicas , updates to its .spec.template will not be propagated to its Pods. In most cases you will not need to use a\npartition, but they are useful if you want to stage an update, roll out a canary, or perform a phased roll out.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n167/684\n\n11/7/25, 4:37 PM\n\nMaximum unavailable Pods\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.24 [alpha]\n\nYou can control the maximum number of Pods that can be unavailable during an update by specifying the\n.spec.updateStrategy.rollingUpdate.maxUnavailable field. The value can be an absolute number (for example, 5 ) or a percentage\nof desired Pods (for example, 10% ). Absolute number is calculated from the percentage value by rounding it up. This field cannot be\n0. The default setting is 1.\nThis field applies to all Pods in the range 0 to replicas - 1 . If there is any unavailable Pod in the range 0 to replicas - 1 , it will\nbe counted towards maxUnavailable .\nNote:\nThe maxUnavailable field is in Alpha stage and it is honored only by API servers that are running with the\nMaxUnavailableStatefulSet feature gate enabled."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0179", "text": "Forced rollback\nWhen using Rolling Updates with the default Pod Management Policy ( OrderedReady ), it's possible to get into a broken state that\nrequires manual intervention to repair.\nIf you update the Pod template to a configuration that never becomes Running and Ready (for example, due to a bad binary or\napplication-level configuration error), StatefulSet will stop the rollout and wait.\nIn this state, it's not enough to revert the Pod template to a good configuration. Due to a known issue, StatefulSet will continue to\nwait for the broken Pod to become Ready (which never happens) before it will attempt to revert it back to the working configuration.\nAfter reverting the template, you must also delete any Pods that StatefulSet had already attempted to run with the bad\nconfiguration. StatefulSet will then begin to recreate the Pods using the reverted template.\n\nRevision history\nControllerRevision is a Kubernetes API resource used by controllers, such as the StatefulSet controller, to track historical\nconfiguration changes.\nStatefulSets use ControllerRevisions to maintain a revision history, enabling rollbacks and version tracking.\n\nHow StatefulSets track changes using ControllerRevisions\nWhen you update a StatefulSet's Pod template ( spec.template ), the StatefulSet controller:\n1. Prepares a new ControllerRevision object\n2. Stores a snapshot of the Pod template and metadata\n3. Assigns an incremental revision number\n\nKey Properties\nSee ControllerRevision to learn more about key properties and other details.\n\nManaging Revision History\nControl retained revisions with .spec.revisionHistoryLimit :\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n168/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: webapp\nspec:\nrevisionHistoryLimit: 5\n\n# Keep last 5 revisions\n\n# ... other spec fields ...\n\nDefault: 10 revisions retained if unspecified\nCleanup: Oldest revisions are garbage-collected when exceeding the limit\n\nPerforming Rollbacks\nYou can revert to a previous configuration using:\n\n# View revision history\nkubectl rollout history statefulset/webapp\n# Rollback to a specific revision\nkubectl rollout undo statefulset/webapp --to-revision=3\n\nThis will:\nApply the Pod template from revision 3\nCreate a new ControllerRevision with an updated revision number\n\nInspecting ControllerRevisions\nTo view associated ControllerRevisions:\n\n# List all revisions for the StatefulSet\nkubectl get controllerrevisions -l app.kubernetes.io/name=webapp\n# View detailed configuration of a specific revision\nkubectl get controllerrevision/webapp-3 -o yaml\n\nBest Practices\nRetention Policy\nSet revisionHistoryLimit between 5â€“10 for most workloads.\nIncrease only if deep rollback history is required.\n\nMonitoring\nRegularly check revisions with:\n\nkubectl get controllerrevisions"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0180", "text": "Alert on rapid revision count growth.\n\nAvoid\nManual edits to ControllerRevision objects.\nUsing revisions as a backup mechanism (use actual backup tools).\nhttps://kubernetes.io/docs/concepts/_print/\n\n169/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nSetting revisionHistoryLimit: 0 (disables rollback capability).\n\nPersistentVolumeClaim retention\nâ“˜ FEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)\n\nThe optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a\nStatefulSet. You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this\nfield. Once enabled, there are two policies you can configure for each StatefulSet:\nwhenDeleted\n\nConfigures the volume retention behavior that applies when the StatefulSet is deleted.\nwhenScaled\n\nConfigures the volume retention behavior that applies when the replica count of the StatefulSet is reduced; for example, when\nscaling down the set.\nFor each policy that you can configure, you can set the value to either Delete or Retain .\nDelete\n\nThe PVCs created from the StatefulSet volumeClaimTemplate are deleted for each Pod affected by the policy. With the\nwhenDeleted policy all PVCs from the volumeClaimTemplate are deleted after their Pods have been deleted. With the whenScaled\npolicy, only PVCs corresponding to Pod replicas being scaled down are deleted, after their Pods have been deleted.\nRetain (default)\n\nPVCs from the volumeClaimTemplate are not affected when their Pod is deleted. This is the behavior before this new feature.\nBear in mind that these policies only apply when Pods are being removed due to the StatefulSet being deleted or scaled down. For\nexample, if a Pod associated with a StatefulSet fails due to node failure, and the control plane creates a replacement Pod, the\nStatefulSet retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to the node where the new Pod is\nabout to launch.\nThe default for policies is Retain , matching the StatefulSet behavior before this new feature.\nHere is an example policy:\n\napiVersion: apps/v1\nkind: StatefulSet\n...\nspec:\npersistentVolumeClaimRetentionPolicy:\nwhenDeleted: Retain\nwhenScaled: Delete\n..."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0181", "text": "The StatefulSet controller adds owner references to its PVCs, which are then deleted by the garbage collector after the Pod is\nterminated. This enables the Pod to cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and\nvolume are deleted, depending on the retain policy). When you set the whenDeleted policy to Delete , an owner reference to the\nStatefulSet instance is placed on all PVCs associated with that StatefulSet.\nThe whenScaled policy must delete PVCs only when a Pod is scaled down, and not when a Pod is deleted for another reason. When\nreconciling, the StatefulSet controller compares its desired replica count to the actual Pods present on the cluster. Any StatefulSet\nPod whose id greater than the replica count is condemned and marked for deletion. If the whenScaled policy is Delete , the\ncondemned Pods are first set as owners to the associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs\nto be garbage collected after only the condemned Pods have terminated.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n170/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThis means that if the controller crashes and restarts, no Pod will be deleted before its owner reference has been updated\nappropriate to the policy. If a condemned Pod is force-deleted while the controller is down, the owner reference may or may not\nhave been set up, depending on when the controller crashed. It may take several reconcile loops to update the owner references, so\nsome condemned Pods may have set up owner references and others may not. For this reason we recommend waiting for the\ncontroller to come back up, which will verify owner references before terminating Pods. If that is not possible, the operator should\nverify the owner references on PVCs to ensure the expected objects are deleted when Pods are force-deleted.\n\nReplicas\n.spec.replicas\n\nis an optional field that specifies the number of desired Pods. It defaults to 1.\n\nShould you manually scale a deployment, example via kubectl scale statefulset statefulset --replicas=X , and then you\nupdate that StatefulSet based on a manifest (for example: by running kubectl apply -f statefulset.yaml ), then applying that\nmanifest overwrites the manual scaling that you previously did.\nIf a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Statefulset, don't set\n.spec.replicas . Instead, allow the Kubernetes control plane to manage the .spec.replicas field automatically."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0182", "text": "What's next\nLearn about Pods.\nFind out how to use StatefulSets\nFollow an example of deploying a stateful application.\nFollow an example of deploying Cassandra with Stateful Sets.\nFollow an example of running a replicated stateful application.\nLearn how to scale a StatefulSet.\nLearn what's involved when you delete a StatefulSet.\nLearn how to configure a Pod to use a volume for storage.\nLearn how to configure a Pod to use a PersistentVolume for storage.\nStatefulSet is a top-level resource in the Kubernetes REST API. Read the StatefulSet object definition to understand the API\nfor stateful sets.\nRead about PodDisruptionBudget and how you can use it to manage application availability during disruptions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n171/684\n\n11/7/25, 4:37 PM\n\n4.2.4 - DaemonSet\n\nConcepts | Kubernetes\n\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of\nyour cluster, such as a networking helper tool, or be part of an add-on.\nA DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As\nnodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nSome typical uses of a DaemonSet are:\nrunning a cluster storage daemon on every node\nrunning a logs collection daemon on every node\nrunning a node monitoring daemon on every node\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use\nmultiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different\nhardware types.\n\nWriting a DaemonSet Spec\nCreate a DaemonSet\nYou can describe a DaemonSet in a YAML file. For example, the daemonset.yaml file below describes a DaemonSet that runs the\nfluentd-elasticsearch Docker image:\ncontrollers/daemonset.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n172/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0183", "text": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\nname: fluentd-elasticsearch\nnamespace: kube-system\nlabels:\nk8s-app: fluentd-logging\nspec:\nselector:\nmatchLabels:\nname: fluentd-elasticsearch\ntemplate:\nmetadata:\nlabels:\nname: fluentd-elasticsearch\nspec:\ntolerations:\n# these tolerations are to have the daemonset runnable on control plane nodes\n# remove them if your control plane nodes should not run pods\n- key: node-role.kubernetes.io/control-plane\noperator: Exists\neffect: NoSchedule\n- key: node-role.kubernetes.io/master\noperator: Exists\neffect: NoSchedule\ncontainers:\n- name: fluentd-elasticsearch\nimage: quay.io/fluentd_elasticsearch/fluentd:v5.0.1\nresources:\nlimits:\nmemory: 200Mi\nrequests:\ncpu: 100m\nmemory: 200Mi\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\n# it may be desirable to set a high priority class to ensure that a DaemonSet Pod\n# preempts running Pods\n# priorityClassName: important\nterminationGracePeriodSeconds: 30\nvolumes:\n- name: varlog\nhostPath:\npath: /var/log\n\nCreate a DaemonSet based on the YAML file:\nkubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml\n\nRequired Fields\nAs with all other Kubernetes config, a DaemonSet needs apiVersion , kind , and metadata fields. For general information about\nworking with config files, see running stateless applications and object management using kubectl.\nThe name of a DaemonSet object must be a valid DNS subdomain name.\nA DaemonSet also needs a .spec section.\n\nPod Template\nThe .spec.template is one of the required fields in .spec .\nhttps://kubernetes.io/docs/concepts/_print/\n\n173/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an\napiVersion or kind .\nIn addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate labels (see pod selector).\nA Pod Template in a DaemonSet must have a RestartPolicy equal to Always , or be unspecified, which defaults to Always .\n\nPod Selector\nThe .spec.selector field is a pod selector. It works the same as the .spec.selector of a Job.\nYou must specify a pod selector that matches the labels of the .spec.template . Also, once a DaemonSet is created, its\n.spec.selector can not be mutated. Mutating the pod selector can lead to the unintentional orphaning of Pods, and it was found\nto be confusing to users.\nThe .spec.selector is an object consisting of two fields:\nmatchLabels\n\n- works the same as the .spec.selector of a ReplicationController.\n\nmatchExpressions\n\n- allows to build more sophisticated selectors by specifying key, list of values and an operator that relates"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0184", "text": "the key and values.\nWhen the two are specified the result is ANDed.\nThe .spec.selector must match the .spec.template.metadata.labels . Config with these two not matching will be rejected by the\nAPI.\n\nRunning Pods on select Nodes\nIf you specify a .spec.template.spec.nodeSelector , then the DaemonSet controller will create Pods on nodes which match that\nnode selector. Likewise if you specify a .spec.template.spec.affinity , then DaemonSet controller will create Pods on nodes which\nmatch that node affinity. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.\n\nHow Daemon Pods are scheduled\nA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet controller creates a Pod for each\neligible node and adds the spec.affinity.nodeAffinity field of the Pod to match the target host. After the Pod is created, the\ndefault scheduler typically takes over and then binds the Pod to the target host by setting the .spec.nodeName field. If the new Pod\ncannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods based on the priority of the new Pod.\nNote:\nIf it's important that the DaemonSet pod run on each node, it's often desirable to set the\n.spec.template.spec.priorityClassName of the DaemonSet to a PriorityClass with a higher priority to ensure that this eviction\noccurs.\nThe user can specify a different scheduler for the Pods of the DaemonSet, by setting the .spec.template.spec.schedulerName field\nof the DaemonSet.\nThe original node affinity specified at the .spec.template.spec.affinity.nodeAffinity field (if specified) is taken into consideration\nby the DaemonSet controller when evaluating the eligible nodes, but is replaced on the created Pod with the node affinity that\nmatches the name of the eligible node.\n\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchFields:\n- key: metadata.name\noperator: In\nvalues:\n- target-host-name\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n174/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTaints and tolerations\n\nThe DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods:\nToleration key\n\nEffect\n\nDetails\n\nnode.kubernetes.io/not-ready\n\nNoExecute\n\nDaemonSet Pods can be scheduled onto nodes that are not\nhealthy or ready to accept Pods. Any DaemonSet Pods running\non such nodes will not be evicted.\n\nnode.kubernetes.io/unreachable\n\nNoExecute\n\nDaemonSet Pods can be scheduled onto nodes that are\nunreachable from the node controller. Any DaemonSet Pods\nrunning on such nodes will not be evicted.\n\nnode.kubernetes.io/disk-pressure\n\nNoSchedule\n\nDaemonSet Pods can be scheduled onto nodes with disk\npressure issues."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0185", "text": "node.kubernetes.io/memory-\n\nNoSchedule\n\nDaemonSet Pods can be scheduled onto nodes with memory\npressure issues.\n\nnode.kubernetes.io/pid-pressure\n\nNoSchedule\n\nDaemonSet Pods can be scheduled onto nodes with process\npressure issues.\n\nnode.kubernetes.io/unschedulable\n\nNoSchedule\n\nDaemonSet Pods can be scheduled onto nodes that are\nunschedulable.\n\nnode.kubernetes.io/network-\n\nNoSchedule\n\nOnly added for DaemonSet Pods that request host\nnetworking, i.e., Pods having spec.hostNetwork: true .\nSuch DaemonSet Pods can be scheduled onto nodes with\nunavailable network.\n\npressure\n\nunavailable\n\nYou can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the Pod template of the DaemonSet.\nBecause the DaemonSet controller sets the node.kubernetes.io/unschedulable:NoSchedule toleration automatically, Kubernetes\ncan run DaemonSet Pods on nodes that are marked as unschedulable.\nIf you use a DaemonSet to provide an important node-level function, such as cluster networking, it is helpful that Kubernetes places\nDaemonSet Pods on nodes before they are ready. For example, without that special toleration, you could end up in a deadlock\nsituation where the node is not marked as ready because the network plugin is not running there, and at the same time the network\nplugin is not running on that node because the node is not yet ready.\n\nCommunicating with Daemon Pods\nSome possible patterns for communicating with Pods in a DaemonSet are:\nPush: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have\nclients.\nNodeIP and Known Port: Pods in the DaemonSet can use a hostPort , so that the pods are reachable via the node IPs. Clients\nknow the list of node IPs somehow, and know the port by convention.\nDNS: Create a headless service with the same pod selector, and then discover DaemonSets using the endpoints resource or\nretrieve multiple A records from DNS.\nService: Create a service with the same Pod selector, and use the service to reach a daemon on a random node. Use Service\nInternal Traffic Policy to limit to pods on the same node.\n\nUpdating a DaemonSet\nIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly notmatching nodes.\nhttps://kubernetes.io/docs/concepts/_print/\n\n175/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0186", "text": "You can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be updated. Also, the DaemonSet\ncontroller will use the original template the next time a node (even with the same name) is created.\nYou can delete a DaemonSet. If you specify --cascade=orphan with kubectl , then the Pods will be left on the nodes. If you\nsubsequently create a new DaemonSet with the same selector, the new DaemonSet adopts the existing Pods. If any Pods need\nreplacing the DaemonSet replaces them according to its updateStrategy .\nYou can perform a rolling update on a DaemonSet.\n\nAlternatives to DaemonSet\nInit scripts\nIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using init , upstartd , or systemd ). This is\nperfectly fine. However, there are several advantages to running such processes via a DaemonSet:\nAbility to monitor and manage logs for daemons in the same way as applications.\nSame config language and tools (e.g. Pod templates, kubectl ) for daemons and applications.\nRunning daemons in containers with resource limits increases isolation between daemons from app containers. However, this\ncan also be accomplished by running the daemons in a container but not in a Pod.\n\nBare Pods\nIt is possible to create Pods directly which specify a particular node to run on. However, a DaemonSet replaces Pods that are deleted\nor terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this\nreason, you should use a DaemonSet rather than creating individual Pods.\n\nStatic Pods\nIt is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike\nDaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the\napiserver, making them useful in cluster bootstrapping cases. Also, static Pods may be deprecated in the future."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0187", "text": "Deployments\nDaemonSets are similar to Deployments in that they both create Pods, and those Pods have processes which are not expected to\nterminate (e.g. web servers, storage servers).\nUse a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates\nare more important than controlling exactly which host the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod\nalways run on all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that\nparticular node.\nFor example, network plugins often include a component that runs as a DaemonSet. The DaemonSet component makes sure that\nthe node where it's running has working cluster networking.\n\nWhat's next\nLearn about Pods:\nLearn about static Pods, which are useful for running Kubernetes control plane components.\nFind out how to use DaemonSets:\nPerform a rolling update on a DaemonSet.\nPerform a rollback on a DaemonSet (for example, if a roll out didn't work how you expected).\nUnderstand how Kubernetes assigns Pods to Nodes.\nLearn about device plugins and add ons, which often run as DaemonSets.\nDaemonSet is a top-level resource in the Kubernetes REST API. Read the DaemonSet object definition to understand the API for\ndaemon sets.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n176/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.2.5 - Jobs\n\nJobs represent one-off tasks that run to completion and then stop.\nA Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully\nterminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful\ncompletions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its\nactive Pods until the Job is resumed again.\nA simple case is to create one Job object in order to reliably run one Pod to completion. The Job object will start a new Pod if the first\nPod fails or is deleted (for example due to a node hardware failure or a node reboot).\nYou can also use a Job to run multiple Pods in parallel.\nIf you want to run a Job (either a single task, or several in parallel) on a schedule, see CronJob."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0188", "text": "Running an example Job\nHere is an example Job config. It computes Ï€ to 2000 places and prints it out. It takes around 10s to complete.\ncontrollers/job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi\nspec:\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\",\n\n\"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n\nrestartPolicy: Never\nbackoffLimit: 4\n\nYou can run the example with this command:\n\nkubectl apply -f https://kubernetes.io/examples/controllers/job.yaml\n\nThe output is similar to this:\njob.batch/pi created\n\nCheck on the status of the Job with kubectl :\nkubectl describe job pi\n\nhttps://kubernetes.io/docs/concepts/_print/\n\nkubectl get job pi -o yaml\n\n177/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\n\npi\n\nNamespace:\nSelector:\n\ndefault\nbatch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\n\nLabels:\n\nbatch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\n...\n\nAnnotations:\nParallelism:\n\nbatch.kubernetes.io/job-tracking: \"\"\n1\n\nCompletions:\nStart Time:\nCompleted At:\n\n1\nMon, 02 Dec 2019 15:20:11 +0200\nMon, 02 Dec 2019 15:21:16 +0200\n\nDuration:\nPods Statuses:\n\n65s\n0 Running / 1 Succeeded / 0 Failed\n\nPod Template:\nLabels: batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c\nbatch.kubernetes.io/job-name=pi\nContainers:\npi:\nImage:\nPort:\n\nperl:5.34.0\n<none>\n\nHost Port:\nCommand:\n\n<none>\n\nperl\n-Mbignum=bpi\n-wle\nprint bpi(2000)\nEnvironment: <none>\nMounts:\nVolumes:\nEvents:\n\n<none>\n<none>\n\nType\n----\n\nReason\n------\n\nAge\n----\n\nFrom\n----\n\nMessage\n-------\n\nNormal\nNormal\n\nSuccessfulCreate\nCompleted\n\n21s\n18s\n\njob-controller\njob-controller\n\nCreated pod: pi-xf9p4\nJob completed\n\nTo view completed Pods of a Job, use kubectl get pods .\nTo list all the Pods that belong to a Job in a machine readable form, you can use a command like this:\n\npods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')\necho $pods\n\nThe output is similar to this:\npi-5rwd7\n\nHere, the selector is the same as the selector for the Job. The --output=jsonpath option specifies an expression with the name from\neach Pod in the returned list.\nView the standard output of one of the pods:\n\nkubectl logs $pods\n\nAnother way to view the logs of a Job:\n\nkubectl logs jobs/pi\n\nThe output is similar to this:\nhttps://kubernetes.io/docs/concepts/_print/\n\n178/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117067982148086513282"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0189", "text": "Writing a Job spec\nAs with all other Kubernetes config, a Job needs apiVersion , kind , and metadata fields.\nWhen the control plane creates new Pods for a Job, the .metadata.name of the Job is part of the basis for naming those Pods. The\nname of a Job must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best\ncompatibility, the name should follow the more restrictive rules for a DNS label. Even when the name is a DNS subdomain, the name\nmust be no longer than 63 characters.\nA Job also needs a .spec section.\n\nJob Labels\nJob labels will have batch.kubernetes.io/ prefix for job-name and controller-uid .\n\nPod Template\nThe .spec.template is the only required field of the .spec .\nThe .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an\napiVersion or kind .\nIn addition to required fields for a Pod, a pod template in a Job must specify appropriate labels (see pod selector) and an appropriate\nrestart policy.\nOnly a RestartPolicy equal to Never or OnFailure is allowed.\n\nPod selector\nThe .spec.selector field is optional. In almost all cases you should not specify it. See section specifying your own pod selector."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0190", "text": "Parallel execution for Jobs\nThere are three main types of task suitable to run as a Job:\n1. Non-parallel Jobs\nnormally, only one Pod is started, unless the Pod fails.\nthe Job is complete as soon as its Pod terminates successfully.\n2. Parallel Jobs with a fixed completion count:\nspecify a non-zero positive value for .spec.completions .\nthe Job represents the overall task, and is complete when there are .spec.completions successful Pods.\nwhen using .spec.completionMode=\"Indexed\" , each Pod gets a different index in the range 0 to .spec.completions-1 .\n3. Parallel Jobs with a work queue:\ndo not specify .spec.completions , default to .spec.parallelism .\nthe Pods must coordinate amongst themselves or an external service to determine what each should work on. For\nexample, a Pod might fetch a batch of up to N items from the work queue.\neach Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is\ndone.\nwhen any Pod from the Job terminates with success, no new Pods are created.\nonce at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.\nonce any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They\nshould all be in the process of exiting.\nFor a non-parallel Job, you can leave both .spec.completions and .spec.parallelism unset. When both are unset, both are\ndefaulted to 1.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n179/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor a fixed completion count Job, you should set .spec.completions to the number of completions needed. You can set\n.spec.parallelism , or leave it unset and it will default to 1.\nFor a work queue Job, you must leave .spec.completions unset, and set .spec.parallelism to a non-negative integer.\nFor more information about how to make use of the different types of job, see the job patterns section."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0191", "text": "Controlling parallelism\nThe requested parallelism ( .spec.parallelism ) can be set to any non-negative value. If it is unspecified, it defaults to 1. If it is\nspecified as 0, then the Job is effectively paused until it is increased.\nActual parallelism (number of pods running at any instant) may be more or less than requested parallelism, for a variety of reasons:\nFor fixed completion count Jobs, the actual number of pods running in parallel will not exceed the number of remaining\ncompletions. Higher values of .spec.parallelism are effectively ignored.\nFor work queue Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete,\nhowever.\nIf the Job Controller has not had time to react.\nIf the Job controller failed to create Pods for any reason (lack of ResourceQuota , lack of permission, etc.), then there may be\nfewer pods than requested.\nThe Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.\nWhen a Pod is gracefully shut down, it takes time to stop.\n\nCompletion mode\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nJobs with fixed completion count - that is, jobs that have non null .spec.completions - can have a completion mode that is specified\nin .spec.completionMode :\n(default): the Job is considered complete when there have been .spec.completions successfully completed Pods.\nIn other words, each Pod completion is homologous to each other. Note that Jobs that have null .spec.completions are\nimplicitly NonIndexed .\nNonIndexed\n\nIndexed : the Pods of a Job get an associated completion index from 0 to .spec.completions-1 . The index is available through"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0192", "text": "four mechanisms:\nThe Pod annotation batch.kubernetes.io/job-completion-index .\nThe Pod label batch.kubernetes.io/job-completion-index (for v1.28 and later). Note the feature gate PodIndexLabel\nmust be enabled to use this label, and it is enabled by default.\nAs part of the Pod hostname, following the pattern $(job-name)-$(index) . When you use an Indexed Job in combination\nwith a Service, Pods within the Job can use the deterministic hostnames to address each other via DNS. For more\ninformation about how to configure this, see Job with Pod-to-Pod Communication.\nFrom the containerized task, in the environment variable JOB_COMPLETION_INDEX .\nThe Job is considered complete when there is one successfully completed Pod for each index. For more information about how\nto use this mode, see Indexed Job for Parallel Processing with Static Work Assignment.\nNote:\nAlthough rare, more than one Pod could be started for the same index (due to various reasons such as node failures, kubelet\nrestarts, or Pod evictions). In this case, only the first Pod that completes successfully will count towards the completion count\nand update the status of the Job. The other Pods that are running or completed for the same index will be deleted by the Job\ncontroller once they are detected.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n180/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nHandling Pod and container failures"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0193", "text": "A container in a Pod may fail for a number of reasons, such as because the process in it exited with a non-zero exit code, or the\ncontainer was killed for exceeding a memory limit, etc. If this happens, and the .spec.template.spec.restartPolicy = \"OnFailure\" ,\nthen the Pod stays on the node, but the container is re-run. Therefore, your program needs to handle the case when it is restarted\nlocally, or else specify .spec.template.spec.restartPolicy = \"Never\" . See pod lifecycle for more information on restartPolicy .\nAn entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node (node is upgraded, rebooted,\ndeleted, etc.), or if a container of the Pod fails and the .spec.template.spec.restartPolicy = \"Never\" . When a Pod fails, then the\nJob controller starts a new Pod. This means that your application needs to handle the case when it is restarted in a new pod. In\nparticular, it needs to handle temporary files, locks, incomplete output and the like caused by previous runs.\nBy default, each pod failure is counted towards the .spec.backoffLimit limit, see pod backoff failure policy. However, you can\ncustomize handling of pod failures by setting the Job's pod failure policy.\nAdditionally, you can choose to count the pod failures independently for each index of an Indexed Job by setting the\n.spec.backoffLimitPerIndex field (for more information, see backoff limit per index).\nNote that even if you specify .spec.parallelism = 1 and .spec.completions = 1 and .spec.template.spec.restartPolicy =\n\"Never\" , the same program may sometimes be started twice.\nIf you do specify .spec.parallelism and .spec.completions both greater than 1, then there may be multiple pods running at once.\nTherefore, your pods must also be tolerant of concurrency.\nIf you specify the .spec.podFailurePolicy field, the Job controller does not consider a terminating Pod (a pod that has a\n.metadata.deletionTimestamp field set) as a failure until that Pod is terminal (its .status.phase is Failed or Succeeded ).\nHowever, the Job controller creates a replacement Pod as soon as the termination becomes apparent. Once the pod terminates, the\nJob controller evaluates .backoffLimit and .podFailurePolicy for the relevant Job, taking this now-terminated Pod into\nconsideration.\nIf either of these requirements is not satisfied, the Job controller counts a terminating Pod as an immediate failure, even if that Pod\nlater terminates with phase: \"Succeeded\" ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0194", "text": "Pod backoff failure policy\nThere are situations where you want to fail a Job after some amount of retries due to a logical error in configuration etc. To do so, set\n.spec.backoffLimit to specify the number of retries before considering a Job as failed.\nThe .spec.backoffLimit is set by default to 6, unless the backoff limit per index (only Indexed Job) is specified. When\n.spec.backoffLimitPerIndex is specified, then .spec.backoffLimit defaults to 2147483647 (MaxInt32).\nFailed Pods associated with the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s ...) capped at\nsix minutes.\nThe number of retries is calculated in two ways:\nThe number of Pods with .status.phase = \"Failed\" .\nWhen using restartPolicy = \"OnFailure\" , the number of retries in all the containers of Pods with .status.phase equal to\nPending or Running .\nIf either of the calculations reaches the .spec.backoffLimit , the Job is considered failed.\nNote:\nIf your Job has restartPolicy = \"OnFailure\", keep in mind that your Pod running the job will be terminated once the job\nbackoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting\nrestartPolicy = \"Never\" when debugging the Job or using a logging system to ensure output from failed Jobs is not lost\ninadvertently.\n\nBackoff limit per index\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\nhttps://kubernetes.io/docs/concepts/_print/\n\n181/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0195", "text": "When you run an indexed Job, you can choose to handle retries for pod failures independently for each index. To do so, set the\n.spec.backoffLimitPerIndex to specify the maximal number of pod failures per index.\nWhen the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the\n.status.failedIndexes field. The succeeded indexes, those with a successfully executed pods, are recorded in the\n.status.completedIndexes field, regardless of whether you set the backoffLimitPerIndex field.\nNote that a failing index does not interrupt execution of other indexes. Once all indexes finish for a Job where you specified a\nbackoff limit per index, if at least one of those indexes did fail, the Job controller marks the overall Job as failed, by setting the Failed\ncondition in the status. The Job gets marked as failed even if some, potentially nearly all, of the indexes were processed successfully.\nYou can additionally limit the maximal number of indexes marked failed by setting the .spec.maxFailedIndexes field. When the\nnumber of failed indexes exceeds the maxFailedIndexes field, the Job controller triggers termination of all remaining running Pods\nfor that Job. Once all pods are terminated, the entire Job is marked failed by the Job controller, by setting the Failed condition in the\nJob status.\nHere is an example manifest for a Job that defines a backoffLimitPerIndex :\n/controllers/job-backoff-limit-per-index-example.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: job-backoff-limit-per-index-example\nspec:\ncompletions: 10\nparallelism: 3\ncompletionMode: Indexed\n\n# required for the feature\n\nbackoffLimitPerIndex: 1\n\n# maximal number of failures per index\n\nmaxFailedIndexes: 5\n\n# maximal number of failed indexes before terminating the Job execution\n\ntemplate:\nspec:\nrestartPolicy: Never # required for the feature\ncontainers:\n- name: example\nimage: python\ncommand:\n\n# The jobs fails as there is at least one failed index\n# (all even indexes fail in here), yet all indexes\n# are executed as maxFailedIndexes is not exceeded.\n\n- python3\n- -c\n- |\nimport os, sys\nprint(\"Hello world\")\nif int(os.environ.get(\"JOB_COMPLETION_INDEX\")) % 2 == 0:\nsys.exit(1)\n\nIn the example above, the Job controller allows for one restart for each of the indexes. When the total number of failed indexes\nexceeds 5, then the entire Job is terminated.\nOnce the job is finished, the Job status looks as follows:\n\nkubectl get -o yaml job job-backoff-limit-per-index-example\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n182/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nstatus:\ncompletedIndexes: 1,3,5,7,9\nfailedIndexes: 0,2,4,6,8\nsucceeded: 5"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0196", "text": "# 1 succeeded pod for each of 5 succeeded indexes\n\nfailed: 10\n\n# 2 failed pods (1 retry) for each of 5 failed indexes\n\nconditions:\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: FailureTarget\n- message: Job has failed indexes\nreason: FailedIndexes\nstatus: \"True\"\ntype: Failed\n\nThe Job controller adds the FailureTarget Job condition to trigger Job termination and cleanup. When all of the Job Pods are\nterminated, the Job controller adds the Failed condition with the same values for reason and message as the FailureTarget Job\ncondition. For details, see Termination of Job Pods.\nAdditionally, you may want to use the per-index backoff along with a pod failure policy. When using per-index backoff, there is a new\nFailIndex action available which allows you to avoid unnecessary retries within an index.\n\nPod failure policy\nâ“˜ FEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)\n\nA Pod failure policy, defined with the .spec.podFailurePolicy field, enables your cluster to handle Pod failures based on the\ncontainer exit codes and the Pod conditions.\nIn some situations, you may want to have a better control when handling Pod failures than the control provided by the Pod backoff\nfailure policy, which is based on the Job's .spec.backoffLimit . These are some examples of use cases:\nTo optimize costs of running workloads by avoiding unnecessary Pod restarts, you can terminate a Job as soon as one of its\nPods fails with an exit code indicating a software bug.\nTo guarantee that your Job finishes even if there are disruptions, you can ignore Pod failures caused by disruptions (such as\npreemption, API-initiated eviction or taint-based eviction) so that they don't count towards the .spec.backoffLimit limit of\nretries.\nYou can configure a Pod failure policy, in the .spec.podFailurePolicy field, to meet the above use cases. This policy can handle Pod\nfailures based on the container exit codes and the Pod conditions.\nHere is a manifest for a Job that defines a podFailurePolicy :\n/controllers/job-pod-failure-policy-example.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n183/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: job-pod-failure-policy-example\nspec:\ncompletions: 12\nparallelism: 3\ntemplate:\nspec:\nrestartPolicy: Never\ncontainers:\n- name: main\nimage: docker.io/library/bash:5\ncommand: [\"bash\"]\n\n# example command simulating a bug which triggers the FailJob action\n\nargs:\n- -c\n- echo \"Hello world!\" && sleep 5 && exit 42\nbackoffLimit: 6\npodFailurePolicy:\nrules:\n- action: FailJob\nonExitCodes:\ncontainerName: main\n\n# optional\n\noperator: In"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0197", "text": "# one of: In, NotIn\n\nvalues: [42]\n- action: Ignore\n\n# one of: Ignore, FailJob, Count\n\nonPodConditions:\n- type: DisruptionTarget\n\n# indicates Pod disruption\n\nIn the example above, the first rule of the Pod failure policy specifies that the Job should be marked failed if the main container fails\nwith the 42 exit code. The following are the rules for the main container specifically:\nan exit code of 0 means that the container succeeded\nan exit code of 42 means that the entire Job failed\nany other exit code represents that the container failed, and hence the entire Pod. The Pod will be re-created if the total\nnumber of restarts is below backoffLimit . If the backoffLimit is reached the entire Job failed.\nNote:\nBecause the Pod template specifies a restartPolicy: Never, the kubelet does not restart the main container in that particular\nPod.\nThe second rule of the Pod failure policy, specifying the Ignore action for failed Pods with condition DisruptionTarget excludes\nPod disruptions from being counted towards the .spec.backoffLimit limit of retries.\nNote:\nIf the Job failed, either by the Pod failure policy or Pod backoff failure policy, and the Job is running multiple Pods, Kubernetes\nterminates all the Pods in that Job that are still Pending or Running.\nThese are some requirements and semantics of the API:\nif you want to use a .spec.podFailurePolicy field for a Job, you must also define that Job's pod template with\n.spec.restartPolicy set to Never .\nthe Pod failure policy rules you specify under spec.podFailurePolicy.rules are evaluated in order. Once a rule matches a Pod\nfailure, the remaining rules are ignored. When no rule matches the Pod failure, the default handling applies.\nyou may want to restrict a rule to a specific container by specifying its name\nin spec.podFailurePolicy.rules[*].onExitCodes.containerName . When not specified the rule applies to all containers. When\nspecified, it should match one the container or initContainer names in the Pod template.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n184/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nyou may specify the action taken when a Pod failure policy is matched by spec.podFailurePolicy.rules[*].action . Possible\nvalues are:\nFailJob : use to indicate that the Pod's job should be marked as Failed and all running Pods should be terminated.\nIgnore : use to indicate that the counter towards the .spec.backoffLimit\n\nshould not be incremented and a"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0198", "text": "replacement Pod should be created.\nCount : use to indicate that the Pod should be handled in the default way. The counter towards the .spec.backoffLimit\nshould be incremented.\nFailIndex : use this action along with backoff limit per index to avoid unnecessary retries within the index of a failed pod.\nNote:\nWhen you use a podFailurePolicy, the job controller only matches Pods in the Failed phase. Pods with a deletion timestamp\nthat are not in a terminal phase (Failed or Succeeded) are considered still terminating. This implies that terminating pods\nretain a tracking finalizer until they reach a terminal phase. Since Kubernetes 1.27, Kubelet transitions deleted pods to a\nterminal phase (see: Pod Phase). This ensures that deleted pods have their finalizers removed by the Job controller.\n\nNote:\nStarting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates terminating Pods only once these\nPods reach the terminal Failed phase. This behavior is similar to podReplacementPolicy: Failed. For more information, see\nPod replacement policy.\nWhen you use the podFailurePolicy , and the Job fails due to the pod matching the rule with the FailJob action, then the Job\ncontroller triggers the Job termination process by adding the FailureTarget condition. For more details, see Job termination and\ncleanup."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0199", "text": "Success policy\nWhen creating an Indexed Job, you can define when a Job can be declared as succeeded using a .spec.successPolicy , based on the\npods that succeeded.\nBy default, a Job succeeds when the number of succeeded Pods equals .spec.completions . These are some situations where you\nmight want additional control for declaring a Job succeeded:\nWhen running simulations with different parameters, you might not need all the simulations to succeed for the overall Job to\nbe successful.\nWhen following a leader-worker pattern, only the success of the leader determines the success or failure of a Job. Examples of\nthis are frameworks like MPI and PyTorch etc.\nYou can configure a success policy, in the .spec.successPolicy field, to meet the above use cases. This policy can handle Job\nsuccess based on the succeeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods. A\nsuccess policy is defined by rules. Each rule can take one of the following forms:\nWhen you specify the succeededIndexes only, once all indexes specified in the succeededIndexes succeed, the job controller\nmarks the Job as succeeded. The succeededIndexes must be a list of intervals between 0 and .spec.completions-1 .\nWhen you specify the succeededCount only, once the number of succeeded indexes reaches the succeededCount , the job\ncontroller marks the Job as succeeded.\nWhen you specify both succeededIndexes and succeededCount , once the number of succeeded indexes from the subset of\nindexes specified in the succeededIndexes reaches the succeededCount , the job controller marks the Job as succeeded.\nNote that when you specify multiple rules in the .spec.successPolicy.rules , the job controller evaluates the rules in order. Once\nthe Job meets a rule, the job controller ignores remaining rules.\nHere is a manifest for a Job with successPolicy :\n/controllers/job-success-policy.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n185/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: job-success\nspec:\nparallelism: 10\ncompletions: 10\ncompletionMode: Indexed # Required for the success policy\nsuccessPolicy:\nrules:\n- succeededIndexes: 0,2-3\nsucceededCount: 1\ntemplate:\nspec:\ncontainers:\n- name: main\nimage: python\ncommand:\n\n# Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,\n# the overall Job is a success.\n\n- python3\n- -c\n- |\nimport os, sys\nif os.environ.get(\"JOB_COMPLETION_INDEX\") == \"2\":\nsys.exit(0)\nelse:\nsys.exit(1)\nrestartPolicy: Never"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0200", "text": "In the example above, both succeededIndexes and succeededCount have been specified. Therefore, the job controller will mark the\nJob as succeeded and terminate the lingering Pods when either of the specified indexes, 0, 2, or 3, succeed. The Job that meets the\nsuccess policy gets the SuccessCriteriaMet condition with a SuccessPolicy reason. After the removal of the lingering Pods is\nissued, the Job gets the Complete condition.\nNote that the succeededIndexes is represented as intervals separated by a hyphen. The number are listed in represented by the\nfirst and last element of the series, separated by a hyphen.\nNote:\nWhen you specify both a success policy and some terminating policies such as .spec.backoffLimit and\n.spec.podFailurePolicy, once the Job meets either policy, the job controller respects the terminating policy and ignores the\nsuccess policy.\n\nJob termination and cleanup\nWhen a Job completes, no more Pods are created, but the Pods are usually not deleted either. Keeping them around allows you to\nstill view the logs of completed pods to check for errors, warnings, or other diagnostic output. The job object also remains after it is\ncompleted so that you can view its status. It is up to the user to delete old jobs after noting their status. Delete the job with kubectl\n(e.g. kubectl delete jobs/pi or kubectl delete -f ./job.yaml ). When you delete the job using kubectl , all the pods it created\nare deleted too.\nBy default, a Job will run uninterrupted unless a Pod fails ( restartPolicy=Never ) or a Container exits in error\n( restartPolicy=OnFailure ), at which point the Job defers to the .spec.backoffLimit described above. Once .spec.backoffLimit\nhas been reached the Job will be marked as failed and any running Pods will be terminated.\nAnother way to terminate a Job is by setting an active deadline. Do this by setting the .spec.activeDeadlineSeconds field of the Job\nto a number of seconds. The activeDeadlineSeconds applies to the duration of the job, no matter how many Pods are created.\nOnce a Job reaches activeDeadlineSeconds , all of its running Pods are terminated and the Job status will become type: Failed\nwith reason: DeadlineExceeded .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n186/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0201", "text": "Note that a Job's .spec.activeDeadlineSeconds takes precedence over its .spec.backoffLimit . Therefore, a Job that is retrying one\nor more failed Pods will not deploy additional Pods once it reaches the time limit specified by activeDeadlineSeconds , even if the\nbackoffLimit is not yet reached.\nExample:\n\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-timeout\nspec:\nbackoffLimit: 5\nactiveDeadlineSeconds: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never\n\nNote that both the Job spec and the Pod template spec within the Job have an activeDeadlineSeconds field. Ensure that you set this\nfield at the proper level.\nKeep in mind that the restartPolicy applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job\nstatus is type: Failed . That is, the Job termination mechanisms activated with .spec.activeDeadlineSeconds and\n.spec.backoffLimit result in a permanent Job failure that requires manual intervention to resolve."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0202", "text": "Terminal Job conditions\nA Job has two possible terminal states, each of which has a corresponding Job condition:\nSucceeded: Job condition Complete\nFailed: Job condition Failed\nJobs fail for the following reasons:\nThe number of Pod failures exceeded the specified .spec.backoffLimit in the Job specification. For details, see Pod backoff\nfailure policy.\nThe Job runtime exceeded the specified .spec.activeDeadlineSeconds\nAn indexed Job that used .spec.backoffLimitPerIndex has failed indexes. For details, see Backoff limit per index.\nThe number of failed indexes in the Job exceeded the specified spec.maxFailedIndexes . For details, see Backoff limit per index\nA failed Pod matches a rule in .spec.podFailurePolicy that has the FailJob action. For details about how Pod failure policy\nrules might affect failure evaluation, see Pod failure policy.\nJobs succeed for the following reasons:\nThe number of succeeded Pods reached the specified .spec.completions\nThe criteria specified in .spec.successPolicy are met. For details, see Success policy.\nIn Kubernetes v1.31 and later the Job controller delays the addition of the terminal conditions, Failed or Complete , until all of the\nJob Pods are terminated.\nIn Kubernetes v1.30 and earlier, the Job controller added the Complete or the Failed Job terminal conditions as soon as the Job\ntermination process was triggered and all Pod finalizers were removed. However, some Pods would still be running or terminating at\nthe moment that the terminal condition was added.\nIn Kubernetes v1.31 and later, the controller only adds the Job terminal conditions after all of the Pods are terminated. You can\ncontrol this behavior by using the JobManagedBy and the JobPodReplacementPolicy (both enabled by default) feature gates.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n187/684\n\n11/7/25, 4:37 PM\n\nTermination of Job pods\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0203", "text": "The Job controller adds the FailureTarget condition or the SuccessCriteriaMet condition to the Job to trigger Pod termination\nafter a Job meets either the success or failure criteria.\nFactors like terminationGracePeriodSeconds might increase the amount of time from the moment that the Job controller adds the\nFailureTarget condition or the SuccessCriteriaMet condition to the moment that all of the Job Pods terminate and the Job\ncontroller adds a terminal condition ( Failed or Complete ).\nYou can use the FailureTarget or the SuccessCriteriaMet condition to evaluate whether the Job has failed or succeeded without\nhaving to wait for the controller to add a terminal condition.\nFor example, you might want to decide when to create a replacement Job that replaces a failed Job. If you replace the failed Job when\nthe FailureTarget condition appears, your replacement Job runs sooner, but could result in Pods from the failed and the\nreplacement Job running at the same time, using extra compute resources.\nAlternatively, if your cluster has limited resource capacity, you could choose to wait until the Failed condition appears on the Job,\nwhich would delay your replacement Job but would ensure that you conserve resources by waiting until all of the failed Pods are\nremoved.\n\nClean up finished jobs automatically\nFinished Jobs are usually no longer needed in the system. Keeping them around in the system will put pressure on the API server. If\nthe Jobs are managed directly by a higher level controller, such as CronJobs, the Jobs can be cleaned up by CronJobs based on the\nspecified capacity-based cleanup policy.\n\nTTL mechanism for finished Jobs\nâ“˜ FEATURE STATE: Kubernetes v1.23 [stable]\n\nAnother way to clean up finished Jobs (either Complete or Failed ) automatically is to use a TTL mechanism provided by a TTL\ncontroller for finished resources, by specifying the .spec.ttlSecondsAfterFinished field of the Job.\nWhen the TTL controller cleans up the Job, it will delete the Job cascadingly, i.e. delete its dependent objects, such as Pods, together\nwith the Job. Note that when the Job is deleted, its lifecycle guarantees, such as finalizers, will be honored.\nFor example:\n\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-with-ttl\nspec:\nttlSecondsAfterFinished: 100\ntemplate:\nspec:\ncontainers:\n- name: pi\nimage: perl:5.34.0\ncommand: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\nrestartPolicy: Never"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0204", "text": "The Job pi-with-ttl will be eligible to be automatically deleted, 100 seconds after it finishes.\nIf the field is set to 0 , the Job will be eligible to be automatically deleted immediately after it finishes. If the field is unset, this Job\nwon't be cleaned up by the TTL controller after it finishes.\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n188/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIt is recommended to set ttlSecondsAfterFinished field because unmanaged jobs (Jobs that you created directly, and not\nindirectly through other workload APIs such as CronJob) have a default deletion policy of orphanDependents causing Pods\ncreated by an unmanaged Job to be left around after that Job is fully deleted. Even though the control plane eventually garbage\ncollects the Pods from a deleted Job after they either fail or complete, sometimes those lingering pods may cause cluster\nperformance degradation or in worst case cause the cluster to go offline due to this degradation.\nYou can use LimitRanges and ResourceQuotas to place a cap on the amount of resources that a particular namespace can\nconsume."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0205", "text": "Job patterns\nThe Job object can be used to process a set of independent but related work items. These might be emails to be sent, frames to be\nrendered, files to be transcoded, ranges of keys in a NoSQL database to scan, and so on.\nIn a complex system, there may be multiple different sets of work items. Here we are just considering one set of work items that the\nuser wants to manage together â€” a batch job.\nThere are several different patterns for parallel computation, each with strengths and weaknesses. The tradeoffs are:\nOne Job object for each work item, versus a single Job object for all work items. One Job per work item creates some overhead\nfor the user and for the system to manage large numbers of Job objects. A single Job for all work items is better for large\nnumbers of items.\nNumber of Pods created equals number of work items, versus each Pod can process multiple work items. When the number of\nPods equals the number of work items, the Pods typically requires less modification to existing code and containers. Having\neach Pod process multiple work items is better for large numbers of items.\nSeveral approaches use a work queue. This requires running a queue service, and modifications to the existing program or\ncontainer to make it use the work queue. Other approaches are easier to adapt to an existing containerised application.\nWhen the Job is associated with a headless Service, you can enable the Pods within a Job to communicate with each other to\ncollaborate in a computation.\nThe tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs. The pattern names are also links to\nexamples and more detailed description.\nPattern\n\nSingle Job object\n\nQueue with Pod Per Work Item\n\nâœ“\n\nQueue with Variable Pod Count\n\nâœ“\n\nIndexed Job with Static Work Assignment\n\nâœ“\n\nJob with Pod-to-Pod Communication\n\nâœ“\n\nFewer pods than work items?\n\nUse app unmodified?\nsometimes\n\nâœ“\nâœ“\n\nsometimes\n\nJob Template Expansion\n\nsometimes\nâœ“"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0206", "text": "When you specify completions with .spec.completions , each Pod created by the Job controller has an identical spec . This means\nthat all pods for a task will have the same command line and the same image, the same volumes, and (almost) the same\nenvironment variables. These patterns are different ways to arrange for pods to work on different things.\nThis table shows the required settings for .spec.parallelism and .spec.completions for each of the patterns. Here, W is the\nnumber of work items.\nPattern\n\n.spec.completions\n\n.spec.parallelism\n\nQueue with Pod Per Work Item\n\nW\n\nany\n\nQueue with Variable Pod Count\n\nnull\n\nany\n\nIndexed Job with Static Work Assignment\n\nW\n\nany\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n189/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPattern\n\n.spec.completions\n\n.spec.parallelism\n\nJob with Pod-to-Pod Communication\n\nW\n\nW\n\nJob Template Expansion\n\n1\n\nshould be 1\n\nAdvanced usage\nSuspending a Job\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nWhen a Job is created, the Job controller will immediately begin creating Pods to satisfy the Job's requirements and will continue to\ndo so until the Job is complete. However, you may want to temporarily suspend a Job's execution and resume it later, or start Jobs in\nsuspended state and have a custom controller decide later when to start them.\nTo suspend a Job, you can update the .spec.suspend field of the Job to true; later, when you want to resume it again, update it to\nfalse. Creating a Job with .spec.suspend set to true will create it in the suspended state.\nWhen a Job is resumed from suspension, its .status.startTime field will be reset to the current time. This means that the\n.spec.activeDeadlineSeconds timer will be stopped and reset when a Job is suspended and resumed.\nWhen you suspend a Job, any running Pods that don't have a status of Completed will be terminated with a SIGTERM signal. The\nPod's graceful termination period will be honored and your Pod must handle this signal in this period. This may involve saving\nprogress for later or undoing changes. Pods terminated this way will not count towards the Job's completions count.\nAn example Job definition in the suspended state can be like so:\n\nkubectl get job myjob -o yaml\n\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: myjob\nspec:\nsuspend: true\nparallelism: 1\ncompletions: 5\ntemplate:\nspec:\n...\n\nYou can also toggle Job suspension by patching the Job using the command line.\nSuspend an active Job:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0207", "text": "kubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":true}}'\n\nResume a suspended Job:\n\nkubectl patch job/myjob --type=strategic --patch '{\"spec\":{\"suspend\":false}}'\n\nThe Job's status can be used to determine if a Job is suspended or has been suspended in the past:\nhttps://kubernetes.io/docs/concepts/_print/\n\n190/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get jobs/myjob -o yaml\n\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nconditions:\n- lastProbeTime: \"2021-02-05T13:14:33Z\"\nlastTransitionTime: \"2021-02-05T13:14:33Z\"\nstatus: \"True\"\ntype: Suspended\nstartTime: \"2021-02-05T13:13:48Z\"\n\nThe Job condition of type \"Suspended\" with status \"True\" means the Job is suspended; the lastTransitionTime field can be used to\ndetermine how long the Job has been suspended for. If the status of that condition is \"False\", then the Job was previously suspended\nand is now running. If such a condition does not exist in the Job's status, the Job has never been stopped.\nEvents are also created when the Job is suspended and resumed:\n\nkubectl describe jobs/myjob\n\nName:\n\nmyjob\n\n...\nEvents:\nType\n----\n\nReason\n------\n\nAge\n----\n\nFrom\n----\n\nMessage\n-------\n\nNormal\nNormal\nNormal\n\nSuccessfulCreate\nSuccessfulDelete\nSuspended\n\n12m\n11m\n11m\n\njob-controller\njob-controller\njob-controller\n\nCreated pod: myjob-hlrpl\nDeleted pod: myjob-hlrpl\nJob suspended\n\nNormal\nNormal\n\nSuccessfulCreate\nResumed\n\n3s\n3s\n\njob-controller\njob-controller\n\nCreated pod: myjob-jvb44\nJob resumed\n\nThe last four events, particularly the \"Suspended\" and \"Resumed\" events, are directly a result of toggling the .spec.suspend field. In\nthe time between these two events, we see that no Pods were created, but Pod creation restarted as soon as the Job was resumed.\n\nMutable Scheduling Directives\nâ“˜ FEATURE STATE: Kubernetes v1.27 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0208", "text": "In most cases, a parallel job will want the pods to run with constraints, like all in the same zone, or all either on GPU model x or y but\nnot a mix of both.\nThe suspend field is the first step towards achieving those semantics. Suspend allows a custom queue controller to decide when a\njob should start; However, once a job is unsuspended, a custom queue controller has no influence on where the pods of a job will\nactually land.\nThis feature allows updating a Job's scheduling directives before it starts, which gives custom queue controllers the ability to\ninfluence pod placement while at the same time offloading actual pod-to-node assignment to kube-scheduler. This is allowed only\nfor suspended Jobs that have never been unsuspended before.\nThe fields in a Job's pod template that can be updated are node affinity, node selector, tolerations, labels, annotations and\nscheduling gates.\n\nSpecifying your own Pod selector\nNormally, when you create a Job object, you do not specify .spec.selector . The system defaulting logic adds this field when the Job\nis created. It picks a selector value that will not overlap with any other jobs.\nhttps://kubernetes.io/docs/concepts/_print/\n\n191/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0209", "text": "However, in some cases, you might need to override this automatically set selector. To do this, you can specify the .spec.selector\nof the Job.\nBe very careful when doing this. If you specify a label selector which is not unique to the pods of that Job, and which matches\nunrelated Pods, then pods of the unrelated job may be deleted, or this Job may count other Pods as completing it, or one or both\nJobs may refuse to create Pods or run to completion. If a non-unique selector is chosen, then other controllers (e.g.\nReplicationController) and their Pods may behave in unpredictable ways too. Kubernetes will not stop you from making a mistake\nwhen specifying .spec.selector .\nHere is an example of a case when you might want to use this feature.\nSay Job old is already running. You want existing Pods to keep running, but you want the rest of the Pods it creates to use a\ndifferent pod template and for the Job to have a new name. You cannot update the Job because these fields are not updatable.\nTherefore, you delete Job old but leave its pods running, using kubectl delete jobs/old --cascade=orphan . Before deleting it, you\nmake a note of what selector it uses:\n\nkubectl get job old -o yaml\n\nThe output is similar to this:\n\nkind: Job\nmetadata:\nname: old\n...\nspec:\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n\nThen you create a new Job with name new and you explicitly specify the same selector. Since the existing Pods have label\nbatch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002 , they are controlled by Job new as well.\nYou need to specify manualSelector: true in the new Job since you are not using the selector that the system normally generates\nfor you automatically.\n\nkind: Job\nmetadata:\nname: new\n...\nspec:\nmanualSelector: true\nselector:\nmatchLabels:\nbatch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002\n...\n\nThe new Job itself will have a different uid from a8f3d00d-c6d2-11e5-9f87-42010af00002 . Setting manualSelector: true tells the\nsystem that you know what you are doing and to allow this mismatch.\n\nJob tracking with finalizers\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0210", "text": "The control plane keeps track of the Pods that belong to any Job and notices if any such Pod is removed from the API server. To do\nthat, the Job controller creates Pods with the finalizer batch.kubernetes.io/job-tracking . The controller removes the finalizer only\nafter the Pod has been accounted for in the Job status, allowing the Pod to be removed by other controllers or users.\nhttps://kubernetes.io/docs/concepts/_print/\n\n192/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nSee My pod stays terminating if you observe that pods from a Job are stuck with the tracking finalizer.\n\nElastic Indexed Jobs\nâ“˜ FEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)\n\nYou can scale Indexed Jobs up or down by mutating both .spec.parallelism and .spec.completions together such that\n.spec.parallelism == .spec.completions . When scaling down, Kubernetes removes the Pods with higher indexes.\nUse cases for elastic Indexed Jobs include batch workloads which require scaling an indexed Job, such as MPI, Horovod, Ray, and\nPyTorch training jobs.\n\nDelayed creation of replacement pods\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nBy default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp). This means that,\nat a given time, when some of the Pods are terminating, the number of running Pods for a Job can be greater than parallelism or\ngreater than one Pod per index (if you are using an Indexed Job).\nYou may choose to create replacement Pods only when the terminating Pod is fully terminal (has status.phase: Failed ). To do this,\nset the .spec.podReplacementPolicy: Failed . The default replacement policy depends on whether the Job has a podFailurePolicy\nset. With no Pod failure policy defined for a Job, omitting the podReplacementPolicy field selects the TerminatingOrFailed\nreplacement policy: the control plane creates replacement Pods immediately upon Pod deletion (as soon as the control plane sees\nthat a Pod for this Job has deletionTimestamp set). For Jobs with a Pod failure policy set, the default podReplacementPolicy is\nFailed , and no other value is permitted. See Pod failure policy to learn more about Pod failure policies for Jobs.\n\nkind: Job\nmetadata:\nname: new\n...\nspec:\npodReplacementPolicy: Failed\n...\n\nProvided your cluster has the feature gate enabled, you can inspect the .status.terminating field of a Job. The value of the field is\nthe number of Pods owned by the Job that are currently terminating."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0211", "text": "kubectl get jobs/myjob -o yaml\n\napiVersion: batch/v1\nkind: Job\n# .metadata and .spec omitted\nstatus:\nterminating: 3 # three Pods are terminating and have not yet reached the Failed phase\n\nDelegation of managing a Job object to external controller\nâ“˜ FEATURE STATE: Kubernetes v1.32 [beta] (enabled by default: true)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n193/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nYou can only set the managedBy field on Jobs if you enable the JobManagedBy feature gate (enabled by default).\nThis feature allows you to disable the built-in Job controller, for a specific Job, and delegate reconciliation of the Job to an external\ncontroller.\nYou indicate the controller that reconciles the Job by setting a custom value for the spec.managedBy field - any value other than\nkubernetes.io/job-controller . The value of the field is immutable.\nNote:\nWhen using this feature, make sure the controller indicated by the field is installed, otherwise the Job may not be reconciled at\nall.\n\nNote:\nWhen developing an external Job controller be aware that your controller needs to operate in a fashion conformant with the\ndefinitions of the API spec and status fields of the Job object.\nPlease review these in detail in the Job API. We also recommend that you run the e2e conformance tests for the Job object to\nverify your implementation.\nFinally, when developing an external Job controller make sure it does not use the batch.kubernetes.io/job-tracking finalizer,\nreserved for the built-in controller.\n\nWarning:\nIf you are considering to disable the JobManagedBy feature gate, or to downgrade the cluster to a version without the feature\ngate enabled, check if there are jobs with a custom value of the spec.managedBy field. If there are such jobs, there is a risk that\nthey might be reconciled by two controllers after the operation: the built-in Job controller and the external controller indicated\nby the field value.\n\nAlternatives\nBare Pods\nWhen the node that a Pod is running on reboots or fails, the pod is terminated and will not be restarted. However, a Job will create\nnew Pods to replace terminated ones. For this reason, we recommend that you use a Job rather than a bare Pod, even if your\napplication requires only a single Pod."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0212", "text": "Replication Controller\nJobs are complementary to Replication Controllers. A Replication Controller manages Pods which are not expected to terminate (e.g.\nweb servers), and a Job manages Pods that are expected to terminate (e.g. batch tasks).\nAs discussed in Pod Lifecycle, Job is only appropriate for pods with RestartPolicy equal to OnFailure or Never .\nNote:\nIf RestartPolicy is not set, the default value is Always.\n\nSingle Job starts controller Pod\nAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort of custom controller for those\nPods. This allows the most flexibility, but may be somewhat complicated to get started with and offers less integration with\nKubernetes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n194/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAn advantage of this approach is that the overall process gets the completion guarantee of a Job object, but maintains complete\ncontrol over what Pods are created and how work is assigned to them.\n\nWhat's next\nLearn about Pods.\nRead about different ways of running Jobs:\nCoarse Parallel Processing Using a Work Queue\nFine Parallel Processing Using a Work Queue\nUse an indexed Job for parallel processing with static work assignment\nCreate multiple Jobs based on a template: Parallel Processing using Expansions\nFollow the links within Clean up finished jobs automatically to learn more about how your cluster can clean up completed and /\nor failed tasks.\nJob is part of the Kubernetes REST API. Read the Job object definition to understand the API for jobs.\nRead about CronJob, which you can use to define a series of Jobs that will run based on a schedule, similar to the UNIX tool\ncron .\nPractice how to configure handling of retriable and non-retriable pod failures using podFailurePolicy , based on the step-bystep examples.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n195/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.2.6 - Automatic Cleanup for Finished Jobs\nA time-to-live mechanism to clean up old Jobs that have finished execution.\nâ“˜ FEATURE STATE: Kubernetes v1.23 [stable]\n\nWhen your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job) so that you can tell whether\nthe Job succeeded or failed.\nKubernetes' TTL-after-finished controller provides a TTL (time to live) mechanism to limit the lifetime of Job objects that have finished\nexecution."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0213", "text": "Cleanup for finished Jobs\nThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean up finished Jobs (either Complete\nor Failed ) automatically by specifying the .spec.ttlSecondsAfterFinished field of a Job, as in this example.\nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up TTL seconds after the Job has finished. The timer\nstarts once the status condition of the Job changes to show that the Job is either Complete or Failed ; once the TTL has expired,\nthat Job becomes eligible for cascading removal. When the TTL-after-finished controller cleans up a job, it will delete it cascadingly,\nthat is to say it will delete its dependent objects together with it.\nKubernetes honors object lifecycle guarantees on the Job, such as waiting for finalizers.\nYou can set the TTL seconds at any time. Here are some examples for setting the .spec.ttlSecondsAfterFinished field of a Job:\nSpecify this field in the Job manifest, so that a Job can be cleaned up automatically some time after it finishes.\nManually set this field of existing, already finished Jobs, so that they become eligible for cleanup.\nUse a mutating admission webhook to set this field dynamically at Job creation time. Cluster administrators can use this to\nenforce a TTL policy for finished jobs.\nUse a mutating admission webhook to set this field dynamically after the Job has finished, and choose different TTL values\nbased on job status, labels. For this case, the webhook needs to detect changes to the .status of the Job and only set a TTL\nwhen the Job is being marked as completed.\nWrite your own controller to manage the cleanup TTL for Jobs that match a particular selector.\n\nCaveats\nUpdating TTL for finished Jobs\nYou can modify the TTL period, e.g. .spec.ttlSecondsAfterFinished field of Jobs, after the job is created or has finished. If you\nextend the TTL period after the existing ttlSecondsAfterFinished period has expired, Kubernetes doesn't guarantee to retain that\nJob, even if an update to extend the TTL returns a successful API response."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0214", "text": "Time skew\nBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to determine whether the TTL has expired\nor not, this feature is sensitive to time skew in your cluster, which may cause the control plane to clean up Job objects at the wrong\ntime.\nClocks aren't always correct, but the difference should be very small. Please be aware of this risk when setting a non-zero TTL.\n\nWhat's next\nRead Clean up Jobs automatically\nRefer to the Kubernetes Enhancement Proposal (KEP) for adding this mechanism.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n196/684\n\n11/7/25, 4:37 PM\n\n4.2.7 - CronJob\n\nConcepts | Kubernetes\n\nA CronJob starts one-time Jobs on a repeating schedule.\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]\n\nA CronJob creates Jobs on a repeating schedule.\nCronJob is meant for performing regular scheduled actions such as backups, report generation, and so on. One CronJob object is like\none line of a crontab (cron table) file on a Unix system. It runs a Job periodically on a given schedule, written in Cron format.\nCronJobs have limitations and idiosyncrasies. For example, in certain circumstances, a single CronJob can create multiple concurrent\nJobs. See the limitations below.\nWhen the control plane creates new Jobs and (indirectly) Pods for a CronJob, the .metadata.name of the CronJob is part of the basis\nfor naming those Pods. The name of a CronJob must be a valid DNS subdomain value, but this can produce unexpected results for\nthe Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label. Even when the name is\na DNS subdomain, the name must be no longer than 52 characters. This is because the CronJob controller will automatically append\n11 characters to the name you provide and there is a constraint that the length of a Job name is no more than 63 characters.\n\nExample\nThis example CronJob manifest prints the current time and a hello message every minute:\napplication/job/cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\nname: hello\nspec:\nschedule: \"* * * * *\"\njobTemplate:\nspec:\ntemplate:\nspec:\ncontainers:\n- name: hello\nimage: busybox:1.28\nimagePullPolicy: IfNotPresent\ncommand:\n- /bin/sh\n- -c\n- date; echo Hello from the Kubernetes cluster\nrestartPolicy: OnFailure\n\n(Running Automated Tasks with a CronJob takes you through this example in more detail).\n\nWriting a CronJob spec\nSchedule syntax\nThe .spec.schedule field is required. The value of that field follows the Cron syntax:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n197/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0215", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\n# â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ hour (0 - 23)\n# â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of the month (1 - 31)\n# â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ month (1 - 12)\n# â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ day of the week (0 - 6) (Sunday to Saturday)\n# â”‚ â”‚ â”‚ â”‚ â”‚\nOR sun, mon, tue, wed, thu, fri, sat\n# â”‚ â”‚ â”‚ â”‚ â”‚\n# â”‚ â”‚ â”‚ â”‚ â”‚\n# * * * * *\n\nFor example, 0 3 * * 1 means this task is scheduled to run weekly on a Monday at 3 AM.\nThe format also includes extended \"Vixie cron\" step values. As explained in the FreeBSD manual:\nStep values can be used in conjunction with ranges. Following a range with /<number> specifies skips of the number's value\nthrough the range. For example, 0-23/2 can be used in the hours field to specify command execution every other hour (the\nalternative in the V7 standard is 0,2,4,6,8,10,12,14,16,18,20,22 ). Steps are also permitted after an asterisk, so if you want to\nsay \"every two hours\", just use */2 .\n\nNote:\nA question mark (?) in the schedule has the same meaning as an asterisk *, that is, it stands for any of available value for a\ngiven field.\nOther than the standard syntax, some macros like @monthly can also be used:\nEntry\n\nDescription\n\nEquivalent to\n\n@yearly (or @annually)\n\nRun once a year at midnight of 1 January\n\n0011*\n\n@monthly\n\nRun once a month at midnight of the first day of the month\n\n001**\n\n@weekly\n\nRun once a week at midnight on Sunday morning\n\n00**0\n\n@daily (or @midnight)\n\nRun once a day at midnight\n\n00***\n\n@hourly\n\nRun once an hour at the beginning of the hour\n\n0****\n\nTo generate CronJob schedule expressions, you can also use web tools like crontab.guru.\n\nJob template\nThe .spec.jobTemplate defines a template for the Jobs that the CronJob creates, and it is required. It has exactly the same schema\nas a Job, except that it is nested and does not have an apiVersion or kind . You can specify common metadata for the templated\nJobs, such as labels or annotations. For information about writing a Job .spec , see Writing a Job Spec."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0216", "text": "Deadline for delayed Job start\nThe .spec.startingDeadlineSeconds field is optional. This field defines a deadline (in whole seconds) for starting the Job, if that Job\nmisses its scheduled time for any reason.\nAfter missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled). For example, if you\nhave a backup Job that runs twice a day, you might allow it to start up to 8 hours late, but no later, because a backup taken any later\nwouldn't be useful: you would instead prefer to wait for the next scheduled run.\nFor Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs. If you don't specify startingDeadlineSeconds for\na CronJob, the Job occurrences have no deadline.\nIf the .spec.startingDeadlineSeconds field is set (not null), the CronJob controller measures the time between when a Job is\nexpected to be created and now. If the difference is higher than that limit, it will skip this execution.\nhttps://kubernetes.io/docs/concepts/_print/\n\n198/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, if it is set to 200 , it allows a Job to be created for up to 200 seconds after the actual schedule.\n\nConcurrency policy\nThe .spec.concurrencyPolicy field is also optional. It specifies how to treat concurrent executions of a Job that is created by this\nCronJob. The spec may specify only one of the following concurrency policies:\nAllow\n\n(default): The CronJob allows concurrently running Jobs\n\nForbid : The CronJob does not allow concurrent runs; if it is time for a new Job run and the previous Job run hasn't finished yet,\n\nthe CronJob skips the new Job run. Also note that when the previous Job run finishes, .spec.startingDeadlineSeconds is still\ntaken into account and may result in a new Job run.\nReplace : If it is time for a new Job run and the previous Job run hasn't finished yet, the CronJob replaces the currently running\nJob run with a new Job run\nNote that concurrency policy only applies to the Jobs created by the same CronJob. If there are multiple CronJobs, their respective\nJobs are always allowed to run concurrently."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0217", "text": "Schedule suspension\nYou can suspend execution of Jobs for a CronJob, by setting the optional .spec.suspend field to true. The field defaults to false.\nThis setting does not affect Jobs that the CronJob has already started.\nIf you do set that field to true, all subsequent executions are suspended (they remain scheduled, but the CronJob controller does not\nstart the Jobs to run the tasks) until you unsuspend the CronJob.\nCaution:\nExecutions that are suspended during their scheduled time count as missed Jobs. When .spec.suspend changes from true to\nfalse on an existing CronJob without a starting deadline, the missed Jobs are scheduled immediately.\n\nJobs history limits\nThe .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields specify how many completed and failed Jobs\nshould be kept. Both fields are optional.\n.spec.successfulJobsHistoryLimit : This field specifies the number of successful finished jobs to keep. The default value is 3 .\n\nSetting this field to 0 will not keep any successful jobs.\n.spec.failedJobsHistoryLimit : This field specifies the number of failed finished jobs to keep. The default value is 1 . Setting\n\nthis field to 0 will not keep any failed jobs.\nFor another way to clean up Jobs automatically, see Clean up finished Jobs automatically.\n\nTime zones\nâ“˜ FEATURE STATE: Kubernetes v1.27 [stable]\n\nFor CronJobs with no time zone specified, the kube-controller-manager interprets schedules relative to its local time zone.\nYou can specify a time zone for a CronJob by setting .spec.timeZone to the name of a valid time zone. For example, setting\n.spec.timeZone: \"Etc/UTC\" instructs Kubernetes to interpret the schedule relative to Coordinated Universal Time.\nA time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is\nnot available on the system.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n199/684\n\n11/7/25, 4:37 PM\n\nCronJob limitations\n\nConcepts | Kubernetes\n\nUnsupported TimeZone specification\nSpecifying a timezone using CRON_TZ or TZ variables inside .spec.schedule is not officially supported (and never has been). If\nyou try to set a schedule that includes TZ or CRON_TZ timezone specification, Kubernetes will fail to create or update the resource\nwith a validation error. You should specify time zones using the time zone field, instead."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0218", "text": "Modifying a CronJob\nBy design, a CronJob contains a template for new Jobs. If you modify an existing CronJob, the changes you make will apply to new\nJobs that start to run after your modification is complete. Jobs (and their Pods) that have already started continue to run without\nchanges. That is, the CronJob does not update existing Jobs, even if those remain running.\n\nJob creation\nA CronJob creates a Job object approximately once per execution time of its schedule. The scheduling is approximate because there\nare certain circumstances where two Jobs might be created, or no Job might be created. Kubernetes tries to avoid those situations,\nbut does not completely prevent them. Therefore, the Jobs that you define should be idempotent.\nStarting with Kubernetes v1.32, CronJobs apply an annotation batch.kubernetes.io/cronjob-scheduled-timestamp to their created\nJobs. This annotation indicates the originally scheduled creation time for the Job and is formatted in RFC3339.\nIf startingDeadlineSeconds is set to a large value or left unset (the default) and if concurrencyPolicy is set to Allow , the Jobs will\nalways run at least once.\nCaution:\nIf startingDeadlineSeconds is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the\nCronJob controller checks things every 10 seconds.\nFor every CronJob, the CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until\nnow. If there are more than 100 missed schedules, then it does not start the Job and logs the error.\nCannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadl"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0219", "text": "It is important to note that if the startingDeadlineSeconds field is set (not nil ), the controller counts how many missed Jobs\noccurred from the value of startingDeadlineSeconds until now rather than from the last scheduled time until now. For example, if\nstartingDeadlineSeconds is 200 , the controller counts how many missed Jobs occurred in the last 200 seconds.\nA CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if concurrencyPolicy is set to\nForbid and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as\nmissed.\nFor example, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00 , and its\nstartingDeadlineSeconds field is not set. If the CronJob controller happens to be down from 08:29:00 to 10:21:00 , the Job will\nnot start as the number of missed Jobs which missed their schedule is greater than 100.\nTo illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00 , and its\nstartingDeadlineSeconds is set to 200 seconds. If the CronJob controller happens to be down for the same period as the previous\nexample ( 08:29:00 to 10:21:00 ,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed\nschedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.\nThe CronJob is only responsible for creating Jobs that match its schedule, and the Job in turn is responsible for the management of\nthe Pods it represents.\n\nWhat's next\nLearn about Pods and Jobs, two concepts that CronJobs rely upon.\nRead about the detailed format of CronJob .spec.schedule fields.\nhttps://kubernetes.io/docs/concepts/_print/\n\n200/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor instructions on creating and working with CronJobs, and for an example of a CronJob manifest, see Running automated\ntasks with CronJobs.\nCronJob is part of the Kubernetes REST API. Read the CronJob API reference for more details.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n201/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n4.2.8 - ReplicationController"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0220", "text": "Legacy API for managing workloads that can scale horizontally. Superseded by the Deployment and ReplicaSet\nAPIs.\nNote:\nA Deployment that configures a ReplicaSet is now the recommended way to set up replication.\nA ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a\nReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.\n\nHow a ReplicationController works\nIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts\nmore pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are\ndeleted, or are terminated. For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\nFor this reason, you should use a ReplicationController even if your application requires only a single pod. A ReplicationController is\nsimilar to a process supervisor, but instead of supervising individual processes on a single node, the ReplicationController supervises\nmultiple pods across multiple nodes.\nReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in kubectl commands.\nA simple case is to create one ReplicationController object to reliably run one instance of a Pod indefinitely. A more complex use\ncase is to run several identical replicas of a replicated service, such as web servers.\n\nRunning an example ReplicationController\nThis example ReplicationController config runs three copies of the nginx web server.\ncontrollers/replication.yaml\napiVersion: v1\nkind: ReplicationController\nmetadata:\nname: nginx\nspec:\nreplicas: 3\nselector:\napp: nginx\ntemplate:\nmetadata:\nname: nginx\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nports:\n- containerPort: 80\n\nRun the example job by downloading the example file and then running this command:\n\nkubectl apply -f https://k8s.io/examples/controllers/replication.yaml\n\nThe output is similar to this:\nhttps://kubernetes.io/docs/concepts/_print/\n\n202/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nreplicationcontroller/nginx created\n\nCheck on the status of the ReplicationController using this command:\n\nkubectl describe replicationcontrollers/nginx\n\nThe output is similar to this:\nName:\nnginx\nNamespace:\ndefault\nSelector:\napp=nginx\nLabels:\napp=nginx\nAnnotations:\n<none>\nReplicas:\n3 current / 3 desired\nPods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed\nPod Template:\nLabels:\napp=nginx\nContainers:\nnginx:\nImage:\nnginx\nPort:\n80/TCP\nEnvironment:\n<none>\nMounts:\n<none>\nVolumes:\nEvents:\nFirstSeen\n--------20s\n20s\n20s\n\n<none>\nLastSeen\n-------20s\n20s\n20s\n\nCount\n----1\n1\n1\n\nFrom\n---{replication-controller }\n{replication-controller }\n{replication-controller }"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0221", "text": "SubobjectPath\n-------------\n\nType\n---Normal\nNormal\nNormal\n\nReason\n-----SuccessfulCreate\nSuccessfulCreate\nSuccessfulCreate\n\nM\nC\nC\nC\n\nHere, three pods are created, but none is running yet, perhaps because the image is being pulled. A little later, the same command\nmay show:\n\nPods Status:\n\n3 Running / 0 Waiting / 0 Succeeded / 0 Failed\n\nTo list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:\n\npods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name})\necho $pods\n\nThe output is similar to this:\nnginx-3ntk0 nginx-4ok8v nginx-qrm3m\n\nHere, the selector is the same as the selector for the ReplicationController (seen in the kubectl describe output), and in a different\nform in replication.yaml . The --output=jsonpath option specifies an expression with the name from each pod in the returned\nlist.\n\nWriting a ReplicationController Manifest\nAs with all other Kubernetes config, a ReplicationController needs apiVersion , kind , and metadata fields.\nWhen the control plane creates new Pods for a ReplicationController, the .metadata.name of the ReplicationController is part of the\nbasis for naming those Pods. The name of a ReplicationController must be a valid DNS subdomain value, but this can produce\nunexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.\nhttps://kubernetes.io/docs/concepts/_print/\n\n203/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor general information about working with configuration files, see object management.\nA ReplicationController also needs a .spec section.\n\nPod Template\nThe .spec.template is the only required field of the .spec .\nThe .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an\napiVersion or kind .\nIn addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate labels and an appropriate\nrestart policy. For labels, make sure not to overlap with other controllers. See pod selector.\nOnly a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.\nFor local container restarts, ReplicationControllers delegate to an agent on the node, for example the Kubelet."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0222", "text": "Labels on the ReplicationController\nThe ReplicationController can itself have labels ( .metadata.labels ). Typically, you would set these the same as the\n.spec.template.metadata.labels ; if .metadata.labels is not specified then it defaults to .spec.template.metadata.labels .\nHowever, they are allowed to be different, and the .metadata.labels do not affect the behavior of the ReplicationController.\n\nPod Selector\nThe .spec.selector field is a label selector. A ReplicationController manages all the pods with labels that match the selector. It does\nnot distinguish between pods that it created or deleted and pods that another person or process created or deleted. This allows the\nReplicationController to be replaced without affecting the running pods.\nIf specified, the .spec.template.metadata.labels must be equal to the .spec.selector , or it will be rejected by the API. If\n.spec.selector is unspecified, it will be defaulted to .spec.template.metadata.labels .\nAlso you should not normally create any pods whose labels match this selector, either directly, with another ReplicationController, or\nwith another controller such as Job. If you do so, the ReplicationController thinks that it created the other pods. Kubernetes does not\nstop you from doing this.\nIf you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself (see below).\n\nMultiple Replicas\nYou can specify how many pods should run concurrently by setting .spec.replicas to the number of pods you would like to have\nrunning concurrently. The number running at any time may be higher or lower, such as if the replicas were just increased or\ndecreased, or if a pod is gracefully shutdown, and a replacement starts early.\nIf you do not specify .spec.replicas , then it defaults to 1.\n\nWorking with ReplicationControllers\nDeleting a ReplicationController and its Pods\nTo delete a ReplicationController and all its pods, use kubectl delete . Kubectl will scale the ReplicationController to zero and wait\nfor it to delete each pod before deleting the ReplicationController itself. If this kubectl command is interrupted, it can be restarted.\nWhen using the REST API or client library, you need to do the steps explicitly (scale replicas to 0, wait for pod deletions, then delete\nthe ReplicationController).\n\nDeleting only a ReplicationController\nYou can delete a ReplicationController without affecting any of its pods.\nUsing kubectl, specify the --cascade=orphan option to kubectl delete .\nhttps://kubernetes.io/docs/concepts/_print/\n\n204/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0223", "text": "When using the REST API or client library, you can delete the ReplicationController object.\nOnce the original is deleted, you can create a new ReplicationController to replace it. As long as the old and new .spec.selector\nare the same, then the new one will adopt the old pods. However, it will not make any effort to make existing pods match a new,\ndifferent pod template. To update pods to a new spec in a controlled way, use a rolling update.\n\nIsolating pods from a ReplicationController\nPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods\nfrom service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the\nnumber of replicas is not also changed).\n\nCommon usage patterns\nRescheduling\nAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the\nspecified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another\ncontrol agent).\n\nScaling\nThe ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by\nupdating the replicas field.\n\nRolling updates\nThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.\nAs explained in #1353, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old\n(-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods\nregardless of unexpected failures.\nIdeally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of\npods were productively serving at any given time.\nThe two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the\nprimary container of the pod, since it is typically image updates that motivate rolling updates."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0224", "text": "Multiple release tracks\nIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases\nfor an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.\nFor instance, a service might target all pods with tier in (frontend), environment in (prod) . Now say you have 10 replicated\npods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a\nReplicationController with replicas set to 9 for the bulk of the replicas, with labels tier=frontend, environment=prod,\ntrack=stable , and another ReplicationController with replicas set to 1 for the canary, with labels tier=frontend,\nenvironment=prod, track=canary . Now the service is covering both the canary and non-canary pods. But you can mess with the\nReplicationControllers separately to test things out, monitor the results, etc.\n\nUsing ReplicationControllers with Services\nMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic goes to the old version, and some\ngoes to the new version.\nA ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be\ncomposed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created\nand destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services\nthemselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n205/684\n\n11/7/25, 4:37 PM\n\nWriting programs for Replication\n\nConcepts | Kubernetes\n\nPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may\nbecome heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used\nto maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work\nassignment mechanisms, such as the RabbitMQ work queues, as opposed to static/one-time customization of the configuration of\neach pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for\nexample, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0225", "text": "Responsibilities of the ReplicationController\nThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only\nterminated pods are excluded from its count. In the future, readiness and other information available from the system may be taken\ninto account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external\nclients to implement arbitrarily sophisticated replacement and/or scale-down policies.\nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes.\nRather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in #492), which would\nchange its replicas field. We will not add scheduling policies (for example, spreading) to the ReplicationController. Nor should it\nverify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated\nprocesses. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere.\nWe even plan to factor out the mechanism for bulk pod creation (#170).\nThe ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be\nbuilt on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently\nsupported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like Asgard\nmanaging ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.\n\nAPI Object\nReplication controller is a top-level resource in the Kubernetes REST API. More details about the API object can be found at:\nReplicationController API object.\n\nAlternatives to ReplicationController\nReplicaSet\nis the next-generation ReplicationController that supports the new set-based label selector. It's mainly used by\nDeployment as a mechanism to orchestrate pod creation, deletion and updates. Note that we recommend using Deployments\ninstead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.\nReplicaSet\n\nDeployment (Recommended)\nis a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if\nyou want the rolling update functionality, because they are declarative, server-side, and have additional features.\nDeployment"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0226", "text": "Bare Pods\nUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any\nreason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we\nrecommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process\nsupervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A\nReplicationController delegates local container restarts to some agent on the node, such as the kubelet.\n\nJob\nUse a Job instead of a ReplicationController for pods that are expected to terminate on their own (that is, batch jobs).\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n206/684\n\n11/7/25, 4:37 PM\n\nDaemonSet\n\nConcepts | Kubernetes\n\nUse a DaemonSet instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or\nmachine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before\nother pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.\n\nWhat's next\nLearn about Pods.\nLearn about Deployment, the replacement for ReplicationController.\nReplicationController is part of the Kubernetes REST API. Read the ReplicationController object definition to understand the\nAPI for replication controllers.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n207/684\n\n11/7/25, 4:37 PM\n\n4.3 - Autoscaling Workloads\n\nConcepts | Kubernetes\n\nWith autoscaling, you can automatically update your workloads in one way or another. This allows your cluster\nto react to changes in resource demand more elastically and efficiently.\nIn Kubernetes, you can scale a workload depending on the current demand of resources. This allows your cluster to react to changes\nin resource demand more elastically and efficiently.\nWhen you scale a workload, you can either increase or decrease the number of replicas managed by the workload, or adjust the\nresources available to the replicas in-place.\nThe first approach is referred to as horizontal scaling, while the second is referred to as vertical scaling.\nThere are manual and automatic ways to scale your workloads, depending on your use case."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0227", "text": "Scaling workloads manually\nKubernetes supports manual scaling of workloads. Horizontal scaling can be done using the kubectl CLI. For vertical scaling, you\nneed to patch the resource definition of your workload.\nSee below for examples of both strategies.\nHorizontal scaling: Running multiple instances of your app\nVertical scaling: Resizing CPU and memory resources assigned to containers\n\nScaling workloads automatically\nKubernetes also supports automatic scaling of workloads, which is the focus of this page.\nThe concept of Autoscaling in Kubernetes refers to the ability to automatically update an object that manages a set of Pods (for\nexample a Deployment).\n\nScaling workloads horizontally\nIn Kubernetes, you can automatically scale a workload horizontally using a HorizontalPodAutoscaler (HPA).\nIt is implemented as a Kubernetes API resource and a controller and periodically adjusts the number of replicas in a workload to\nmatch observed resource utilization such as CPU or memory usage.\nThere is a walkthrough tutorial of configuring a HorizontalPodAutoscaler for a Deployment.\n\nScaling workloads vertically\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nYou can automatically scale a workload vertically using a VerticalPodAutoscaler (VPA). Unlike the HPA, the VPA doesn't come with\nKubernetes by default, but is a separate project that can be found on GitHub.\nOnce installed, it allows you to create CustomResourceDefinitions (CRDs) for your workloads which define how and when to scale the\nresources of the managed replicas.\nNote:\nYou will need to have the Metrics Server installed to your cluster for the VPA to work.\nAt the moment, the VPA can operate in four different modes:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n208/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nMode\n\nDescription\n\nAuto\n\nCurrently Recreate . This might change to in-place updates in the future.\n\nRecreate\n\nThe VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them\nwhen the requested resources differ significantly from the new recommendation\n\nInitial\n\nThe VPA only assigns resource requests on pod creation and never changes them later.\n\nOff\n\nThe VPA does not automatically change the resource requirements of the pods. The recommendations are\ncalculated and can be inspected in the VPA object.\n\nIn-place pod vertical scaling\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)\n\nAs of Kubernetes 1.34, VPA does not support resizing pods in-place, but this integration is being worked on. For manually resizing\npods in-place, see Resize Container Resources In-Place."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0228", "text": "Autoscaling based on cluster size\nFor workloads that need to be scaled based on the size of the cluster (for example cluster-dns or other system components), you\ncan use the Cluster Proportional Autoscaler. Just like the VPA, it is not part of the Kubernetes core, but hosted as its own project on\nGitHub.\nThe Cluster Proportional Autoscaler watches the number of schedulable nodes and cores and scales the number of replicas of the\ntarget workload accordingly.\nIf the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using the Cluster\nProportional Vertical Autoscaler. The project is currently in beta and can be found on GitHub.\nWhile the Cluster Proportional Autoscaler scales the number of replicas of a workload, the Cluster Proportional Vertical Autoscaler\nadjusts the resource requests for a workload (for example a Deployment or DaemonSet) based on the number of nodes and/or\ncores in the cluster.\n\nEvent driven Autoscaling\nIt is also possible to scale workloads based on events, for example using the Kubernetes Event Driven Autoscaler (KEDA).\nKEDA is a CNCF-graduated project enabling you to scale your workloads based on the number of events to be processed, for\nexample the amount of messages in a queue. There exists a wide range of adapters for different event sources to choose from.\n\nAutoscaling based on schedules\nAnother strategy for scaling your workloads is to schedule the scaling operations, for example in order to reduce resource\nconsumption during off-peak hours.\nSimilar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with its Cron scaler. The Cron scaler\nallows you to define schedules (and time zones) for scaling your workloads in or out.\n\nScaling cluster infrastructure\nIf scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.\nScaling the cluster infrastructure normally means adding or removing nodes. Read Node autoscaling for more information.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n209/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nLearn more about scaling horizontally\nScale a StatefulSet\nHorizontalPodAutoscaler Walkthrough\nResize Container Resources In-Place\nAutoscale the DNS Service in a Cluster\nLearn about Node autoscaling\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n210/684\n\n11/7/25, 4:37 PM\n\n4.4 - Managing Workloads\n\nConcepts | Kubernetes\n\nYou've deployed your application and exposed it via a Service. Now what? Kubernetes provides a number of tools to help you\nmanage your application deployment, including scaling and updating."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0229", "text": "Organizing resource configurations\nMany applications require multiple resources to be created, such as a Deployment along with a Service. Management of multiple\nresources can be simplified by grouping them together in the same file (separated by --- in YAML). For example:\napplication/nginx-app.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: my-nginx-svc\nlabels:\napp: nginx\nspec:\ntype: LoadBalancer\nports:\n- port: 80\nselector:\napp: nginx\n--apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: my-nginx\nlabels:\napp: nginx\nspec:\nreplicas: 3\nselector:\nmatchLabels:\napp: nginx\ntemplate:\nmetadata:\nlabels:\napp: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx:1.14.2\nports:\n- containerPort: 80\n\nMultiple resources can be created the same way as a single resource:\n\nkubectl apply -f https://k8s.io/examples/application/nginx-app.yaml\n\nservice/my-nginx-svc created\ndeployment.apps/my-nginx created\n\nThe resources will be created in the order they appear in the manifest. Therefore, it's best to specify the Service first, since that will\nensure the scheduler can spread the pods associated with the Service as they are created by the controller(s), such as Deployment.\nkubectl apply\n\nalso accepts multiple -f arguments:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n211/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml \\\n-f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n\nIt is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all\nof the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, you\ncan deploy all of the components of your stack together.\nA URL can also be specified as a configuration source, which is handy for deploying directly from manifests in your source control\nsystem:\n\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n\ndeployment.apps/my-nginx created\n\nIf you need to define more manifests, such as adding a ConfigMap, you can do that too.\n\nExternal tools\nThis section lists only the most common tools used for managing workloads on Kubernetes. To see a larger list, view Application\ndefinition and image build in the CNCF Landscape.\n\nHelm\nðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. More information\n\nHelm is a tool for managing packages of pre-configured Kubernetes resources. These packages are known as Helm charts.\n\nKustomize\nKustomize traverses a Kubernetes manifest to add, remove or update configuration options. It is available both as a standalone\nbinary and as a native feature of kubectl."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0230", "text": "Bulk operations in kubectl\nResource creation isn't the only operation that kubectl can perform in bulk. It can also extract resource names from configuration\nfiles in order to perform other operations, in particular to delete the same resources you created:\n\nkubectl delete -f https://k8s.io/examples/application/nginx-app.yaml\n\ndeployment.apps \"my-nginx\" deleted\nservice \"my-nginx-svc\" deleted\n\nIn the case of two resources, you can specify both resources on the command line using the resource/name syntax:\n\nkubectl delete deployments/my-nginx services/my-nginx-svc\n\nFor larger numbers of resources, you'll find it easier to specify the selector (label query) specified using -l or --selector , to filter\nresources by their labels:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n212/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl delete deployment,services -l app=nginx\n\ndeployment.apps \"my-nginx\" deleted\nservice \"my-nginx-svc\" deleted\n\nChaining and filtering\nBecause kubectl outputs resource names in the same syntax it accepts, you can chain operations using $() or xargs :\n\nkubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ )\nkubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ | xargs -i kubectl get '{}'\n\nThe output might be similar to:\nNAME\nmy-nginx-svc\n\nTYPE\nLoadBalancer\n\nCLUSTER-IP\n10.0.0.208\n\nEXTERNAL-IP\n<pending>\n\nPORT(S)\n80/TCP\n\nAGE\n0s\n\nWith the above commands, first you create resources under docs/concepts/cluster-administration/nginx/ and print the\nresources created with -o name output format (print each resource as resource/name). Then you grep only the Service, and then\nprint it with kubectl get .\n\nRecursive operations on local files\nIf you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the\noperations on the subdirectories also, by specifying --recursive or -R alongside the --filename / -f argument.\nFor instance, assume there is a directory project/k8s/development that holds all of the manifests needed for the development\nenvironment, organized by resource type:\nproject/k8s/development\nâ”œâ”€â”€ configmap\nâ”‚\nâ””â”€â”€ my-configmap.yaml\nâ”œâ”€â”€ deployment\nâ”‚\nâ””â”€â”€ my-deployment.yaml\nâ””â”€â”€ pvc\nâ””â”€â”€ my-pvc.yaml\n\nBy default, performing a bulk operation on project/k8s/development will stop at the first level of the directory, not processing any\nsubdirectories. If you had tried to create the resources in this directory using the following command, we would have encountered\nan error:\n\nkubectl apply -f project/k8s/development\n\nerror: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin)\n\nInstead, specify the --recursive or -R command line argument along with the --filename / -f argument:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0231", "text": "kubectl apply -f project/k8s/development --recursive\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n213/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nconfigmap/my-config created\ndeployment.apps/my-deployment created\npersistentvolumeclaim/my-pvc created\n\nThe --recursive argument works with any operation that accepts the --filename / -f argument such as: kubectl create ,\nkubectl get , kubectl delete , kubectl describe , or even kubectl rollout .\nThe --recursive argument also works when multiple -f arguments are provided:\n\nkubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive\n\nnamespace/development created\nnamespace/staging created\nconfigmap/my-config created\ndeployment.apps/my-deployment created\npersistentvolumeclaim/my-pvc created\n\nIf you're interested in learning more about kubectl , go ahead and read Command line tool (kubectl).\n\nUpdating your application without an outage\nAt some point, you'll eventually need to update your deployed application, typically by specifying a new image or image tag.\nkubectl supports several update operations, each of which is applicable to different scenarios.\nYou can run multiple copies of your app, and use a rollout to gradually shift the traffic to new healthy Pods. Eventually, all the\nrunning Pods would have the new software.\nThis section of the page guides you through how to create and update applications with Deployments.\nLet's say you were running version 1.14.2 of nginx:\n\nkubectl create deployment my-nginx --image=nginx:1.14.2\n\ndeployment.apps/my-nginx created\n\nEnsure that there is 1 replica:\n\nkubectl scale --replicas 1 deployments/my-nginx --subresource='scale' --type='merge' -p '{\"spec\":{\"replicas\": 1}}'\n\ndeployment.apps/my-nginx scaled\n\nand allow Kubernetes to add more temporary replicas during a rollout, by setting a surge maximum of 100%:\n\nkubectl patch --type='merge' -p '{\"spec\":{\"strategy\":{\"rollingUpdate\":{\"maxSurge\": \"100%\" }}}}'\n\ndeployment.apps/my-nginx patched\n\nTo update to version 1.16.1, change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1 using\nkubectl edit :\nhttps://kubernetes.io/docs/concepts/_print/\n\n214/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl edit deployment/my-nginx\n# Change the manifest to use the newer container image, then save your changes\n\nThat's it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that\nonly a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be\ncreated above the desired number of pods. To learn more details about how this happens, visit Deployment.\nYou can use rollouts with DaemonSets, Deployments, or StatefulSets."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0232", "text": "Managing rollouts\nYou can use kubectl rollout to manage a progressive update of an existing application.\nFor example:\n\nkubectl apply -f my-deployment.yaml\n# wait for rollout to finish\nkubectl rollout status deployment/my-deployment --timeout 10m # 10 minute timeout\n\nor\n\nkubectl apply -f backing-stateful-component.yaml\n# don't wait for rollout to finish, just check the status\nkubectl rollout status statefulsets/backing-stateful-component --watch=false\n\nYou can also pause, resume or cancel a rollout. Visit kubectl rollout to learn more.\n\nCanary deployments\nAnother scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same\ncomponent. It is common practice to deploy a canary of a new application release (specified via image tag in the pod template) side\nby side with the previous release so that the new release can receive live production traffic before fully rolling it out.\nFor instance, you can use a track label to differentiate different releases.\nThe primary, stable release would have a track label with value as stable :\nname: frontend\nreplicas: 3\n...\nlabels:\napp: guestbook\ntier: frontend\ntrack: stable\n...\nimage: gb-frontend:v3\n\nand then you can create a new release of the guestbook frontend that carries the track label with different value (i.e. canary ), so\nthat two sets of pods would not overlap:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n215/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nname: frontend-canary\nreplicas: 1\n...\nlabels:\napp: guestbook\ntier: frontend\ntrack: canary\n...\nimage: gb-frontend:v4\n\nThe frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the track label),\nso that the traffic will be redirected to both applications:\n\nselector:\napp: guestbook\ntier: frontend\n\nYou can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live\nproduction traffic (in this case, 3:1). Once you're confident, you can update the stable track to the new application release and\nremove the canary one.\n\nUpdating annotations\nSometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by\nAPI clients such as tools or libraries. This can be done with kubectl annotate . For example:\n\nkubectl annotate pods my-nginx-v4-9gw19 description='my frontend running nginx'\nkubectl get pods my-nginx-v4-9gw19 -o yaml\n\napiVersion: v1\nkind: pod\nmetadata:\nannotations:\ndescription: my frontend running nginx\n...\n\nFor more information, see annotations and kubectl annotate."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0233", "text": "Scaling your application\nWhen load on your application grows or shrinks, use kubectl to scale your application. For instance, to decrease the number of\nnginx replicas from 3 to 1, do:\n\nkubectl scale deployment/my-nginx --replicas=1\n\ndeployment.apps/my-nginx scaled\n\nNow you only have one pod managed by the deployment.\n\nkubectl get pods -l app=my-nginx\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n216/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nmy-nginx-2035384211-j5fhi\n\n1/1\n\nRunning\n\n0\n\n30m\n\nTo have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:\n\n# This requires an existing source of container and Pod metrics\nkubectl autoscale deployment/my-nginx --min=1 --max=3\n\nhorizontalpodautoscaler.autoscaling/my-nginx autoscaled\n\nNow your nginx replicas will be scaled up and down as needed, automatically.\nFor more information, please see kubectl scale, kubectl autoscale and horizontal pod autoscaler document.\n\nIn-place updates of resources\nSometimes it's necessary to make narrow, non-disruptive updates to resources you've created.\n\nkubectl apply\nIt is suggested to maintain a set of configuration files in source control (see configuration as code), so that they can be maintained\nand versioned along with the code for the resources they configure. Then, you can use kubectl apply to push your configuration\nchanges to the cluster.\nThis command will compare the version of the configuration that you're pushing with the previous version and apply the changes\nyou've made, without overwriting any automated changes to properties you haven't specified.\n\nkubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml\n\ndeployment.apps/my-nginx configured\n\nTo learn more about the underlying mechanism, read server-side apply.\n\nkubectl edit\nAlternatively, you may also update resources with kubectl edit :\n\nkubectl edit deployment/my-nginx\n\nThis is equivalent to first get the resource, edit it in text editor, and then apply the resource with the updated version:\n\nkubectl get deployment my-nginx -o yaml > /tmp/nginx.yaml\nvi /tmp/nginx.yaml\n# do some edit, and then save the file\nkubectl apply -f /tmp/nginx.yaml\ndeployment.apps/my-nginx configured\nrm /tmp/nginx.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n217/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThis allows you to do more significant changes more easily. Note that you can specify the editor with your EDITOR or KUBE_EDITOR\nenvironment variables.\nFor more information, please see kubectl edit.\n\nkubectl patch\nYou can use kubectl patch to update API objects in place. This subcommand supports JSON patch, JSON merge patch, and strategic\nmerge patch.\nSee Update API Objects in Place Using kubectl patch for more details."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0234", "text": "Disruptive updates\nIn some cases, you may need to update resource fields that cannot be updated once initialized, or you may want to make a recursive\nchange immediately, such as to fix broken pods created by a Deployment. To change such fields, use replace --force , which\ndeletes and re-creates the resource. In this case, you can modify your original configuration file:\n\nkubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force\n\ndeployment.apps/my-nginx deleted\ndeployment.apps/my-nginx replaced\n\nWhat's next\nLearn about how to use kubectl for application introspection and debugging.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n218/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5 - Services, Load Balancing, and Networking\nConcepts and resources behind networking in Kubernetes.\n\nThe Kubernetes network model\nThe Kubernetes network model is built out of several pieces:\nEach pod in a cluster gets its own unique cluster-wide IP address.\nA pod has its own private network namespace which is shared by all of the containers within the pod. Processes running\nin different containers in the same pod can communicate with each other over localhost .\nThe pod network (also called a cluster network) handles communication between pods. It ensures that (barring intentional\nnetwork segmentation):\nAll pods can communicate with all other pods, whether they are on the same node or on different nodes. Pods can\ncommunicate with each other directly, without the use of proxies or address translation (NAT).\nOn Windows, this rule does not apply to host-network pods.\nAgents on a node (such as system daemons, or kubelet) can communicate with all pods on that node.\nThe Service API lets you provide a stable (long lived) IP address or hostname for a service implemented by one or more\nbackend pods, where the individual pods making up the service can change over time.\nKubernetes automatically manages EndpointSlice objects to provide information about the pods currently backing a\nService.\nA service proxy implementation monitors the set of Service and EndpointSlice objects, and programs the data plane to\nroute service traffic to its backends, by using operating system or cloud provider APIs to intercept or rewrite packets.\nThe Gateway API (or its predecessor, Ingress) allows you to make Services accessible to clients that are outside the cluster.\nA simpler, but less-configurable, mechanism for cluster ingress is available via the Service API's type: LoadBalancer, when\nusing a supported Cloud Provider.\nNetworkPolicy is a built-in Kubernetes API that allows you to control traffic between pods, or between pods and the outside\nworld.\nIn older container systems, there was no automatic connectivity between containers on different hosts, and so it was often\nnecessary to explicitly create links between containers, or to map container ports to host ports to make them reachable by\ncontainers on other hosts. This is not needed in Kubernetes; Kubernetes's model is that pods can be treated much like VMs or\nphysical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and\nmigration.\nOnly a few parts of this model are implemented by Kubernetes itself. For the other parts, Kubernetes defines the APIs, but the\ncorresponding functionality is provided by external components, some of which are optional:\nPod network namespace setup is handled by system-level software implementing the Container Runtime Interface.\nThe pod network itself is managed by a pod network implementation. On Linux, most container runtimes use the\nContainer Networking Interface (CNI) to interact with the pod network implementation, so these implementations are often\ncalled CNI plugins.\nKubernetes provides a default implementation of service proxying, called kube-proxy, but some pod network implementations\ninstead use their own service proxy that is more tightly integrated with the rest of the implementation.\nNetworkPolicy is generally also implemented by the pod network implementation. (Some simpler pod network\nimplementations don't implement NetworkPolicy, or an administrator may choose to configure the pod network without\nNetworkPolicy support. In these cases, the API will still be present, but it will have no effect.)\nThere are many implementations of the Gateway API, some of which are specific to particular cloud environments, some more\nfocused on \"bare metal\" environments, and others more generic."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0235", "text": "https://kubernetes.io/docs/concepts/_print/\n\n219/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nThe Connecting Applications with Services tutorial lets you learn about Services and Kubernetes networking with a hands-on\nexample.\nCluster Networking explains how to set up networking for your cluster, and also provides an overview of the technologies involved.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n220/684\n\n11/7/25, 4:37 PM\n\n5.1 - Service\n\nConcepts | Kubernetes\n\nExpose an application running in your cluster behind a single outward-facing endpoint, even when the\nworkload is split across multiple backends.\nIn Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster.\nA key aim of Services in Kubernetes is that you don't need to modify your existing application to use an unfamiliar service discovery\nmechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you've\ncontainerized. You use a Service to make that set of Pods available on the network so that clients can interact with it.\nIf you use a Deployment to run your app, that Deployment can create and destroy Pods dynamically. From one moment to the next,\nyou don't know how many of those Pods are working and healthy; you might not even know what those healthy Pods are named.\nKubernetes Pods are created and destroyed to match the desired state of your cluster. Pods are ephemeral resources (you should\nnot expect that an individual Pod is reliable and durable).\nEach Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a given Deployment in your cluster, the\nset of Pods running in one moment in time could be different from the set of Pods running that application a moment later.\nThis leads to a problem: if some set of Pods (call them \"backends\") provides functionality to other Pods (call them \"frontends\") inside\nyour cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the\nbackend part of the workload?\nEnter Services."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0236", "text": "Services in Kubernetes\nThe Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a network. Each Service object defines\na logical set of endpoints (usually these endpoints are Pods) along with a policy about how to make those pods accessible.\nFor example, consider a stateless image-processing backend which is running with 3 replicas. Those replicas are fungibleâ€”frontends\ndo not care which backend they use. While the actual Pods that compose the backend set may change, the frontend clients should\nnot need to be aware of that, nor should they need to keep track of the set of backends themselves.\nThe Service abstraction enables this decoupling.\nThe set of Pods targeted by a Service is usually determined by a selector that you define. To learn about other ways to define Service\nendpoints, see Services without selectors.\nIf your workload speaks HTTP, you might choose to use an Ingress to control how web traffic reaches that workload. Ingress is not a\nService type, but it acts as the entry point for your cluster. An Ingress lets you consolidate your routing rules into a single resource,\nso that you can expose multiple components of your workload, running separately in your cluster, behind a single listener.\nThe Gateway API for Kubernetes provides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster - it is a\nfamily of extension APIs, implemented using CustomResourceDefinitions - and then use these to configure access to network\nservices that are running in your cluster.\n\nCloud-native service discovery\nIf you're able to use Kubernetes APIs for service discovery in your application, you can query the API server for matching\nEndpointSlices. Kubernetes updates the EndpointSlices for a Service whenever the set of Pods in a Service changes.\nFor non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the\nbackend Pods.\nEither way, your workload can use these service discovery mechanisms to find the target it wants to connect to.\n\nDefining a Service\nA Service is an object (the same way that a Pod or a ConfigMap is an object). You can create, view or modify Service definitions using\nthe Kubernetes API. Usually you use a tool such as kubectl to make those API calls for you.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n221/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0237", "text": "For example, suppose you have a set of Pods that each listen on TCP port 9376 and are labelled as app.kubernetes.io/name=MyApp .\nYou can define a Service to publish that TCP listener:\nservice/simple-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\n\nApplying this manifest creates a new Service named \"my-service\" with the default ClusterIP service type. The Service targets TCP port\n9376 on any Pod with the app.kubernetes.io/name: MyApp label.\nKubernetes assigns this Service an IP address (the cluster IP), that is used by the virtual IP address mechanism. For more details on\nthat mechanism, read Virtual IPs and Service Proxies.\nThe controller for that Service continuously scans for Pods that match its selector, and then makes any necessary updates to the set\nof EndpointSlices for the Service.\nThe name of a Service object must be a valid RFC 1035 label name.\nNote:\nA Service can map any incoming port to a targetPort. By default and for convenience, the targetPort is set to the same value\nas the port field.\n\nRelaxed naming requirements for Service objects\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\nThe RelaxedServiceNameValidation feature gate allows Service object names to start with a digit. When this feature gate is enabled,\nService object names must be valid RFC 1123 label names.\n\nPort definitions\nPort definitions in Pods have names, and you can reference these names in the targetPort attribute of a Service. For example, we\ncan bind the targetPort of the Service to the Pod port in the following way:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n222/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\napp.kubernetes.io/name: proxy\nspec:\ncontainers:\n- name: nginx\nimage: nginx:stable\nports:\n- containerPort: 80\nname: http-web-svc\n--apiVersion: v1\nkind: Service\nmetadata:\nname: nginx-service\nspec:\nselector:\napp.kubernetes.io/name: proxy\nports:\n- name: name-of-service-port\nprotocol: TCP\nport: 80\ntargetPort: http-web-svc"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0238", "text": "This works even if there is a mixture of Pods in the Service using a single configured name, with the same network protocol available\nvia different port numbers. This offers a lot of flexibility for deploying and evolving your Services. For example, you can change the\nport numbers that Pods expose in the next version of your backend software, without breaking clients.\nThe default protocol for Services is TCP; you can also use any other supported protocol.\nBecause many Services need to expose more than one port, Kubernetes supports multiple port definitions for a single Service. Each\nport definition can have the same protocol , or a different one.\n\nServices without selectors\nServices most commonly abstract access to Kubernetes Pods thanks to the selector, but when used with a corresponding set of\nEndpointSlices objects and without a selector, the Service can abstract other kinds of backends, including ones that run outside the\ncluster.\nFor example:\nYou want to have an external database cluster in production, but in your test environment you use your own databases.\nYou want to point your Service to a Service in a different Namespace or on another cluster.\nYou are migrating a workload to Kubernetes. While evaluating the approach, you run only a portion of your backends in\nKubernetes.\nIn any of these scenarios you can define a Service without specifying a selector to match Pods. For example:\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nports:\n- name: http\nprotocol: TCP\nport: 80\ntargetPort: 9376\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n223/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nBecause this Service has no selector, the corresponding EndpointSlice objects are not created automatically. You can map the\nService to the network address and port where it's running, by adding an EndpointSlice object manually. For example:\n\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\nname: my-service-1 # by convention, use the name of the Service\n# as a prefix for the name of the EndpointSlice\nlabels:\n# You should set the \"kubernetes.io/service-name\" label.\n# Set its value to match the name of the Service\nkubernetes.io/service-name: my-service\naddressType: IPv4\nports:\n- name: http # should match with the name of the service port defined above\nappProtocol: http\nprotocol: TCP\nport: 9376\nendpoints:\n- addresses:\n- \"10.4.5.6\"\n- addresses:\n- \"10.1.2.3\""}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0239", "text": "Custom EndpointSlices\nWhen you create an EndpointSlice object for a Service, you can use any name for the EndpointSlice. Each EndpointSlice in a\nnamespace must have a unique name. You link an EndpointSlice to a Service by setting the kubernetes.io/service-name label on\nthat EndpointSlice.\nNote:\nThe endpoint IPs must not be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and 224.0.0.0/24 for\nIPv4, fe80::/64 for IPv6).\nThe endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because kube-proxy doesn't support virtual\nIPs as a destination.\n\nFor an EndpointSlice that you create yourself, or in your own code, you should also pick a value to use for the label\nendpointslice.kubernetes.io/managed-by . If you create your own controller code to manage EndpointSlices, consider using a value\nsimilar to \"my-domain.example/name-of-controller\" . If you are using a third party tool, use the name of the tool in all-lowercase\nand change spaces and other punctuation to dashes ( - ). If people are directly using a tool such as kubectl to manage\nEndpointSlices, use a name that describes this manual management, such as \"staff\" or \"cluster-admins\" . You should avoid\nusing the reserved value \"controller\" , which identifies EndpointSlices managed by Kubernetes' own control plane.\n\nAccessing a Service without a selector\nAccessing a Service without a selector works the same as if it had a selector. In the example for a Service without a selector, traffic is\nrouted to one of the two endpoints defined in the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.\nNote:\nThe Kubernetes API server does not allow proxying to endpoints that are not mapped to pods. Actions such as kubectl portforward service/<service-name> forwardedPort:servicePort where the service has no selector will fail due to this\nconstraint. This prevents the Kubernetes API server from being used as a proxy to endpoints the caller may not be authorized\nto access.\nAn ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead. For more\ninformation, see the ExternalName section.\nhttps://kubernetes.io/docs/concepts/_print/\n\n224/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nEndpointSlices\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0240", "text": "EndpointSlices are objects that represent a subset (a slice) of the backing network endpoints for a Service.\nYour Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are so many endpoints for a Service that\na threshold is reached, then Kubernetes adds another empty EndpointSlice and stores new endpoint information there. By default,\nKubernetes makes a new EndpointSlice once the existing EndpointSlices all contain at least 100 endpoints. Kubernetes does not\nmake the new EndpointSlice until an extra endpoint needs to be added.\nSee EndpointSlices for more information about this API.\n\nEndpoints (deprecated)\nâ“˜ FEATURE STATE: Kubernetes v1.33 [deprecated]\n\nThe EndpointSlice API is the evolution of the older Endpoints API. The deprecated Endpoints API has several problems relative to\nEndpointSlice:\nIt does not support dual-stack clusters.\nIt does not contain information needed to support newer features, such as trafficDistribution.\nIt will truncate the list of endpoints if it is too long to fit in a single object.\nBecause of this, it is recommended that all clients use the EndpointSlice API rather than Endpoints.\n\nOver-capacity endpoints\nKubernetes limits the number of endpoints that can fit in a single Endpoints object. When there are over 1000 backing endpoints for\na Service, Kubernetes truncates the data in the Endpoints object. Because a Service can be linked with more than one EndpointSlice,\nthe 1000 backing endpoint limit only affects the legacy Endpoints API.\nIn that case, Kubernetes selects at most 1000 possible backend endpoints to store into the Endpoints object, and sets an annotation\non the Endpoints: endpoints.kubernetes.io/over-capacity: truncated . The control plane also removes that annotation if the\nnumber of backend Pods drops below 1000.\nTraffic is still sent to backends, but any load balancing mechanism that relies on the legacy Endpoints API only sends traffic to at\nmost 1000 of the available backing endpoints.\nThe same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.\n\nApplication protocol\nâ“˜ FEATURE STATE: Kubernetes v1.20 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0241", "text": "The appProtocol field provides a way to specify an application protocol for each Service port. This is used as a hint for\nimplementations to offer richer behavior for protocols that they understand. The value of this field is mirrored by the corresponding\nEndpoints and EndpointSlice objects.\nThis field follows standard Kubernetes label syntax. Valid values are one of:\nIANA standard service names.\nImplementation-defined prefixed names such as mycompany.com/my-custom-protocol .\nKubernetes-defined prefixed names:\nProtocol\n\nDescription\n\nkubernetes.io/h2c\n\nHTTP/2 over cleartext as described in RFC 7540\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n225/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nProtocol\n\nDescription\n\nkubernetes.io/ws\n\nWebSocket over cleartext as described in RFC 6455\n\nkubernetes.io/wss\n\nWebSocket over TLS as described in RFC 6455\n\nMulti-port Services\nFor some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service\nobject. When using multiple ports for a Service, you must give all of your ports names so that these are unambiguous. For example:\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- name: http\nprotocol: TCP\nport: 80\ntargetPort: 9376\n- name: https\nprotocol: TCP\nport: 443\ntargetPort: 9377\n\nNote:\nAs with Kubernetes names in general, names for ports must only contain lowercase alphanumeric characters and - . Port\nnames must also start and end with an alphanumeric character.\nFor example, the names 123-abc and web are valid, but 123_abc and -web are not.\n\nService type\nFor some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, one that's\naccessible from outside of your cluster.\nKubernetes Service types allow you to specify what kind of Service you want.\nThe available type values and their behaviors are:\nClusterIP\n\nExposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is\nthe default that is used if you don't explicitly specify a type for a Service. You can expose the Service to the public internet using\nan Ingress or a Gateway.\nNodePort\n\nExposes the Service on each Node's IP at a static port (the NodePort). To make the node port available, Kubernetes sets up a\ncluster IP address, the same as if you had requested a Service of type: ClusterIP.\nLoadBalancer"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0242", "text": "Exposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component;\nyou must provide one, or you can integrate your Kubernetes cluster with a cloud provider.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n226/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nExternalName\n\nMaps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). The mapping\nconfigures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set\nup.\nThe type field in the Service API is designed as nested functionality - each level adds to the previous. However there is an exception\nto this nested design. You can define a LoadBalancer Service by disabling the load balancer NodePort allocation.\n\ntype: ClusterIP\nThis default Service type assigns an IP address from a pool of IP addresses that your cluster has reserved for that purpose.\nSeveral of the other types for Service build on the ClusterIP type as a foundation.\nIf you define a Service that has the .spec.clusterIP set to \"None\" then Kubernetes does not assign an IP address. See headless\nServices for more information.\n\nChoosing your own IP address\nYou can specify your own cluster IP address as part of a Service creation request. To do this, set the .spec.clusterIP field. For\nexample, if you already have an existing DNS entry that you wish to reuse, or legacy systems that are configured for a specific IP\naddress and difficult to re-configure.\nThe IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is\nconfigured for the API server. If you try to create a Service with an invalid clusterIP address value, the API server will return a 422\nHTTP status code to indicate that there's a problem.\nRead avoiding collisions to learn how Kubernetes helps reduce the risk and impact of two different Services both trying to use the\nsame IP address."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0243", "text": "type: NodePort\nIf you set the type field to NodePort , the Kubernetes control plane allocates a port from a range specified by --service-nodeport-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. Your\nService reports the allocated port in its .spec.ports[*].nodePort field.\nUsing a NodePort gives you the freedom to set up your own load balancing solution, to configure environments that are not fully\nsupported by Kubernetes, or even to expose one or more nodes' IP addresses directly.\nFor a node port Service, Kubernetes additionally allocates a port (TCP, UDP or SCTP to match the protocol of the Service). Every node\nin the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with\nthat Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the\nappropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service).\n\nChoosing your own port\nIf you want a specific port number, you can specify a value in the nodePort field. The control plane will either allocate you that port\nor report that the API transaction failed. This means that you need to take care of possible port collisions yourself. You also have to\nuse a valid port number, one that's inside the range configured for NodePort use.\nHere is an example manifest for a Service of type: NodePort that specifies a NodePort value (30007, in this example):\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n227/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\ntype: NodePort\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- port: 80\n# By default and for convenience, the `targetPort` is set to\n# the same value as the `port` field.\ntargetPort: 80\n# Optional field\n# By default and for convenience, the Kubernetes control plane\n# will allocate a port from a range (default: 30000-32767)\nnodePort: 30007"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0244", "text": "Reserve Nodeport ranges to avoid collisions\nThe policy for assigning ports to NodePort services applies to both the auto-assignment and the manual assignment scenarios.\nWhen a user wants to create a NodePort service that uses a specific port, the target port may conflict with another port that has\nalready been assigned.\nTo avoid this problem, the port range for NodePort services is divided into two bands. Dynamic port assignment uses the upper\nband by default, and it may use the lower band once the upper band has been exhausted. Users can then allocate from the lower\nband with a lower risk of port collision.\n\nCustom IP address configuration for type: NodePort Services\nYou can set up nodes in your cluster to use a particular IP address for serving node port services. You might want to do this if each\nnode is connected to multiple networks (for example: one network for application traffic, and another network for traffic between\nnodes and the control plane).\nIf you want to specify particular IP address(es) to proxy the port, you can set the --nodeport-addresses flag for kube-proxy or the\nequivalent nodePortAddresses field of the kube-proxy configuration file to particular IP block(s).\nThis flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8 , 192.0.2.0/25 ) to specify IP address ranges that kube-proxy\nshould consider as local to this node.\nFor example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8 flag, kube-proxy only selects the loopback\ninterface for NodePort Services. The default for --nodeport-addresses is an empty list. This means that kube-proxy should consider\nall available network interfaces for NodePort. (That's also compatible with earlier Kubernetes releases.)\nNote:\nThis Service is visible as <NodeIP>:spec.ports[*].nodePort and .spec.clusterIP:spec.ports[*].port. If the --nodeportaddresses flag for kube-proxy or the equivalent field in the kube-proxy configuration file is set, <NodeIP> would be a filtered\nnode IP address (or possibly IP addresses).\n\ntype: LoadBalancer\nOn cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for\nyour Service. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer is\npublished in the Service's .status.loadBalancer field. For example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n228/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\nclusterIP: 10.0.171.239\ntype: LoadBalancer\nstatus:\nloadBalancer:\ningress:\n- ip: 192.0.2.127"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0245", "text": "Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced.\nTo implement a Service of type: LoadBalancer , Kubernetes typically starts off by making the changes that are equivalent to you\nrequesting a Service of type: NodePort . The cloud-controller-manager component then configures the external load balancer to\nforward traffic to that assigned node port.\nYou can configure a load balanced Service to omit assigning a node port, provided that the cloud provider implementation supports\nthis.\nSome cloud providers allow you to specify the loadBalancerIP . In those cases, the load-balancer is created with the user-specified\nloadBalancerIP . If the loadBalancerIP field is not specified, the load balancer is set up with an ephemeral IP address. If you\nspecify a loadBalancerIP but your cloud provider does not support the feature, the loadbalancerIP field that you set is ignored.\nNote:\nThe .spec.loadBalancerIP field for a Service was deprecated in Kubernetes v1.24.\nThis field was under-specified and its meaning varies across implementations. It also cannot support dual-stack networking.\nThis field may be removed in a future API version.\nIf you're integrating with a provider that supports specifying the load balancer IP address(es) for a Service via a (provider\nspecific) annotation, you should switch to doing that.\nIf you are writing code for a load balancer integration with Kubernetes, avoid using this field. You can integrate with Gateway\nrather than Service, or you can define your own (provider specific) annotations on the Service that specify the equivalent detail.\n\nNode liveness impact on load balancer traffic\nLoad balancer health checks are critical to modern applications. They are used to determine which server (virtual machine, or IP\naddress) the load balancer should dispatch traffic to. The Kubernetes APIs do not define how health checks have to be implemented\nfor Kubernetes managed load balancers, instead it's the cloud providers (and the people implementing integration code) who decide\non the behavior. Load balancer health checks are extensively used within the context of supporting the externalTrafficPolicy field\nfor Services.\n\nLoad balancers with mixed protocol types\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0246", "text": "By default, for LoadBalancer type of Services, when there is more than one port defined, all ports must have the same protocol, and\nthe protocol must be one which is supported by the cloud provider.\nThe feature gate MixedProtocolLBService (enabled by default for the kube-apiserver as of v1.24) allows the use of different\nprotocols for LoadBalancer type of Services, when there is more than one port defined.\nhttps://kubernetes.io/docs/concepts/_print/\n\n229/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nThe set of protocols that can be used for load balanced Services is defined by your cloud provider; they may impose\nrestrictions beyond what the Kubernetes API enforces.\n\nDisabling load balancer NodePort allocation\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nYou can optionally disable node port allocation for a Service of type: LoadBalancer , by setting the field\nspec.allocateLoadBalancerNodePorts to false . This should only be used for load balancer implementations that route traffic\ndirectly to pods as opposed to using node ports. By default, spec.allocateLoadBalancerNodePorts is true and type LoadBalancer\nServices will continue to allocate node ports. If spec.allocateLoadBalancerNodePorts is set to false on an existing Service with\nallocated node ports, those node ports will not be de-allocated automatically. You must explicitly remove the nodePorts entry in\nevery Service port to de-allocate those node ports.\n\nSpecifying class of load balancer implementation\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nFor a Service with type set to LoadBalancer , the .spec.loadBalancerClass field enables you to use a load balancer\nimplementation other than the cloud provider default.\nBy default, .spec.loadBalancerClass is not set and a LoadBalancer type of Service uses the cloud provider's default load balancer\nimplementation if the cluster is configured with a cloud provider using the --cloud-provider component flag.\nIf you specify .spec.loadBalancerClass , it is assumed that a load balancer implementation that matches the specified class is\nwatching for Services. Any default load balancer implementation (for example, the one provided by the cloud provider) will ignore\nServices that have this field set. spec.loadBalancerClass can be set on a Service of type LoadBalancer only. Once set, it cannot be\nchanged. The value of spec.loadBalancerClass must be a label-style identifier, with an optional prefix such as \" internal-vip \" or\n\" example.com/internal-vip \". Unprefixed names are reserved for end-users.\n\nLoad balancer IP address mode\nâ“˜ FEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0247", "text": "For a Service of type: LoadBalancer , a controller can set .status.loadBalancer.ingress.ipMode . The\n.status.loadBalancer.ingress.ipMode specifies how the load-balancer IP behaves. It may be specified only when the\n.status.loadBalancer.ingress.ip field is also specified.\nThere are two possible values for .status.loadBalancer.ingress.ipMode : \"VIP\" and \"Proxy\". The default value is \"VIP\" meaning that\ntraffic is delivered to the node with the destination set to the load-balancer's IP and port. There are two cases when setting this to\n\"Proxy\", depending on how the load-balancer from the cloud provider delivers the traffics:\nIf the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;\nIf the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.\nService implementations may use this information to adjust traffic routing.\n\nInternal load balancer\nIn a mixed environment it is sometimes necessary to route traffic from Services inside the same (virtual) network address block.\nIn a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your\nendpoints.\nTo set an internal load balancer, add one of the following annotations to your Service depending on the cloud service provider you're\nusing:\nhttps://kubernetes.io/docs/concepts/_print/\n\n230/684\n\n11/7/25, 4:37 PM\n\nDefault\n\nConcepts | Kubernetes\n\nGCP\n\nAWS\n\nAzure\n\nIBM Cloud\n\nOpenStack\n\nBaidu Cloud\n\nTencent Cloud\n\nAlibaba Cloud\n\nOCI\n\nSelect one of the tabs.\n\ntype: ExternalName\nServices of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service or cassandra . You\nspecify these Services with the spec.externalName parameter.\nThis Service definition, for example, maps the my-service Service in the prod namespace to my.database.example.com :\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nnamespace: prod\nspec:\ntype: ExternalName\nexternalName: my.database.example.com\n\nNote:\nA Service of type: ExternalName accepts an IPv4 address string, but treats that string as a DNS name comprised of digits, not\nas an IP address (the internet does not however allow such names in DNS). Services with external names that resemble IPv4\naddresses are not resolved by DNS servers.\nIf you want to map a Service directly to a specific IP address, consider using headless Services."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0248", "text": "When looking up the host my-service.prod.svc.cluster.local , the cluster DNS Service returns a CNAME record with the value\nmy.database.example.com . Accessing my-service works in the same way as other Services but with the crucial difference that\nredirection happens at the DNS level rather than via proxying or forwarding. Should you later decide to move your database into\nyour cluster, you can start its Pods, add appropriate selectors or endpoints, and change the Service's type .\nCaution:\nYou may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName\nthen the hostname used by clients inside your cluster is different from the name that the ExternalName references.\nFor protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a\nHost: header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the\nhostname that the client connected to.\n\nHeadless Services\nSometimes you don't need load-balancing and a single Service IP. In this case, you can create what are termed headless Services, by\nexplicitly specifying \"None\" for the cluster IP address ( .spec.clusterIP ).\nYou can use a headless Service to interface with other service discovery mechanisms, without being tied to Kubernetes'\nimplementation.\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or\nproxying done by the platform for them.\nA headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't configure routes\nand packet forwarding using virtual IP addresses and proxies; instead, headless Services report the endpoint IP addresses of the\nindividual pods via internal DNS records, served through the cluster's DNS service. To define a headless Service, you make a Service\nwith .spec.type set to ClusterIP (which is also the default for type ), and you additionally set .spec.clusterIP to None.\nhttps://kubernetes.io/docs/concepts/_print/\n\n231/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe string value None is a special case and is not the same as leaving the .spec.clusterIP field unset.\nHow DNS is automatically configured depends on whether the Service has selectors defined:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0249", "text": "With selectors\nFor headless Services that define selectors, the endpoints controller creates EndpointSlices in the Kubernetes API, and modifies the\nDNS configuration to return A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service.\n\nWithout selectors\nFor headless Services that do not define selectors, the control plane does not create EndpointSlice objects. However, the DNS\nsystem looks for and configures either:\nDNS CNAME records for type: ExternalName Services.\nDNS A / AAAA records for all IP addresses of the Service's ready endpoints, for all Service types other than ExternalName .\nFor IPv4 endpoints, the DNS system creates A records.\nFor IPv6 endpoints, the DNS system creates AAAA records.\nWhen you define a headless Service without a selector, the port must match the targetPort .\n\nDiscovering services\nFor clients running inside your cluster, Kubernetes supports two primary modes of finding a Service: environment variables and\nDNS.\n\nEnvironment variables\nWhen a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service. It adds\n{SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted\nto underscores.\nFor example, the Service redis-primary which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11,\nproduces the following environment variables:\n\nREDIS_PRIMARY_SERVICE_HOST=10.0.0.11\nREDIS_PRIMARY_SERVICE_PORT=6379\nREDIS_PRIMARY_PORT=tcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP=tcp://10.0.0.11:6379\nREDIS_PRIMARY_PORT_6379_TCP_PROTO=tcp\nREDIS_PRIMARY_PORT_6379_TCP_PORT=6379\nREDIS_PRIMARY_PORT_6379_TCP_ADDR=10.0.0.11\n\nNote:\nWhen you have a Pod that needs to access a Service, and you are using the environment variable method to publish the port\nand cluster IP to the client Pods, you must create the Service before the client Pods come into existence. Otherwise, those\nclient Pods won't have their environment variables populated.\nIf you only use DNS to discover the cluster IP for a Service, you don't need to worry about this ordering issue.\n\nKubernetes also supports and provides variables that are compatible with Docker Engine's \"legacy container links\" feature. You can\nread makeLinkVariables to see how this is implemented in Kubernetes.\n\nDNS\nYou can (and almost always should) set up a DNS service for your Kubernetes cluster using an add-on.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n232/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0250", "text": "A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services and creates a set of DNS records for\neach one. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their\nDNS name.\nFor example, if you have a Service called my-service in a Kubernetes namespace my-ns , the control plane and the DNS Service\nacting together create a DNS record for my-service.my-ns . Pods in the my-ns namespace should be able to find the service by\ndoing a name lookup for my-service ( my-service.my-ns would also work).\nPods in other namespaces must qualify the name as my-service.my-ns . These names will resolve to the cluster IP assigned for the\nService.\nKubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns Service has a port named http with\nthe protocol set to TCP , you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for http , as\nwell as the IP address.\nThe Kubernetes DNS server is the only way to access ExternalName Services. You can find more information about ExternalName\nresolution in DNS for Services and Pods.\n\nVirtual IP addressing mechanism\nRead Virtual IPs and Service Proxies explains the mechanism Kubernetes provides to expose a Service with a virtual IP address.\n\nTraffic policies\nYou can set the .spec.internalTrafficPolicy and .spec.externalTrafficPolicy fields to control how Kubernetes routes traffic to\nhealthy (â€œreadyâ€) backends.\nSee Traffic Policies for more details.\n\nTraffic distribution\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\n\nThe .spec.trafficDistribution field provides another way to influence traffic routing within a Kubernetes Service. While traffic\npolicies focus on strict semantic guarantees, traffic distribution allows you to express preferences (such as routing to topologically\ncloser endpoints). This can help optimize for performance, cost, or reliability. In Kubernetes 1.34, the following field value is\nsupported:\nPreferClose\n\nIndicates a preference for routing traffic to endpoints that are in the same zone as the client.\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)\n\nIn Kubernetes 1.34, two additional values are available (unless the PreferSameTrafficDistribution feature gate is disabled):\nPreferSameZone\n\nThis is an alias for PreferClose that is clearer about the intended semantics.\nPreferSameNode"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0251", "text": "Indicates a preference for routing traffic to endpoints that are on the same node as the client.\nIf the field is not set, the implementation will apply its default routing strategy.\nSee Traffic Distribution for more details\n\nSession stickiness\nIf you want to make sure that connections from a particular client are passed to the same Pod each time, you can configure session\naffinity based on the client's IP address. Read session affinity to learn more.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n233/684\n\n11/7/25, 4:37 PM\n\nExternal IPs\n\nConcepts | Kubernetes\n\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs . When\nnetwork traffic arrives into the cluster, with the external IP (as destination IP) and the port matching that Service, rules and routes\nthat Kubernetes has configured ensure that the traffic is routed to one of the endpoints for that Service.\nWhen you define a Service, you can specify externalIPs for any service type. In the example below, the Service named \"myservice\" can be accessed by clients using TCP, on \"198.51.100.32:80\" (calculated from .spec.externalIPs[] and\n.spec.ports[].port ).\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- name: http\nprotocol: TCP\nport: 80\ntargetPort: 49152\nexternalIPs:\n- 198.51.100.32\n\nNote:\nKubernetes does not manage allocation of externalIPs; these are the responsibility of the cluster administrator.\n\nAPI Object\nService is a top-level resource in the Kubernetes REST API. You can find more details about the Service API object.\n\nWhat's next\nLearn more about Services and how they fit into Kubernetes:\nFollow the Connecting Applications with Services tutorial.\nRead about Ingress, which exposes HTTP and HTTPS routes from outside the cluster to Services within your cluster.\nRead about Gateway, an extension to Kubernetes that provides more flexibility than Ingress.\nFor more context, read the following:\nVirtual IPs and Service Proxies\nEndpointSlices\nService API reference\nEndpointSlice API reference\nEndpoint API reference (legacy)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n234/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.2 - Ingress\n\nMake your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that\nunderstands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to\ndifferent backends based on rules you define via the Kubernetes API.\nâ“˜ FEATURE STATE: Kubernetes v1.19 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0252", "text": "An API object that manages external access to the services in a cluster, typically HTTP.\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\nNote:\nIngress is frozen. New features are being added to the Gateway API.\n\nTerminology\nFor clarity, this guide defines the following terms:\nNode: A worker machine in Kubernetes, part of a cluster.\nCluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common\nKubernetes deployments, nodes in the cluster are not part of the public internet.\nEdge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or\na physical piece of hardware.\nCluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes\nnetworking model.\nService: A Kubernetes Service that identifies a set of Pods using label selectors. Unless mentioned otherwise, Services are\nassumed to have virtual IPs only routable within the cluster network.\n\nWhat is Ingress?\nIngress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules\ndefined on the Ingress resource.\nHere is a simple example where an Ingress sends all its traffic to one Service:\ncluster\nPod\nclient\n\nIngress-managed\nload balancer\n\nIngress\n\nrouting rule\n\nService\nPod\n\nFigure. Ingress\nAn Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer namebased virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also\nconfigure your edge router or additional frontends to help handle the traffic.\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses\na service of type Service.Type=NodePort or Service.Type=LoadBalancer.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n235/684\n\n11/7/25, 4:37 PM\n\nPrerequisites\n\nConcepts | Kubernetes\n\nYou must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.\nYou may need to deploy an Ingress controller such as ingress-nginx. You can choose from a number of Ingress controllers.\nIdeally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly\ndifferently.\nNote:\nMake sure you review your Ingress controller's documentation to understand the caveats of choosing it."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0253", "text": "The Ingress resource\nA minimal Ingress resource example:\nservice/networking/minimal-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: minimal-ingress\nannotations:\nnginx.ingress.kubernetes.io/rewrite-target: /\nspec:\ningressClassName: nginx-example\nrules:\n- http:\npaths:\n- path: /testpath\npathType: Prefix\nbackend:\nservice:\nname: test\nport:\nnumber: 80\n\nAn Ingress needs apiVersion , kind , metadata and spec fields. The name of an Ingress object must be a valid DNS subdomain\nname. For general information about working with config files, see deploying applications, configuring containers, managing\nresources. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which\nis the rewrite-target annotation. Different Ingress controllers support different annotations. Review the documentation for your\nchoice of Ingress controller to learn which annotations are supported.\nThe Ingress spec has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of\nrules matched against all incoming requests. Ingress resource only supports rules for directing HTTP(S) traffic.\nIf the ingressClassName is omitted, a default Ingress class should be defined.\nThere are some ingress controllers, that work without the definition of a default IngressClass . For example, the Ingress-NGINX\ncontroller can be configured with a flag --watch-ingress-without-class . It is recommended though, to specify the default\nIngressClass as shown below.\n\nIngress rules\nEach HTTP rule contains the following information:\nAn optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address\nspecified. If a host is provided (for example, foo.bar.com), the rules apply to that host.\nA list of paths (for example, /testpath ), each of which has an associated backend defined with a service.name and a\nservice.port.name or service.port.number . Both the host and path must match the content of an incoming request before\nthe load balancer directs traffic to the referenced Service.\nhttps://kubernetes.io/docs/concepts/_print/\n\n236/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA backend is a combination of Service and port names as described in the Service doc or a custom resource backend by way of\na CRD. HTTP (and HTTPS) requests to the Ingress that match the host and path of the rule are sent to the listed backend.\nA defaultBackend is often configured in an Ingress controller to service any requests that do not match a path in the spec."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0254", "text": "DefaultBackend\nAn Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend is the backend that should handle\nrequests in that case. The defaultBackend is conventionally a configuration option of the Ingress controller and is not specified in\nyour Ingress resources. If no .spec.rules are specified, .spec.defaultBackend must be specified. If defaultBackend is not set, the\nhandling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your\ningress controller to find out how it handles this case).\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.\n\nResource backends\nA Resource backend is an ObjectRef to another Kubernetes resource within the same namespace as the Ingress object. A Resource\nis a mutually exclusive setting with Service, and will fail validation if both are specified. A common usage for a Resource backend is\nto ingress data to an object storage backend with static assets.\nservice/networking/ingress-resource-backend.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: ingress-resource-backend\nspec:\ndefaultBackend:\nresource:\napiGroup: k8s.example.com\nkind: StorageBucket\nname: static-assets\nrules:\n- http:\npaths:\n- path: /icons\npathType: ImplementationSpecific\nbackend:\nresource:\napiGroup: k8s.example.com\nkind: StorageBucket\nname: icon-assets\n\nAfter creating the Ingress above, you can view it with the following command:\n\nkubectl describe ingress ingress-resource-backend\n\nName:\n\ningress-resource-backend\n\nNamespace:\nAddress:\n\ndefault\n\nDefault backend:\n\nAPIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets\n\nRules:\nHost\n----\n\nPath\n----\n\nBackends\n--------\n\n*\nAnnotations:\n\n/icons\n<none>\n\nEvents:\n\n<none>\n\nhttps://kubernetes.io/docs/concepts/_print/\n\nAPIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets\n\n237/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPath types\n\nEach path in an Ingress is required to have a corresponding path type. Paths that do not include an explicit pathType will fail\nvalidation. There are three supported path types:\nImplementationSpecific : With this path type, matching is up to the IngressClass. Implementations can treat this as a separate\npathType\n\nor treat it identically to Prefix or Exact path types.\n\nExact : Matches the URL path exactly and with case sensitivity.\nPrefix : Matches based on a URL path prefix split by / . Matching is case sensitive and done on a path element by element"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0255", "text": "basis. A path element refers to the list of labels in the path split by the / separator. A request is a match for path p if every p is\nan element-wise prefix of p of the request path.\nNote:\nIf the last element of the path is a substring of the last element in request path, it is not a match (for example: /foo/bar\nmatches /foo/bar/baz, but does not match /foo/barbaz).\n\nExamples\nKind\n\nPath(s)\n\nRequest path(s)\n\nMatches?\n\nPrefix\n\n/\n\n(all paths)\n\nYes\n\nExact\n\n/foo\n\n/foo\n\nYes\n\nExact\n\n/foo\n\n/bar\n\nNo\n\nExact\n\n/foo\n\n/foo/\n\nNo\n\nExact\n\n/foo/\n\n/foo\n\nNo\n\nPrefix\n\n/foo\n\n/foo , /foo/\n\nYes\n\nPrefix\n\n/foo/\n\n/foo , /foo/\n\nYes\n\nPrefix\n\n/aaa/bb\n\n/aaa/bbb\n\nNo\n\nPrefix\n\n/aaa/bbb\n\n/aaa/bbb\n\nYes\n\nPrefix\n\n/aaa/bbb/\n\n/aaa/bbb\n\nYes, ignores trailing slash\n\nPrefix\n\n/aaa/bbb\n\n/aaa/bbb/\n\nYes, matches trailing slash\n\nPrefix\n\n/aaa/bbb\n\n/aaa/bbb/ccc\n\nYes, matches subpath\n\nPrefix\n\n/aaa/bbb\n\n/aaa/bbbxyz\n\nNo, does not match string prefix\n\nPrefix\n\n/ , /aaa\n\n/aaa/ccc\n\nYes, matches /aaa prefix\n\nPrefix\n\n/ , /aaa , /aaa/bbb\n\n/aaa/bbb\n\nYes, matches /aaa/bbb prefix\n\nPrefix\n\n/ , /aaa , /aaa/bbb\n\n/ccc\n\nYes, matches / prefix\n\nPrefix\n\n/aaa\n\n/ccc\n\nNo, uses default backend\n\nMixed\n\n/foo (Prefix), /foo (Exact)\n\n/foo\n\nYes, prefers Exact\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n238/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nMultiple matches\n\nIn some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest\nmatching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.\n\nHostname wildcards\nHosts can be precise matches (for example â€œ foo.bar.com â€) or a wildcard (for example â€œ *.foo.com â€). Precise matches require that\nthe HTTP host header matches the host field. Wildcard matches require the HTTP host header is equal to the suffix of the\nwildcard rule.\nHost\n\nHost header\n\nMatch?\n\n*.foo.com\n\nbar.foo.com\n\nMatches based on shared suffix\n\n*.foo.com\n\nbaz.bar.foo.com\n\nNo match, wildcard only covers a single DNS label\n\n*.foo.com\n\nfoo.com\n\nNo match, wildcard only covers a single DNS label\n\nservice/networking/ingress-wildcard-host.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: ingress-wildcard-host\nspec:\nrules:\n- host: \"foo.bar.com\"\nhttp:\npaths:\n- pathType: Prefix\npath: \"/bar\"\nbackend:\nservice:\nname: service1\nport:\nnumber: 80\n- host: \"*.foo.com\"\nhttp:\npaths:\n- pathType: Prefix\npath: \"/foo\"\nbackend:\nservice:\nname: service2\nport:\nnumber: 80"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0256", "text": "Ingress class\nIngresses can be implemented by different controllers, often with different configuration. Each Ingress should specify a class, a\nreference to an IngressClass resource that contains additional configuration including the name of the controller that should\nimplement the class.\nservice/networking/external-lb.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n239/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\nname: external-lb\nspec:\ncontroller: example.com/ingress-controller\nparameters:\napiGroup: k8s.example.com\nkind: IngressParameters\nname: external-lb\n\nThe .spec.parameters field of an IngressClass lets you reference another resource that provides configuration related to that\nIngressClass.\nThe specific type of parameters to use depends on the ingress controller that you specify in the .spec.controller field of the\nIngressClass.\n\nIngressClass scope\nDepending on your ingress controller, you may be able to use parameters that you set cluster-wide, or just for one namespace.\nCluster\n\nNamespaced\n\nThe default scope for IngressClass parameters is cluster-wide.\nIf you set the .spec.parameters field and don't set .spec.parameters.scope , or if you set\n.spec.parameters.scope to Cluster , then the IngressClass refers to a cluster-scoped resource. The\nkind (in combination the apiGroup ) of the parameters refers to a cluster-scoped API (possibly a\ncustom resource), and the name of the parameters identifies a specific cluster scoped resource for\nthat API.\nFor example:\n--apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\nname: external-lb-1\nspec:\ncontroller: example.com/ingress-controller\nparameters:\n# The parameters for this IngressClass are specified in a\n# ClusterIngressParameter (API group k8s.example.net) named\n# \"external-config-1\". This definition tells Kubernetes to\n# look for a cluster-scoped parameter resource.\nscope: Cluster\napiGroup: k8s.example.net\nkind: ClusterIngressParameter\nname: external-config-1\n\nDeprecated annotation\nBefore the IngressClass resource and ingressClassName field were added in Kubernetes 1.18, Ingress classes were specified with a\nkubernetes.io/ingress.class annotation on the Ingress. This annotation was never formally defined, but was widely supported by\nIngress controllers.\nThe newer ingressClassName field on Ingresses is a replacement for that annotation, but is not a direct equivalent. While the\nannotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field is a\nreference to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n240/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDefault IngressClass"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0257", "text": "You can mark a particular IngressClass as default for your cluster. Setting the ingressclass.kubernetes.io/is-default-class\nannotation to true on an IngressClass resource will ensure that new Ingresses without an ingressClassName field specified will be\nassigned this default IngressClass.\nCaution:\nIf you have more than one IngressClass marked as the default for your cluster, the admission controller prevents creating new\nIngress objects that don't have an ingressClassName specified. You can resolve this by ensuring that at most 1 IngressClass is\nmarked as default in your cluster.\nThere are some ingress controllers, that work without the definition of a default IngressClass . For example, the Ingress-NGINX\ncontroller can be configured with a flag --watch-ingress-without-class . It is recommended though, to specify the default\nIngressClass :\nservice/networking/default-ingressclass.yaml\napiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\nlabels:\napp.kubernetes.io/component: controller\nname: nginx-example\nannotations:\ningressclass.kubernetes.io/is-default-class: \"true\"\nspec:\ncontroller: k8s.io/ingress-nginx\n\nTypes of Ingress\nIngress backed by a single Service\nThere are existing Kubernetes concepts that allow you to expose a single Service (see alternatives). You can also do this with an\nIngress by specifying a default backend with no rules.\nservice/networking/test-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: test-ingress\nspec:\ndefaultBackend:\nservice:\nname: test\nport:\nnumber: 80\n\nIf you create it using kubectl apply -f you should be able to view the state of the Ingress you added:\n\nkubectl get ingress test-ingress\n\nNAME\n\nCLASS\n\nHOSTS\n\nADDRESS\n\nPORTS\n\nAGE\n\ntest-ingress\n\nexternal-lb\n\n*\n\n203.0.113.123\n\n80\n\n59s\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n241/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhere 203.0.113.123 is the IP allocated by the Ingress controller to satisfy this Ingress.\nNote:\nIngress controllers and load balancers may take a minute or two to allocate an IP address. Until that time, you often see the\naddress listed as <pending>.\n\nSimple fanout\nA fanout configuration routes traffic from a single IP address to more than one Service, based on the HTTP URI being requested. An\nIngress allows you to keep the number of load balancers down to a minimum. For example, a setup like:\ncluster\nPod\n/foo\n\nService service1:4200\nPod\n\nIngress-managed\nload balancer\n\nclient\n\nIngress, 178.91.123.132\nPod\n/bar\n\nService service2:8080\nPod"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0258", "text": "Figure. Ingress Fan Out\nIt would require an Ingress such as:\nservice/networking/simple-fanout-example.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: simple-fanout-example\nspec:\nrules:\n- host: foo.bar.com\nhttp:\npaths:\n- path: /foo\npathType: Prefix\nbackend:\nservice:\nname: service1\nport:\nnumber: 4200\n- path: /bar\npathType: Prefix\nbackend:\nservice:\nname: service2\nport:\nnumber: 8080\n\nWhen you create the Ingress with kubectl apply -f :\nhttps://kubernetes.io/docs/concepts/_print/\n\n242/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl describe ingress simple-fanout-example\n\nName:\nNamespace:\n\nsimple-fanout-example\ndefault\n\nAddress:\n\n178.91.123.132\n\nDefault backend:\nRules:\n\ndefault-http-backend:80 (10.8.2.3:8080)\n\nHost\n\nPath\n\nBackends\n\n---foo.bar.com\n\n----\n\n--------\n\n/foo\n\nservice1:4200 (10.8.0.90:4200)\n\n/bar\n\nservice2:8080 (10.8.0.91:8080)\n\nEvents:\nType\n\nReason\n\nAge\n\nFrom\n\nMessage\n\n----\n\n------\n\n----\n\n----\n\n-------\n\nNormal\n\nADD\n\n22s\n\nloadbalancer-controller\n\ndefault/test\n\nThe Ingress controller provisions an implementation-specific load balancer that satisfies the Ingress, as long as the Services\n( service1 , service2 ) exist. When it has done so, you can see the address of the load balancer at the Address field.\nNote:\nDepending on the Ingress controller you are using, you may need to create a default-http-backend Service.\n\nName based virtual hosting\nName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.\ncluster\nPod\nHost: foo.bar.com\n\nService service1:80\nPod\n\nclient\n\nIngress-managed\nload balancer\n\nIngress, 178.91.123.132\nPod\nHost: bar.foo.com\n\nService service2:80\nPod\n\nFigure. Ingress Name Based Virtual hosting\nThe following Ingress tells the backing load balancer to route requests based on the Host header.\nservice/networking/name-virtual-host-ingress.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n243/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: name-virtual-host-ingress\nspec:\nrules:\n- host: foo.bar.com\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: service1\nport:\nnumber: 80\n- host: bar.foo.com\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: service2\nport:\nnumber: 80\n\nIf you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress\ncontroller can be matched without a name based virtual host being required.\nFor example, the following Ingress routes traffic requested for first.bar.com to service1 , second.bar.com to service2 , and any\ntraffic whose request host header doesn't match first.bar.com and second.bar.com to service3 .\nservice/networking/name-virtual-host-ingress-no-third-host.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n244/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0259", "text": "apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: name-virtual-host-ingress-no-third-host\nspec:\nrules:\n- host: first.bar.com\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: service1\nport:\nnumber: 80\n- host: second.bar.com\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: service2\nport:\nnumber: 80\n- http:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: service3\nport:\nnumber: 80\n\nTLS\nYou can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. The Ingress resource only supports a\nsingle TLS port, 443, and assumes TLS termination at the ingress point (traffic to the Service and its Pods is in plaintext). If the TLS\nconfiguration section in an Ingress specifies different hosts, they are multiplexed on the same port according to the hostname\nspecified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named\ntls.crt and tls.key that contain the certificate and private key to use for TLS. For example:\n\napiVersion: v1\nkind: Secret\nmetadata:\nname: testsecret-tls\nnamespace: default\ndata:\ntls.crt: base64 encoded cert\ntls.key: base64 encoded key\ntype: kubernetes.io/tls\n\nReferencing this secret in an Ingress tells the Ingress controller to secure the channel from the client to the load balancer using TLS.\nYou need to make sure the TLS secret you created came from a certificate that contains a Common Name (CN), also known as a Fully\nQualified Domain Name (FQDN) for https-example.foo.com .\nNote:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n245/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nKeep in mind that TLS will not work on the default rule because the certificates would have to be issued for all the possible\nsub-domains. Therefore, hosts in the tls section need to explicitly match the host in the rules section.\n\nservice/networking/tls-example-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: tls-example-ingress\nspec:\ntls:\n- hosts:\n- https-example.foo.com\nsecretName: testsecret-tls\nrules:\n- host: https-example.foo.com\nhttp:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: service1\nport:\nnumber: 80\n\nNote:\nThere is a gap between TLS features supported by various Ingress controllers. Please refer to documentation on nginx, GCE, or\nany other platform specific Ingress controller to understand how TLS works in your environment."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0260", "text": "Load balancing\nAn Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load\nbalancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g. persistent sessions, dynamic\nweights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.\nIt's also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in\nKubernetes such as readiness probes that allow you to achieve the same end result. Please review the controller specific\ndocumentation to see how they handle health checks (for example: nginx, or GCE).\n\nUpdating an Ingress\nTo update an existing Ingress to add a new Host, you can update it by editing the resource:\n\nkubectl describe ingress test\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n246/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\nNamespace:\n\ntest\ndefault\n\nAddress:\n\n178.91.123.132\n\nDefault backend:\n\ndefault-http-backend:80 (10.8.2.3:8080)\n\nRules:\nHost\n\nPath\n\nBackends\n\n----\n\n----\n\n--------\n\n/foo\n\nservice1:80 (10.8.0.90:80)\n\nfoo.bar.com\nAnnotations:\nnginx.ingress.kubernetes.io/rewrite-target:\n\n/\n\nEvents:\nType\n\nReason\n\nAge\n\nFrom\n\nMessage\n\n----\n\n------\n\n----\n\n----\n\n-------\n\nNormal\n\nADD\n\n35s\n\nloadbalancer-controller\n\ndefault/test\n\nkubectl edit ingress test\n\nThis pops up an editor with the existing configuration in YAML format. Modify it to include the new Host:\n\nspec:\nrules:\n- host: foo.bar.com\nhttp:\npaths:\n- backend:\nservice:\nname: service1\nport:\nnumber: 80\npath: /foo\npathType: Prefix\n- host: bar.baz.com\nhttp:\npaths:\n- backend:\nservice:\nname: service2\nport:\nnumber: 80\npath: /foo\npathType: Prefix\n..\n\nAfter you save your changes, kubectl updates the resource in the API server, which tells the Ingress controller to reconfigure the load\nbalancer.\nVerify this:\n\nkubectl describe ingress test\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n247/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\n\ntest\n\nNamespace:\nAddress:\n\ndefault\n178.91.123.132\n\nDefault backend:\n\ndefault-http-backend:80 (10.8.2.3:8080)\n\nRules:\nHost\n\nPath\n\nBackends\n\n----\n\n----\n\n--------\n\n/foo\n\nservice1:80 (10.8.0.90:80)\n\n/foo\n\nservice2:80 (10.8.0.91:80)\n\nfoo.bar.com\nbar.baz.com\nAnnotations:\nnginx.ingress.kubernetes.io/rewrite-target:\n\n/\n\nEvents:\nType\n\nReason\n\nAge\n\nFrom\n\nMessage\n\n---Normal\n\n-----ADD\n\n---45s\n\n---loadbalancer-controller\n\n------default/test\n\nYou can achieve the same outcome by invoking kubectl replace -f on a modified Ingress YAML file.\n\nFailing across availability zones\nTechniques for spreading traffic across failure domains differ between cloud providers. Please check the documentation of the\nrelevant Ingress controller for details.\n\nAlternatives\nYou can expose a Service in multiple ways that don't directly involve the Ingress resource:\nUse Service.Type=LoadBalancer\nUse Service.Type=NodePort"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0261", "text": "What's next\nLearn about the Ingress API\nLearn about Ingress controllers\nSet up Ingress on Minikube with the NGINX Controller\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n248/684\n\n11/7/25, 4:37 PM\n\n5.3 - Ingress Controllers\n\nConcepts | Kubernetes\n\nIn order for an Ingress to work in your cluster, there must be an ingress controller running. You need to select\nat least one ingress controller and make sure it is set up in your cluster. This page lists common ingress\ncontrollers that you can deploy.\nIn order for the Ingress resource to work, the cluster must have an ingress controller running.\nUnlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started\nautomatically with a cluster. Use this page to choose the ingress controller implementation that best fits your cluster.\nKubernetes as a project supports and maintains AWS, GCE, and nginx ingress controllers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0262", "text": "Additional controllers\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nAKS Application Gateway Ingress Controller is an ingress controller that configures the Azure Application Gateway.\nAlibaba Cloud MSE Ingress is an ingress controller that configures the Alibaba Cloud Native Gateway, which is also the\ncommercial version of Higress.\nApache APISIX ingress controller is an Apache APISIX-based ingress controller.\nAvi Kubernetes Operator provides L4-L7 load-balancing using VMware NSX Advanced Load Balancer.\nBFE Ingress Controller is a BFE-based ingress controller.\nCilium Ingress Controller is an ingress controller powered by Cilium.\nThe Citrix ingress controller works with Citrix Application Delivery Controller.\nContour is an Envoy based ingress controller.\nEmissary-Ingress API Gateway is an Envoy-based ingress controller.\nEnRoute is an Envoy based API gateway that can run as an ingress controller.\nEasegress IngressController is an Easegress based API gateway that can run as an ingress controller.\nF5 BIG-IP Container Ingress Services for Kubernetes lets you use an Ingress to configure F5 BIG-IP virtual servers.\nFortiADC Ingress Controller support the Kubernetes Ingress resources and allows you to manage FortiADC objects from\nKubernetes\nGloo is an open-source ingress controller based on Envoy, which offers API gateway functionality.\nHAProxy Ingress is an ingress controller for HAProxy.\nHigress is an Envoy based API gateway that can run as an ingress controller.\nThe HAProxy Ingress Controller for Kubernetes is also an ingress controller for HAProxy.\nIstio Ingress is an Istio based ingress controller.\nThe Kong Ingress Controller for Kubernetes is an ingress controller driving Kong Gateway.\nKusk Gateway is an OpenAPI-driven ingress controller based on Envoy.\nThe NGINX Ingress Controller for Kubernetes works with the NGINX webserver (as a proxy).\nThe ngrok Kubernetes Ingress Controller is an open source controller for adding secure public access to your K8s services using\nthe ngrok platform.\nThe OCI Native Ingress Controller is an Ingress controller for Oracle Cloud Infrastructure which allows you to manage the OCI\nLoad Balancer.\nOpenNJet Ingress Controller is a OpenNJet-based ingress controller.\nThe Pomerium Ingress Controller is based on Pomerium, which offers context-aware access policy.\nSkipper HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a\nlibrary to build your custom proxy.\nThe Traefik Kubernetes Ingress provider is an ingress controller for the Traefik proxy.\nTyk Operator extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works\nwith the Open Source Tyk Gateway & Tyk Cloud control plane.\nVoyager is an ingress controller for HAProxy.\nhttps://kubernetes.io/docs/concepts/_print/"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0263", "text": "249/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWallarm Ingress Controller is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.\n\nUsing multiple Ingress controllers\nYou may deploy any number of ingress controllers using ingress class within a cluster. Note the .metadata.name of your ingress\nclass resource. When you create an ingress you would need that name to specify the ingressClassName field on your Ingress object\n(refer to IngressSpec v1 reference). ingressClassName is a replacement of the older annotation method.\nIf you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then\nKubernetes applies the cluster's default IngressClass to the Ingress. You mark an IngressClass as default by setting the\ningressclass.kubernetes.io/is-default-class annotation on that IngressClass, with the string value \"true\" .\nIdeally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.\nNote:\nMake sure you review your ingress controller's documentation to understand the caveats of choosing it.\n\nWhat's next\nLearn more about Ingress.\nSet up Ingress on Minikube with the NGINX Controller.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n250/684\n\n11/7/25, 4:37 PM\n\n5.4 - Gateway API\n\nConcepts | Kubernetes\n\nGateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic\nrouting.\nMake network services available by using an extensible, role-oriented, protocol-aware configuration mechanism. Gateway API is an\nadd-on containing API kinds that provide dynamic infrastructure provisioning and advanced traffic routing."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0264", "text": "Design principles\nThe following principles shaped the design and architecture of Gateway API:\nRole-oriented: Gateway API kinds are modeled after organizational roles that are responsible for managing Kubernetes service\nnetworking:\nInfrastructure Provider: Manages infrastructure that allows multiple isolated clusters to serve multiple tenants, e.g. a\ncloud provider.\nCluster Operator: Manages clusters and is typically concerned with policies, network access, application permissions, etc.\nApplication Developer: Manages an application running in a cluster and is typically concerned with application-level\nconfiguration and Service composition.\nPortable: Gateway API specifications are defined as custom resources and are supported by many implementations.\nExpressive: Gateway API kinds support functionality for common traffic routing use cases such as header-based matching,\ntraffic weighting, and others that were only possible in Ingress by using custom annotations.\nExtensible: Gateway allows for custom resources to be linked at various layers of the API. This makes granular customization\npossible at the appropriate places within the API structure.\n\nResource model\nGateway API has four stable API kinds:\nGatewayClass: Defines a set of gateways with common configuration and managed by a controller that implements the class.\nGateway: Defines an instance of traffic handling infrastructure, such as cloud load balancer.\nHTTPRoute: Defines HTTP-specific rules for mapping traffic from a Gateway listener to a representation of backend network\nendpoints. These endpoints are often represented as a Service.\nGRPCRoute: Defines gRPC-specific rules for mapping traffic from a Gateway listener to a representation of backend network\nendpoints. These endpoints are often represented as a Service.\nGateway API is organized into different API kinds that have interdependent relationships to support the role-oriented nature of\norganizations. A Gateway object is associated with exactly one GatewayClass; the GatewayClass describes the gateway controller\nresponsible for managing Gateways of this class. One or more route kinds such as HTTPRoute, are then associated to Gateways. A\nGateway can filter the routes that may be attached to its listeners , forming a bidirectional trust model with routes.\nThe following figure illustrates the relationships of the three stable Gateway API kinds:\ncluster\nHTTPRoute\n\nGateway\n\nGatewayClass\n\nGatewayClass\nGateways can be implemented by different controllers, often with different configurations. A Gateway must reference a\nGatewayClass that contains the name of the controller that implements the class.\nA minimal GatewayClass example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n251/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0265", "text": "apiVersion: gateway.networking.k8s.io/v1\nkind: GatewayClass\nmetadata:\nname: example-class\nspec:\ncontrollerName: example.com/gateway-controller\n\nIn this example, a controller that has implemented Gateway API is configured to manage GatewayClasses with the controller name\nexample.com/gateway-controller . Gateways of this class will be managed by the implementation's controller.\nSee the GatewayClass reference for a full definition of this API kind.\n\nGateway\nA Gateway describes an instance of traffic handling infrastructure. It defines a network endpoint that can be used for processing\ntraffic, i.e. filtering, balancing, splitting, etc. for backends such as a Service. For example, a Gateway may represent a cloud load\nbalancer or an in-cluster proxy server that is configured to accept HTTP traffic.\nA minimal Gateway resource example:\n\napiVersion: gateway.networking.k8s.io/v1\nkind: Gateway\nmetadata:\nname: example-gateway\nspec:\ngatewayClassName: example-class\nlisteners:\n- name: http\nprotocol: HTTP\nport: 80\n\nIn this example, an instance of traffic handling infrastructure is programmed to listen for HTTP traffic on port 80. Since the\naddresses field is unspecified, an address or hostname is assigned to the Gateway by the implementation's controller. This address\nis used as a network endpoint for processing traffic of backend network endpoints defined in routes.\nSee the Gateway reference for a full definition of this API kind.\n\nHTTPRoute\nThe HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to backend network endpoints. For a\nService backend, an implementation may represent the backend network endpoint as a Service IP or the backing EndpointSlices of\nthe Service. An HTTPRoute represents configuration that is applied to the underlying Gateway implementation. For example,\ndefining a new HTTPRoute may result in configuring additional traffic routes in a cloud load balancer or in-cluster proxy server.\nA minimal HTTPRoute example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n252/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\nname: example-httproute\nspec:\nparentRefs:\n- name: example-gateway\nhostnames:\n- \"www.example.com\"\nrules:\n- matches:\n- path:\ntype: PathPrefix\nvalue: /login\nbackendRefs:\n- name: example-svc\nport: 8080\n\nIn this example, HTTP traffic from Gateway example-gateway with the Host: header set to www.example.com and the request path\nspecified as /login will be routed to Service example-svc on port 8080 .\nSee the HTTPRoute reference for a full definition of this API kind."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0266", "text": "GRPCRoute\nThe GRPCRoute kind specifies routing behavior of gRPC requests from a Gateway listener to backend network endpoints. For a\nService backend, an implementation may represent the backend network endpoint as a Service IP or the backing EndpointSlices of\nthe Service. A GRPCRoute represents configuration that is applied to the underlying Gateway implementation. For example, defining\na new GRPCRoute may result in configuring additional traffic routes in a cloud load balancer or in-cluster proxy server.\nGateways supporting GRPCRoute are required to support HTTP/2 without an initial upgrade from HTTP/1, so gRPC traffic is\nguaranteed to flow properly.\nA minimal GRPCRoute example:\n\napiVersion: gateway.networking.k8s.io/v1\nkind: GRPCRoute\nmetadata:\nname: example-grpcroute\nspec:\nparentRefs:\n- name: example-gateway\nhostnames:\n- \"svc.example.com\"\nrules:\n- backendRefs:\n- name: example-svc\nport: 50051\n\nIn this example, gRPC traffic from Gateway example-gateway with the host set to svc.example.com will be directed to the service\nexample-svc on port 50051 from the same namespace.\nGRPCRoute allows matching specific gRPC services, as per the following example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n253/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: gateway.networking.k8s.io/v1\nkind: GRPCRoute\nmetadata:\nname: example-grpcroute\nspec:\nparentRefs:\n- name: example-gateway\nhostnames:\n- \"svc.example.com\"\nrules:\n- matches:\n- method:\nservice: com.example\nmethod: Login\nbackendRefs:\n- name: foo-svc\nport: 50051\n\nIn this case, the GRPCRoute will match any traffic for svc.example.com and apply its routing rules to forward the traffic to the correct\nbackend. Since there is only one match specified,only requests for the com.example.User.Login method to svc.example.com will be\nforwarded. RPCs of any other method` will not be matched by this Route.\nSee the GRPCRoute reference for a full definition of this API kind.\n\nRequest flow\nHere is a simple example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0267", "text": "In this example, the request flow for a Gateway implemented as a reverse proxy is:\n1. The client starts to prepare an HTTP request for the URL http://www.example.com\n2. The client's DNS resolver queries for the destination name and learns a mapping to one or more IP addresses associated with\nthe Gateway.\n3. The client sends a request to the Gateway IP address; the reverse proxy receives the HTTP request and uses the Host: header\nto match a configuration that was derived from the Gateway and attached HTTPRoute.\n4. Optionally, the reverse proxy can perform request header and/or path matching based on match rules of the HTTPRoute.\n5. Optionally, the reverse proxy can modify the request; for example, to add or remove headers, based on filter rules of the\nHTTPRoute.\n6. Lastly, the reverse proxy forwards the request to one or more backends.\n\nConformance\nGateway API covers a broad set of features and is widely implemented. This combination requires clear conformance definitions and\ntests to ensure that the API provides a consistent experience wherever it is used.\nSee the conformance documentation to understand details such as release channels, support levels, and running conformance\ntests.\n\nMigrating from Ingress\nGateway API is the successor to the Ingress API. However, it does not include the Ingress kind. As a result, a one-time conversion\nfrom your existing Ingress resources to Gateway API resources is necessary.\nhttps://kubernetes.io/docs/concepts/_print/\n\n254/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nRefer to the ingress migration guide for details on migrating Ingress resources to Gateway API resources.\n\nWhat's next\nInstead of Gateway API resources being natively implemented by Kubernetes, the specifications are defined as Custom Resources\nsupported by a wide range of implementations. Install the Gateway API CRDs or follow the installation instructions of your selected\nimplementation. After installing an implementation, use the Getting Started guide to help you quickly start working with Gateway\nAPI.\nNote:\nMake sure to review the documentation of your selected implementation to understand any caveats.\nRefer to the API specification for additional details of all Gateway API kinds.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n255/684\n\n11/7/25, 4:37 PM\n\n5.5 - EndpointSlices\n\nConcepts | Kubernetes\n\nThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large\nnumbers of backends, and allows the cluster to update its list of healthy backends efficiently.\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0268", "text": "EndpointSlices track the IP addresses of backend endpoints. EndpointSlices are normally associated with a Service and the backend\nendpoints typically represent Pods.\n\nEndpointSlice API\nIn Kubernetes, an EndpointSlice contains references to a set of network endpoints. The control plane automatically creates\nEndpointSlices for any Kubernetes Service that has a selector specified. These EndpointSlices include references to all the Pods that\nmatch the Service selector. EndpointSlices group network endpoints together by unique combinations of IP family, protocol, port\nnumber, and Service name. The name of a EndpointSlice object must be a valid DNS subdomain name.\nAs an example, here's a sample EndpointSlice object, that's owned by the example Kubernetes Service.\n\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\nname: example-abc\nlabels:\nkubernetes.io/service-name: example\naddressType: IPv4\nports:\n- name: http\nprotocol: TCP\nport: 80\nendpoints:\n- addresses:\n- \"10.1.2.3\"\nconditions:\nready: true\nhostname: pod-1\nnodeName: node-1\nzone: us-west2-a\n\nBy default, the control plane creates and manages EndpointSlices to have no more than 100 endpoints each. You can configure this\nwith the --max-endpoints-per-slice kube-controller-manager flag, up to a maximum of 1000.\nEndpointSlices act as the source of truth for kube-proxy when it comes to how to route internal traffic.\n\nAddress types\nEndpointSlices support two address types:\nIPv4\nIPv6\nEach EndpointSlice object represents a specific IP address type. If you have a Service that is available via IPv4 and IPv6, there will\nbe at least two EndpointSlice objects (one for IPv4, and one for IPv6).\n\nConditions\nThe EndpointSlice API stores conditions about endpoints that may be useful for consumers. The three conditions are serving ,\nterminating , and ready .\nhttps://kubernetes.io/docs/concepts/_print/\n\n256/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nServing\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]\n\nThe serving condition indicates that the endpoint is currently serving responses, and so it should be used as a target for Service\ntraffic. For endpoints backed by a Pod, this maps to the Pod's Ready condition.\n\nTerminating\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0269", "text": "The terminating condition indicates that the endpoint is terminating. For endpoints backed by a Pod, this condition is set when the\nPod is first deleted (that is, when it receives a deletion timestamp, but most likely before the Pod's containers exit).\nService proxies will normally ignore endpoints that are terminating , but they may route traffic to endpoints that are both serving\nand terminating if all available endpoints are terminating . (This helps to ensure that no Service traffic is lost during rolling\nupdates of the underlying Pods.)\n\nReady\nThe ready condition is essentially a shortcut for checking \" serving and not terminating \" (though it will also always be true for\nServices with spec.publishNotReadyAddresses set to true ).\n\nTopology information\nEach endpoint within an EndpointSlice can contain relevant topology information. The topology information includes the location of\nthe endpoint and information about the corresponding Node and zone. These are available in the following per endpoint fields on\nEndpointSlices:\nnodeName\nzone\n\n- The name of the Node this endpoint is on.\n\n- The zone this endpoint is in.\n\nManagement\nMost often, the control plane (specifically, the endpoint slice controller) creates and manages EndpointSlice objects. There are a\nvariety of other use cases for EndpointSlices, such as service mesh implementations, that could result in other entities or controllers\nmanaging additional sets of EndpointSlices.\nTo ensure that multiple entities can manage EndpointSlices without interfering with each other, Kubernetes defines the label\nendpointslice.kubernetes.io/managed-by , which indicates the entity managing an EndpointSlice. The endpoint slice controller sets\nendpointslice-controller.k8s.io as the value for this label on all EndpointSlices it manages. Other entities managing\nEndpointSlices should also set a unique value for this label.\n\nOwnership\nIn most use cases, EndpointSlices are owned by the Service that the endpoint slice object tracks endpoints for. This ownership is\nindicated by an owner reference on each EndpointSlice as well as a kubernetes.io/service-name label that enables simple lookups\nof all EndpointSlices belonging to a Service."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0270", "text": "Distribution of EndpointSlices\nEach EndpointSlice has a set of ports that applies to all endpoints within the resource. When named ports are used for a Service,\nPods may end up with different target port numbers for the same named port, requiring different EndpointSlices.\nThe control plane tries to fill EndpointSlices as full as possible, but does not actively rebalance them. The logic is fairly\nstraightforward:\n1. Iterate through existing EndpointSlices, remove endpoints that are no longer desired and update matching endpoints that have\nchanged.\n2. Iterate through EndpointSlices that have been modified in the first step and fill them up with any new endpoints needed.\nhttps://kubernetes.io/docs/concepts/_print/\n\n257/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3. If there's still new endpoints left to add, try to fit them into a previously unchanged slice and/or create new ones.\nImportantly, the third step prioritizes limiting EndpointSlice updates over a perfectly full distribution of EndpointSlices. As an\nexample, if there are 10 new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each, this approach will create\na new EndpointSlice instead of filling up the 2 existing EndpointSlices. In other words, a single EndpointSlice creation is preferable to\nmultiple EndpointSlice updates.\nWith kube-proxy running on each Node and watching EndpointSlices, every change to an EndpointSlice becomes relatively expensive\nsince it will be transmitted to every Node in the cluster. This approach is intended to limit the number of changes that need to be\nsent to every Node, even if it may result with multiple EndpointSlices that are not full.\nIn practice, this less than ideal distribution should be rare. Most changes processed by the EndpointSlice controller will be small\nenough to fit in an existing EndpointSlice, and if not, a new EndpointSlice is likely going to be necessary soon anyway. Rolling updates\nof Deployments also provide a natural repacking of EndpointSlices with all Pods and their corresponding endpoints getting replaced."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0271", "text": "Duplicate endpoints\nDue to the nature of EndpointSlice changes, endpoints may be represented in more than one EndpointSlice at the same time. This\nnaturally occurs as changes to different EndpointSlice objects can arrive at the Kubernetes client watch / cache at different times.\nNote:\nClients of the EndpointSlice API must iterate through all the existing EndpointSlices associated to a Service and build a\ncomplete list of unique network endpoints. It is important to mention that endpoints may be duplicated in different\nEndpointSlices.\nYou can find a reference implementation for how to perform this endpoint aggregation and deduplication as part of the\nEndpointSliceCache code within kube-proxy .\n\nEndpointSlice mirroring\nâ“˜ FEATURE STATE: Kubernetes v1.33 [deprecated]\n\nThe EndpointSlice API is a replacement for the older Endpoints API. To preserve compatibility with older controllers and user\nworkloads that expect kube-proxy to route traffic based on Endpoints resources, the cluster's control plane mirrors most usercreated Endpoints resources to corresponding EndpointSlices.\n(However, this feature, like the rest of the Endpoints API, is deprecated. Users who manually specify endpoints for selectorless\nServices should do so by creating EndpointSlice resources directly, rather than by creating Endpoints resources and allowing them to\nbe mirrored.)\nThe control plane mirrors Endpoints resources unless:\nthe Endpoints resource has a endpointslice.kubernetes.io/skip-mirror label set to true .\nthe Endpoints resource has a control-plane.alpha.kubernetes.io/leader annotation.\nthe corresponding Service resource does not exist.\nthe corresponding Service resource has a non-nil selector.\nIndividual Endpoints resources may translate into multiple EndpointSlices. This will occur if an Endpoints resource has multiple\nsubsets or includes endpoints with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per subset will be mirrored to\nEndpointSlices.\n\nWhat's next\nFollow the Connecting Applications with Services tutorial\nRead the API reference for the EndpointSlice API\nRead the API reference for the Endpoints API\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n258/684\n\n11/7/25, 4:37 PM\n\n5.6 - Network Policies\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0272", "text": "If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to\nspecify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster\nmust use a network plugin that supports NetworkPolicy enforcement.\nIf you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols, then you might consider using\nKubernetes NetworkPolicies for particular applications in your cluster. NetworkPolicies are an application-centric construct which\nallow you to specify how a pod is allowed to communicate with various network \"entities\" (we use the word \"entity\" here to avoid\noverloading the more common terms such as \"endpoints\" and \"services\", which have specific Kubernetes connotations) over the\nnetwork. NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.\nThe entities that a Pod can communicate with are identified through a combination of the following three identifiers:\n1. Other pods that are allowed (exception: a pod cannot block access to itself)\n2. Namespaces that are allowed\n3. IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the\nPod or the node)\nWhen defining a pod- or namespace-based NetworkPolicy, you use a selector to specify what traffic is allowed to and from the Pod(s)\nthat match the selector.\nMeanwhile, when IP-based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).\n\nPrerequisites\nNetwork policies are implemented by the network plugin. To use network policies, you must be using a networking solution which\nsupports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0273", "text": "The two sorts of pod isolation\nThere are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They concern what connections may be\nestablished. \"Isolation\" here is not absolute, rather it means \"some restrictions apply\". The alternative, \"non-isolated for $direction\",\nmeans that no restrictions apply in the stated direction. The two sorts of isolation (or not) are declared independently, and are both\nrelevant for a connection from one pod to another.\nBy default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is isolated for egress if there is any\nNetworkPolicy that both selects the pod and has \"Egress\" in its policyTypes ; we say that such a policy applies to the pod for egress.\nWhen a pod is isolated for egress, the only allowed connections from the pod are those allowed by the egress list of some\nNetworkPolicy that applies to the pod for egress. Reply traffic for those allowed connections will also be implicitly allowed. The\neffects of those egress lists combine additively.\nBy default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is isolated for ingress if there is any\nNetworkPolicy that both selects the pod and has \"Ingress\" in its policyTypes ; we say that such a policy applies to the pod for\ningress. When a pod is isolated for ingress, the only allowed connections into the pod are those from the pod's node and those\nallowed by the ingress list of some NetworkPolicy that applies to the pod for ingress. Reply traffic for those allowed connections\nwill also be implicitly allowed. The effects of those ingress lists combine additively.\nNetwork policies do not conflict; they are additive. If any policy or policies apply to a given pod for a given direction, the connections\nallowed in that direction from that pod is the union of what the applicable policies allow. Thus, order of evaluation does not affect\nthe policy result.\nFor a connection from a source pod to a destination pod to be allowed, both the egress policy on the source pod and the ingress\npolicy on the destination pod need to allow the connection. If either side does not allow the connection, it will not happen.\n\nThe NetworkPolicy resource\nSee the NetworkPolicy reference for a full definition of the resource.\nAn example NetworkPolicy might look like this:\nhttps://kubernetes.io/docs/concepts/_print/\n\n259/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0274", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nservice/networking/networkpolicy.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: test-network-policy\nnamespace: default\nspec:\npodSelector:\nmatchLabels:\nrole: db\npolicyTypes:\n- Ingress\n- Egress\ningress:\n- from:\n- ipBlock:\ncidr: 172.17.0.0/16\nexcept:\n- 172.17.1.0/24\n- namespaceSelector:\nmatchLabels:\nproject: myproject\n- podSelector:\nmatchLabels:\nrole: frontend\nports:\n- protocol: TCP\nport: 6379\negress:\n- to:\n- ipBlock:\ncidr: 10.0.0.0/24\nports:\n- protocol: TCP\nport: 5978\n\nNote:\nPOSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network\npolicy.\nMandatory Fields: As with all other Kubernetes config, a NetworkPolicy needs apiVersion , kind , and metadata fields. For general\ninformation about working with config files, see Configure a Pod to Use a ConfigMap, and Object Management.\nspec: NetworkPolicy spec has all the information needed to define a particular network policy in the given namespace.\npodSelector: Each NetworkPolicy includes a podSelector which selects the grouping of pods to which the policy applies. The\nexample policy selects pods with the label \"role=db\". An empty podSelector selects all pods in the namespace.\npolicyTypes: Each NetworkPolicy includes a policyTypes list which may include either Ingress , Egress , or both. The\npolicyTypes field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected\npods, or both. If no policyTypes are specified on a NetworkPolicy then by default Ingress will always be set and Egress will be\nset if the NetworkPolicy has any egress rules.\ningress: Each NetworkPolicy may include a list of allowed ingress rules. Each rule allows traffic which matches both the from and\nports sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first\nspecified via an ipBlock , the second via a namespaceSelector and the third via a podSelector .\negress: Each NetworkPolicy may include a list of allowed egress rules. Each rule allows traffic which matches both the to and\nports sections. The example policy contains a single rule, which matches traffic on a single port to any destination in 10.0.0.0/24 .\nSo, the example NetworkPolicy:\nhttps://kubernetes.io/docs/concepts/_print/\n\n260/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0275", "text": "1. isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)\n2. (Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:\nany pod in the default namespace with the label role=frontend\nany pod in a namespace with the label project=myproject\nIP addresses in the ranges 172.17.0.0 â€“ 172.17.0.255 and 172.17.2.0 â€“ 172.17.255.255 (ie, all of 172.17.0.0/16\nexcept 172.17.1.0/24 )\n3. (Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on\nTCP port 5978\nSee the Declare Network Policy walkthrough for further examples.\n\nBehavior of to and from selectors\nThere are four kinds of selectors that can be specified in an ingress\n\nfrom\n\nsection or egress\n\nto\n\nsection:\n\npodSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources\nor egress destinations.\nnamespaceSelector: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress\ndestinations.\nnamespaceSelector and podSelector: A single to / from entry that specifies both namespaceSelector and podSelector selects\nparticular Pods within particular namespaces. Be careful to use correct YAML syntax. For example:\n\n...\ningress:\n- from:\n- namespaceSelector:\nmatchLabels:\nuser: alice\npodSelector:\nmatchLabels:\nrole: client\n...\n\nThis policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the\nlabel user=alice . But the following policy is different:\n\n...\ningress:\n- from:\n- namespaceSelector:\nmatchLabels:\nuser: alice\n- podSelector:\nmatchLabels:\nrole: client\n...\n\nIt contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client ,\nor from any Pod in any namespace with the label user=alice .\nWhen in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.\nipBlock: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external\nIPs, since Pod IPs are ephemeral and unpredictable.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n261/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0276", "text": "Cluster ingress and egress mechanisms often require rewriting the source or destination IP of packets. In cases where this happens,\nit is not defined whether this happens before or after NetworkPolicy processing, and the behavior may be different for different\ncombinations of network plugin, cloud provider, Service implementation, etc.\nIn the case of ingress, this means that in some cases you may be able to filter incoming packets based on the actual original source\nIP, while in other cases, the \"source IP\" that the NetworkPolicy acts on may be the IP of a LoadBalancer or of the Pod's node, etc.\nFor egress, this means that connections from pods to Service IPs that get rewritten to cluster-external IPs may or may not be\nsubject to ipBlock -based policies.\n\nDefault policies\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The\nfollowing examples let you change the default behavior in that namespace.\n\nDefault deny all ingress traffic\nYou can create a \"default\" ingress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not\nallow any ingress traffic to those pods.\nservice/networking/network-policy-default-deny-ingress.yaml\n--apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-ingress\nspec:\npodSelector: {}\npolicyTypes:\n- Ingress\n\nThis ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated for ingress. This policy does not\naffect isolation for egress from any pod.\n\nAllow all ingress traffic\nIf you want to allow all incoming connections to all pods in a namespace, you can create a policy that explicitly allows that.\nservice/networking/network-policy-allow-all-ingress.yaml\n--apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-all-ingress\nspec:\npodSelector: {}\ningress:\n- {}\npolicyTypes:\n- Ingress\n\nWith this policy in place, no additional policy or policies can cause any incoming connection to those pods to be denied. This policy\nhas no effect on isolation for egress from any pod.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n262/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDefault deny all egress traffic\n\nYou can create a \"default\" egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not\nallow any egress traffic from those pods.\nservice/networking/network-policy-default-deny-egress.yaml\n--apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-egress\nspec:\npodSelector: {}\npolicyTypes:\n- Egress"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0277", "text": "This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not\nchange the ingress isolation behavior of any pod.\n\nAllow all egress traffic\nIf you want to allow all connections from all pods in a namespace, you can create a policy that explicitly allows all outgoing\nconnections from pods in that namespace.\nservice/networking/network-policy-allow-all-egress.yaml\n--apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-all-egress\nspec:\npodSelector: {}\negress:\n- {}\npolicyTypes:\n- Egress\n\nWith this policy in place, no additional policy or policies can cause any outgoing connection from those pods to be denied. This policy\nhas no effect on isolation for ingress to any pod.\n\nDefault deny all ingress and all egress traffic\nYou can create a \"default\" policy for a namespace which prevents all ingress AND egress traffic by creating the following\nNetworkPolicy in that namespace.\nservice/networking/network-policy-default-deny-all.yaml\n--apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-all\nspec:\npodSelector: {}\npolicyTypes:\n- Ingress\n- Egress\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n263/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThis ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed ingress or egress traffic.\n\nNetwork traffic filtering\nNetworkPolicy is defined for layer 4 connections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary\nacross network plugins.\nNote:\nYou must be using a CNI plugin that supports SCTP protocol NetworkPolicies.\nWhen a deny all network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP connections. For other protocols, such\nas ARP or ICMP, the behaviour is undefined. The same applies to allow rules: when a specific pod is allowed as ingress source or\negress destination, it is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some\nnetwork plugins and denied by others.\n\nTargeting a range of ports\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nWhen writing a NetworkPolicy, you can target a range of ports instead of a single port.\nThis is achievable with the usage of the endPort field, as the following example:\nservice/networking/networkpolicy-multiport-egress.yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: multi-port-egress\nnamespace: default\nspec:\npodSelector:\nmatchLabels:\nrole: db\npolicyTypes:\n- Egress\negress:\n- to:\n- ipBlock:\ncidr: 10.0.0.0/24\nports:\n- protocol: TCP\nport: 32000\nendPort: 32768"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0278", "text": "The above rule allows any Pod with label role=db on the namespace default to communicate with any IP within the range\n10.0.0.0/24 over TCP, provided that the target port is between the range 32000 and 32768.\nThe following restrictions apply when using this field:\nThe endPort field must be equal to or greater than the port field.\nendPort\n\ncan only be defined if port is also defined.\n\nBoth ports must be numeric.\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n264/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYour cluster must be using a CNI plugin that supports the endPort field in NetworkPolicy specifications. If your network plugin\ndoes not support the endPort field and you specify a NetworkPolicy with that, the policy will be applied only for the single port\nfield.\n\nTargeting multiple namespaces by label\nIn this scenario, your Egress NetworkPolicy targets more than one namespace using their label names. For this to work, you need\nto label the target namespaces. For example:\n\nkubectl label namespace frontend namespace=frontend\nkubectl label namespace backend namespace=backend\n\nAdd the labels under namespaceSelector in your NetworkPolicy document. For example:\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: egress-namespaces\nspec:\npodSelector:\nmatchLabels:\napp: myapp\npolicyTypes:\n- Egress\negress:\n- to:\n- namespaceSelector:\nmatchExpressions:\n- key: namespace\noperator: In\nvalues: [\"frontend\", \"backend\"]\n\nNote:\nIt is not possible to directly specify the name of the namespaces in a NetworkPolicy. You must use a namespaceSelector with\nmatchLabels or matchExpressions to select the namespaces based on their labels.\n\nTargeting a Namespace by its name\nThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces, the value of the label is\nthe namespace name.\nWhile NetworkPolicy cannot target a namespace by its name with some object field, you can use the standardized label to target a\nspecific namespace.\n\nPod lifecycle\nNote:\nThe following applies to clusters with a conformant networking plugin and a conformant implementation of NetworkPolicy.\nWhen a new NetworkPolicy object is created, it may take some time for a network plugin to handle the new object. If a pod that is\naffected by a NetworkPolicy is created before the network plugin has completed NetworkPolicy handling, that pod may be started\nunprotected, and isolation rules will be applied when the NetworkPolicy handling is completed.\nhttps://kubernetes.io/docs/concepts/_print/\n\n265/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0279", "text": "Once the NetworkPolicy is handled by a network plugin,\n1. All newly created pods affected by a given NetworkPolicy will be isolated before they are started. Implementations of\nNetworkPolicy must ensure that filtering is effective throughout the Pod lifecycle, even from the very first instant that any\ncontainer in that Pod is started. Because they are applied at Pod level, NetworkPolicies apply equally to init containers, sidecar\ncontainers, and regular containers.\n2. Allow rules will be applied eventually after the isolation rules (or may be applied at the same time). In the worst case, a newly\ncreated pod may have no network connectivity at all when it is first started, if isolation rules were already applied, but no allow\nrules were applied yet.\nEvery created NetworkPolicy will be handled by a network plugin eventually, but there is no way to tell from the Kubernetes API\nwhen exactly that happens.\nTherefore, pods must be resilient against being started up with different network connectivity than expected. If you need to make\nsure the pod can reach certain destinations before being started, you can use an init container to wait for those destinations to be\nreachable before kubelet starts the app containers.\nEvery NetworkPolicy will be applied to all selected pods eventually. Because the network plugin may implement NetworkPolicy in a\ndistributed manner, it is possible that pods may see a slightly inconsistent view of network policies when the pod is first created, or\nwhen pods or policies change. For example, a newly-created pod that is supposed to be able to reach both Pod A on Node 1 and Pod\nB on Node 2 may find that it can reach Pod A immediately, but cannot reach Pod B until a few seconds later."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0280", "text": "NetworkPolicy and hostNetwork pods\nNetworkPolicy behaviour for hostNetwork pods is undefined, but it should be limited to 2 possibilities:\nThe network plugin can distinguish hostNetwork pod traffic from all other traffic (including being able to distinguish traffic\nfrom different hostNetwork pods on the same node), and will apply NetworkPolicy to hostNetwork pods just like it does to\npod-network pods.\nThe network plugin cannot properly distinguish hostNetwork pod traffic, and so it ignores hostNetwork pods when matching\npodSelector and namespaceSelector . Traffic to/from hostNetwork pods is treated the same as all other traffic to/from the\nnode IP. (This is the most common implementation.)\nThis applies when\n1. a hostNetwork pod is selected by spec.podSelector .\n\n...\nspec:\npodSelector:\nmatchLabels:\nrole: client\n...\n\n2. a hostNetwork pod is selected by a podSelector or namespaceSelector in an ingress or egress rule.\n\n...\ningress:\n- from:\n- podSelector:\nmatchLabels:\nrole: client\n...\n\nAt the same time, since hostNetwork pods have the same IP addresses as the nodes they reside on, their connections will be treated\nas node connections. For example, you can allow traffic from a hostNetwork Pod using an ipBlock rule.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n266/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0281", "text": "What you can't do with network policies (at least, not yet)\nAs of Kubernetes 1.34, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement\nworkarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies\n(Ingress controllers, Service Mesh implementations) or admission controllers. In case you are new to network security in Kubernetes,\nits worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.\nForcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).\nAnything TLS related (use a service mesh or ingress controller for this).\nNode specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities\nspecifically).\nTargeting of services by name (you can, however, target pods or namespaces by their labels, which is often a viable\nworkaround).\nCreation or management of \"Policy requests\" that are fulfilled by a third party.\nDefault policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects\nwhich can do this).\nAdvanced policy querying and reachability tooling.\nThe ability to log network security events (for example connections that are blocked or accepted).\nThe ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add\nallow rules).\nThe ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the\nability to block access from their resident node).\n\nNetworkPolicy's impact on existing connections\nWhen the set of NetworkPolicies that applies to an existing connection changes - this could happen either due to a change in\nNetworkPolicies or if the relevant labels of the namespaces/pods selected by the policy (both subject and peers) are changed in the\nmiddle of an existing connection - it is implementation defined as to whether the change will take effect for that existing connection\nor not. Example: A policy is created that leads to denying a previously allowed connection, the underlying network plugin\nimplementation is responsible for defining if that new policy will close the existing connections or not. It is recommended not to\nmodify policies/pods/namespaces in ways that might affect existing connections."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0282", "text": "What's next\nSee the Declare Network Policy walkthrough for further examples.\nSee more recipes for common scenarios enabled by the NetworkPolicy resource.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n267/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.7 - DNS for Services and Pods\n\nYour workload can discover Services within your cluster using DNS; this page explains how that works.\nKubernetes creates DNS records for Services and Pods. You can contact Services with consistent DNS names instead of IP addresses.\nKubernetes publishes information about Pods and Services which is used to program DNS. kubelet configures Pods' DNS so that\nrunning containers can look up Services by name rather than IP.\nServices defined in the cluster are assigned DNS names. By default, a client Pod's DNS search list includes the Pod's own namespace\nand the cluster's default domain.\n\nNamespaces of Services\nA DNS query may return different results based on the namespace of the Pod making it. DNS queries that don't specify a namespace\nare limited to the Pod's namespace. Access Services in other namespaces by specifying it in the DNS query.\nFor example, consider a Pod in a test namespace. A data Service is in the prod namespace.\nA query for data returns no results, because it uses the Pod's test namespace.\nA query for data.prod returns the intended result, because it specifies the namespace.\nDNS queries may be expanded using the Pod's /etc/resolv.conf . kubelet configures this file for each Pod. For example, a query for\njust data may be expanded to data.test.svc.cluster.local . The values of the search option are used to expand queries. To\nlearn more about DNS queries, see the resolv.conf manual page.\nnameserver 10.32.0.10\nsearch <namespace>.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n\nIn summary, a Pod in the test namespace can successfully resolve either data.prod or data.prod.svc.cluster.local .\n\nDNS Records\nWhat objects get DNS records?\n1. Services\n2. Pods\nThe following sections detail the supported DNS record types and layout that is supported. Any other layout or names or queries\nthat happen to work are considered implementation details and are subject to change without warning. For more up-to-date\nspecification, see Kubernetes DNS-Based Service Discovery."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0283", "text": "Services\nA/AAAA records\n\"Normal\" (not headless) Services are assigned DNS A and/or AAAA records, depending on the IP family or families of the Service, with\na name of the form my-svc.my-namespace.svc.cluster-domain.example . This resolves to the cluster IP of the Service.\nHeadless Services (without a cluster IP) are also assigned DNS A and/or AAAA records, with a name of the form my-svc.mynamespace.svc.cluster-domain.example . Unlike normal Services, this resolves to the set of IPs of all of the Pods selected by the\nService. Clients are expected to consume the set or else use standard round-robin selection from the set.\n\nSRV records\nSRV Records are created for named ports that are part of normal or headless services.\nFor each named port, the SRV record has the form _port-name._port-protocol.my-svc.my-namespace.svc.clusterdomain.example .\nhttps://kubernetes.io/docs/concepts/_print/\n\n268/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor a regular Service, this resolves to the port number and the domain name: my-svc.my-namespace.svc.clusterdomain.example .\nFor a headless Service, this resolves to multiple answers, one for each Pod that is backing the Service, and contains the port\nnumber and the domain name of the Pod of the form hostname.my-svc.my-namespace.svc.cluster-domain.example .\n\nPods\nA/AAAA records\nKube-DNS versions, prior to the implementation of the DNS specification, had the following DNS resolution:\n<pod-IPv4-address>.<namespace>.pod.<cluster-domain>\n\nFor example, if a Pod in the default namespace has the IP address 172.17.0.3, and the domain name for your cluster is\ncluster.local , then the Pod has a DNS name:\n172-17-0-3.default.pod.cluster.local\n\nSome cluster DNS mechanisms, like CoreDNS, also provide A records for:\n<pod-ipv4-address>.<service-name>.<my-namespace>.svc.<cluster-domain.example>\n\nFor example, if a Pod in the cafe namespace has the IP address 172.17.0.3, is an endpoint of a Service named barista , and the\ndomain name for your cluster is cluster.local , then the Pod would have this service-scoped DNS A record.\n172-17-0-3.barista.cafe.svc.cluster.local"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0284", "text": "Pod's hostname and subdomain fields\nCurrently when a Pod is created, its hostname (as observed from within the Pod) is the Pod's metadata.name value.\nThe Pod spec has an optional hostname field, which can be used to specify a different hostname. When specified, it takes\nprecedence over the Pod's name to be the hostname of the Pod (again, as observed from within the Pod). For example, given a Pod\nwith spec.hostname set to \"my-host\" , the Pod will have its hostname set to \"my-host\" .\nThe Pod spec also has an optional subdomain field which can be used to indicate that the pod is part of sub-group of the\nnamespace. For example, a Pod with spec.hostname set to \"foo\" , and spec.subdomain set to \"bar\" , in namespace \"mynamespace\" , will have its hostname set to \"foo\" and its fully qualified domain name (FQDN) set to \"foo.bar.mynamespace.svc.cluster.local\" (once more, as observed from within the Pod).\nIf there exists a headless Service in the same namespace as the Pod, with the same name as the subdomain, the cluster's DNS\nServer also returns A and/or AAAA records for the Pod's fully qualified hostname.\nExample:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n269/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: busybox-subdomain\nspec:\nselector:\nname: busybox\nclusterIP: None\nports:\n- name: foo # name is not required for single-port Services\nport: 1234\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: busybox1\nlabels:\nname: busybox\nspec:\nhostname: busybox-1\nsubdomain: busybox-subdomain\ncontainers:\n- image: busybox:1.28\ncommand:\n- sleep\n- \"3600\"\nname: busybox\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: busybox2\nlabels:\nname: busybox\nspec:\nhostname: busybox-2\nsubdomain: busybox-subdomain\ncontainers:\n- image: busybox:1.28\ncommand:\n- sleep\n- \"3600\"\nname: busybox"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0285", "text": "Given the above Service \"busybox-subdomain\" and the Pods which set spec.subdomain to \"busybox-subdomain\" , the first Pod will\nsee its own FQDN as \"busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example\" . DNS serves A and/or AAAA\nrecords at that name, pointing to the Pod's IP. Both Pods \" busybox1 \" and \" busybox2 \" will have their own address records.\nAn EndpointSlice can specify the DNS hostname for any endpoint addresses, along with its IP.\nNote:\nA and AAAA records are not created for Pod names since hostname is missing for the Pod. A Pod with no hostname but with\nsubdomain will only create the A or AAAA record for the headless Service (busybox-subdomain.my-namespace.svc.clusterdomain.example), pointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have a record unless\npublishNotReadyAddresses=True is set on the Service.\n\nPod's setHostnameAsFQDN field\nâ“˜ FEATURE STATE: Kubernetes v1.22 [stable]\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n270/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have\na Pod with the fully qualified domain name busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example , then by\ndefault the hostname command inside that Pod returns busybox-1 and the hostname --fqdn command returns the FQDN.\nWhen you set setHostnameAsFQDN: true in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's\nnamespace. In this case, both hostname and hostname --fqdn return the Pod's FQDN.\nNote:\nIn Linux, the hostname field of the kernel (the nodename field of struct utsname ) is limited to 64 characters.\nIf a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status\n( ContainerCreating as seen by kubectl ) generating error events, such as Failed to construct FQDN from Pod hostname and\ncluster domain, FQDN long-FQDN is too long (64 characters is the max, 70 characters requested). One way of improving user\nexperience for this scenario is to create an admission webhook controller to control FQDN size when users create top level\nobjects, for example, Deployment."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0286", "text": "Pod's DNS Policy\nDNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-specific DNS policies. These policies are\nspecified in the dnsPolicy field of a Pod Spec.\n\" Default \": The Pod inherits the name resolution configuration from the node that the Pods run on. See related discussion for\nmore details.\n\" ClusterFirst \": Any DNS query that does not match the configured cluster domain suffix, such as \" www.kubernetes.io \", is\nforwarded to an upstream nameserver by the DNS server. Cluster administrators may have extra stub-domain and upstream\nDNS servers configured. See related discussion for details on how DNS queries are handled in those cases.\n\" ClusterFirstWithHostNet \": For Pods running with hostNetwork, you should explicitly set its DNS policy to\n\" ClusterFirstWithHostNet \". Otherwise, Pods running with hostNetwork and \"ClusterFirst\" will fallback to the behavior of\nthe \"Default\" policy.\nNote:\nThis is not supported on Windows. See below for details.\n\" None \": It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided\nusing the dnsConfig field in the Pod Spec. See Pod's DNS config subsection below.\nNote:\n\"Default\" is not the default DNS policy. If dnsPolicy is not explicitly specified, then \"ClusterFirst\" is used.\nThe example below shows a Pod with its DNS policy set to \" ClusterFirstWithHostNet \" because it has hostNetwork set to true .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n271/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: busybox\nnamespace: default\nspec:\ncontainers:\n- image: busybox:1.28\ncommand:\n- sleep\n- \"3600\"\nimagePullPolicy: IfNotPresent\nname: busybox\nrestartPolicy: Always\nhostNetwork: true\ndnsPolicy: ClusterFirstWithHostNet\n\nPod's DNS Config\nâ“˜ FEATURE STATE: Kubernetes v1.14 [stable]\n\nPod's DNS Config allows users more control on the DNS settings for a Pod.\nThe dnsConfig field is optional and it can work with any dnsPolicy settings. However, when a Pod's dnsPolicy is set to \" None \",\nthe dnsConfig field has to be specified.\nBelow are the properties a user can specify in the dnsConfig field:\nnameservers : a list of IP addresses that will be used as DNS servers for the Pod. There can be at most 3 IP addresses specified."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0287", "text": "When the Pod's dnsPolicy is set to \" None \", the list must contain at least one IP address, otherwise this property is optional.\nThe servers listed will be combined to the base nameservers generated from the specified DNS policy with duplicate addresses\nremoved.\nsearches : a list of DNS search domains for hostname lookup in the Pod. This property is optional. When specified, the\nprovided list will be merged into the base search domain names generated from the chosen DNS policy. Duplicate domain\nnames are removed. Kubernetes allows up to 32 search domains.\noptions : an optional list of objects where each object may have a name property (required) and a value property (optional).\nThe contents in this property will be merged to the options generated from the specified DNS policy. Duplicate entries are\nremoved.\nThe following is an example Pod with custom DNS settings:\nservice/networking/custom-dns.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n272/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nnamespace: default\nname: dns-example\nspec:\ncontainers:\n- name: test\nimage: nginx\ndnsPolicy: \"None\"\ndnsConfig:\nnameservers:\n- 192.0.2.1 # this is an example\nsearches:\n- ns1.svc.cluster-domain.example\n- my.dns.search.suffix\noptions:\n- name: ndots\nvalue: \"2\"\n- name: edns0\n\nWhen the Pod above is created, the container test gets the following contents in its /etc/resolv.conf file:\nnameserver 192.0.2.1\nsearch ns1.svc.cluster-domain.example my.dns.search.suffix\noptions ndots:2 edns0\n\nFor IPv6 setup, search path and name server should be set up like this:\n\nkubectl exec -it dns-example -- cat /etc/resolv.conf\n\nThe output is similar to this:\nnameserver 2001:db8:30::a\nsearch default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example\noptions ndots:5\n\nDNS search domain list limits\nâ“˜ FEATURE STATE: Kubernetes 1.28 [stable]\n\nKubernetes itself does not limit the DNS Config until the length of the search domain list exceeds 32 or the total length of all search\ndomains exceeds 2048. This limit applies to the node's resolver configuration file, the Pod's DNS Config, and the merged DNS Config\nrespectively.\nNote:\nSome container runtimes of earlier versions may have their own restrictions on the number of DNS search domains.\nDepending on the container runtime environment, the pods with a large number of DNS search domains may get stuck in the\npending state.\nIt is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have this problem.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n273/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDNS resolution on Windows nodes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0288", "text": "is not supported for Pods that run on Windows nodes. Windows treats all names with a . as a\nFQDN and skips FQDN resolution.\nOn Windows, there are multiple DNS resolvers that can be used. As these come with slightly different behaviors, using the\nResolve-DNSName powershell cmdlet for name query resolutions is recommended.\nClusterFirstWithHostNet\n\nOn Linux, you have a DNS suffix list, which is used after resolution of a name as fully qualified has failed. On Windows, you can\nonly have 1 DNS suffix, which is the DNS suffix associated with that Pod's namespace (example: mydns.svc.cluster.local ).\nWindows can resolve FQDNs, Services, or network name which can be resolved with this single suffix. For example, a Pod\nspawned in the default namespace, will have the DNS suffix default.svc.cluster.local . Inside a Windows Pod, you can\nresolve both kubernetes.default.svc.cluster.local and kubernetes , but not the partially qualified names\n( kubernetes.default or kubernetes.default.svc ).\n\nWhat's next\nFor guidance on administering DNS configurations, check Configure DNS Service.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n274/684\n\n11/7/25, 4:37 PM\n\n5.8 - IPv4/IPv6 dual-stack\n\nConcepts | Kubernetes\n\nKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack\nnetworking with both network families active. This page explains how.\nâ“˜ FEATURE STATE: Kubernetes v1.23 [stable]\n\nIPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.\nIPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in 1.21, allowing the simultaneous\nassignment of both IPv4 and IPv6 addresses.\n\nSupported Features\nIPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:\nDual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)\nIPv4 and IPv6 enabled Services\nPod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces\n\nPrerequisites\nThe following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:\nKubernetes 1.20 or later\nFor information about using dual-stack services with earlier Kubernetes versions, refer to the documentation for that version of\nKubernetes.\nProvider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with\nroutable IPv4/IPv6 network interfaces)\nA network plugin that supports dual-stack networking.\n\nConfigure IPv4/IPv6 dual-stack\nTo configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:\nkube-apiserver:\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0289", "text": "kube-controller-manager:\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\n--service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>\n--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6\n\ndefaults to /24 for IPv4 and /64 for IPv6\n\nkube-proxy:\n--cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>\n\nkubelet:\n--node-ip=<IPv4 IP>,<IPv6 IP>\n\nThis option is required for bare metal dual-stack nodes (nodes that do not define a cloud provider with the --cloudprovider flag). If you are using a cloud provider and choose to override the node IPs chosen by the cloud provider,\nset the --node-ip option.\n(The legacy built-in cloud providers do not support dual-stack --node-ip .)\nNote:\nAn example of an IPv4 CIDR: 10.244.0.0/16 (though you would supply your own address range)\nhttps://kubernetes.io/docs/concepts/_print/\n\n275/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAn example of an IPv6 CIDR: fdXY:IJKL:MNOP:15::/64 (this shows the format but is not a valid address - see RFC 4193)\n\nServices\nYou can create Services which can use IPv4, IPv6, or both.\nThe address family of a Service defaults to the address family of the first service cluster IP range (configured via the --servicecluster-ip-range flag to the kube-apiserver).\nWhen you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you set the\n.spec.ipFamilyPolicy field to one of the following values:\nSingleStack : Single-stack service. The control plane allocates a cluster IP for the Service, using the first configured service\n\ncluster IP range.\nPreferDualStack : Allocates both IPv4 and IPv6 cluster IPs for the Service when dual-stack is enabled. If dual-stack is not\n\nenabled or supported, it falls back to single-stack behavior.\nRequireDualStack : Allocates Service .spec.clusterIPs from both IPv4 and IPv6 address ranges when dual-stack is enabled. If\ndual-stack is not enabled or supported, the Service API object creation fails.\nSelects the .spec.clusterIP from the list of .spec.clusterIPs based on the address family of the first element in the\n.spec.ipFamilies array.\nIf you would like to define which IP family to use for single stack or define the order of IP families for dual-stack, you can choose the\naddress families by setting an optional field, .spec.ipFamilies , on the Service.\nNote:\nThe .spec.ipFamilies field is conditionally mutable: you can add or remove a secondary IP address family, but you cannot\nchange the primary IP address family of an existing Service.\nYou can set .spec.ipFamilies to any of the following array values:\n[\"IPv4\"]\n[\"IPv6\"]\n[\"IPv4\",\"IPv6\"]\n\n(dual stack)\n\n[\"IPv6\",\"IPv4\"]\n\n(dual stack)\n\nThe first family you list is used for the legacy .spec.clusterIP field."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0290", "text": "Dual-stack Service configuration scenarios\nThese examples demonstrate the behavior of various dual-stack Service configuration scenarios.\n\nDual-stack options on new Services\n1. This Service specification does not explicitly define .spec.ipFamilyPolicy . When you create this Service, Kubernetes assigns a\ncluster IP for the Service from the first configured service-cluster-ip-range and sets the .spec.ipFamilyPolicy to\nSingleStack . (Services without selectors and headless Services with selectors will behave in this same way.)\nservice/networking/dual-stack-default-svc.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n276/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\n\n2. This Service specification explicitly defines PreferDualStack in .spec.ipFamilyPolicy . When you create this Service on a dualstack cluster, Kubernetes assigns both IPv4 and IPv6 addresses for the service. The control plane updates the .spec for the\nService to record the IP address assignments. The field .spec.clusterIPs is the primary field, and contains both assigned IP\naddresses; .spec.clusterIP is a secondary field with its value calculated from .spec.clusterIPs .\nFor the .spec.clusterIP field, the control plane records the IP address that is from the same address family as the first\nservice cluster IP range.\nOn a single-stack cluster, the .spec.clusterIPs and .spec.clusterIP fields both only list one address.\nOn a cluster with dual-stack enabled, specifying RequireDualStack in .spec.ipFamilyPolicy behaves the same as\nPreferDualStack .\nservice/networking/dual-stack-preferred-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\nipFamilyPolicy: PreferDualStack\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\n\n3. This Service specification explicitly defines IPv6 and IPv4 in .spec.ipFamilies as well as defining PreferDualStack in\n.spec.ipFamilyPolicy . When Kubernetes assigns an IPv6 and IPv4 address in .spec.clusterIPs , .spec.clusterIP is set to\nthe IPv6 address because that is the first element in the .spec.clusterIPs array, overriding the default.\nservice/networking/dual-stack-preferred-ipfamilies-svc.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n277/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\nipFamilyPolicy: PreferDualStack\nipFamilies:\n- IPv6\n- IPv4\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0291", "text": "Dual-stack defaults on existing Services\nThese examples demonstrate the default behavior when dual-stack is newly enabled on a cluster where Services already exist.\n(Upgrading an existing cluster to 1.21 or beyond will enable dual-stack.)\n1. When dual-stack is enabled on a cluster, existing Services (whether IPv4 or IPv6 ) are configured by the control plane to set\n.spec.ipFamilyPolicy to SingleStack and set .spec.ipFamilies to the address family of the existing Service. The existing\nService cluster IP will be stored in .spec.clusterIPs .\nservice/networking/dual-stack-default-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\n\nYou can validate this behavior by using kubectl to inspect an existing service.\n\nkubectl get svc my-service -o yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n278/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp.kubernetes.io/name: MyApp\nname: my-service\nspec:\nclusterIP: 10.0.197.123\nclusterIPs:\n- 10.0.197.123\nipFamilies:\n- IPv4\nipFamilyPolicy: SingleStack\nports:\n- port: 80\nprotocol: TCP\ntargetPort: 80\nselector:\napp.kubernetes.io/name: MyApp\ntype: ClusterIP\nstatus:\nloadBalancer: {}\n\n2. When dual-stack is enabled on a cluster, existing headless Services with selectors are configured by the control plane to set\n.spec.ipFamilyPolicy to SingleStack and set .spec.ipFamilies to the address family of the first service cluster IP range\n(configured via the --service-cluster-ip-range flag to the kube-apiserver) even though .spec.clusterIP is set to None .\nservice/networking/dual-stack-default-svc.yaml\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nlabels:\napp.kubernetes.io/name: MyApp\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\n\nYou can validate this behavior by using kubectl to inspect an existing headless service with selectors.\n\nkubectl get svc my-service -o yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n279/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\napp.kubernetes.io/name: MyApp\nname: my-service\nspec:\nclusterIP: None\nclusterIPs:\n- None\nipFamilies:\n- IPv4\nipFamilyPolicy: SingleStack\nports:\n- port: 80\nprotocol: TCP\ntargetPort: 80\nselector:\napp.kubernetes.io/name: MyApp"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0292", "text": "Switching Services between single-stack and dual-stack\nServices can be changed from single-stack to dual-stack and from dual-stack to single-stack.\n1. To change a Service from single-stack to dual-stack, change .spec.ipFamilyPolicy from SingleStack to PreferDualStack or\nRequireDualStack as desired. When you change this Service from single-stack to dual-stack, Kubernetes assigns the missing\naddress family so that the Service now has IPv4 and IPv6 addresses.\nEdit the Service specification updating the .spec.ipFamilyPolicy from SingleStack to PreferDualStack .\nBefore:\n\nspec:\nipFamilyPolicy: SingleStack\n\nAfter:\n\nspec:\nipFamilyPolicy: PreferDualStack\n\n2. To change a Service from dual-stack to single-stack, change .spec.ipFamilyPolicy from PreferDualStack or\nRequireDualStack to SingleStack . When you change this Service from dual-stack to single-stack, Kubernetes retains only the\nfirst element in the .spec.clusterIPs array, and sets .spec.clusterIP to that IP address and sets .spec.ipFamilies to the\naddress family of .spec.clusterIPs .\n\nHeadless Services without selector\nFor Headless Services without selectors and without .spec.ipFamilyPolicy explicitly set, the .spec.ipFamilyPolicy field defaults\nto RequireDualStack .\n\nService type LoadBalancer\nTo provision a dual-stack load balancer for your Service:\nSet the .spec.type field to LoadBalancer\nSet .spec.ipFamilyPolicy field to PreferDualStack or RequireDualStack\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n280/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nTo use a dual-stack LoadBalancer type Service, your cloud provider must support IPv4 and IPv6 load balancers.\n\nEgress traffic\nIf you want to enable egress traffic in order to reach off-cluster destinations (eg. the public Internet) from a Pod that uses nonpublicly routable IPv6 addresses, you need to enable the Pod to use a publicly routed IPv6 address via a mechanism such as\ntransparent proxying or IP masquerading. The ip-masq-agent project supports IP masquerading on dual-stack clusters.\nNote:\nEnsure your CNI provider supports IPv6.\n\nWindows support\nKubernetes on Windows does not support single-stack \"IPv6-only\" networking. However, dual-stack IPv4/IPv6 networking for pods\nand nodes with single-family services is supported.\nYou can use IPv4/IPv6 dual-stack networking with l2bridge networks.\nNote:\nOverlay (VXLAN) networks on Windows do not support dual-stack networking.\nYou can read more about the different network modes for Windows within the Networking on Windows topic.\n\nWhat's next\nValidate IPv4/IPv6 dual-stack networking\nEnable dual-stack networking using kubeadm\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n281/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.9 - Topology Aware Routing"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0293", "text": "Topology Aware Routing provides a mechanism to help keep network traffic within the zone where it\noriginated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance\n(network latency and throughput), or cost.\nâ“˜ FEATURE STATE: Kubernetes v1.23 [beta]\n\nNote:\nPrior to Kubernetes 1.27, this feature was known as Topology Aware Hints.\nTopology Aware Routing adjusts routing behavior to prefer keeping traffic in the zone it originated from. In some cases this can help\nreduce costs or improve network performance.\n\nMotivation\nKubernetes clusters are increasingly deployed in multi-zone environments. Topology Aware Routing provides a mechanism to help\nkeep traffic within the zone it originated from. When calculating the endpoints for a Service, the EndpointSlice controller considers\nthe topology (region and zone) of each endpoint and populates the hints field to allocate it to a zone. Cluster components such as\nkube-proxy can then consume those hints, and use them to influence how the traffic is routed (favoring topologically closer\nendpoints).\n\nEnabling Topology Aware Routing\nNote:\nPrior to Kubernetes 1.27, this behavior was controlled using the service.kubernetes.io/topology-aware-hints annotation.\nYou can enable Topology Aware Routing for a Service by setting the service.kubernetes.io/topology-mode annotation to Auto .\nWhen there are enough endpoints available in each zone, Topology Hints will be populated on EndpointSlices to allocate individual\nendpoints to specific zones, resulting in traffic being routed closer to where it originated from.\n\nWhen it works best\nThis feature works best when:\n\n1. Incoming traffic is evenly distributed\nIf a large proportion of traffic is originating from a single zone, that traffic could overload the subset of endpoints that have been\nallocated to that zone. This feature is not recommended when incoming traffic is expected to originate from a single zone.\n\n2. The Service has 3 or more endpoints per zone\nIn a three zone cluster, this means 9 or more endpoints. If there are fewer than 3 endpoints per zone, there is a high (â‰ˆ50%)\nprobability that the EndpointSlice controller will not be able to allocate endpoints evenly and instead will fall back to the default\ncluster-wide routing approach.\n\nHow It Works\nThe \"Auto\" heuristic attempts to proportionally allocate a number of endpoints to each zone. Note that this heuristic works best for\nServices that have a significant number of endpoints.\nhttps://kubernetes.io/docs/concepts/_print/\n\n282/684\n\n11/7/25, 4:37 PM\n\nEndpointSlice controller\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0294", "text": "The EndpointSlice controller is responsible for setting hints on EndpointSlices when this heuristic is enabled. The controller allocates\na proportional amount of endpoints to each zone. This proportion is based on the allocatable CPU cores for nodes running in that\nzone. For example, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller would allocate twice as many\nendpoints to the zone with 2 CPU cores.\nThe following example shows what an EndpointSlice looks like when hints have been populated:\n\napiVersion: discovery.k8s.io/v1\nkind: EndpointSlice\nmetadata:\nname: example-hints\nlabels:\nkubernetes.io/service-name: example-svc\naddressType: IPv4\nports:\n- name: http\nprotocol: TCP\nport: 80\nendpoints:\n- addresses:\n- \"10.1.2.3\"\nconditions:\nready: true\nhostname: pod-1\nzone: zone-a\nhints:\nforZones:\n- name: \"zone-a\"\n\nkube-proxy\nThe kube-proxy component filters the endpoints it routes to based on the hints set by the EndpointSlice controller. In most cases,\nthis means that the kube-proxy is able to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints\nfrom a different zone to ensure more even distribution of endpoints between zones. This would result in some traffic being routed\nto other zones."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0295", "text": "Safeguards\nThe Kubernetes control plane and the kube-proxy on each node apply some safeguard rules before using Topology Aware Hints. If\nthese don't check out, the kube-proxy selects endpoints from anywhere in your cluster, regardless of the zone.\n1. Insufficient number of endpoints: If there are less endpoints than zones in a cluster, the controller will not assign any hints.\n2. Impossible to achieve balanced allocation: In some cases, it will be impossible to achieve a balanced allocation of endpoints\namong zones. For example, if zone-a is twice as large as zone-b, but there are only 2 endpoints, an endpoint allocated to zone-a\nmay receive twice as much traffic as zone-b. The controller does not assign hints if it can't get this \"expected overload\" value\nbelow an acceptable threshold for each zone. Importantly this is not based on real-time feedback. It is still possible for\nindividual endpoints to become overloaded.\n3. One or more Nodes has insufficient information: If any node does not have a topology.kubernetes.io/zone label or is not\nreporting a value for allocatable CPU, the control plane does not set any topology-aware endpoint hints and so kube-proxy\ndoes not filter endpoints by zone.\n4. One or more endpoints does not have a zone hint: When this happens, the kube-proxy assumes that a transition from or to\nTopology Aware Hints is underway. Filtering endpoints for a Service in this state would be dangerous so the kube-proxy falls\nback to using all endpoints.\n5. A zone is not represented in hints: If the kube-proxy is unable to find at least one endpoint with a hint targeting the zone it is\nrunning in, it falls back to using endpoints from all zones. This is most likely to happen as you add a new zone into your existing\ncluster.\nhttps://kubernetes.io/docs/concepts/_print/\n\n283/684\n\n11/7/25, 4:37 PM\n\nConstraints\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0296", "text": "Topology Aware Hints are not used when internalTrafficPolicy is set to Local on a Service. It is possible to use both\nfeatures in the same cluster on different Services, just not on the same Service.\nThis approach will not work well for Services that have a large proportion of traffic originating from a subset of zones. Instead\nthis assumes that incoming traffic will be roughly proportional to the capacity of the Nodes in each zone.\nThe EndpointSlice controller ignores unready nodes as it calculates the proportions of each zone. This could have unintended\nconsequences if a large portion of nodes are unready.\nThe EndpointSlice controller ignores nodes with the node-role.kubernetes.io/control-plane or noderole.kubernetes.io/master label set. This could be problematic if workloads are also running on those nodes.\nThe EndpointSlice controller does not take into account tolerations when deploying or calculating the proportions of each zone.\nIf the Pods backing a Service are limited to a subset of Nodes in the cluster, this will not be taken into account.\nThis may not work well with autoscaling. For example, if a lot of traffic is originating from a single zone, only the endpoints\nallocated to that zone will be handling that traffic. That could result in Horizontal Pod Autoscaler either not picking up on this\nevent, or newly added pods starting in a different zone.\n\nCustom heuristics\nKubernetes is deployed in many different ways, there is no single heuristic for allocating endpoints to zones will work for every use\ncase. A key goal of this feature is to enable custom heuristics to be developed if the built in heuristic does not work for your use\ncase. The first steps to enable custom heuristics were included in the 1.27 release. This is a limited implementation that may not yet\ncover some relevant and plausible situations.\n\nWhat's next\nFollow the Connecting Applications with Services tutorial\nLearn about the trafficDistribution field, which is closely related to the service.kubernetes.io/topology-mode annotation and\nprovides flexible options for traffic routing within Kubernetes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n284/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.10 - Networking on Windows\n\nKubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node within a single cluster. This page\nprovides an overview to networking specific to the Windows operating system."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0297", "text": "Container networking on Windows\nNetworking for Windows containers is exposed through CNI plugins. Windows containers function similarly to virtual machines in\nregards to networking. Each container has a virtual network adapter (vNIC) which is connected to a Hyper-V virtual switch (vSwitch).\nThe Host Networking Service (HNS) and the Host Compute Service (HCS) work together to create containers and attach container\nvNICs to networks. HCS is responsible for the management of containers whereas HNS is responsible for the management of\nnetworking resources such as:\nVirtual networks (including creation of vSwitches)\nEndpoints / vNICs\nNamespaces\nPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.\nThe Windows HNS and vSwitch implement namespacing and can create virtual NICs as needed for a pod or container. However,\nmany configurations such as DNS, routes, and metrics are stored in the Windows registry database rather than as files inside /etc ,\nwhich is how Linux stores those configurations. The Windows registry for the container is separate from that of the host, so concepts\nlike mapping /etc/resolv.conf from the host into a container don't have the same effect they would on Linux. These must be\nconfigured using Windows APIs run in the context of that container. Therefore CNI implementations need to call the HNS instead of\nrelying on file mappings to pass network details into the pod or container.\n\nNetwork modes\nWindows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay (Beta), Transparent, and NAT. In a\nheterogeneous cluster with Windows and Linux worker nodes, you need to select a networking solution that is compatible on both\nWindows and Linux. The following table lists the out-of-tree plugins are supported on Windows, with recommendations on when to\nuse each CNI:\nNetwork\nDriver\n\nDescription\n\nContainer Packet\nModifications\n\nNetwork\nPlugins\n\nNetwork Plugin Characteristics\n\nL2bridge\n\nContainers are attached to\nan external vSwitch.\nContainers are attached to\nthe underlay network,\nalthough the physical\nnetwork doesn't need to\nlearn the container MACs\nbecause they are rewritten\non ingress/egress.\n\nMAC is rewritten to\nhost MAC, IP may be\nrewritten to host IP\nusing HNS\nOutboundNAT policy.\n\nwin-bridge,\nAzure-CNI,\nFlannel hostgateway uses\nwin-bridge\n\nwin-bridge uses L2bridge network\nmode, connects containers to the\nunderlay of hosts, offering best\nperformance. Requires user-defined\nroutes (UDR) for inter-node\nconnectivity.\n\nL2Tunnel\n\nThis is a special case of\nl2bridge, but only used on\nAzure. All packets are sent\nto the virtualization host\nwhere SDN policy is applied."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0298", "text": "MAC rewritten, IP\nvisible on the\nunderlay network\n\nAzure-CNI\n\nAzure-CNI allows integration of\ncontainers with Azure vNET, and\nallows them to leverage the set of\ncapabilities that Azure Virtual\nNetwork provides. For example,\nsecurely connect to Azure services\nor use Azure NSGs. See azure-cni for\nsome examples\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n285/684\n\n11/7/25, 4:37 PM\n\nNetwork\nDriver\n\nConcepts | Kubernetes\n\nDescription\n\nContainer Packet\nModifications\n\nNetwork\nPlugins\n\nNetwork Plugin Characteristics\n\nOverlay\n\nContainers are given a vNIC\nconnected to an external\nvSwitch. Each overlay\nnetwork gets its own IP\nsubnet, defined by a custom\nIP prefix.The overlay\nnetwork driver uses VXLAN\nencapsulation.\n\nEncapsulated with an\nouter header.\n\nwin-overlay,\nFlannel VXLAN\n(uses winoverlay)\n\nwin-overlay should be used when\nvirtual container networks are\ndesired to be isolated from underlay\nof hosts (e.g. for security reasons).\nAllows for IPs to be re-used for\ndifferent overlay networks (which\nhave different VNID tags) if you are\nrestricted on IPs in your datacenter.\nThis option requires KB4489899 on\nWindows Server 2019.\n\nTransparent\n(special use\ncase for ovnkubernetes)\n\nRequires an external\nvSwitch. Containers are\nattached to an external\nvSwitch which enables intrapod communication via\nlogical networks (logical\nswitches and routers).\n\nPacket is\nencapsulated either\nvia GENEVE or STT\ntunneling to reach\npods which are not\non the same host.\nPackets are\nforwarded or\ndropped via the\ntunnel metadata\ninformation supplied\nby the ovn network\ncontroller.\nNAT is done for northsouth\ncommunication.\n\novnkubernetes\n\nDeploy via ansible. Distributed ACLs\ncan be applied via Kubernetes\npolicies. IPAM support. Loadbalancing can be achieved without\nkube-proxy. NATing is done without\nusing iptables/netsh.\n\nNAT (not used\nin Kubernetes)\n\nContainers are given a vNIC\nconnected to an internal\nvSwitch. DNS/DHCP is\nprovided using an internal\ncomponent called WinNAT\n\nMAC and IP is\nrewritten to host\nMAC/IP.\n\nnat\n\nIncluded here for completeness"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0299", "text": "As outlined above, the Flannel CNI plugin is also supported on Windows via the VXLAN network backend (Beta support ; delegates\nto win-overlay) and host-gateway network backend (stable support; delegates to win-bridge).\nThis plugin supports delegating to one of the reference CNI plugins (win-overlay, win-bridge), to work in conjunction with Flannel\ndaemon on Windows (Flanneld) for automatic node subnet lease assignment and HNS network creation. This plugin reads in its own\nconfiguration file (cni.conf), and aggregates it with the environment variables from the FlannelD generated subnet.env file. It then\ndelegates to one of the reference CNI plugins for network plumbing, and sends the correct configuration containing the nodeassigned subnet to the IPAM plugin (for example: host-local ).\nFor Node, Pod, and Service objects, the following network flows are supported for TCP/UDP traffic:\nPod â†’ Pod (IP)\nPod â†’ Pod (Name)\nPod â†’ Service (Cluster IP)\nPod â†’ Service (PQDN, but only if there are no \".\")\nPod â†’ Service (FQDN)\nPod â†’ external (IP)\nPod â†’ external (DNS)\nNode â†’ Pod\nPod â†’ Node\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n286/684\n\n11/7/25, 4:37 PM\n\nIP address management (IPAM)\n\nConcepts | Kubernetes\n\nThe following IPAM options are supported on Windows:\nhost-local\nazure-vnet-ipam (for azure-cni only)\nWindows Server IPAM (fallback option if no IPAM is set)\n\nDirect Server Return (DSR)\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nLoad balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives\nwith the source IP set as the originating pod IP. This provides performance optimizations by allowing the return traffic routed\nthrough load balancers to bypass the load balancer and respond directly to the client; reducing load on the load balancer and also\nreducing overall latency. For more information, read Direct Server Return (DSR) in a nutshell.\n\nLoad balancing and Services\nA Kubernetes Service is an abstraction that defines a logical set of Pods and a means to access them over a network. In a cluster that\nincludes Windows nodes, you can use the following types of Service:\nNodePort\nClusterIP\nLoadBalancer\nExternalName\n\nWindows container networking differs in some important ways from Linux networking. The Microsoft documentation for Windows\nContainer Networking provides additional details and background.\nOn Windows, you can use the following settings to configure Services and load balancing behavior:\nMinimum\nSupported\nWindows OS\nbuild\n\nHow to enable\n\nFeature\n\nDescription\n\nSession affinity"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0300", "text": "Ensures that connections from a\nparticular client are passed to the\nsame Pod each time.\n\nWindows\nServer 2022\n\nSet service.spec.sessionAffinity to\n\"ClientIP\"\n\nDirect Server\nReturn (DSR)\n\nSee DSR notes above.\n\nWindows\nServer 2019\n\nSet the following command line argument (assuming\nversion 1.34): --enable-dsr=true\n\nPreserveDestination\n\nSkips DNAT of service traffic,\nthereby preserving the virtual IP of\nthe target service in packets\nreaching the backend Pod. Also\ndisables node-node forwarding.\n\nWindows\nServer, version\n1903\n\nSet \"preserve-destination\": \"true\" in\nservice annotations and enable DSR in kube-proxy.\n\nIPv4/IPv6 dualstack\nnetworking\n\nNative IPv4-to-IPv4 in parallel with\nIPv6-to-IPv6 communications to,\nfrom, and within a cluster\n\nWindows\nServer 2019\n\nSee IPv4/IPv6 dual-stack\n\nClient IP\npreservation\n\nEnsures that source IP of incoming\ningress traffic gets preserved. Also\ndisables node-node forwarding.\n\nWindows\nServer 2019\n\nSet service.spec.externalTrafficPolicy to\n\"Local\" and enable DSR in kube-proxy\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n287/684\n\n11/7/25, 4:37 PM\n\nLimitations\n\nConcepts | Kubernetes\n\nThe following networking functionality is not supported on Windows nodes:\nHost networking mode\nLocal NodePort access from the node itself (works for other nodes or external clients)\nMore than 64 backend pods (or unique destination addresses) for a single Service\nIPv6 communication between Windows pods connected to overlay networks\nLocal Traffic Policy in non-DSR mode\nOutbound communication using the ICMP protocol via the win-overlay , win-bridge , or using the Azure-CNI plugin.\nSpecifically, the Windows data plane (VFP) doesn't support ICMP packet transpositions, and this means:\nICMP packets directed to destinations within the same network (such as pod to pod communication via ping) work as\nexpected;\nTCP/UDP packets work as expected;\nICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping) cannot be\ntransposed and thus will not be routed back to their source;\nSince TCP/UDP packets can still be transposed, you can substitute ping <destination> with curl <destination> when\ndebugging connectivity with the outside world.\nOther limitations:\nWindows reference network plugins win-bridge and win-overlay do not implement CNI spec v0.4.0, due to a missing CHECK\nimplementation.\nThe Flannel VXLAN CNI plugin has the following limitations on Windows:\nNode-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).\nFlannel is restricted to using VNI 4096 and UDP port 4789. See the official Flannel VXLAN backend docs for more details on\nthese parameters.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n288/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.11 - Service ClusterIP allocation"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0301", "text": "In Kubernetes, Services are an abstract way to expose an application running on a set of Pods. Services can have a cluster-scoped\nvirtual IP address (using a Service of type: ClusterIP ). Clients can connect using that virtual IP address, and Kubernetes then loadbalances traffic to that Service across the different backing Pods.\n\nHow Service ClusterIPs are allocated?\nWhen Kubernetes needs to assign a virtual IP address for a Service, that assignment happens one of two ways:\ndynamically\nthe cluster's control plane automatically picks a free IP address from within the configured IP range for type: ClusterIP Services.\nstatically\nyou specify an IP address of your choice, from within the configured IP range for Services.\nAcross your whole cluster, every Service ClusterIP must be unique. Trying to create a Service with a specific ClusterIP that has\nalready been allocated will return an error.\n\nWhy do you need to reserve Service Cluster IPs?\nSometimes you may want to have Services running in well-known IP addresses, so other components and users in the cluster can\nuse them.\nThe best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address\nfrom the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range 10.96.0.0/16 and you want\nyour DNS Service IP to be 10.96.0.10, you'd have to create a Service like this:\n\napiVersion: v1\nkind: Service\nmetadata:\nlabels:\nk8s-app: kube-dns\nkubernetes.io/cluster-service: \"true\"\nkubernetes.io/name: CoreDNS\nname: kube-dns\nnamespace: kube-system\nspec:\nclusterIP: 10.96.0.10\nports:\n- name: dns\nport: 53\nprotocol: UDP\ntargetPort: 53\n- name: dns-tcp\nport: 53\nprotocol: TCP\ntargetPort: 53\nselector:\nk8s-app: kube-dns\ntype: ClusterIP\n\nBut, as it was explained before, the IP address 10.96.0.10 has not been reserved. If other Services are created before or in parallel\nwith dynamic allocation, there is a chance they can allocate this IP. Hence, you will not be able to create the DNS Service because it\nwill fail with a conflict error.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n289/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0302", "text": "How can you avoid Service ClusterIP conflicts?\nThe allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces the risk of collision.\nThe ClusterIP range is divided, based on the formula min(max(16, cidrSize / 16), 256) , described as never less than 16 or more\nthan 256 with a graduated step between them.\nDynamic IP assignment uses the upper band by default, once this has been exhausted it will use the lower range. This will allow\nusers to use static allocations on the lower band with a low risk of collision.\n\nExamples\nExample 1\nThis example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses of Services.\nRange Size: 28 - 2 = 254\nBand Offset: min(max(16, 256/16), 256) = min(16, 256) = 16\nStatic band start: 10.96.0.1\nStatic band end: 10.96.0.16\nRange end: 10.96.0.254\npie showData title 10.96.0.0/24 \"Static\" : 16 \"Dynamic\" : 238\n\nExample 2\nThis example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses of Services.\nRange Size: 212 - 2 = 4094\nBand Offset: min(max(16, 4096/16), 256) = min(256, 256) = 256\nStatic band start: 10.96.0.1\nStatic band end: 10.96.1.0\nRange end: 10.96.15.254\npie showData title 10.96.0.0/20 \"Static\" : 256 \"Dynamic\" : 3838\n\nExample 3\nThis example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses of Services.\nRange Size: 216 - 2 = 65534\nBand Offset: min(max(16, 65536/16), 256) = min(4096, 256) = 256\nStatic band start: 10.96.0.1\nStatic band ends: 10.96.1.0\nRange end: 10.96.255.254\npie showData title 10.96.0.0/16 \"Static\" : 256 \"Dynamic\" : 65278\n\nWhat's next\nRead about Service External Traffic Policy\nRead about Connecting Applications with Services\nRead about Services\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n290/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5.12 - Service Internal Traffic Policy\nIf two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use\nService Internal Traffic Policy to keep network traffic within that node. Avoiding a round trip via the cluster\nnetwork can help with reliability, performance (network latency and throughput), or cost.\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]\n\nService Internal Traffic Policy enables internal traffic restrictions to only route internal traffic to endpoints within the node the traffic\noriginated from. The \"internal\" traffic here refers to traffic originated from Pods in the current cluster. This can help to reduce costs\nand improve performance."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0303", "text": "Using Service Internal Traffic Policy\nYou can enable the internal-only traffic policy for a Service, by setting its .spec.internalTrafficPolicy to Local . This tells kubeproxy to only use node local endpoints for cluster internal traffic.\nNote:\nFor pods on nodes with no endpoints for a given Service, the Service behaves as if it has zero endpoints (for Pods on this node)\neven if the service does have endpoints on other nodes.\nThe following example shows what a Service looks like when you set .spec.internalTrafficPolicy to Local :\n\napiVersion: v1\nkind: Service\nmetadata:\nname: my-service\nspec:\nselector:\napp.kubernetes.io/name: MyApp\nports:\n- protocol: TCP\nport: 80\ntargetPort: 9376\ninternalTrafficPolicy: Local\n\nHow it works\nThe kube-proxy filters the endpoints it routes to based on the spec.internalTrafficPolicy setting. When it's set to Local , only\nnode local endpoints are considered. When it's Cluster (the default), or is not set, Kubernetes considers all endpoints.\n\nWhat's next\nRead about Topology Aware Routing\nRead about Service External Traffic Policy\nFollow the Connecting Applications with Services tutorial\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n291/684\n\n11/7/25, 4:37 PM\n\n6 - Storage\n\nConcepts | Kubernetes\n\nWays to provide both long-term and temporary storage to Pods in your cluster.\n\n6.1 - Volumes\nKubernetes volumes provide a way for containers in a pod to access and share data via the filesystem. There are different kinds of\nvolume that you can use for different purposes, such as:\npopulating a configuration file based on a ConfigMap or a Secret\nproviding some temporary scratch space for a pod\nsharing a filesystem between two different containers in the same pod\nsharing a filesystem between two different pods (even if those Pods run on different nodes)\ndurably storing data so that it stays available even if the Pod restarts or is replaced\npassing configuration information to an app running in a container, based on details of the Pod the container is in (for\nexample: telling a sidecar container what namespace the Pod is running in)\nproviding read-only access to data in a different container image\nData sharing can be between different local processes within a container, or between different containers, or between Pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0304", "text": "Why volumes are important\nData persistence: On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when\nrunning in containers. One problem occurs when a container crashes or is stopped, the container state is not saved so all of\nthe files that were created or modified during the lifetime of the container are lost. After a crash, kubelet restarts the container\nwith a clean state.\nShared storage: Another problem occurs when multiple containers are running in a Pod and need to share files. It can be\nchallenging to set up and access a shared filesystem across all of the containers.\nThe Kubernetes volume abstraction can help you to solve both of these problems.\nBefore you learn about volumes, PersistentVolumes and PersistentVolumeClaims, you should read up about Pods and make sure\nthat you understand how Kubernetes uses Pods to run containers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0305", "text": "How volumes work\nKubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types\nhave a lifetime linked to a specific Pod, but persistent volumes exist beyond the lifetime of any individual pod. When a pod ceases to\nexist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume\nin a given pod, data is preserved across container restarts.\nAt its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory\ncomes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.\nTo use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into\ncontainers in .spec.containers[*].volumeMounts .\nWhen a pod is launched, a process in the container sees a filesystem view composed from the initial contents of the container image\n, plus volumes (if defined) mounted inside the container. The process sees a root filesystem that initially matches the contents of the\ncontainer image. Any writes to within that filesystem hierarchy, if allowed, affect what that process views when it performs a\nsubsequent filesystem access. Volumes are mounted at specified paths within the container filesystem. For each container defined\nwithin a Pod, you must independently specify where to mount each volume that the container uses.\nVolumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard\nlink to anything in a different volume.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n292/684\n\n11/7/25, 4:37 PM\n\nTypes of volumes\n\nConcepts | Kubernetes\n\nKubernetes supports several types of volumes.\n\nawsElasticBlockStore (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree awsElasticBlockStore type are redirected to the ebs.csi.aws.com CSI driver.\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the\nv1.27 release.\nThe Kubernetes project suggests that you use the AWS EBS third party storage driver instead.\n\nazureDisk (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree azureDisk type are redirected to the disk.csi.azure.com CSI driver.\nThe AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.\nThe Kubernetes project suggests that you use the Azure Disk third party storage driver instead."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0306", "text": "azureFile (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree azureFile type are redirected to the file.csi.azure.com CSI driver.\nThe AzureFile in-tree storage driver was deprecated in the Kubernetes v1.21 release and then removed entirely in the v1.30 release.\nThe Kubernetes project suggests that you use the Azure File third party storage driver instead.\n\ncephfs (removed)\nKubernetes 1.34 does not include a cephfs volume type.\nThe cephfs in-tree storage driver was deprecated in the Kubernetes v1.28 release and then removed entirely in the v1.31 release.\n\ncinder (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree cinder type are redirected to the cinder.csi.openstack.org CSI driver.\nThe OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release and then removed entirely in the v1.26\nrelease.\nThe Kubernetes project suggests that you use the OpenStack Cinder third party storage driver instead.\n\nconfigMap\nA ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of\ntype configMap and then consumed by containerized applications running in a pod.\nWhen referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You can customize the path to use for a\nspecific entry in the ConfigMap. The following configuration shows how to mount the log-config ConfigMap onto a Pod called\nconfigmap-pod :\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n293/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: configmap-pod\nspec:\ncontainers:\n- name: test\nimage: busybox:1.28\ncommand: ['sh', '-c', 'echo \"The app is running!\" && tail -f /dev/null']\nvolumeMounts:\n- name: config-vol\nmountPath: /etc/config\nvolumes:\n- name: config-vol\nconfigMap:\nname: log-config\nitems:\n- key: log_level\npath: log_level.conf\n\nThe log-config ConfigMap is mounted as a volume, and all contents stored in its log_level entry are mounted into the Pod at\npath /etc/config/log_level.conf . Note that this path is derived from the volume's mountPath and the path keyed with\nlog_level .\nNote:\nYou must create a ConfigMap before you can use it.\nA ConfigMap is always mounted as readOnly .\nA container using a ConfigMap as a subPath volume mount will not receive updates when the ConfigMap changes.\nText data is exposed as files using the UTF-8 character encoding. For other character encodings, use binaryData ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0307", "text": "downwardAPI\nA downwardAPI volume makes downward API data available to applications. Within the volume, you can find the exposed data as\nread-only files in plain text format.\nNote:\nA container using the downward API as a subPath volume mount does not receive updates when field values change.\nSee Expose Pod Information to Containers Through Files to learn more.\n\nemptyDir\nFor a Pod that defines an emptyDir volume, the volume is created when the Pod is assigned to a node. As the name says, the\nemptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that\nvolume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the\ndata in the emptyDir is deleted permanently.\nNote:\nA container crashing does not remove a Pod from a node. The data in an emptyDir volume is safe across container crashes.\nSome uses for an emptyDir are:\nscratch space, such as for a disk-based merge sort\ncheckpointing a long computation for recovery from crashes\nhttps://kubernetes.io/docs/concepts/_print/\n\n294/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nholding files that a content-manager container fetches while a webserver container serves the data\nThe emptyDir.medium field controls where emptyDir volumes are stored. By default emptyDir volumes are stored on whatever\nmedium that backs the node such as disk, SSD, or network storage, depending on your environment. If you set the emptyDir.medium\nfield to \"Memory\" , Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that,\nunlike disks, files you write count against the memory limit of the container that wrote them.\nA size limit can be specified for the default medium, which limits the capacity of the emptyDir volume. The storage is allocated from\nnode ephemeral storage. If that is filled up from another source (for example, log files or image overlays), the emptyDir may run out\nof capacity before this limit. If no size is specified, memory-backed volumes are sized to node allocatable memory.\nCaution:\nPlease check here for points to note in terms of resource management when using memory-backed emptyDir.\n\nemptyDir configuration example\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-pd\nspec:\ncontainers:\n- image: registry.k8s.io/test-webserver\nname: test-container\nvolumeMounts:\n- mountPath: /cache\nname: cache-volume\nvolumes:\n- name: cache-volume\nemptyDir:\nsizeLimit: 500Mi"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0308", "text": "emptyDir memory configuration example\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-pd\nspec:\ncontainers:\n- image: registry.k8s.io/test-webserver\nname: test-container\nvolumeMounts:\n- mountPath: /cache\nname: cache-volume\nvolumes:\n- name: cache-volume\nemptyDir:\nsizeLimit: 500Mi\nmedium: Memory\n\nfc (fibre channel)\nAn fc volume type allows an existing fibre channel block storage volume to be mounted in a Pod. You can specify single or multiple\ntarget world wide names (WWNs) using the parameter targetWWNs in your Volume configuration. If multiple WWNs are specified,\ntargetWWNs expect that those WWNs are from multi-path connections.\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n295/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs beforehand so that\nKubernetes hosts can access them.\n\ngcePersistentDisk (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree gcePersistentDisk type are redirected to the pd.csi.storage.gke.io CSI driver.\nThe gcePersistentDisk in-tree storage driver was deprecated in the Kubernetes v1.17 release and then removed entirely in the\nv1.28 release.\nThe Kubernetes project suggests that you use the Google Compute Engine Persistent Disk CSI third party storage driver instead.\n\ngitRepo (deprecated)\nWarning:\nThe gitRepo volume plugin is deprecated and is disabled by default.\nTo provision a Pod that has a Git repository mounted, you can mount an emptyDir volume into an init container that clones\nthe repo using Git, then mount the EmptyDir into the Pod's container.\nYou can restrict the use of gitRepo volumes in your cluster using policies, such as ValidatingAdmissionPolicy. You can use the\nfollowing Common Expression Language (CEL) expression as part of a policy to reject use of gitRepo volumes:\n!has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))\n\nYou can use this deprecated storage plugin in your cluster if you explicitly enable the GitRepoVolumeDriver feature gate.\nA gitRepo volume is an example of a volume plugin. This plugin mounts an empty directory and clones a git repository into this\ndirectory for your Pod to use.\nHere is an example of a gitRepo volume:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: server\nspec:\ncontainers:\n- image: nginx\nname: nginx\nvolumeMounts:\n- mountPath: /mypath\nname: git-volume\nvolumes:\n- name: git-volume\ngitRepo:\nrepository: \"git@somewhere:me/my-git-repository.git\"\nrevision: \"22f1d8406d464b0c0874075539c1f2e96c253775\"\n\nglusterfs (removed)\nKubernetes 1.34 does not include a glusterfs volume type.\nThe GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release and then removed entirely in the v1.26 release.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n296/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0309", "text": "hostPath\n\nA hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods\nwill need, but it offers a powerful escape hatch for some applications.\n\nWarning:\nUsing the hostPath volume type presents many security risks. If you can avoid using a hostPath volume, you should. For\nexample, define a local PersistentVolume, and use that instead.\nIf you are restricting access to specific directories on the node using admission-time validation, that restriction is only effective\nwhen you additionally require that any mounts of that hostPath volume are read only. If you allow a read-write mount of any\nhost path by an untrusted Pod, the containers in that Pod may be able to subvert the read-write host mount.\nTake care when using hostPath volumes, whether these are mounted as read-only or as read-write, because:\nAccess to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs (such as\nthe container runtime socket) that can be used for container escape or to attack other parts of the cluster.\nPods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to\ndifferent files on the nodes.\nhostPath volume usage is not treated as ephemeral storage usage. You need to monitor the disk usage by yourself\nbecause excessive hostPath disk usage will lead to disk pressure on the node.\n\nSome uses for a hostPath are:\nrunning a container that needs access to node-level system components (such as a container that transfers system logs to a\ncentral location, accessing those logs using a read-only mount of /var/log )\nmaking a configuration file stored on the host system available read-only to a static pod; unlike normal Pods, static Pods cannot\naccess ConfigMaps\nhostPath volume types\n\nIn addition to the required path property, you can optionally specify a type for a hostPath volume.\nThe available values for type are:\nValue\n\nBehavior\n\n\"\"\nâ€Œ\n\nEmpty string (default) is for backward compatibility, which means that no checks will be performed\nbefore mounting the hostPath volume.\n\nDirectoryOrCreate\n\nIf nothing exists at the given path, an empty directory will be created there as needed with permission\nset to 0755, having the same group and ownership with Kubelet.\n\nDirectory\n\nA directory must exist at the given path\n\nFileOrCreate"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0310", "text": "If nothing exists at the given path, an empty file will be created there as needed with permission set to\n0644, having the same group and ownership with Kubelet.\n\nFile\n\nA file must exist at the given path\n\nSocket\n\nA UNIX socket must exist at the given path\n\nCharDevice\n\n(Linux nodes only) A character device must exist at the given path\n\nBlockDevice\n\n(Linux nodes only) A block device must exist at the given path\n\nCaution:\nhttps://kubernetes.io/docs/concepts/_print/\n\n297/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe FileOrCreate mode does not create the parent directory of the file. If the parent directory of the mounted file does not\nexist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown in\nthe FileOrCreate example for hostPath.\nSome files or directories created on the underlying hosts might only be accessible by root. You then either need to run your process\nas root in a privileged container or modify the file permissions on the host to read from or write to a hostPath volume.\n\nhostPath configuration example\nLinux node\n\nWindows node\n\n--# This manifest mounts /data/foo on the host as /foo inside the\n# single container that runs within the hostpath-example-linux Pod.\n#\n# The mount into the container is read-only.\napiVersion: v1\nkind: Pod\nmetadata:\nname: hostpath-example-linux\nspec:\nos: { name: linux }\nnodeSelector:\nkubernetes.io/os: linux\ncontainers:\n- name: example-container\nimage: registry.k8s.io/test-webserver\nvolumeMounts:\n- mountPath: /foo\nname: example-volume\nreadOnly: true\nvolumes:\n- name: example-volume\n# mount /data/foo, but only if that directory already exists\nhostPath:\npath: /data/foo # directory location on host\ntype: Directory # this field is optional\n\nhostPath FileOrCreate configuration example\nThe following manifest defines a Pod that mounts /var/local/aaa inside the single container in the Pod. If the node does not\nalready have a path /var/local/aaa , the kubelet creates it as a directory and then mounts it into the Pod.\nIf /var/local/aaa already exists but is not a directory, the Pod fails. Additionally, the kubelet attempts to make a file named\n/var/local/aaa/1.txt inside that directory (as seen from the host); if something already exists at that path and isn't a regular file,\nthe Pod fails.\nHere's the example manifest:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n298/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0311", "text": "apiVersion: v1\nkind: Pod\nmetadata:\nname: test-webserver\nspec:\nos: { name: linux }\nnodeSelector:\nkubernetes.io/os: linux\ncontainers:\n- name: test-webserver\nimage: registry.k8s.io/test-webserver:latest\nvolumeMounts:\n- mountPath: /var/local/aaa\nname: mydir\n- mountPath: /var/local/aaa/1.txt\nname: myfile\nvolumes:\n- name: mydir\nhostPath:\n# Ensure the file directory is created.\npath: /var/local/aaa\ntype: DirectoryOrCreate\n- name: myfile\nhostPath:\npath: /var/local/aaa/1.txt\ntype: FileOrCreate\n\nimage\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)\n\nAn image volume source represents an OCI object (a container image or artifact) which is available on the kubelet's host machine.\nAn example of using the image volume source is:\npods/image-volumes.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: image-volume\nspec:\ncontainers:\n- name: shell\ncommand: [\"sleep\", \"infinity\"]\nimage: debian\nvolumeMounts:\n- name: volume\nmountPath: /volume\nvolumes:\n- name: volume\nimage:\nreference: quay.io/crio/artifact:v2\npullPolicy: IfNotPresent\n\nThe volume is resolved at pod startup depending on which pullPolicy value is provided:\nAlways\n\nthe kubelet always attempts to pull the reference. If the pull fails, the kubelet sets the Pod to Failed.\nNever\nhttps://kubernetes.io/docs/concepts/_print/\n\n299/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nthe kubelet never pulls the reference and only uses a local image or artifact. The Pod becomes Failed if any layers of the image\naren't already present locally, or if the manifest for that image isn't already cached.\nIfNotPresent\n\nthe kubelet pulls if the reference isn't already present on disk. The Pod becomes Failed if the reference isn't present and the pull\nfails.\nThe volume gets re-resolved if the pod gets deleted and recreated, which means that new remote content will become available on\npod recreation. A failure to resolve or pull the image during pod startup will block containers from starting and may add significant\nlatency. Failures will be retried using normal volume backoff and will be reported on the pod reason and message.\nThe types of objects that may be mounted by this volume are defined by the container runtime implementation on a host machine.\nAt a minimum, they must include all valid types supported by the container image field. The OCI object gets mounted in a single\ndirectory ( spec.containers[*].volumeMounts.mountPath ) and will be mounted read-only. On Linux, the container runtime typically\nalso mounts the volume with file execution blocked ( noexec ).\nBesides that:\nsubPath or subPathExpr mounts for containers ( spec.containers[*].volumeMounts.[subPath,subPathExpr] ) are only"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0312", "text": "supported from Kubernetes v1.33.\nThe field spec.securityContext.fsGroupChangePolicy has no effect on this volume type.\nThe AlwaysPullImages Admission Controller does also work for this volume source like for container images.\nThe following fields are available for the image type:\nreference\n\nArtifact reference to be used. For example, you could specify registry.k8s.io/conformance:v1.34.0 to load the files from the\nKubernetes conformance test image. Behaves in the same way as pod.spec.containers[*].image. Pull secrets will be assembled\nin the same way as for the container image by looking up node credentials, service account image pull secrets, and pod spec\nimage pull secrets. This field is optional to allow higher level config management to default or override container images in\nworkload controllers like Deployments and StatefulSets. More info about container images\npullPolicy\n\nPolicy for pulling OCI objects. Possible values are: Always, Never or IfNotPresent. Defaults to Always if :latest tag is specified, or\nIfNotPresent otherwise.\nSee the Use an Image Volume With a Pod example for more details on how to use the volume source.\n\niscsi\nAn iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your Pod. Unlike emptyDir , which is erased\nwhen a Pod is removed, the contents of an iscsi volume are preserved and the volume is merely unmounted. This means that an\niscsi volume can be pre-populated with data, and that data can be shared between pods.\nNote:\nYou must have your own iSCSI server running with the volume created before you can use it.\nA feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can prepopulate a volume with your dataset and then serve it in parallel from as many Pods as you need. Unfortunately, iSCSI volumes can\nonly be mounted by a single consumer in read-write mode. Simultaneous writers are not allowed.\n\nlocal\nA local volume represents a mounted local storage device such as a disk, partition or directory.\nLocal volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.\nCompared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to\nnodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n300/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0313", "text": "However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node\nbecomes unhealthy, then the local volume becomes inaccessible to the pod. The pod using this volume is unable to run.\nApplications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the\ndurability characteristics of the underlying disk.\nThe following example shows a PersistentVolume using a local volume and nodeAffinity :\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: example-pv\nspec:\ncapacity:\nstorage: 100Gi\nvolumeMode: Filesystem\naccessModes:\n- ReadWriteOnce\npersistentVolumeReclaimPolicy: Delete\nstorageClassName: local-storage\nlocal:\npath: /mnt/disks/ssd1\nnodeAffinity:\nrequired:\nnodeSelectorTerms:\n- matchExpressions:\n- key: kubernetes.io/hostname\noperator: In\nvalues:\n- example-node\n\nYou must set a PersistentVolume nodeAffinity when using local volumes. The Kubernetes scheduler uses the PersistentVolume\nnodeAffinity to schedule these Pods to the correct node.\nPersistentVolume volumeMode can be set to \"Block\" (instead of the default value \"Filesystem\") to expose the local volume as a raw\nblock device.\nWhen using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer . For\nmore details, see the local StorageClass example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision\nwill also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod\naffinity, and Pod anti-affinity.\nAn external static provisioner can be run separately for improved management of the local volume lifecycle. Note that this\nprovisioner does not support dynamic provisioning yet. For an example on how to run an external local provisioner, see the local\nvolume provisioner user guide.\nNote:\nThe local PersistentVolume requires manual cleanup and deletion by the user if the external static provisioner is not used to\nmanage the volume lifecycle.\n\nnfs\nAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir , which is erased\nwhen a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an\nNFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers\nsimultaneously.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n301/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0314", "text": "apiVersion: v1\nkind: Pod\nmetadata:\nname: test-pd\nspec:\ncontainers:\n- image: registry.k8s.io/test-webserver\nname: test-container\nvolumeMounts:\n- mountPath: /my-nfs-data\nname: test-volume\nvolumes:\n- name: test-volume\nnfs:\nserver: my-nfs-server.example.com\npath: /my-nfs-volume\nreadOnly: true\n\nNote:\nYou must have your own NFS server running with the share exported before you can use it.\nAlso note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or use\n/etc/nfsmount.conf. You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.\n\npersistentVolumeClaim\nA persistentVolumeClaim volume is used to mount a PersistentVolume into a Pod. PersistentVolumeClaims are a way for users to\n\"claim\" durable storage (such as an iSCSI volume) without knowing the details of the particular cloud environment.\nSee the information about PersistentVolumes for more details.\n\nportworxVolume (deprecated)\nâ“˜ FEATURE STATE: Kubernetes v1.25 [deprecated]\n\nA portworxVolume is an elastic block storage layer that runs hyperconverged with Kubernetes. Portworx fingerprints storage in a\nserver, tiers based on capabilities, and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on\nbare metal Linux nodes.\nA portworxVolume can be dynamically created through Kubernetes or it can also be pre-provisioned and referenced inside a Pod.\nHere is an example Pod referencing a pre-provisioned Portworx volume:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-portworx-volume-pod\nspec:\ncontainers:\n- image: registry.k8s.io/test-webserver\nname: test-container\nvolumeMounts:\n- mountPath: /mnt\nname: pxvol\nvolumes:\n- name: pxvol\n# This Portworx volume must already exist.\nportworxVolume:\nvolumeID: \"pxvol\"\nfsType: \"<fs-type>\"\nhttps://kubernetes.io/docs/concepts/_print/\n\n302/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nMake sure you have an existing PortworxVolume with name pxvol before using it in the Pod.\n\nPortworx CSI migration\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\n\nIn Kubernetes 1.34, all operations for the in-tree Portworx volumes are redirected to the pxd.portworx.com Container Storage\nInterface (CSI) Driver by default.\nPortworx CSI Driver must be installed on the cluster.\n\nprojected\nA projected volume maps several existing volume sources into the same directory. For more details, see projected volumes.\n\nrbd (removed)\nKubernetes 1.34 does not include a rbd volume type.\nThe Rados Block Device (RBD) in-tree storage driver and its csi migration support were deprecated in the Kubernetes v1.28 release\nand then removed entirely in the v1.31 release."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0315", "text": "secret\nA secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and\nmount them as files for use by pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed\nfilesystem) so they are never written to non-volatile storage.\nNote:\nYou must create a Secret in the Kubernetes API before you can use it.\nA Secret is always mounted as readOnly .\nA container using a Secret as a subPath volume mount will not receive Secret updates.\n\nFor more details, see Configuring Secrets.\n\nvsphereVolume (deprecated)\nIn Kubernetes 1.34, all operations for the in-tree vsphereVolume type are redirected to the csi.vsphere.vmware.com CSI driver.\nThe vsphereVolume in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.30\nrelease.\nThe Kubernetes project suggests that you use the vSphere CSI third party storage driver instead.\n\nUsing subPath\nSometimes, it is useful to share one volume for multiple uses in a single pod. The volumeMounts[*].subPath property specifies a\nsub-path inside the referenced volume instead of its root.\nThe following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP) using a single, shared volume.\nThis sample subPath configuration is not recommended for production use.\nThe PHP application's code and assets map to the volume's html folder and the MySQL database is stored in the volume's mysql\nfolder. For example:\nhttps://kubernetes.io/docs/concepts/_print/\n\n303/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: my-lamp-site\nspec:\ncontainers:\n- name: mysql\nimage: mysql\nenv:\n- name: MYSQL_ROOT_PASSWORD\nvalue: \"rootpasswd\"\nvolumeMounts:\n- mountPath: /var/lib/mysql\nname: site-data\nsubPath: mysql\n- name: php\nimage: php:7.0-apache\nvolumeMounts:\n- mountPath: /var/www/html\nname: site-data\nsubPath: html\nvolumes:\n- name: site-data\npersistentVolumeClaim:\nclaimName: my-lamp-site-data\n\nUsing subPath with expanded environment variables\nâ“˜ FEATURE STATE: Kubernetes v1.17 [stable]\n\nUse the subPathExpr field to construct subPath directory names from downward API environment variables. The subPath and\nsubPathExpr properties are mutually exclusive.\nIn this example, a Pod uses subPathExpr to create a directory pod1 within the hostPath volume /var/log/pods . The hostPath\nvolume takes the Pod name from the downwardAPI . The host directory /var/log/pods/pod1 is mounted at /logs in the container."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0316", "text": "apiVersion: v1\nkind: Pod\nmetadata:\nname: pod1\nspec:\ncontainers:\n- name: container1\nenv:\n- name: POD_NAME\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: metadata.name\nimage: busybox:1.28\ncommand: [ \"sh\", \"-c\", \"while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt\" ]\nvolumeMounts:\n- name: workdir1\nmountPath: /logs\n# The variable expansion uses round brackets (not curly brackets).\nsubPathExpr: $(POD_NAME)\nrestartPolicy: Never\nvolumes:\n- name: workdir1\nhostPath:\npath: /var/log/pods\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n304/684\n\n11/7/25, 4:37 PM\n\nResources\n\nConcepts | Kubernetes\n\nThe storage medium (such as Disk or SSD) of an emptyDir volume is determined by the medium of the filesystem holding the\nkubelet root dir (typically /var/lib/kubelet ). There is no limit on how much space an emptyDir or hostPath volume can consume,\nand no isolation between containers or pods.\nTo learn about requesting space using a resource specification, see how to manage resources.\n\nOut-of-tree volume plugins\nThe out-of-tree volume plugins include Container Storage Interface (CSI), and also FlexVolume (which is deprecated). These plugins\nenable storage vendors to create custom storage plugins without adding their plugin source code to the Kubernetes repository.\nPreviously, all volume plugins were \"in-tree\". The \"in-tree\" plugins were built, linked, compiled, and shipped with the core Kubernetes\nbinaries. This meant that adding a new storage system to Kubernetes (a volume plugin) required checking code into the core\nKubernetes code repository.\nBoth CSI and FlexVolume allow volume plugins to be developed independently of the Kubernetes code base, and deployed (installed)\non Kubernetes clusters as extensions.\nFor storage vendors looking to create an out-of-tree volume plugin, please refer to the volume plugin FAQ.\n\ncsi\nContainer Storage Interface (CSI) defines a standard interface for container orchestration systems (like Kubernetes) to expose\narbitrary storage systems to their container workloads.\nPlease read the CSI design proposal for more information.\nNote:\nSupport for CSI spec versions 0.2 and 0.3 is deprecated in Kubernetes v1.13 and will be removed in a future release."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0317", "text": "Note:\nCSI drivers may not be compatible across all Kubernetes releases. Please check the specific CSI driver's documentation for\nsupported deployments steps for each Kubernetes release and a compatibility matrix.\nOnce a CSI-compatible volume driver is deployed on a Kubernetes cluster, users may use the csi volume type to attach or mount\nthe volumes exposed by the CSI driver.\nA csi volume can be used in a Pod in three different ways:\nthrough a reference to a PersistentVolumeClaim\nwith a generic ephemeral volume\nwith a CSI ephemeral volume if the driver supports that\nThe following fields are available to storage administrators to configure a CSI persistent volume:\ndriver : A string value that specifies the name of the volume driver to use. This value must correspond to the value returned in\n\nthe GetPluginInfoResponse by the CSI driver as defined in the CSI spec. It is used by Kubernetes to identify which CSI driver to\ncall out to, and by CSI driver components to identify which PV objects belong to the CSI driver.\nvolumeHandle : A string value that uniquely identifies the volume. This value must correspond to the value returned in the\nvolume.id field of the CreateVolumeResponse by the CSI driver as defined in the CSI spec. The value is passed as volume_id in\nall calls to the CSI volume driver when referencing the volume.\nreadOnly : An optional boolean value indicating whether the volume is to be \"ControllerPublished\" (attached) as read only.\nDefault is false. This value is passed to the CSI driver via the readonly field in the ControllerPublishVolumeRequest .\nfsType : If the PV's VolumeMode\n\nis Filesystem , then this field may be used to specify the filesystem that should be used to\nmount the volume. If the volume has not been formatted and formatting is supported, this value will be used to format the\nvolume. This value is passed to the CSI driver via the VolumeCapability field of ControllerPublishVolumeRequest ,\nNodeStageVolumeRequest , and NodePublishVolumeRequest .\nhttps://kubernetes.io/docs/concepts/_print/\n\n305/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nvolumeAttributes : A map of string to string that specifies static properties of a volume. This map must correspond to the map"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0318", "text": "returned in the volume.attributes field of the CreateVolumeResponse by the CSI driver as defined in the CSI spec. The map is\npassed to the CSI driver via the volume_context field in the ControllerPublishVolumeRequest , NodeStageVolumeRequest , and\nNodePublishVolumeRequest .\ncontrollerPublishSecretRef : A reference to the secret object containing sensitive information to pass to the CSI driver to\n\ncomplete the CSI ControllerPublishVolume and ControllerUnpublishVolume calls. This field is optional, and may be empty if\nno secret is required. If the Secret contains more than one secret, all secrets are passed.\nnodeExpandSecretRef : A reference to the secret containing sensitive information to pass to the CSI driver to complete the CSI\nNodeExpandVolume call. This field is optional and may be empty if no secret is required. If the object contains more than one\nsecret, all secrets are passed. When you have configured secret data for node-initiated volume expansion, the kubelet passes\nthat data via the NodeExpandVolume() call to the CSI driver. All supported versions of Kubernetes offer the\nnodeExpandSecretRef field, and have it available by default. Kubernetes releases prior to v1.25 did not include this support.\nEnable the feature gate named CSINodeExpandSecret for each kube-apiserver and for the kubelet on every node. Since\nKubernetes version 1.27, this feature has been enabled by default and no explicit enablement of the feature gate is required.\nYou must also be using a CSI driver that supports or requires secret data during node-initiated storage resize operations.\nnodePublishSecretRef : A reference to the secret object containing sensitive information to pass to the CSI driver to complete\nthe CSI NodePublishVolume call. This field is optional and may be empty if no secret is required. If the secret object contains\nmore than one secret, all secrets are passed.\nnodeStageSecretRef : A reference to the secret object containing sensitive information to pass to the CSI driver to complete the\nCSI NodeStageVolume call. This field is optional and may be empty if no secret is required. If the Secret contains more than one\nsecret, all secrets are passed.\n\nCSI raw block volume support\nâ“˜ FEATURE STATE: Kubernetes v1.18 [stable]\n\nVendors with external CSI drivers can implement raw block volume support in Kubernetes workloads.\nYou can set up your PersistentVolume/PersistentVolumeClaim with raw block volume support as usual, without any CSI-specific\nchanges.\n\nCSI ephemeral volumes\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0319", "text": "You can directly configure CSI volumes within the Pod specification. Volumes specified in this way are ephemeral and do not persist\nacross pod restarts. See Ephemeral Volumes for more information.\nFor more information on how to develop a CSI driver, refer to the kubernetes-csi documentation\n\nWindows CSI proxy\nâ“˜ FEATURE STATE: Kubernetes v1.22 [stable]\n\nCSI node plugins need to perform various privileged operations like scanning of disk devices and mounting of file systems. These\noperations differ for each host operating system. For Linux worker nodes, containerized CSI node plugins are typically deployed as\nprivileged containers. For Windows worker nodes, privileged operations for containerized CSI node plugins is supported using csiproxy, a community-managed, stand-alone binary that needs to be pre-installed on each Windows node.\nFor more details, refer to the deployment guide of the CSI plugin you wish to deploy.\n\nMigrating to CSI drivers from in-tree plugins\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n306/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe CSIMigration feature directs operations against existing in-tree plugins to corresponding CSI plugins (which are expected to be\ninstalled and configured). As a result, operators do not have to make any configuration changes to existing Storage Classes,\nPersistentVolumes or PersistentVolumeClaims (referring to in-tree plugins) when transitioning to a CSI driver that supersedes an intree plugin.\nNote:\nExisting PVs created by an in-tree volume plugin can still be used in the future without any configuration changes, even after\nthe migration to CSI is completed for that volume type, and even after you upgrade to a version of Kubernetes that doesn't\nhave compiled-in support for that kind of storage.\nAs part of that migration, you - or another cluster administrator - must have installed and configured the appropriate CSI\ndriver for that storage. The core of Kubernetes does not install that software for you.\nAfter that migration, you can also define new PVCs and PVs that refer to the legacy, built-in storage integrations. Provided you\nhave the appropriate CSI driver installed and configured, the PV creation continues to work, even for brand new volumes. The\nactual storage management now happens through the CSI driver.\n\nThe operations and features that are supported include: provisioning/delete, attach/detach, mount/unmount and resizing of\nvolumes.\nIn-tree plugins that support CSIMigration and have a corresponding CSI driver implemented are listed in Types of Volumes.\n\nflexVolume (deprecated)\nâ“˜ FEATURE STATE: Kubernetes v1.23 [deprecated]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0320", "text": "FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface with storage drivers. The FlexVolume driver\nbinaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well.\nPods interact with FlexVolume drivers through the flexVolume in-tree volume plugin.\nThe following FlexVolume plugins, deployed as PowerShell scripts on the host, support Windows nodes:\nSMB\niSCSI\nNote:\nFlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with\nKubernetes.\nMaintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI. Users\nof FlexVolume should move their workloads to use the equivalent CSI Driver.\n\nMount propagation\nCaution:\nMount propagation is a low-level feature that does not work consistently on all volume types. The Kubernetes project\nrecommends only using mount propagation with hostPath or memory-backed emptyDir volumes. See Kubernetes issue\n#95049 for more context.\nMount propagation allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods\non the same node.\nMount propagation of a volume is controlled by the mountPropagation field in containers[*].volumeMounts . Its values are:\nhttps://kubernetes.io/docs/concepts/_print/\n\n307/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n- This volume mount will not receive any subsequent mounts that are mounted to this volume or any of its\nsubdirectories by the host. In similar fashion, no mounts created by the container will be visible on the host. This is the default\nmode.\nNone\n\nThis mode is equal to rprivate mount propagation as described in mount(8)\nHowever, the CRI runtime may choose rslave mount propagation (i.e., HostToContainer ) instead, when rprivate\npropagation is not applicable. cri-dockerd (Docker) is known to choose rslave mount propagation when the mount source\ncontains the Docker daemon's root directory ( /var/lib/docker ).\nHostToContainer\n\n- This volume mount will receive all subsequent mounts that are mounted to this volume or any of its"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0321", "text": "subdirectories.\nIn other words, if the host mounts anything inside the volume mount, the container will see it mounted there.\nSimilarly, if any Pod with Bidirectional mount propagation to the same volume mounts anything there, the container with\nHostToContainer mount propagation will see it.\nThis mode is equal to rslave mount propagation as described in the mount(8)\n- This volume mount behaves the same the HostToContainer mount. In addition, all volume mounts created\nby the container will be propagated back to the host and to all containers of all pods that use the same volume.\nBidirectional\n\nA typical use case for this mode is a Pod with a FlexVolume or CSI driver or a Pod that needs to mount something on the host\nusing a hostPath volume.\nThis mode is equal to rshared mount propagation as described in the mount(8)\n\nWarning:\nBidirectional mount propagation can be dangerous. It can damage the host operating system and therefore it is\n\nallowed only in privileged containers. Familiarity with Linux kernel behavior is strongly recommended. In addition, any\nvolume mounts created by containers in pods must be destroyed (unmounted) by the containers on termination.\n\nRead-only mounts\nA mount can be made read-only by setting the .spec.containers[].volumeMounts[].readOnly field to true . This does not make the\nvolume itself read-only, but that specific container will not be able to write to it. Other containers in the Pod may mount the same\nvolume as read-write.\nOn Linux, read-only mounts are not recursively read-only by default. For example, consider a Pod which mounts the hosts /mnt as a\nhostPath volume. If there is another filesystem mounted read-write on /mnt/<SUBMOUNT> (such as tmpfs, NFS, or USB storage), the\nvolume mounted into the container(s) will also have a writeable /mnt/<SUBMOUNT> , even if the mount itself was specified as readonly.\n\nRecursive read-only mounts\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\n\nRecursive read-only mounts can be enabled by setting the RecursiveReadOnlyMounts feature gate for kubelet and kube-apiserver,\nand setting the .spec.containers[].volumeMounts[].recursiveReadOnly field for a pod.\nThe allowed values are:\nDisabled\n\n(default): no effect.\n\nEnabled : makes the mount recursively read-only. Needs all the following requirements to be satisfied:\nreadOnly\n\nis set to true\n\nmountPropagation\n\nis unset, or, set to None\n\nThe host is running with Linux kernel v5.12 or later\nThe CRI-level container runtime supports recursive read-only mounts\nhttps://kubernetes.io/docs/concepts/_print/\n\n308/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0322", "text": "The OCI-level container runtime supports recursive read-only mounts.\nIt will fail if any of these is not true.\nIfPossible : attempts to apply Enabled , and falls back to Disabled if the feature is not supported by the kernel or the\nruntime class.\nExample:\nstorage/rro.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: rro\nspec:\nvolumes:\n- name: mnt\nhostPath:\n# tmpfs is mounted on /mnt/tmpfs\npath: /mnt\ncontainers:\n- name: busybox\nimage: busybox\nargs: [\"sleep\", \"infinity\"]\nvolumeMounts:\n# /mnt-rro/tmpfs is not writable\n- name: mnt\nmountPath: /mnt-rro\nreadOnly: true\nmountPropagation: None\nrecursiveReadOnly: Enabled\n# /mnt-ro/tmpfs is writable\n- name: mnt\nmountPath: /mnt-ro\nreadOnly: true\n# /mnt-rw/tmpfs is writable\n- name: mnt\nmountPath: /mnt-rw\n\nWhen this property is recognized by kubelet and kube-apiserver, the\n.status.containerStatuses[].volumeMounts[].recursiveReadOnly field is set to either Enabled or Disabled .\n\nImplementations\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nThe following container runtimes are known to support recursive read-only mounts.\nCRI-level:\ncontainerd, since v2.0\nCRI-O, since v1.30\nOCI-level:\nrunc, since v1.1\ncrun, since v1.8.6\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n309/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nFollow an example of deploying WordPress and MySQL with Persistent Volumes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n310/684\n\n11/7/25, 4:37 PM\n\n6.2 - Persistent Volumes\n\nConcepts | Kubernetes\n\nThis document describes persistent volumes in Kubernetes. Familiarity with volumes, StorageClasses and VolumeAttributesClasses is\nsuggested."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0323", "text": "Introduction\nManaging storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for\nusers and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two\nnew API resources: PersistentVolume and PersistentVolumeClaim.\nA PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned\nusing Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but\nhave a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the\nstorage, be that NFS, iSCSI, or a cloud-provider-specific storage system.\nA PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs\nconsume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access\nmodes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod, see AccessModes).\nWhile PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes\nwith varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of\nPersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes\nare implemented. For these needs, there is the StorageClass resource.\nSee the detailed walkthrough with working examples.\n\nLifecycle of a volume and claim\nPVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction\nbetween PVs and PVCs follows this lifecycle:\n\nProvisioning\nThere are two ways PVs may be provisioned: statically or dynamically.\n\nStatic\nA cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users.\nThey exist in the Kubernetes API and are available for consumption."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0324", "text": "Dynamic\nWhen none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically\nprovision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the\nadministrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class \"\"\neffectively disable dynamic provisioning for themselves.\nTo enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass\nadmission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the commadelimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API\nserver command-line flags, check kube-apiserver documentation.\n\nBinding\nA user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of\nstorage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if\npossible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n311/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOtherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once\nbound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one\nmapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.\nClaims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become\navailable. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound\nwhen a 100Gi PV is added to the cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0325", "text": "Using\nPods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes\nthat support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.\nOnce a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods\nand access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block. See Claims As Volumes for\nmore details on this.\n\nStorage Object in Use Protection\nThe purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod\nand PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.\nNote:\nPVC is in active use by a Pod when a Pod object exists that is using the PVC.\nIf a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no\nlonger actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV\nremoval is postponed until the PV is no longer bound to a PVC.\nYou can see that a PVC is protected when the PVC's status is Terminating and the Finalizers list includes kubernetes.io/pvcprotection :\n\nkubectl describe pvc hostpath\nName:\nNamespace:\n\nhostpath\ndefault\n\nStorageClass:\n\nexample-hostpath\n\nStatus:\n\nTerminating\n\nVolume:\nLabels:\n\n<none>\n\nAnnotations:\n\nvolume.beta.kubernetes.io/storage-class=example-hostpath\n\nFinalizers:\n\nvolume.beta.kubernetes.io/storage-provisioner=example.com/hostpath\n[kubernetes.io/pvc-protection]\n\n...\n\nYou can see that a PV is protected when the PV's status is Terminating and the Finalizers list includes kubernetes.io/pvprotection too:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n312/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl describe pv task-pv-volume\nName:\nLabels:\n\ntask-pv-volume\ntype=local\n\nAnnotations:\n\n<none>\n\nFinalizers:\n\n[kubernetes.io/pv-protection]\n\nStorageClass:\nStatus:\n\nstandard\nTerminating\n\nClaim:\nReclaim Policy:\nAccess Modes:\n\nDelete\nRWO\n\nCapacity:\n\n1Gi\n\nMessage:\nSource:\nType:\nPath:\nHostPathType:\nEvents:\n\nHostPath (bare host directory volume)\n/tmp/data\n<none>\n\nReclaiming\nWhen a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The\nreclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently,\nvolumes can either be Retained, Recycled, or Deleted."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0326", "text": "Retain\nThe Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the\nPersistentVolume still exists and the volume is considered \"released\". But it is not yet available for another claim because the\nprevious claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.\n1. Delete the PersistentVolume. The associated storage asset in external infrastructure still exists after the PV is deleted.\n2. Manually clean up the data on the associated storage asset accordingly.\n3. Manually delete the associated storage asset.\nIf you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.\n\nDelete\nFor volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as\nwell as the associated storage asset in the external infrastructure. Volumes that were dynamically provisioned inherit the reclaim\npolicy of their StorageClass, which defaults to Delete . The administrator should configure the StorageClass according to users'\nexpectations; otherwise, the PV must be edited or patched after it is created. See Change the Reclaim Policy of a PersistentVolume.\n\nRecycle\nWarning:\nThe Recycle reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.\nIf supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub ( rm -rf /thevolume/* ) on the\nvolume and makes it available again for a new claim.\nHowever, an administrator can configure a custom recycler Pod template using the Kubernetes controller manager command line\narguments as described in the reference. The custom recycler Pod template must contain a volumes specification, as shown in the\nexample below:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n313/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: pv-recycler\nnamespace: default\nspec:\nrestartPolicy: Never\nvolumes:\n- name: vol\nhostPath:\npath: /any/path/it/will/be/replaced\ncontainers:\n- name: pv-recycler\nimage: \"registry.k8s.io/busybox\"\ncommand: [\"/bin/sh\", \"-c\", \"test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*\n\n&& test -z \\\"$(ls -A /s\n\nvolumeMounts:\n- name: vol\nmountPath: /scrub\n\nHowever, the particular path specified in the custom recycler Pod template in the volumes part is replaced with the particular path\nof the volume that is being recycled.\n\nPersistentVolume deletion protection finalizer\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0327", "text": "Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes having Delete reclaim policy are deleted only after\nthe backing storage are deleted.\nThe finalizer external-provisioner.volume.kubernetes.io/finalizer (introduced in v1.31) is added to both dynamically provisioned\nand statically provisioned CSI volumes.\nThe finalizer kubernetes.io/pv-controller (introduced in v1.31) is added to dynamically provisioned in-tree plugin volumes and\nskipped for statically provisioned in-tree plugin volumes.\nThe following is an example of dynamically provisioned in-tree plugin volume:\n\nkubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nName:\npvc-74a498d6-3929-47e8-8c02-078c1ece4d78\nLabels:\n<none>\nAnnotations:\nkubernetes.io/createdby: vsphere-volume-dynamic-provisioner\npv.kubernetes.io/bound-by-controller: yes\npv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume\nFinalizers:\n[kubernetes.io/pv-protection kubernetes.io/pv-controller]\nStorageClass:\nvcp-sc\nStatus:\nBound\nClaim:\ndefault/vcp-pvc-1\nReclaim Policy: Delete\nAccess Modes:\nRWO\nVolumeMode:\nFilesystem\nCapacity:\n1Gi\nNode Affinity:\n<none>\nMessage:\nSource:\nType:\nvSphereVolume (a Persistent Disk resource in vSphere)\nVolumePath:\n[vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47\nFSType:\next4\nStoragePolicyName: vSAN Default Storage Policy\nEvents:\n<none>\n\nThe finalizer external-provisioner.volume.kubernetes.io/finalizer is added for CSI volumes. The following is an example:\nhttps://kubernetes.io/docs/concepts/_print/\n\n314/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:\nLabels:\n\npvc-2f0bab97-85a8-4552-8044-eb8be45cf48d\n<none>\n\nAnnotations:\n\npv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com\n\nFinalizers:\nStorageClass:\n\n[kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer]\nfast\n\nStatus:\n\nBound\n\nClaim:\n\ndemo-app/nginx-logs\n\nReclaim Policy:\nAccess Modes:\n\nDelete\nRWO\n\nVolumeMode:\n\nFilesystem\n\nCapacity:\nNode Affinity:\n\n200Mi\n<none>\n\nMessage:\nSource:\nType:\n\nCSI (a Container Storage Interface (CSI) volume source)\n\nDriver:\n\ncsi.vsphere.vmware.com\n\nFSType:\n\next4\n\nVolumeHandle:\nReadOnly:\n\n44830fa8-79b4-406b-8b58-621ba25353fd\nfalse\n\nVolumeAttributes:\nEvents:\n\nstorage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com\ntype=vSphere CNS Block Volume\n<none>\n\nWhen the CSIMigration{provider} feature flag is enabled for a specific in-tree volume plugin, the kubernetes.io/pv-controller\nfinalizer is replaced by the external-provisioner.volume.kubernetes.io/finalizer finalizer.\nThe finalizers ensure that the PV object is removed only after the volume is deleted from the storage backend provided the reclaim\npolicy of the PV is Delete . This also ensures that the volume is deleted from storage backend irrespective of the order of deletion of\nPV and PVC."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0328", "text": "Reserving a PersistentVolume\nThe control plane can bind PersistentVolumeClaims to matching PersistentVolumes in the cluster. However, if you want a PVC to\nbind to a specific PV, you need to pre-bind them.\nBy specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC. If the\nPersistentVolume exists and has not reserved PersistentVolumeClaims through its claimRef field, then the PersistentVolume and\nPersistentVolumeClaim will be bound.\nThe binding happens regardless of some volume matching criteria, including node affinity. The control plane still checks that storage\nclass, access modes, and requested storage size are valid.\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: foo-pvc\nnamespace: foo\nspec:\nstorageClassName: \"\" # Empty string must be explicitly set otherwise default StorageClass will be set\nvolumeName: foo-pv\n...\n\nThis method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV\nthat you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the claimRef field of\nthe PV so that other PVCs can not bind to it.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n315/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: foo-pv\nspec:\nstorageClassName: \"\"\nclaimRef:\nname: foo-pvc\nnamespace: foo\n...\n\nThis is useful if you want to consume PersistentVolumes that have their persistentVolumeReclaimPolicy set to Retain , including\ncases where you are reusing an existing PV.\n\nExpanding Persistent Volumes Claims\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nSupport for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand the following types of volumes:\ncsi (including some CSI migrated volme types)\nflexVolume (deprecated)\nportworxVolume (deprecated)\nYou can only expand a PVC if its storage class's allowVolumeExpansion field is set to true.\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: example-vol-default\nprovisioner: vendor-name.example/magicstorage\nparameters:\nresturl: \"http://192.168.10.100:8080\"\nrestuser: \"\"\nsecretNamespace: \"\"\nsecretName: \"\"\nallowVolumeExpansion: true\n\nTo request a larger volume for a PVC, edit the PVC object and specify a larger size. This triggers expansion of the volume that backs\nthe underlying PersistentVolume. A new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is\nresized."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0329", "text": "Warning:\nDirectly editing the size of a PersistentVolume can prevent an automatic resize of that volume. If you edit the capacity of a\nPersistentVolume, and then edit the .spec of a matching PersistentVolumeClaim to make the size of the\nPersistentVolumeClaim match the PersistentVolume, then no storage resize happens. The Kubernetes control plane will see\nthat the desired state of both resources matches, conclude that the backing volume size has been manually increased and that\nno resize is necessary.\n\nCSI Volume expansion\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nSupport for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer\nto documentation of the specific CSI driver for more information.\nhttps://kubernetes.io/docs/concepts/_print/\n\n316/684\n\n11/7/25, 4:37 PM\n\nResizing a volume containing a file system\n\nConcepts | Kubernetes\n\nYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.\nWhen a volume contains a file system, the file system is only resized when a new Pod is using the PersistentVolumeClaim in\nReadWrite mode. File system expansion is either done when a Pod is starting up or when a Pod is running and the underlying file\nsystem supports online expansion.\nFlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the RequiresFSResize capability to\ntrue . The FlexVolume can be resized on Pod restart.\n\nResizing an in-use PersistentVolumeClaim\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nIn this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC. Any in-use PVC automatically\nbecomes available to its Pod as soon as its file system has been expanded. This feature has no effect on PVCs that are not in use by a\nPod or deployment. You must create a Pod that uses the PVC before the expansion can complete.\nSimilar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.\nNote:\nFlexVolume resize is possible only when the underlying driver supports resize.\n\nRecovering from Failure when Expanding Volumes\nIf a user specifies a new size that is too big to be satisfied by underlying storage system, expansion of PVC will be continuously\nretried until user or cluster administrator takes some action. This can be undesirable and hence Kubernetes provides following\nmethods of recovering from such failures.\nManually with Cluster Administrator access"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0330", "text": "By requesting expansion to smaller size\n\nIf expanding underlying storage fails, the cluster administrator can manually recover the Persistent\nVolume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are\ncontinuously retried by the controller without administrator intervention.\n1. Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with Retain\nreclaim policy.\n2. Delete the PVC. Since PV has Retain reclaim policy - we will not lose any data when we recreate\nthe PVC.\n3. Delete the claimRef entry from PV specs, so as new PVC can bind to it. This should make the\nPV Available .\n4. Re-create the PVC with smaller size than PV and set volumeName field of the PVC to the name of\nthe PV. This should bind new PVC to existing PV.\n5. Don't forget to restore the reclaim policy of the PV.\n\nTypes of Persistent Volumes\nPersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:\ncsi - Container Storage Interface (CSI)\nfc - Fibre Channel (FC) storage\nhostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local\n\nvolume instead)\niscsi - iSCSI (SCSI over IP) storage\nlocal - local storage devices mounted on nodes.\nhttps://kubernetes.io/docs/concepts/_print/\n\n317/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nnfs - Network File System (NFS) storage\n\nThe following types of PersistentVolume are deprecated but still available. If you are using these volume types except for\nflexVolume , cephfs and rbd , please install corresponding CSI drivers.\nawsElasticBlockStore - AWS Elastic Block Store (EBS) (migration on by default starting v1.23)\nazureDisk - Azure Disk (migration on by default starting v1.23)\nazureFile - Azure File (migration on by default starting v1.24)\ncinder - Cinder (OpenStack block storage) (migration on by default starting v1.21)\nflexVolume - FlexVolume (deprecated starting v1.23, no migration plan and no plan to remove support)\ngcePersistentDisk - GCE Persistent Disk (migration on by default starting v1.23)\nportworxVolume - Portworx volume (migration on by default starting v1.31)\nvsphereVolume - vSphere VMDK volume (migration on by default starting v1.25)\n\nOlder versions of Kubernetes also supported the following in-tree PersistentVolume types:\ncephfs (not available starting v1.31)\nflocker\n\n- Flocker storage. (not available starting v1.25)\n\nglusterfs\n\n- GlusterFS storage. (not available starting v1.26)\n\nphotonPersistentDisk\nquobyte\n\n- Photon controller persistent disk. (not available starting v1.15)\n\n- Quobyte volume. (not available starting v1.25)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0331", "text": "rbd - Rados Block Device (RBD) volume (not available starting v1.31)\nscaleIO\n\n- ScaleIO volume. (not available starting v1.21)\n\nstorageos\n\n- StorageOS volume. (not available starting v1.25)\n\nPersistent Volumes\nEach PV contains a spec and status, which is the specification and status of the volume. The name of a PersistentVolume object must\nbe a valid DNS subdomain name.\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: pv0003\nspec:\ncapacity:\nstorage: 5Gi\nvolumeMode: Filesystem\naccessModes:\n- ReadWriteOnce\npersistentVolumeReclaimPolicy: Recycle\nstorageClassName: slow\nmountOptions:\n- hard\n- nfsvers=4.1\nnfs:\npath: /tmp\nserver: 172.17.0.2\n\nNote:\nHelper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster. In this\nexample, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of\nNFS filesystems.\n\nCapacity\nGenerally, a PV will have a specific storage capacity. This is set using the PV's capacity attribute which is a Quantity value.\nhttps://kubernetes.io/docs/concepts/_print/\n\n318/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nCurrently, storage size is the only resource that can be set or requested. Future attributes may include IOPS, throughput, etc.\n\nVolume Mode\nâ“˜ FEATURE STATE: Kubernetes v1.18 [stable]\n\nKubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block .\nvolumeMode\n\nis an optional API parameter. Filesystem is the default mode used when volumeMode parameter is omitted.\n\nA volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volume is backed by a block device and the\ndevice is empty, Kubernetes creates a filesystem on the device before mounting it for the first time.\nYou can set the value of volumeMode to Block to use a volume as a raw block device. Such volume is presented into a Pod as a\nblock device, without any filesystem on it. This mode is useful to provide a Pod the fastest possible way to access a volume, without\nany filesystem layer between the Pod and the volume. On the other hand, the application running in the Pod must know how to\nhandle a raw block device. See Raw Block Volume Support for an example on how to use a volume with volumeMode: Block in a\nPod."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0332", "text": "Access Modes\nA PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below,\nproviders will have different capabilities and each PV's access modes are set to the specific modes supported by that particular\nvolume. For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as readonly. Each PV gets its own set of access modes describing that specific PV's capabilities.\nThe access modes are:\nReadWriteOnce\n\nthe volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access\n(read from or write to) that volume when the pods are running on the same node. For single pod access, please see\nReadWriteOncePod.\nReadOnlyMany\n\nthe volume can be mounted as read-only by many nodes.\nReadWriteMany\n\nthe volume can be mounted as read-write by many nodes.\nReadWriteOncePod\n\nâ“˜ FEATURE STATE: Kubernetes v1.29 [stable]\n\nthe volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only\none pod across the whole cluster can read that PVC or write to it.\nNote:\nThe ReadWriteOncePod access mode is only supported for CSI volumes and Kubernetes version 1.22+. To use this feature you\nwill need to update the following CSI sidecars to these versions or greater:\ncsi-provisioner:v3.0.0+\ncsi-attacher:v3.3.0+\ncsi-resizer:v1.3.0+\n\nIn the CLI, the access modes are abbreviated to:\nRWO - ReadWriteOnce\nROX - ReadOnlyMany\nhttps://kubernetes.io/docs/concepts/_print/\n\n319/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nRWX - ReadWriteMany\nRWOP - ReadWriteOncePod\nNote:\nKubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes. In some cases, the volume\naccess modes also constrain where the PersistentVolume can be mounted. Volume access modes do not enforce write\nprotection once the storage has been mounted. Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or\nReadWriteMany, they don't set any constraints on the volume. For example, even if a PersistentVolume is created as\nReadOnlyMany, it is no guarantee that it will be read-only. If the access modes are specified as ReadWriteOncePod, the volume\nis constrained and can be mounted on only a single Pod.\nImportant! A volume can only be mounted using one access mode at a time, even if it supports many.\nVolume Plugin\n\nReadWriteOnce\n\nReadOnlyMany\n\nReadWriteMany\n\nReadWriteOncePod\n\nAzureFile\n\nâœ“\n\nâœ“\n\nâœ“\n\n-\n\nCephFS\n\nâœ“\n\nâœ“\n\nâœ“\n\n-\n\nCSI\n\ndepends on the\ndriver"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0333", "text": "depends on the\ndriver\n\ndepends on the driver\n\nFC\n\nâœ“\n\nâœ“\n\n-\n\n-\n\nFlexVolume\n\nâœ“\n\nâœ“\n\ndepends on the driver\n\n-\n\nHostPath\n\nâœ“\n\n-\n\n-\n\n-\n\niSCSI\n\nâœ“\n\nâœ“\n\n-\n\n-\n\nNFS\n\nâœ“\n\nâœ“\n\nâœ“\n\n-\n\nRBD\n\nâœ“\n\nâœ“\n\n-\n\n-\n\nVsphereVolume\n\nâœ“\n\n-\n\n- (works when Pods are\ncollocated)\n\n-\n\nPortworxVolume\n\nâœ“\n\n-\n\nâœ“\n\n-\n\ndepends on the\ndriver\n\nClass\nA PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a\nparticular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be\nbound to PVCs that request no particular class.\nIn the past, the annotation volume.beta.kubernetes.io/storage-class was used instead of the storageClassName attribute. This\nannotation is still working; however, it will become fully deprecated in a future Kubernetes release.\n\nReclaim Policy\nCurrent reclaim policies are:\nRetain -- manual reclamation\nRecycle -- basic scrub ( rm -rf /thevolume/* )\nDelete -- delete the volume\nFor Kubernetes 1.34, only nfs and hostPath volume types support recycling.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n320/684\n\n11/7/25, 4:37 PM\n\nMount Options\n\nConcepts | Kubernetes\n\nA Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.\nNote:\nNot all Persistent Volume types support mount options.\nThe following volume types support mount options:\ncsi\n\n(including CSI migrated volume types)\n\niscsi\nnfs\n\nMount options are not validated. If a mount option is invalid, the mount fails.\nIn the past, the annotation volume.beta.kubernetes.io/mount-options was used instead of the mountOptions attribute. This\nannotation is still working; however, it will become fully deprecated in a future Kubernetes release.\n\nNode Affinity\nNote:\nFor most volume types, you do not need to set this field. You need to explicitly set this for local volumes.\nA PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will\nonly be scheduled to nodes that are selected by the node affinity. To specify node affinity, set nodeAffinity in the .spec of a PV.\nThe PersistentVolume API reference has more details on this field.\n\nPhase\nA PersistentVolume will be in one of the following phases:\nAvailable\n\na free resource that is not yet bound to a claim\nBound\n\nthe volume is bound to a claim\nReleased"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0334", "text": "the claim has been deleted, but the associated storage resource is not yet reclaimed by the cluster\nFailed\n\nthe volume has failed its (automated) reclamation\nYou can see the name of the PVC bound to the PV using kubectl describe persistentvolume <name> .\n\nPhase transition timestamp\nâ“˜ FEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)\n\nThe .status field for a PersistentVolume can include an alpha lastPhaseTransitionTime field. This field records the timestamp of\nwhen the volume last transitioned its phase. For newly created volumes the phase is set to Pending and lastPhaseTransitionTime\nis set to the current time.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n321/684\n\n11/7/25, 4:37 PM\n\nPersistentVolumeClaims\n\nConcepts | Kubernetes\n\nEach PVC contains a spec and status, which is the specification and status of the claim. The name of a PersistentVolumeClaim object\nmust be a valid DNS subdomain name.\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: myclaim\nspec:\naccessModes:\n- ReadWriteOnce\nvolumeMode: Filesystem\nresources:\nrequests:\nstorage: 8Gi\nstorageClassName: slow\nselector:\nmatchLabels:\nrelease: \"stable\"\nmatchExpressions:\n- {key: environment, operator: In, values: [dev]}\n\nAccess Modes\nClaims use the same conventions as volumes when requesting storage with specific access modes.\n\nVolume Modes\nClaims use the same convention as volumes to indicate the consumption of the volume as either a filesystem or block device.\n\nVolume Name\nClaims can use the volumeName field to explicitly bind to a specific PersistentVolume. You can also leave volumeName unset,\nindicating that you'd like Kubernetes to set up a new PersistentVolume that matches the claim. If the specified PV is already bound to\nanother PVC, the binding will be stuck in a pending state.\n\nResources\nClaims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same resource model\napplies to both volumes and claims.\nNote:\nFor Filesystem volumes, the storage request refers to the \"outer\" volume size (i.e. the allocated size from the storage\nbackend). This means that the writeable size may be slightly lower for providers that build a filesystem on top of a block device,\ndue to filesystem overhead. This is especially visible with XFS, where many metadata features are enabled by default.\n\nSelector\nClaims can specify a label selector to further filter the set of volumes. Only the volumes whose labels match the selector can be\nbound to the claim. The selector can consist of two fields:\nmatchLabels\n\n- the volume must have a label with this value"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0335", "text": "- a list of requirements made by specifying key, list of values, and operator that relates the key and values.\nValid operators include In , NotIn , Exists , and DoesNotExist .\nmatchExpressions\n\nAll of the requirements, from both matchLabels and matchExpressions , are ANDed together â€“ they must all be satisfied in order to\nmatch.\nhttps://kubernetes.io/docs/concepts/_print/\n\n322/684\n\n11/7/25, 4:37 PM\n\nClass\n\nConcepts | Kubernetes\n\nA claim can request a particular class by specifying the name of a StorageClass using the attribute storageClassName . Only PVs of\nthe requested class, ones with the same storageClassName as the PVC, can be bound to the PVC.\nPVCs don't necessarily have to request a class. A PVC with its storageClassName set equal to \"\" is always interpreted to be\nrequesting a PV with no class, so it can only be bound to PVs with no class (no annotation or one set equal to \"\" ). A PVC with no\nstorageClassName is not quite the same and is treated differently by the cluster, depending on whether the DefaultStorageClass\nadmission plugin is turned on.\nIf the admission plugin is turned on, the administrator may specify a default StorageClass. All PVCs that have no\nstorageClassName can be bound only to PVs of that default. Specifying a default StorageClass is done by setting the annotation\nstorageclass.kubernetes.io/is-default-class equal to true in a StorageClass object. If the administrator does not specify a\ndefault, the cluster responds to PVC creation as if the admission plugin were turned off. If more than one default StorageClass\nis specified, the newest default is used when the PVC is dynamically provisioned.\nIf the admission plugin is turned off, there is no notion of a default StorageClass. All PVCs that have storageClassName set to\n\"\" can be bound only to PVs that have storageClassName also set to \"\" . However, PVCs with missing storageClassName can\nbe updated later once default StorageClass becomes available. If the PVC gets updated it will no longer bind to PVs that have\nstorageClassName also set to \"\" .\nSee retroactive default StorageClass assignment for more details.\nDepending on installation method, a default StorageClass may be deployed to a Kubernetes cluster by addon manager during\ninstallation.\nWhen a PVC specifies a selector in addition to requesting a StorageClass, the requirements are ANDed together: only a PV of the\nrequested class and with the requested labels may be bound to the PVC.\nNote:\nCurrently, a PVC with a non-empty selector can't have a PV dynamically provisioned for it.\nIn the past, the annotation volume.beta.kubernetes.io/storage-class was used instead of storageClassName attribute. This\nannotation is still working; however, it won't be supported in a future Kubernetes release."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0336", "text": "Retroactive default StorageClass assignment\nâ“˜ FEATURE STATE: Kubernetes v1.28 [stable]\n\nYou can create a PersistentVolumeClaim without specifying a storageClassName for the new PVC, and you can do so even when no\ndefault StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the storageClassName of that PVC\nremains unset until default becomes available.\nWhen a default StorageClass becomes available, the control plane identifies any existing PVCs without storageClassName . For the\nPVCs that either have an empty value for storageClassName or do not have this key, the control plane then updates those PVCs to\nset storageClassName to match the new default StorageClass. If you have an existing PVC where the storageClassName is \"\" , and\nyou configure a default StorageClass, then this PVC will not get updated.\nIn order to keep binding to PVs with storageClassName set to \"\" (while a default StorageClass is present), you need to set the\nstorageClassName of the associated PVC to \"\" .\nThis behavior helps administrators change default StorageClass by removing the old one first and then creating or setting another\none. This brief window while there is no default causes PVCs without storageClassName created at that time to not have any default,\nbut due to the retroactive default StorageClass assignment this way of changing defaults is safe.\n\nClaims As Volumes\nPods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The\ncluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then\nmounted to the host and into the Pod.\nhttps://kubernetes.io/docs/concepts/_print/\n\n323/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: mypod\nspec:\ncontainers:\n- name: myfrontend\nimage: nginx\nvolumeMounts:\n- mountPath: \"/var/www/html\"\nname: mypd\nvolumes:\n- name: mypd\npersistentVolumeClaim:\nclaimName: myclaim\n\nA Note on Namespaces\nPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with \"Many\"\nmodes ( ROX , RWX ) is only possible within one namespace.\n\nPersistentVolumes typed hostPath\nA hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage. See an example of\nhostPath typed volume.\n\nRaw Block Volume Support\nâ“˜ FEATURE STATE: Kubernetes v1.18 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0337", "text": "The following volume plugins support raw block volumes, including dynamic provisioning where applicable:\nCSI (including some CSI migrated volume types)\nFC (Fibre Channel)\niSCSI\nLocal volume\n\nPersistentVolume using a Raw Block Volume\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: block-pv\nspec:\ncapacity:\nstorage: 10Gi\naccessModes:\n- ReadWriteOnce\nvolumeMode: Block\npersistentVolumeReclaimPolicy: Retain\nfc:\ntargetWWNs: [\"50060e801049cfd1\"]\nlun: 0\nreadOnly: false\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n324/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPersistentVolumeClaim requesting a Raw Block Volume\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: block-pvc\nspec:\naccessModes:\n- ReadWriteOnce\nvolumeMode: Block\nresources:\nrequests:\nstorage: 10Gi\n\nPod specification adding Raw Block Device path in container\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod-with-block-volume\nspec:\ncontainers:\n- name: fc-container\nimage: fedora:26\ncommand: [\"/bin/sh\", \"-c\"]\nargs: [ \"tail -f /dev/null\" ]\nvolumeDevices:\n- name: data\ndevicePath: /dev/xvda\nvolumes:\n- name: data\npersistentVolumeClaim:\nclaimName: block-pvc\n\nNote:\nWhen adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.\n\nBinding Block Volumes\nIf a user requests a raw block volume by indicating this using the volumeMode field in the PersistentVolumeClaim spec, the binding\nrules differ slightly from previous releases that didn't consider this mode as part of the spec. Listed is a table of possible\ncombinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or\nnot given the combinations: Volume binding matrix for statically provisioned volumes:\nPV volumeMode\n\nPVC volumeMode\n\nResult\n\nunspecified\n\nunspecified\n\nBIND\n\nunspecified\n\nBlock\n\nunspecified\n\nFilesystem\n\nBIND\n\nBlock\n\nunspecified\n\nNO BIND\n\nBlock\n\nBlock\n\nBlock\n\nFilesystem\n\nhttps://kubernetes.io/docs/concepts/_print/\n\nNO BIND\n\nBIND\nNO BIND\n325/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPV volumeMode\n\nPVC volumeMode\n\nResult\n\nFilesystem\n\nFilesystem\n\nBIND\n\nFilesystem\n\nBlock\n\nFilesystem\n\nunspecified\n\nNO BIND\nBIND\n\nNote:\nOnly statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values\nwhen working with raw block devices.\n\nVolume Snapshot and Restore Volume from Snapshot Support\nâ“˜ FEATURE STATE: Kubernetes v1.20 [stable]\n\nVolume snapshots only support the out-of-tree CSI volume plugins. For details, see Volume Snapshots. In-tree volume plugins are\ndeprecated. You can read about the deprecated volume plugins in the Volume Plugin FAQ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0338", "text": "Create a PersistentVolumeClaim from a Volume Snapshot\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: restore-pvc\nspec:\nstorageClassName: csi-hostpath-sc\ndataSource:\nname: new-snapshot-test\nkind: VolumeSnapshot\napiGroup: snapshot.storage.k8s.io\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 10Gi\n\nVolume Cloning\nVolume Cloning only available for CSI volume plugins.\n\nCreate PersistentVolumeClaim from an existing PVC\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n326/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: cloned-pvc\nspec:\nstorageClassName: my-csi-plugin\ndataSource:\nname: existing-src-pvc-name\nkind: PersistentVolumeClaim\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 10Gi\n\nVolume populators and data sources\nâ“˜ FEATURE STATE: Kubernetes v1.24 [beta]\n\nKubernetes supports custom volume populators. To use custom volume populators, you must enable the AnyVolumeDataSource\nfeature gate for the kube-apiserver and kube-controller-manager.\nVolume populators take advantage of a PVC spec field called dataSourceRef . Unlike the dataSource field, which can only contain\neither a reference to another PersistentVolumeClaim or to a VolumeSnapshot, the dataSourceRef field can contain a reference to\nany object in the same namespace, except for core objects other than PVCs. For clusters that have the feature gate enabled, use of\nthe dataSourceRef is preferred over dataSource .\n\nCross namespace data sources\nâ“˜ FEATURE STATE: Kubernetes v1.26 [alpha]\n\nKubernetes supports cross namespace volume data sources. To use cross namespace volume data sources, you must enable the\nAnyVolumeDataSource and CrossNamespaceVolumeDataSource feature gates for the kube-apiserver and kube-controller-manager.\nAlso, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.\nEnabling the CrossNamespaceVolumeDataSource feature gate allows you to specify a namespace in the dataSourceRef field.\nNote:\nWhen you specify a namespace for a volume data source, Kubernetes checks for a ReferenceGrant in the other namespace\nbefore accepting the reference. ReferenceGrant is part of the gateway.networking.k8s.io extension APIs. See ReferenceGrant\nin the Gateway API documentation for details. This means that you must extend your Kubernetes cluster with at least\nReferenceGrant from the Gateway API before you can use this mechanism."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0339", "text": "Data source references\nThe dataSourceRef field behaves almost the same as the dataSource field. If one is specified while the other is not, the API server\nwill give both fields the same value. Neither field can be changed after creation, and attempting to specify different values for the\ntwo fields will result in a validation error. Therefore the two fields will always have the same contents.\nThere are two differences between the dataSourceRef field and the dataSource field that users should be aware of:\nThe dataSource field ignores invalid values (as if the field was blank) while the dataSourceRef field never ignores values and\nwill cause an error if an invalid value is used. Invalid values are any core object (objects with no apiGroup) except for PVCs.\nThe dataSourceRef field may contain different types of objects, while the dataSource field only allows PVCs and\nVolumeSnapshots.\nhttps://kubernetes.io/docs/concepts/_print/\n\n327/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen the CrossNamespaceVolumeDataSource feature is enabled, there are additional differences:\nThe dataSource field only allows local objects, while the dataSourceRef field allows objects in any namespaces.\nWhen namespace is specified, dataSource and dataSourceRef are not synced.\nUsers should always use dataSourceRef on clusters that have the feature gate enabled, and fall back to dataSource on clusters\nthat do not. It is not necessary to look at both fields under any circumstance. The duplicated values with slightly different semantics\nexist only for backwards compatibility. In particular, a mixture of older and newer controllers are able to interoperate because the\nfields are the same.\n\nUsing volume populators\nVolume populators are controllers that can create non-empty volumes, where the contents of the volume are determined by a\nCustom Resource. Users create a populated volume by referring to a Custom Resource using the dataSourceRef field:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: populated-pvc\nspec:\ndataSourceRef:\nname: example-name\nkind: ExampleDataSource\napiGroup: example.storage.k8s.io\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 10Gi"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0340", "text": "Because volume populators are external components, attempts to create a PVC that uses one can fail if not all the correct\ncomponents are installed. External controllers should generate events on the PVC to provide feedback on the status of the creation,\nincluding warnings if the PVC cannot be created due to some missing component.\nYou can install the alpha volume data source validator controller into your cluster. That controller generates warning Events on a\nPVC in the case that no populator is registered to handle that kind of data source. When a suitable populator is installed for a PVC,\nit's the responsibility of that populator controller to report Events that relate to volume creation and issues during the process.\n\nUsing a cross-namespace volume data source\nâ“˜ FEATURE STATE: Kubernetes v1.26 [alpha]\n\nCreate a ReferenceGrant to allow the namespace owner to accept the reference. You define a populated volume by specifying a\ncross namespace volume data source using the dataSourceRef field. You must already have a valid ReferenceGrant in the source\nnamespace:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n328/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: gateway.networking.k8s.io/v1beta1\nkind: ReferenceGrant\nmetadata:\nname: allow-ns1-pvc\nnamespace: default\nspec:\nfrom:\n- group: \"\"\nkind: PersistentVolumeClaim\nnamespace: ns1\nto:\n- group: snapshot.storage.k8s.io\nkind: VolumeSnapshot\nname: new-snapshot-demo\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: foo-pvc\nnamespace: ns1\nspec:\nstorageClassName: example\naccessModes:\n- ReadWriteOnce\nresources:\nrequests:\nstorage: 1Gi\ndataSourceRef:\napiGroup: snapshot.storage.k8s.io\nkind: VolumeSnapshot\nname: new-snapshot-demo\nnamespace: default\nvolumeMode: Filesystem"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0341", "text": "Writing Portable Configuration\nIf you're writing configuration templates or examples that run on a wide range of clusters and need persistent storage, it is\nrecommended that you use the following pattern:\nInclude PersistentVolumeClaim objects in your bundle of config (alongside Deployments, ConfigMaps, etc).\nDo not include PersistentVolume objects in the config, since the user instantiating the config may not have permission to create\nPersistentVolumes.\nGive the user the option of providing a storage class name when instantiating the template.\nIf the user provides a storage class name, put that value into the persistentVolumeClaim.storageClassName field. This will\ncause the PVC to match the right storage class if the cluster has StorageClasses enabled by the admin.\nIf the user does not provide a storage class name, leave the persistentVolumeClaim.storageClassName field as nil. This\nwill cause a PV to be automatically provisioned for the user with the default StorageClass in the cluster. Many cluster\nenvironments have a default StorageClass installed, or administrators can create their own default StorageClass.\nIn your tooling, watch for PVCs that are not getting bound after some time and surface this to the user, as this may indicate\nthat the cluster has no dynamic storage support (in which case the user should create a matching PV) or the cluster has no\nstorage system (in which case the user cannot deploy config requiring PVCs).\n\nWhat's next\nLearn more about Creating a PersistentVolume.\nLearn more about Creating a PersistentVolumeClaim.\nRead the Persistent Storage design document.\nhttps://kubernetes.io/docs/concepts/_print/\n\n329/684\n\n11/7/25, 4:37 PM\n\nAPI references\n\nConcepts | Kubernetes\n\nRead about the APIs described in this page:\nPersistentVolume\nPersistentVolumeClaim\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n330/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n6.3 - Projected Volumes\n\nThis document describes projected volumes in Kubernetes. Familiarity with volumes is suggested.\n\nIntroduction\nA projected volume maps several existing volume sources into the same directory.\nCurrently, the following types of volume sources can be projected:\nsecret\ndownwardAPI\nconfigMap\nserviceAccountToken\nclusterTrustBundle\npodCertificate\n\nAll sources are required to be in the same namespace as the Pod. For more details, see the all-in-one volume design document."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0342", "text": "Example configuration with a secret, a downwardAPI, and a configMap\npods/storage/projected-secret-downwardapi-configmap.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: volume-test\nspec:\ncontainers:\n- name: container-test\nimage: busybox:1.28\ncommand: [\"sleep\", \"3600\"]\nvolumeMounts:\n- name: all-in-one\nmountPath: \"/projected-volume\"\nreadOnly: true\nvolumes:\n- name: all-in-one\nprojected:\nsources:\n- secret:\nname: mysecret\nitems:\n- key: username\npath: my-group/my-username\n- downwardAPI:\nitems:\n- path: \"labels\"\nfieldRef:\nfieldPath: metadata.labels\n- path: \"cpu_limit\"\nresourceFieldRef:\ncontainerName: container-test\nresource: limits.cpu\n- configMap:\nname: myconfigmap\nitems:\n- key: config\npath: my-group/my-config\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n331/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nExample configuration: secrets with a non-default permission mode set\npods/storage/projected-secrets-nondefault-permission-mode.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: volume-test\nspec:\ncontainers:\n- name: container-test\nimage: busybox:1.28\ncommand: [\"sleep\", \"3600\"]\nvolumeMounts:\n- name: all-in-one\nmountPath: \"/projected-volume\"\nreadOnly: true\nvolumes:\n- name: all-in-one\nprojected:\nsources:\n- secret:\nname: mysecret\nitems:\n- key: username\npath: my-group/my-username\n- secret:\nname: mysecret2\nitems:\n- key: password\npath: my-group/my-password\nmode: 511\n\nEach projected volume source is listed in the spec under sources . The parameters are nearly the same with two exceptions:\nFor secrets, the secretName field has been changed to name to be consistent with ConfigMap naming.\nThe defaultMode can only be specified at the projected level and not for each volume source. However, as illustrated above,\nyou can explicitly set the mode for each individual projection.\n\nserviceAccountToken projected volumes\nYou can inject the token for the current service account into a Pod at a specified path. For example:\npods/storage/projected-service-account-token.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n332/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: sa-token-test\nspec:\ncontainers:\n- name: container-test\nimage: busybox:1.28\ncommand: [\"sleep\", \"3600\"]\nvolumeMounts:\n- name: token-vol\nmountPath: \"/service-account\"\nreadOnly: true\nserviceAccountName: default\nvolumes:\n- name: token-vol\nprojected:\nsources:\n- serviceAccountToken:\naudience: api\nexpirationSeconds: 3600\npath: token"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0343", "text": "The example Pod has a projected volume containing the injected service account token. Containers in this Pod can use that token to\naccess the Kubernetes API server, authenticating with the identity of the pod's ServiceAccount. The audience field contains the\nintended audience of the token. A recipient of the token must identify itself with an identifier specified in the audience of the token,\nand otherwise should reject the token. This field is optional and it defaults to the identifier of the API server.\nThe expirationSeconds is the expected duration of validity of the service account token. It defaults to 1 hour and must be at least\n10 minutes (600 seconds). An administrator can also limit its maximum value by specifying the --service-account-max-tokenexpiration option for the API server. The path field specifies a relative path to the mount point of the projected volume.\nNote:\nA container using a projected volume source as a subPath volume mount will not receive updates for those volume sources.\n\nclusterTrustBundle projected volumes\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)\n\nNote:\nTo use this feature in Kubernetes 1.34, you must enable support for ClusterTrustBundle objects with the ClusterTrustBundle\nfeature gate and --runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true kube-apiserver flag, then\nenable the ClusterTrustBundleProjection feature gate.\nThe clusterTrustBundle projected volume source injects the contents of one or more ClusterTrustBundle objects as an\nautomatically-updating file in the container filesystem.\nClusterTrustBundles can be selected either by name or by signer name.\nTo select by name, use the name field to designate a single ClusterTrustBundle object.\nTo select by signer name, use the signerName field (and optionally the labelSelector field) to designate a set of\nClusterTrustBundle objects that use the given signer name. If labelSelector is not present, then all ClusterTrustBundles for that\nsigner are selected.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n333/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0344", "text": "The kubelet deduplicates the certificates in the selected ClusterTrustBundle objects, normalizes the PEM representations (discarding\ncomments and headers), reorders the certificates, and writes them into the file named by path . As the set of selected\nClusterTrustBundles or their content changes, kubelet keeps the file up-to-date.\nBy default, the kubelet will prevent the pod from starting if the named ClusterTrustBundle is not found, or if signerName /\nlabelSelector do not match any ClusterTrustBundles. If this behavior is not what you want, then set the optional field to true ,\nand the pod will start up with an empty file at path .\npods/storage/projected-clustertrustbundle.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: sa-ctb-name-test\nspec:\ncontainers:\n- name: container-test\nimage: busybox\ncommand: [\"sleep\", \"3600\"]\nvolumeMounts:\n- name: token-vol\nmountPath: \"/root-certificates\"\nreadOnly: true\nserviceAccountName: default\nvolumes:\n- name: token-vol\nprojected:\nsources:\n- clusterTrustBundle:\nname: example\npath: example-roots.pem\n- clusterTrustBundle:\nsignerName: \"example.com/mysigner\"\nlabelSelector:\nmatchLabels:\nversion: live\npath: mysigner-roots.pem\noptional: true\n\npodCertificate projected volumes\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\nNote:\nIn Kubernetes 1.34, you must enable support for Pod Certificates using the PodCertificateRequest feature gate and the -runtime-config=certificates.k8s.io/v1alpha1/podcertificaterequests=true kube-apiserver flag.\nThe podCertificate projected volumes source securely provisions a private key and X.509 certificate chain for pod to use as client\nor server credentials. Kubelet will then handle refreshing the private key and certificate chain when they get close to expiration. The\napplication just has to make sure that it reloads the file promptly when it changes, with a mechanism like inotify or polling.\nEach podCertificate projection supports the following configuration fields:\nsignerName : The signer you want to issue the certificate. Note that signers may have their own access requirements, and may\n\nrefuse to issue certificates to your pod.\nkeyType : The type of private key that should be generated. Valid values are ED25519 , ECDSAP256 , ECDSAP384 , ECDSAP521 ,\nRSA3072 , and RSA4096 .\nmaxExpirationSeconds : The maximum lifetime you will accept for the certificate issued to the pod. If not set, will be defaulted\n\nto 86400 (24 hours). Must be at least 3600 (1 hour), and at most 7862400 (91 days). Kubernetes built-in signers are restricted\nhttps://kubernetes.io/docs/concepts/_print/\n\n334/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0345", "text": "to a max lifetime of 86400 (1 day). The signer is allowed to issue a certificate with a lifetime shorter than what you've specified.\ncredentialBundlePath : Relative path within the projection where the credential bundle should be written. The credential\n\nbundle is a PEM-formatted file, where the first block is a \"PRIVATE KEY\" block that contains a PKCS#8-serialized private key, and\nthe remaining blocks are \"CERTIFICATE\" blocks that comprise the certificate chain (leaf certificate and any intermediates).\nkeyPath and certificateChainPath : Separate paths where Kubelet should write just the private key or certificate chain.\nNote:\nMost applications should prefer using credentialBundlePath unless they need the key and certificates in separate files for\ncompatibility reasons. Kubelet uses an atomic writing strategy based on symlinks to make sure that when you open the files it\nprojects, you read either the old content or the new content. However, if you read the key and certificate chain from separate\nfiles, Kubelet may rotate the credentials after your first read and before your second read, resulting in your application loading\na mismatched key and certificate.\n\npods/storage/projected-podcertificate.yaml\n# Sample Pod spec that uses a podCertificate projection to request an ED25519\n# private key, a certificate from the `coolcert.example.com/foo` signer, and\n# write the results to `/var/run/my-x509-credentials/credentialbundle.pem`.\napiVersion: v1\nkind: Pod\nmetadata:\nnamespace: default\nname: podcertificate-pod\nspec:\nserviceAccountName: default\ncontainers:\n- image: debian\nname: main\ncommand: [\"sleep\", \"infinity\"]\nvolumeMounts:\n- name: my-x509-credentials\nmountPath: /var/run/my-x509-credentials\nvolumes:\n- name: my-x509-credentials\nprojected:\ndefaultMode: 420\nsources:\n- podCertificate:\nkeyType: ED25519\nsignerName: coolcert.example.com/foo\ncredentialBundlePath: credentialbundle.pem\n\nSecurityContext interactions\nThe proposal for file permission handling in projected service account volume enhancement introduced the projected files having\nthe correct owner permissions set.\n\nLinux\nIn Linux pods that have a projected volume and RunAsUser set in the Pod SecurityContext , the projected files have the correct\nownership set including container user ownership.\nWhen all containers in a pod have the same runAsUser set in their PodSecurityContext or container SecurityContext , then the\nkubelet ensures that the contents of the serviceAccountToken volume are owned by that user, and the token file has its permission\nmode set to 0600 .\nNote:\nEphemeral containers added to a Pod after it is created do not change volume permissions that were set when the pod was\ncreated.\nhttps://kubernetes.io/docs/concepts/_print/\n\n335/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0346", "text": "If a Pod's serviceAccountToken volume permissions were set to 0600 because all other containers in the Pod have the same\nrunAsUser , ephemeral containers must use the same runAsUser to be able to read the token.\n\nWindows\nIn Windows pods that have a projected volume and RunAsUsername set in the Pod SecurityContext , the ownership is not enforced\ndue to the way user accounts are managed in Windows. Windows stores and manages local user and group accounts in a database\nfile called Security Account Manager (SAM). Each container maintains its own instance of the SAM database, to which the host has no\nvisibility into while the container is running. Windows containers are designed to run the user mode portion of the OS in isolation\nfrom the host, hence the maintenance of a virtual SAM database. As a result, the kubelet running on the host does not have the\nability to dynamically configure host file ownership for virtualized container accounts. It is recommended that if files on the host\nmachine are to be shared with the container then they should be placed into their own volume mount outside of C:\\ .\nBy default, the projected files will have the following ownership as shown for an example projected volume file:\n\nPS C:\\> Get-Acl C:\\var\\run\\secrets\\kubernetes.io\\serviceaccount\\..2021_08_31_22_22_18.318230061\\ca.crt | Format-List\nPath\n: Microsoft.PowerShell.Core\\FileSystem::C:\\var\\run\\secrets\\kubernetes.io\\serviceaccount\\..2021_08_31_22_22_18\nOwner : BUILTIN\\Administrators\nGroup : NT AUTHORITY\\SYSTEM\nAccess : NT AUTHORITY\\SYSTEM Allow FullControl\nBUILTIN\\Administrators Allow FullControl\nBUILTIN\\Users Allow ReadAndExecute, Synchronize\nAudit :\nSddl\n: O:BAG:SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)\n\nThis implies all administrator users like ContainerAdministrator will have read, write and execute access while, non-administrator\nusers will have read and execute access.\nNote:\nIn general, granting the container access to the host is discouraged as it can open the door for potential security exploits.\nCreating a Windows Pod with RunAsUser in it's SecurityContext will result in the Pod being stuck at ContainerCreating\nforever. So it is advised to not use the Linux only RunAsUser option with Windows Pods.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n336/684\n\n11/7/25, 4:37 PM\n\n6.4 - Ephemeral Volumes\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0347", "text": "This document describes ephemeral volumes in Kubernetes. Familiarity with volumes is suggested, in particular\nPersistentVolumeClaim and PersistentVolume.\nSome applications need additional storage but don't care whether that data is stored persistently across restarts. For example,\ncaching services are often limited by memory size and can move infrequently used data into storage that is slower than memory\nwith little impact on overall performance.\nOther applications expect some read-only input data to be present in files, like configuration data or secret keys.\nEphemeral volumes are designed for these use cases. Because volumes follow the Pod's lifetime and get created and deleted along\nwith the Pod, Pods can be stopped and restarted without being limited to where some persistent volume is available.\nEphemeral volumes are specified inline in the Pod spec, which simplifies application deployment and management.\n\nTypes of ephemeral volumes\nKubernetes supports several different kinds of ephemeral volumes for different purposes:\nemptyDir: empty at Pod startup, with storage coming locally from the kubelet base directory (usually the root disk) or RAM\nconfigMap, downwardAPI, secret: inject different kinds of Kubernetes data into a Pod\nimage: allows mounting container image files or artifacts, directly to a Pod.\nCSI ephemeral volumes: similar to the previous volume kinds, but provided by special CSI drivers which specifically support this\nfeature\ngeneric ephemeral volumes, which can be provided by all storage drivers that also support persistent volumes\nemptyDir , configMap , downwardAPI , secret\n\nare provided as local ephemeral storage. They are managed by kubelet on each node.\n\nCSI ephemeral volumes must be provided by third-party CSI storage drivers.\nGeneric ephemeral volumes can be provided by third-party CSI storage drivers, but also by any other storage driver that supports\ndynamic provisioning. Some CSI drivers are written specifically for CSI ephemeral volumes and do not support dynamic provisioning:\nthose then cannot be used for generic ephemeral volumes.\nThe advantage of using third-party drivers is that they can offer functionality that Kubernetes itself does not support, for example\nstorage with different performance characteristics than the disk that is managed by kubelet, or injecting different data.\n\nCSI ephemeral volumes\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0348", "text": "Note:\nCSI ephemeral volumes are only supported by a subset of CSI drivers. The Kubernetes CSI Drivers list shows which drivers\nsupport ephemeral volumes.\nConceptually, CSI ephemeral volumes are similar to configMap , downwardAPI and secret volume types: the storage is managed\nlocally on each node and is created together with other local resources after a Pod has been scheduled onto a node. Kubernetes has\nno concept of rescheduling Pods anymore at this stage. Volume creation has to be unlikely to fail, otherwise Pod startup gets stuck.\nIn particular, storage capacity aware Pod scheduling is not supported for these volumes. They are currently also not covered by the\nstorage resource usage limits of a Pod, because that is something that kubelet can only enforce for storage that it manages itself.\nHere's an example manifest for a Pod that uses CSI ephemeral storage:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n337/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkind: Pod\napiVersion: v1\nmetadata:\nname: my-csi-app\nspec:\ncontainers:\n- name: my-frontend\nimage: busybox:1.28\nvolumeMounts:\n- mountPath: \"/data\"\nname: my-csi-inline-vol\ncommand: [ \"sleep\", \"1000000\" ]\nvolumes:\n- name: my-csi-inline-vol\ncsi:\ndriver: inline.storage.kubernetes.io\nvolumeAttributes:\nfoo: bar\n\nThe volumeAttributes determine what volume is prepared by the driver. These attributes are specific to each driver and not\nstandardized. See the documentation of each CSI driver for further instructions.\n\nCSI driver restrictions\nCSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. A CSI driver\nallowing volumeAttributes that are typically restricted to administrators is NOT suitable for use in an inline ephemeral volume. For\nexample, parameters that are normally defined in the StorageClass should not be exposed to users through the use of inline\nephemeral volumes.\nCluster administrators who need to restrict the CSI drivers that are allowed to be used as inline volumes within a Pod spec may do so\nby:\nRemoving Ephemeral from volumeLifecycleModes in the CSIDriver spec, which prevents the driver from being used as an\ninline ephemeral volume.\nUsing an admission webhook to restrict how this driver is used.\n\nGeneric ephemeral volumes\nâ“˜ FEATURE STATE: Kubernetes v1.23 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0349", "text": "Generic ephemeral volumes are similar to emptyDir volumes in the sense that they provide a per-pod directory for scratch data\nthat is usually empty after provisioning. But they may also have additional features:\nStorage can be local or network-attached.\nVolumes can have a fixed size that Pods are not able to exceed.\nVolumes may have some initial data, depending on the driver and parameters.\nTypical operations on volumes are supported assuming that the driver supports them, including snapshotting, cloning, resizing,\nand storage capacity tracking.\nExample:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n338/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkind: Pod\napiVersion: v1\nmetadata:\nname: my-app\nspec:\ncontainers:\n- name: my-frontend\nimage: busybox:1.28\nvolumeMounts:\n- mountPath: \"/scratch\"\nname: scratch-volume\ncommand: [ \"sleep\", \"1000000\" ]\nvolumes:\n- name: scratch-volume\nephemeral:\nvolumeClaimTemplate:\nmetadata:\nlabels:\ntype: my-frontend-volume\nspec:\naccessModes: [ \"ReadWriteOnce\" ]\nstorageClassName: \"scratch-storage-class\"\nresources:\nrequests:\nstorage: 1Gi"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0350", "text": "Lifecycle and PersistentVolumeClaim\nThe key design idea is that the parameters for a volume claim are allowed inside a volume source of the Pod. Labels, annotations\nand the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets created, the ephemeral volume\ncontroller then creates an actual PersistentVolumeClaim object in the same namespace as the Pod and ensures that the\nPersistentVolumeClaim gets deleted when the Pod gets deleted.\nThat triggers volume binding and/or provisioning, either immediately if the StorageClass uses immediate volume binding or when\nthe Pod is tentatively scheduled onto a node ( WaitForFirstConsumer volume binding mode). The latter is recommended for generic\nephemeral volumes because then the scheduler is free to choose a suitable node for the Pod. With immediate binding, the\nscheduler is forced to select a node that has access to the volume once it is available.\nIn terms of resource ownership, a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s) that provide\nthat ephemeral storage. When the Pod is deleted, the Kubernetes garbage collector deletes the PVC, which then usually triggers\ndeletion of the volume because the default reclaim policy of storage classes is to delete volumes. You can create quasi-ephemeral\nlocal storage using a StorageClass with a reclaim policy of retain : the storage outlives the Pod, and in this case you need to ensure\nthat volume clean up happens separately.\nWhile these PVCs exist, they can be used like any other PVC. In particular, they can be referenced as data source in volume cloning or\nsnapshotting. The PVC object also holds the current status of the volume."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0351", "text": "PersistentVolumeClaim naming\nNaming of the automatically created PVCs is deterministic: the name is a combination of the Pod name and volume name, with a\nhyphen ( - ) in the middle. In the example above, the PVC name will be my-app-scratch-volume . This deterministic naming makes it\neasier to interact with the PVC because one does not have to search for it once the Pod name and volume name are known.\nThe deterministic naming also introduces a potential conflict between different Pods (a Pod \"pod-a\" with volume \"scratch\" and\nanother Pod with name \"pod\" and volume \"a-scratch\" both end up with the same PVC name \"pod-a-scratch\") and between Pods and\nmanually created PVCs.\nSuch conflicts are detected: a PVC is only used for an ephemeral volume if it was created for the Pod. This check is based on the\nownership relationship. An existing PVC is not overwritten or modified. But this does not resolve the conflict because without the\nright PVC, the Pod cannot start.\nCaution:\nhttps://kubernetes.io/docs/concepts/_print/\n\n339/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTake care when naming Pods and volumes inside the same namespace, so that these conflicts can't occur.\n\nSecurity\nUsing generic ephemeral volumes allows users to create PVCs indirectly if they can create Pods, even if they do not have permission\nto create PVCs directly. Cluster administrators must be aware of this. If this does not fit their security model, they should use an\nadmission webhook that rejects objects like Pods that have a generic ephemeral volume.\nThe normal namespace quota for PVCs still applies, so even if users are allowed to use this new mechanism, they cannot use it to\ncircumvent other policies.\n\nWhat's next\nEphemeral volumes managed by kubelet\nSee local ephemeral storage.\n\nCSI ephemeral volumes\nFor more information on the design, see the Ephemeral Inline CSI volumes KEP.\nFor more information on further development of this feature, see the enhancement tracking issue #596.\n\nGeneric ephemeral volumes\nFor more information on the design, see the Generic ephemeral inline volumes KEP.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n340/684\n\n11/7/25, 4:37 PM\n\n6.5 - Storage Classes\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0352", "text": "This document describes the concept of a StorageClass in Kubernetes. Familiarity with volumes and persistent volumes is suggested.\nA StorageClass provides a way for administrators to describe the classes of storage they offer. Different classes might map to qualityof-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is\nunopinionated about what classes represent.\nThe Kubernetes concept of a storage class is similar to â€œprofilesâ€ in some other storage system designs.\n\nStorageClass objects\nEach StorageClass contains the fields provisioner , parameters , and reclaimPolicy , which are used when a PersistentVolume\nbelonging to the class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).\nThe name of a StorageClass object is significant, and is how users can request a particular class. Administrators set the name and\nother parameters of a class when first creating StorageClass objects.\nAs an administrator, you can specify a default StorageClass that applies to any PVCs that don't request a specific class. For more\ndetails, see the PersistentVolumeClaim concept.\nHere's an example of a StorageClass:\nstorage/storageclass-low-latency.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: low-latency\nannotations:\nstorageclass.kubernetes.io/is-default-class: \"false\"\nprovisioner: csi-driver.example-vendor.example\nreclaimPolicy: Retain # default value is Delete\nallowVolumeExpansion: true\nmountOptions:\n- discard # this might enable UNMAP / TRIM at the block storage layer\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\nguaranteedReadWriteLatency: \"true\" # provider-specific\n\nDefault StorageClass\nYou can mark a StorageClass as the default for your cluster. For instructions on setting the default StorageClass, see Change the\ndefault StorageClass.\nWhen a PVC does not specify a storageClassName , the default StorageClass is used.\nIf you set the storageclass.kubernetes.io/is-default-class annotation to true on more than one StorageClass in your cluster,\nand you then create a PersistentVolumeClaim with no storageClassName set, Kubernetes uses the most recently created default\nStorageClass.\nNote:\nYou should try to only have one StorageClass in your cluster that is marked as the default. The reason that Kubernetes allows\nyou to have multiple default StorageClasses is to allow for seamless migration.\nYou can create a PersistentVolumeClaim without specifying a storageClassName for the new PVC, and you can do so even when no\ndefault StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the storageClassName of that PVC\nremains unset until a default becomes available.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n341/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0353", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can have a cluster without any default StorageClass. If you don't mark any StorageClass as default (and one hasn't been set for\nyou by, for example, a cloud provider), then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need it.\nIf or when a default StorageClass becomes available, the control plane identifies any existing PVCs without storageClassName . For\nthe PVCs that either have an empty value for storageClassName or do not have this key, the control plane then updates those PVCs\nto set storageClassName to match the new default StorageClass. If you have an existing PVC where the storageClassName is \"\" ,\nand you configure a default StorageClass, then this PVC will not get updated.\nIn order to keep binding to PVs with storageClassName set to \"\" (while a default StorageClass is present), you need to set the\nstorageClassName of the associated PVC to \"\" .\n\nProvisioner\nEach StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.\nVolume Plugin\n\nInternal Provisioner\n\nConfig Example\n\nAzureFile\n\nâœ“\n\nAzure File\n\nCephFS\n\n-\n\n-\n\nFC\n\n-\n\n-\n\nFlexVolume\n\n-\n\n-\n\niSCSI\n\n-\n\n-\n\nLocal\n\n-\n\nLocal\n\nNFS\n\n-\n\nNFS\n\nPortworxVolume\n\nâœ“\n\nPortworx Volume\n\nRBD\n\n-\n\nCeph RBD\n\nVsphereVolume\n\nâœ“\n\nvSphere\n\nYou are not restricted to specifying the \"internal\" provisioners listed here (whose names are prefixed with \"kubernetes.io\" and\nshipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a\nspecification defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the\nprovisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository kubernetes-sigs/sigstorage-lib-external-provisioner houses a library for writing external provisioners that implements the bulk of the specification.\nSome external provisioners are listed under the repository kubernetes-sigs/sig-storage-lib-external-provisioner.\nFor example, NFS doesn't provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd\nparty storage vendors provide their own external provisioner."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0354", "text": "Reclaim policy\nPersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of\nthe class, which can be either Delete or Retain . If no reclaimPolicy is specified when a StorageClass object is created, it will\ndefault to Delete .\nPersistentVolumes that are created manually and managed via a StorageClass will have whatever reclaim policy they were assigned\nat creation.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n342/684\n\n11/7/25, 4:37 PM\n\nVolume expansion\n\nConcepts | Kubernetes\n\nPersistentVolumes can be configured to be expandable. This allows you to resize the volume by editing the corresponding PVC\nobject, requesting a new larger amount of storage.\nThe following types of volumes support volume expansion, when the underlying StorageClass has the field allowVolumeExpansion\nset to true.\nVolume type\n\nRequired Kubernetes version for volume expansion\n\nAzure File\n\n1.11\n\nCSI\n\n1.24\n\nFlexVolume\n\n1.13\n\nPortworx\n\n1.11\n\nrbd\n\n1.11\n\nNote:\nYou can only use the volume expansion feature to grow a Volume, not to shrink it.\n\nMount options\nPersistentVolumes that are dynamically created by a StorageClass will have the mount options specified in the mountOptions field of\nthe class.\nIf the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are not\nvalidated on either the class or PV. If a mount option is invalid, the PV mount fails."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0355", "text": "Volume binding mode\nThe volumeBindingMode field controls when volume binding and dynamic provisioning should occur. When unset, Immediate mode\nis used by default.\nThe Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created.\nFor storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will\nbe bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods.\nA cluster administrator can address this issue by specifying the WaitForFirstConsumer mode which will delay the binding and\nprovisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or\nprovisioned conforming to the topology that is specified by the Pod's scheduling constraints. These include, but are not limited to,\nresource requirements, node selectors, pod affinity and anti-affinity, and taints and tolerations.\nThe following plugins support WaitForFirstConsumer with dynamic provisioning:\nCSI volumes, provided that the specific CSI driver supports this\nThe following plugins support WaitForFirstConsumer with pre-created PersistentVolume binding:\nCSI volumes, provided that the specific CSI driver supports this\nlocal\n\nNote:\nIf you choose to use WaitForFirstConsumer , do not use nodeName in the Pod spec to specify node affinity. If nodeName is used\nin this case, the scheduler will be bypassed and PVC will remain in pending state.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n343/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nInstead, you can use node selector for kubernetes.io/hostname :\n\nstorage/storageclass/pod-volume-binding.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: task-pv-pod\nspec:\nnodeSelector:\nkubernetes.io/hostname: kube-01\nvolumes:\n- name: task-pv-storage\npersistentVolumeClaim:\nclaimName: task-pv-claim\ncontainers:\n- name: task-pv-container\nimage: nginx\nports:\n- containerPort: 80\nname: \"http-server\"\nvolumeMounts:\n- mountPath: \"/usr/share/nginx/html\"\nname: task-pv-storage\n\nAllowed topologies\nWhen a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessary to restrict provisioning\nto specific topologies in most situations. However, if still required, allowedTopologies can be specified.\nThis example demonstrates how to restrict the topology of provisioned volumes to specific zones and should be used as a\nreplacement for the zone and zones parameters for the supported plugins.\nstorage/storageclass/storageclass-topology.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: standard\nprovisioner:\n\nexample.com/example"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0356", "text": "parameters:\ntype: pd-standard\nvolumeBindingMode: WaitForFirstConsumer\nallowedTopologies:\n- matchLabelExpressions:\n- key: topology.kubernetes.io/zone\nvalues:\n- us-central-1a\n- us-central-1b\n\nParameters\nStorageClasses have parameters that describe volumes belonging to the storage class. Different parameters may be accepted\ndepending on the provisioner . When a parameter is omitted, some default is used.\nThere can be at most 512 parameters defined for a StorageClass. The total length of the parameters object including its keys and\nvalues cannot exceed 256 KiB.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n344/684\n\n11/7/25, 4:37 PM\n\nAWS EBS\n\nConcepts | Kubernetes\n\nKubernetes 1.34 does not include a awsElasticBlockStore volume type.\nThe AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the\nv1.27 release.\nThe Kubernetes project suggests that you use the AWS EBS out-of-tree storage driver instead.\nHere is an example StorageClass for the AWS EBS CSI driver:\nstorage/storageclass/storageclass-aws-ebs.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: ebs-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\ncsi.storage.k8s.io/fstype: xfs\ntype: io1\niopsPerGB: \"50\"\nencrypted: \"true\"\ntagSpecification_1: \"key1=value1\"\ntagSpecification_2: \"key2=value2\"\nallowedTopologies:\n- matchLabelExpressions:\n- key: topology.ebs.csi.aws.com/zone\nvalues:\n- us-east-2c\n\ntagSpecification : Tags with this prefix are applied to dynamically provisioned EBS volumes.\n\nAWS EFS\nTo configure AWS EFS storage, you can use the out-of-tree AWS_EFS_CSI_DRIVER.\nstorage/storageclass/storageclass-aws-efs.yaml\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\nname: efs-sc\nprovisioner: efs.csi.aws.com\nparameters:\nprovisioningMode: efs-ap\nfileSystemId: fs-92107410\ndirectoryPerms: \"700\"\n\nprovisioningMode : The type of volume to be provisioned by Amazon EFS. Currently, only access point based provisioning is\n\nsupported ( efs-ap ).\nfileSystemId : The file system under which the access point is created.\ndirectoryPerms : The directory permissions of the root directory created by the access point.\n\nFor more details, refer to the AWS_EFS_CSI_Driver Dynamic Provisioning documentation.\n\nNFS\nTo configure NFS storage, you can use the in-tree driver or the NFS CSI driver for Kubernetes (recommended).\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n345/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nstorage/storageclass/storageclass-nfs.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: example-nfs\nprovisioner: example.com/external-nfs\nparameters:\nserver: nfs-server.example.com\npath: /share\nreadOnly: \"false\""}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0357", "text": "server : Server is the hostname or IP address of the NFS server.\npath : Path that is exported by the NFS server.\nreadOnly : A flag indicating whether the storage will be mounted as read only (default false).\n\nKubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS.\nHere are some examples:\nNFS Ganesha server and external provisioner\nNFS subdir external provisioner\n\nvSphere\nThere are two types of provisioners for vSphere storage classes:\nCSI provisioner: csi.vsphere.vmware.com\nvCP provisioner: kubernetes.io/vsphere-volume\nIn-tree provisioners are deprecated. For more information on the CSI provisioner, see Kubernetes vSphere CSI Driver and\nvSphereVolume CSI migration.\n\nCSI Provisioner\nThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the vSphere CSI\nrepository.\n\nvCP Provisioner\nThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.\n1. Create a StorageClass with a user specified disk format.\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: fast\nprovisioner: kubernetes.io/vsphere-volume\nparameters:\ndiskformat: zeroedthick\n\ndiskformat : thin , zeroedthick\n\nand eagerzeroedthick . Default: \"thin\" .\n\n2. Create a StorageClass with a disk format on a user specified datastore.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n346/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: fast\nprovisioner: kubernetes.io/vsphere-volume\nparameters:\ndiskformat: zeroedthick\ndatastore: VSANDatastore\n\ndatastore : The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0358", "text": "in the StorageClass, which in this case is VSANDatastore . This field is optional. If the datastore is not specified, then the volume\nwill be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.\n3. Storage Policy Management inside kubernetes\nUsing existing vCenter SPBM policy\nOne of the most important features of vSphere for Storage Management is policy based Management. Storage Policy\nBased Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad\nrange of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage\nprovisioning challenges, such as capacity planning, differentiated service levels and managing capacity headroom.\nThe SPBM policies can be specified in the StorageClass using the storagePolicyName parameter.\nVirtual SAN policy support inside Kubernetes\nVsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual SAN Storage Capabilities during dynamic\nvolume provisioning. You can now define storage requirements, such as performance and availability, in the form of\nstorage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual\nSAN policy which are then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is being created.\nThe virtual disk is distributed across the Virtual SAN datastore to meet the requirements.\nYou can see Storage Policy Based Management for dynamic provisioning of volumes for more details on how to use\nstorage policies for persistent volumes management.\nThere are few vSphere examples which you try out for persistent volume management inside Kubernetes for vSphere.\n\nCeph RBD (deprecated)\nNote:\nâ“˜ FEATURE STATE: Kubernetes v1.28 [deprecated]\n\nThis internal provisioner of Ceph RBD is deprecated. Please use CephFS RBD CSI driver.\n\nstorage/storageclass/storageclass-ceph-rbd.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n347/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: fast\nprovisioner: kubernetes.io/rbd # This provisioner is deprecated\nparameters:\nmonitors: 198.19.254.105:6789\nadminId: kube\nadminSecretName: ceph-secret\nadminSecretNamespace: kube-system\npool: kube\nuserId: kube\nuserSecretName: ceph-secret-user\nuserSecretNamespace: default\nfsType: ext4\nimageFormat: \"2\"\nimageFeatures: \"layering\"\n\nmonitors : Ceph monitors, comma delimited. This parameter is required.\nadminId : Ceph client ID that is capable of creating images in the pool. Default is \"admin\".\nadminSecretName : Secret Name for adminId . This parameter is required. The provided secret must have type"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0359", "text": "\"kubernetes.io/rbd\".\nadminSecretNamespace : The namespace for adminSecretName . Default is \"default\".\npool : Ceph RBD pool. Default is \"rbd\".\nuserId : Ceph client ID that is used to map the RBD image. Default is the same as adminId .\nuserSecretName : The name of Ceph Secret for userId\n\nto map RBD image. It must exist in the same namespace as PVCs. This\nparameter is required. The provided secret must have type \"kubernetes.io/rbd\", for example created in this way:\n\nkubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" \\\n--from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \\\n--namespace=kube-system\n\nuserSecretNamespace : The namespace for userSecretName .\nfsType : fsType that is supported by kubernetes. Default: \"ext4\" .\nimageFormat : Ceph RBD image format, \"1\" or \"2\". Default is \"2\".\nimageFeatures : This parameter is optional and should only be used if you set imageFormat\n\nto \"2\". Currently supported\n\nfeatures are layering only. Default is \"\", and no features are turned on.\n\nAzure Disk\nKubernetes 1.34 does not include a azureDisk volume type.\nThe azureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27\nrelease.\nThe Kubernetes project suggests that you use the Azure Disk third party storage driver instead.\n\nAzure File (deprecated)\nstorage/storageclass/storageclass-azure-file.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n348/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: azurefile\nprovisioner: kubernetes.io/azure-file\nparameters:\nskuName: Standard_LRS\nlocation: eastus\nstorageAccount: azure_storage_account_name # example value\n\nskuName : Azure storage account SKU tier. Default is empty.\nlocation : Azure storage account location. Default is empty.\nstorageAccount : Azure storage account name. Default is empty. If a storage account is not provided, all storage accounts\n\nassociated with the resource group are searched to find one that matches skuName and location . If a storage account is\nprovided, it must reside in the same resource group as the cluster, and skuName and location are ignored.\nsecretNamespace : the namespace of the secret that contains the Azure Storage Account Name and Key. Default is the same as\n\nthe Pod.\nsecretName : the name of the secret that contains the Azure Storage Account Name and Key. Default is azure-storageaccount-<accountName>-secret\nreadOnly : a flag indicating whether the storage will be mounted as read only. Defaults to false which means a read/write"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0360", "text": "mount. This setting will impact the ReadOnly setting in VolumeMounts as well.\nDuring storage provisioning, a secret named by secretName is created for the mounting credentials. If the cluster has enabled both\nRBAC and Controller Roles, add the create permission of resource secret for clusterrole system:controller:persistent-volumebinder .\nIn a multi-tenancy context, it is strongly recommended to set the value for secretNamespace explicitly, otherwise the storage\naccount credentials may be read by other users.\n\nPortworx volume (deprecated)\nstorage/storageclass/storageclass-portworx-volume.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: portworx-io-priority-high\nprovisioner: kubernetes.io/portworx-volume # This provisioner is deprecated\nparameters:\nrepl: \"1\"\nsnap_interval: \"70\"\npriority_io: \"high\"\n\nfs : filesystem to be laid out: none/xfs/ext4\n\n(default: ext4 ).\n\nblock_size : block size in Kbytes (default: 32 ).\nrepl : number of synchronous replicas to be provided in the form of replication factor 1..3\n\n(default: 1 ) A string is expected\n\nhere i.e. \"1\" and not 1 .\npriority_io : determines whether the volume will be created from higher performance or a lower priority storage\nhigh/medium/low\n\n(default: low ).\n\nsnap_interval : clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on difference\n\nwith the prior snapshot, 0 disables snaps (default: 0 ). A string is expected here i.e. \"70\" and not 70 .\naggregation_level : specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated\n\nvolume (default: 0 ). A string is expected here i.e. \"0\" and not 0\nephemeral : specifies whether the volume should be cleaned-up after unmount or should be persistent. emptyDir\n\nuse case\ncan set this value to true and persistent volumes use case such as for databases like Cassandra should set to false,\ntrue/false (default false ). A string is expected here i.e. \"true\" and not true .\nhttps://kubernetes.io/docs/concepts/_print/\n\n349/684\n\n11/7/25, 4:37 PM\n\nLocal\n\nConcepts | Kubernetes\n\nstorage/storageclass/storageclass-local.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: local-storage\nprovisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisionin\nvolumeBindingMode: WaitForFirstConsumer\n\nLocal volumes do not support dynamic provisioning in Kubernetes 1.34; however a StorageClass should still be created to delay\nvolume binding until a Pod is actually scheduled to the appropriate node. This is specified by the WaitForFirstConsumer volume\nbinding mode.\nDelaying volume binding allows the scheduler to consider all of a Pod's scheduling constraints when choosing an appropriate\nPersistentVolume for a PersistentVolumeClaim."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0361", "text": "https://kubernetes.io/docs/concepts/_print/\n\n350/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n6.6 - Volume Attributes Classes\n\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nThis page assumes that you are familiar with StorageClasses, volumes and PersistentVolumes in Kubernetes.\nA VolumeAttributesClass provides a way for administrators to describe the mutable \"classes\" of storage they offer. Different classes\nmight map to different quality-of-service levels. Kubernetes itself is un-opinionated about what these classes represent.\nThis feature is generally available (GA) as of version 1.34, and users have the option to disable it.\nYou can also only use VolumeAttributesClasses with storage backed by Container Storage Interface, and only where the relevant CSI\ndriver implements the ModifyVolume API.\n\nThe VolumeAttributesClass API\nEach VolumeAttributesClass contains the driverName and parameters , which are used when a PersistentVolume (PV) belonging to\nthe class needs to be dynamically provisioned or modified.\nThe name of a VolumeAttributesClass object is significant and is how users can request a particular class. Administrators set the\nname and other parameters of a class when first creating VolumeAttributesClass objects. While the name of a VolumeAttributesClass\nobject in a PersistentVolumeClaim is mutable, the parameters in an existing class are immutable.\n\napiVersion: storage.k8s.io/v1\nkind: VolumeAttributesClass\nmetadata:\nname: silver\ndriverName: pd.csi.storage.gke.io\nparameters:\nprovisioned-iops: \"3000\"\nprovisioned-throughput: \"50\"\n\nProvisioner\nEach VolumeAttributesClass has a provisioner that determines what volume plugin is used for provisioning PVs. The field\ndriverName must be specified.\nThe feature support for VolumeAttributesClass is implemented in kubernetes-csi/external-provisioner.\nYou are not restricted to specifying the kubernetes-csi/external-provisioner. You can also run and specify external provisioners,\nwhich are independent programs that follow a specification defined by Kubernetes. Authors of external provisioners have full\ndiscretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses, etc.\nTo understand how the provisioner works with VolumeAttributesClass, refer to the CSI external-provisioner documentation.\n\nResizer\nEach VolumeAttributesClass has a resizer that determines what volume plugin is used for modifying PVs. The field driverName must\nbe specified.\nThe modifying volume feature support for VolumeAttributesClass is implemented in kubernetes-csi/external-resizer.\nFor example, an existing PersistentVolumeClaim is using a VolumeAttributesClass named silver:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n351/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0362", "text": "apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: test-pv-claim\nspec:\nâ€¦\nvolumeAttributesClassName: silver\nâ€¦\n\nA new VolumeAttributesClass gold is available in the cluster:\n\napiVersion: storage.k8s.io/v1\nkind: VolumeAttributesClass\nmetadata:\nname: gold\ndriverName: pd.csi.storage.gke.io\nparameters:\niops: \"4000\"\nthroughput: \"60\"\n\nThe end user can update the PVC with the new VolumeAttributesClass gold and apply:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: test-pv-claim\nspec:\nâ€¦\nvolumeAttributesClassName: gold\nâ€¦\n\nTo understand how the resizer works with VolumeAttributesClass, refer to the CSI external-resizer documentation.\n\nParameters\nVolumeAttributeClasses have parameters that describe volumes belonging to them. Different parameters may be accepted\ndepending on the provisioner or the resizer. For example, the value 4000 , for the parameter iops , and the parameter throughput\nare specific to GCE PD. When a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a\ndifferent VolumeAttributesClass with omitted parameters, the default value of the parameters may be used depending on the CSI\ndriver implementation. Please refer to the related CSI driver documentation for more details.\nThere can be at most 512 parameters defined for a VolumeAttributesClass. The total length of the parameters object including its\nkeys and values cannot exceed 256 KiB.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n352/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n6.7 - Dynamic Volume Provisioning\nDynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster\nadministrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create\nPersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster\nadministrators to pre-provision storage. Instead, it automatically provisions storage when users create PersistentVolumeClaim\nobjects."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0363", "text": "Background\nThe implementation of dynamic volume provisioning is based on the API object StorageClass from the API group storage.k8s.io .\nA cluster administrator can define as many StorageClass objects as needed, each specifying a volume plugin (aka provisioner) that\nprovisions a volume and the set of parameters to pass to that provisioner when provisioning. A cluster administrator can define and\nexpose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of\nparameters. This design also ensures that end users don't have to worry about the complexity and nuances of how storage is\nprovisioned, but still have the ability to select from multiple storage options.\nFor more details, see the Storage Classes concept.\n\nEnabling Dynamic Provisioning\nTo enable dynamic provisioning, a cluster administrator needs to pre-create one or more StorageClass objects for users.\nStorageClass objects define which provisioner should be used and what parameters should be passed to that provisioner when\ndynamic provisioning is invoked. The name of a StorageClass object must be a valid DNS subdomain name.\nThe following manifest creates a storage class \"slow\" which provisions standard disk-like persistent disks.\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: slow\nprovisioner: kubernetes.io/gce-pd\nparameters:\ntype: pd-standard\n\nThe following manifest creates a storage class \"fast\" which provisions SSD-like persistent disks.\n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: fast\nprovisioner: kubernetes.io/gce-pd\nparameters:\ntype: pd-ssd\n\nUsing Dynamic Provisioning\nUsers request dynamically provisioned storage by including a storage class in their PersistentVolumeClaim . Before Kubernetes v1.6,\nthis was done via the volume.beta.kubernetes.io/storage-class annotation. However, this annotation is deprecated since v1.9.\nUsers now can and should instead use the storageClassName field of the PersistentVolumeClaim object. The value of this field\nmust match the name of a StorageClass configured by the administrator (see Enabling Dynamic Provisioning).\nTo select the \"fast\" storage class, for example, a user would create the following PersistentVolumeClaim:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n353/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: claim1\nspec:\naccessModes:\n- ReadWriteOnce\nstorageClassName: fast\nresources:\nrequests:\nstorage: 30Gi\n\nThis claim results in an SSD-like Persistent Disk being automatically provisioned. When the claim is deleted, the volume is destroyed."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0364", "text": "Defaulting Behavior\nDynamic provisioning can be enabled on a cluster such that all claims are dynamically provisioned if no storage class is specified. A\ncluster administrator can enable this behavior by:\nMarking one StorageClass object as default.\nMaking sure that the DefaultStorageClass admission controller is enabled on the API server.\nAn administrator can mark a specific StorageClass as default by adding the storageclass.kubernetes.io/is-default-class\nannotation to it. When a default StorageClass exists in a cluster and a user creates a PersistentVolumeClaim with\nstorageClassName unspecified, the DefaultStorageClass admission controller automatically adds the storageClassName field\npointing to the default storage class.\nNote that if you set the storageclass.kubernetes.io/is-default-class annotation to true on more than one StorageClass in your\ncluster, and you then create a PersistentVolumeClaim with no storageClassName set, Kubernetes uses the most recently created\ndefault StorageClass.\n\nTopology Awareness\nIn Multi-Zone clusters, Pods can be spread across Zones in a Region. Single-Zone storage backends should be provisioned in the\nZones where Pods are scheduled. This can be accomplished by setting the Volume Binding Mode.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n354/684\n\n11/7/25, 4:37 PM\n\n6.8 - Volume Snapshots\n\nConcepts | Kubernetes\n\nIn Kubernetes, a VolumeSnapshot represents a snapshot of a volume on a storage system. This document assumes that you are\nalready familiar with Kubernetes persistent volumes.\n\nIntroduction\nSimilar to how API resources PersistentVolume and PersistentVolumeClaim are used to provision volumes for users and\nadministrators, VolumeSnapshotContent and VolumeSnapshot API resources are provided to create volume snapshots for users and\nadministrators.\nA VolumeSnapshotContent is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a\nresource in the cluster just like a PersistentVolume is a cluster resource.\nA VolumeSnapshot is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.\nallows you to specify different attributes belonging to a VolumeSnapshot . These attributes may differ among\nsnapshots taken from the same volume on the storage system and therefore cannot be expressed by using the same StorageClass\nof a PersistentVolumeClaim .\nVolumeSnapshotClass"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0365", "text": "Volume snapshots provide Kubernetes users with a standardized way to copy a volume's contents at a particular point in time\nwithout creating an entirely new volume. This functionality enables, for example, database administrators to backup databases\nbefore performing edit or delete modifications.\nUsers need to be aware of the following when using this feature:\nAPI Objects VolumeSnapshot , VolumeSnapshotContent , and VolumeSnapshotClass are CRDs, not part of the core API.\nVolumeSnapshot\n\nsupport is only available for CSI drivers.\n\nAs part of the deployment process of VolumeSnapshot , the Kubernetes team provides a snapshot controller to be deployed\ninto the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver. The\nsnapshot controller watches VolumeSnapshot and VolumeSnapshotContent objects and is responsible for the creation and\ndeletion of VolumeSnapshotContent object. The sidecar csi-snapshotter watches VolumeSnapshotContent objects and triggers\nCreateSnapshot and DeleteSnapshot operations against a CSI endpoint.\nThere is also a validating webhook server which provides tightened validation on snapshot objects. This should be installed by\nthe Kubernetes distros along with the snapshot controller and CRDs, not CSI drivers. It should be installed in all Kubernetes\nclusters that has the snapshot feature enabled.\nCSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support\nfor volume snapshot will likely use the csi-snapshotter. See CSI Driver documentation for details.\nThe CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.\nFor advanced use cases, such as creating group snapshots of multiple volumes, see the external CSI Volume Group Snapshot\ndocumentation.\n\nLifecycle of a volume snapshot and volume snapshot content\nare resources in the cluster. VolumeSnapshots are requests for those resources. The interaction between\nVolumeSnapshotContents and VolumeSnapshots follow this lifecycle:\nVolumeSnapshotContents\n\nProvisioning Volume Snapshot\nThere are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.\n\nPre-provisioned\nA cluster administrator creates a number of VolumeSnapshotContents . They carry the details of the real volume snapshot on the\nstorage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.\n\nDynamic\nhttps://kubernetes.io/docs/concepts/_print/\n\n355/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0366", "text": "Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim.\nThe VolumeSnapshotClass specifies storage provider-specific parameters to use when taking a snapshot.\n\nBinding\nThe snapshot controller handles the binding of a VolumeSnapshot object with an appropriate VolumeSnapshotContent object, in\nboth pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.\nIn the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent\nobject is created.\n\nPersistent Volume Claim as Snapshot Source Protection\nThe purpose of this protection is to ensure that in-use PersistentVolumeClaim API objects are not removed from the system while a\nsnapshot is being taken from it (as this may result in data loss).\nWhile a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a\nPersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately.\nInstead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.\n\nDelete\nDeletion is triggered by deleting the VolumeSnapshot object, and the DeletionPolicy will be followed. If the DeletionPolicy is\nDelete , then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the DeletionPolicy\nis Retain , then both the underlying snapshot and VolumeSnapshotContent remain.\n\nVolumeSnapshots\nEach VolumeSnapshot contains a spec and a status.\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\nname: new-snapshot-test\nspec:\nvolumeSnapshotClassName: csi-hostpath-snapclass\nsource:\npersistentVolumeClaimName: pvc-test\n\nis the name of the PersistentVolumeClaim data source for the snapshot. This field is required for\ndynamically provisioning a snapshot.\npersistentVolumeClaimName\n\nA volume snapshot can request a particular class by specifying the name of a VolumeSnapshotClass using the attribute\nvolumeSnapshotClassName . If nothing is set, then the default class is used if available.\nFor pre-provisioned snapshots, you need to specify a volumeSnapshotContentName as the source for the snapshot as shown in the\nfollowing example. The volumeSnapshotContentName source field is required for pre-provisioned snapshots.\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshot\nmetadata:\nname: test-snapshot\nspec:\nsource:\nvolumeSnapshotContentName: test-content\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n356/684\n\n11/7/25, 4:37 PM\n\nVolume Snapshot Contents\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0367", "text": "Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates\nVolumeSnapshotContent objects. Here is an example:\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\nname: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455\nspec:\ndeletionPolicy: Delete\ndriver: hostpath.csi.k8s.io\nsource:\nvolumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002\nsourceVolumeMode: Filesystem\nvolumeSnapshotClassName: csi-hostpath-snapclass\nvolumeSnapshotRef:\nname: new-snapshot-test\nnamespace: default\nuid: 72d9a349-aacd-42d2-a240-d775650d2455\n\nis the unique identifier of the volume created on the storage backend and returned by the CSI driver during the\nvolume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.\nvolumeHandle\n\nFor pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the VolumeSnapshotContent object as\nfollows.\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\nname: new-snapshot-content-test\nspec:\ndeletionPolicy: Delete\ndriver: hostpath.csi.k8s.io\nsource:\nsnapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\nsourceVolumeMode: Filesystem\nvolumeSnapshotRef:\nname: new-snapshot-test\nnamespace: default\n\nis the unique identifier of the volume snapshot created on the storage backend. This field is required for the preprovisioned snapshots. It specifies the CSI snapshot id on the storage system that this VolumeSnapshotContent represents.\nsnapshotHandle\n\nis the mode of the volume whose snapshot is taken. The value of the sourceVolumeMode field can be either\nFilesystem or Block . If the source volume mode is not specified, Kubernetes treats the snapshot as if the source volume's mode is\nunknown.\nsourceVolumeMode\n\nis the reference of the corresponding VolumeSnapshot . Note that when the VolumeSnapshotContent is being\ncreated as a pre-provisioned snapshot, the VolumeSnapshot referenced in volumeSnapshotRef might not exist yet.\nvolumeSnapshotRef\n\nConverting the volume mode of a Snapshot\nIf the VolumeSnapshots API installed on your cluster supports the sourceVolumeMode field, then the API has the capability to prevent\nunauthorized users from converting the mode of a volume.\nTo check if your cluster has capability for this feature, run the following command:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n357/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n$ kubectl get crd volumesnapshotcontent -o yaml"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0368", "text": "If you want to allow users to create a PersistentVolumeClaim from an existing VolumeSnapshot , but with a different volume mode\nthan the source, the annotation snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\" needs to be added to the\nVolumeSnapshotContent that corresponds to the VolumeSnapshot .\nFor pre-provisioned snapshots, spec.sourceVolumeMode needs to be populated by the cluster administrator.\nAn example VolumeSnapshotContent resource with this feature enabled would look like:\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotContent\nmetadata:\nname: new-snapshot-content-test\nannotations:\n- snapshot.storage.kubernetes.io/allow-volume-mode-change: \"true\"\nspec:\ndeletionPolicy: Delete\ndriver: hostpath.csi.k8s.io\nsource:\nsnapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002\nsourceVolumeMode: Filesystem\nvolumeSnapshotRef:\nname: new-snapshot-test\nnamespace: default\n\nProvisioning Volumes from Snapshots\nYou can provision a new volume, pre-populated with data from a snapshot, by using the dataSource field in the\nPersistentVolumeClaim object.\nFor more details, see Volume Snapshot and Restore Volume from Snapshot.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n358/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n6.9 - Volume Snapshot Classes\n\nThis document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity with volume snapshots and storage classes\nis suggested.\n\nIntroduction\nJust like StorageClass provides a way for administrators to describe the \"classes\" of storage they offer when provisioning a volume,\nVolumeSnapshotClass provides a way to describe the \"classes\" of storage when provisioning a volume snapshot.\n\nThe VolumeSnapshotClass Resource\nEach VolumeSnapshotClass contains the fields driver , deletionPolicy , and parameters , which are used when a VolumeSnapshot\nbelonging to the class needs to be dynamically provisioned.\nThe name of a VolumeSnapshotClass object is significant, and is how users can request a particular class. Administrators set the\nname and other parameters of a class when first creating VolumeSnapshotClass objects, and the objects cannot be updated once\nthey are created.\nNote:\nInstallation of the CRDs is the responsibility of the Kubernetes distribution. Without the required CRDs present, the creation of\na VolumeSnapshotClass fails.\n\napiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\nname: csi-hostpath-snapclass\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n\nAdministrators can specify a default VolumeSnapshotClass for VolumeSnapshots that don't request any particular class to bind to by\nadding the snapshot.storage.kubernetes.io/is-default-class: \"true\" annotation:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0369", "text": "apiVersion: snapshot.storage.k8s.io/v1\nkind: VolumeSnapshotClass\nmetadata:\nname: csi-hostpath-snapclass\nannotations:\nsnapshot.storage.kubernetes.io/is-default-class: \"true\"\ndriver: hostpath.csi.k8s.io\ndeletionPolicy: Delete\nparameters:\n\nIf multiple CSI drivers exist, a default VolumeSnapshotClass can be specified for each of them.\n\nVolumeSnapshotClass dependencies\nWhen you create a VolumeSnapshot without specifying a VolumeSnapshotClass, Kubernetes automatically selects a default\nVolumeSnapshotClass that has a CSI driver matching the CSI driver of the PVCâ€™s StorageClass.\nThis behavior allows multiple default VolumeSnapshotClass objects to coexist in a cluster, as long as each one is associated with a\nunique CSI driver.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n359/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAlways ensure that there is only one default VolumeSnapshotClass for each CSI driver. If multiple default VolumeSnapshotClass\nobjects are created using the same CSI driver, a VolumeSnapshot creation will fail because Kubernetes cannot determine which one\nto use.\n\nDriver\nVolume snapshot classes have a driver that determines what CSI volume plugin is used for provisioning VolumeSnapshots. This field\nmust be specified.\n\nDeletionPolicy\nVolume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the\nVolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can either be Retain or\nDelete . This field must be specified.\nIf the deletionPolicy is Delete , then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object.\nIf the deletionPolicy is Retain , then both the underlying snapshot and VolumeSnapshotContent remain.\n\nParameters\nVolume snapshot classes have parameters that describe volume snapshots belonging to the volume snapshot class. Different\nparameters may be accepted depending on the driver .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n360/684\n\n11/7/25, 4:37 PM\n\n6.10 - CSI Volume Cloning\n\nConcepts | Kubernetes\n\nThis document describes the concept of cloning existing CSI Volumes in Kubernetes. Familiarity with Volumes is suggested."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0370", "text": "Introduction\nThe CSI Volume Cloning feature adds support for specifying existing PVCs in the dataSource field to indicate a user would like to\nclone a Volume.\nA Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be. The\nonly difference is that upon provisioning, rather than creating a \"new\" empty Volume, the back end device creates an exact duplicate\nof the specified Volume.\nThe implementation of cloning, from the perspective of the Kubernetes API, adds the ability to specify an existing PVC as a\ndataSource during new PVC creation. The source PVC must be bound and available (not in use).\nUsers need to be aware of the following when using this feature:\nCloning support ( VolumePVCDataSource ) is only available for CSI drivers.\nCloning support is only available for dynamic provisioners.\nCSI drivers may or may not have implemented the volume cloning functionality.\nYou can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the\nsame namespace).\nCloning is supported with a different Storage Class.\nDestination volume can be the same or a different storage class as the source.\nDefault storage class can be used and storageClassName omitted in the spec.\nCloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode\nvolume, the source MUST also be block mode)\n\nProvisioning\nClones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same\nnamespace.\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: clone-of-pvc-1\nnamespace: myns\nspec:\naccessModes:\n- ReadWriteOnce\nstorageClassName: cloning\nresources:\nrequests:\nstorage: 5Gi\ndataSource:\nkind: PersistentVolumeClaim\nname: pvc-1\n\nNote:\nYou must specify a capacity value for spec.resources.requests.storage, and the value you specify must be the same or larger\nthan the capacity of the source volume.\nThe result is a new PVC with the name clone-of-pvc-1 that has the exact same content as the specified source pvc-1 .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n361/684\n\n11/7/25, 4:37 PM\n\nUsage\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0371", "text": "Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC. It's also expected at this point that the newly\ncreated PVC is an independent object. It can be consumed, cloned, snapshotted, or deleted independently and without\nconsideration for it's original dataSource PVC. This also implies that the source is not linked in any way to the newly created clone, it\nmay also be modified or deleted without affecting the newly created clone.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n362/684\n\n11/7/25, 4:37 PM\n\n6.11 - Storage Capacity\n\nConcepts | Kubernetes\n\nStorage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be\naccessible by all nodes, or storage is local to a node to begin with.\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nThis page describes how Kubernetes keeps track of storage capacity and how the scheduler uses that information to schedule Pods\nonto nodes that have access to enough storage capacity for the remaining missing volumes. Without storage capacity tracking, the\nscheduler may choose a node that doesn't have enough capacity to provision a volume and multiple scheduling retries will be\nneeded.\n\nBefore you begin\nKubernetes v1.34 includes cluster-level API support for storage capacity tracking. To use this you must also be using a CSI driver that\nsupports capacity tracking. Consult the documentation for the CSI drivers that you use to find out whether this support is available\nand, if so, how to use it. If you are not running Kubernetes v1.34, check the documentation for that version of Kubernetes.\n\nAPI\nThere are two API extensions for this feature:\nCSIStorageCapacity objects: these get produced by a CSI driver in the namespace where the driver is installed. Each object\ncontains capacity information for one storage class and defines which nodes have access to that storage.\nThe CSIDriverSpec.StorageCapacity field: when set to true , the Kubernetes scheduler will consider storage capacity for\nvolumes that use the CSI driver."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0372", "text": "Scheduling\nStorage capacity information is used by the Kubernetes scheduler if:\na Pod uses a volume that has not been created yet,\nthat volume uses a StorageClass which references a CSI driver and uses WaitForFirstConsumer volume binding mode, and\nthe CSIDriver object for the driver has StorageCapacity set to true.\nIn that case, the scheduler only considers nodes for the Pod which have enough storage available to them. This check is very\nsimplistic and only compares the size of the volume against the capacity listed in CSIStorageCapacity objects with a topology that\nincludes the node.\nFor volumes with Immediate volume binding mode, the storage driver decides where to create the volume, independently of Pods\nthat will use the volume. The scheduler then schedules Pods onto nodes where the volume is available after the volume has been\ncreated.\nFor CSI ephemeral volumes, scheduling always happens without considering storage capacity. This is based on the assumption that\nthis volume type is only used by special CSI drivers which are local to a node and do not need significant resources there.\n\nRescheduling\nWhen a node has been selected for a Pod with WaitForFirstConsumer volumes, that decision is still tentative. The next step is that\nthe CSI storage driver gets asked to create the volume with a hint that the volume is supposed to be available on the selected node.\nBecause Kubernetes might have chosen a node based on out-dated capacity information, it is possible that the volume cannot really\nbe created. The node selection is then reset and the Kubernetes scheduler tries again to find a node for the Pod.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n363/684\n\n11/7/25, 4:37 PM\n\nLimitations\n\nConcepts | Kubernetes\n\nStorage capacity tracking increases the chance that scheduling works on the first try, but cannot guarantee this because the\nscheduler has to decide based on potentially out-dated information. Usually, the same retry mechanism as for scheduling without\nany storage capacity information handles scheduling failures.\nOne situation where scheduling can fail permanently is when a Pod uses multiple volumes: one volume might have been created\nalready in a topology segment which then does not have enough capacity left for another volume. Manual intervention is necessary\nto recover from this, for example by increasing capacity or deleting the volume that was already created.\n\nWhat's next\nFor more information on the design, see the Storage Capacity Constraints for Pod Scheduling KEP.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n364/684\n\n11/7/25, 4:37 PM"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0373", "text": "Concepts | Kubernetes\n\n6.12 - Node-specific Volume Limits\n\nThis page describes the maximum number of volumes that can be attached to a Node for various cloud providers.\nCloud providers like Google, Amazon, and Microsoft typically have a limit on how many volumes can be attached to a Node. It is\nimportant for Kubernetes to respect those limits. Otherwise, Pods scheduled on a Node could get stuck waiting for volumes to\nattach.\n\nKubernetes default limits\nThe Kubernetes scheduler has default limits on the number of volumes that can be attached to a Node:\nCloud service\n\nMaximum volumes per Node\n\nAmazon Elastic Block Store (EBS)\n\n39\n\nGoogle Persistent Disk\n\n16\n\nMicrosoft Azure Disk Storage\n\n16\n\nDynamic volume limits\nâ“˜ FEATURE STATE: Kubernetes v1.17 [stable]\n\nDynamic volume limits are supported for following volume types.\nAmazon EBS\nGoogle Persistent Disk\nAzure Disk\nCSI\nFor volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node type and enforces the appropriate\nmaximum number of volumes for the node. For example:\nOn Google Compute Engine, up to 127 volumes can be attached to a node, depending on the node type.\nFor Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25 volumes to be attached to a Node.\nFor other instance types on Amazon Elastic Compute Cloud (EC2), Kubernetes allows 39 volumes to be attached to a Node.\nOn Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to Sizes for virtual\nmachines in Azure.\nIf a CSI storage driver advertises a maximum number of volumes for a Node (using NodeGetInfo ), the kube-scheduler honors\nthat limit. Refer to the CSI specifications for details.\nFor volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the\none reported by the CSI driver.\n\nMutable CSI Node Allocatable Count\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: false)\n\nCSI drivers can dynamically adjust the maximum number of volumes that can be attached to a Node at runtime. This enhances\nscheduling accuracy and reduces pod scheduling failures due to changes in resource availability.\nTo use this feature, you must enable the MutableCSINodeAllocatableCount feature gate on the following components:\nkube-apiserver\nhttps://kubernetes.io/docs/concepts/_print/\n\n365/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubelet"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0374", "text": "Periodic Updates\nWhen enabled, CSI drivers can request periodic updates to their volume limits by setting the nodeAllocatableUpdatePeriodSeconds\nfield in the CSIDriver specification. For example:\n\napiVersion: storage.k8s.io/v1\nkind: CSIDriver\nmetadata:\nname: hostpath.csi.k8s.io\nspec:\nnodeAllocatableUpdatePeriodSeconds: 60\n\nKubelet will periodically call the corresponding CSI driverâ€™s NodeGetInfo endpoint to refresh the maximum number of attachable\nvolumes, using the interval specified in nodeAllocatableUpdatePeriodSeconds . The minimum allowed value for this field is 10\nseconds.\nIf a volume attachment operation fails with a ResourceExhausted error (gRPC code 8), Kubernetes triggers an immediate update to\nthe allocatable volume count for that Node. Additionally, kubelet marks affected pods as Failed, allowing their controllers to handle\nrecreation. This prevents pods from getting stuck indefinitely in the ContainerCreating state.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n366/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n6.13 - Volume Health Monitoring\nâ“˜ FEATURE STATE: Kubernetes v1.21 [alpha]\n\nCSI volume health monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and\nreport them as events on PVCs or Pods.\n\nVolume health monitoring\nKubernetes volume health monitoring is part of how Kubernetes implements the Container Storage Interface (CSI). Volume health\nmonitoring feature is implemented in two components: an External Health Monitor controller, and the kubelet.\nIf a CSI Driver supports Volume Health Monitoring feature from the controller side, an event will be reported on the related\nPersistentVolumeClaim (PVC) when an abnormal volume condition is detected on a CSI volume.\nThe External Health Monitor controller also watches for node failure events. You can enable node failure monitoring by setting the\nenable-node-watcher flag to true. When the external health monitor detects a node failure event, the controller reports an Event\nwill be reported on the PVC to indicate that pods using this PVC are on a failed node.\nIf a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be reported on every Pod using the PVC\nwhen an abnormal volume condition is detected on a CSI volume. In addition, Volume Health information is exposed as Kubelet\nVolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal is added. This metric includes two labels:\nnamespace and persistentvolumeclaim . The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume is\nhealthy. For more information, please check KEP.\nNote:\nYou need to enable the CSIVolumeHealth feature gate to use this feature from the node side."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0375", "text": "What's next\nSee the CSI driver documentation to find out which CSI drivers have implemented this feature.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n367/684\n\n11/7/25, 4:37 PM\n\n6.14 - Windows Storage\n\nConcepts | Kubernetes\n\nThis page provides an storage overview specific to the Windows operating system.\n\nPersistent storage\nWindows has a layered filesystem driver to mount container layers and create a copy filesystem based on NTFS. All file paths in the\ncontainer are resolved only within the context of that container.\nWith Docker, volume mounts can only target a directory in the container, and not an individual file. This limitation does not\napply to containerd.\nVolume mounts cannot project files or directories back to the host filesystem.\nRead-only filesystems are not supported because write access is always required for the Windows registry and SAM database.\nHowever, read-only volumes are supported.\nVolume user-masks and permissions are not available. Because the SAM is not shared between the host & container, there's no\nmapping between them. All permissions are resolved within the context of the container.\nAs a result, the following storage functionality is not supported on Windows nodes:\nVolume subpath mounts: only the entire volume can be mounted in a Windows container\nSubpath volume mounting for Secrets\nHost mount projection\nRead-only root filesystem (mapped volumes still support readOnly )\nBlock device mapping\nMemory as the storage medium (for example, emptyDir.medium set to Memory )\nFile system features like uid/gid; per-user Linux filesystem permissions\nSetting secret permissions with DefaultMode (due to UID/GID dependency)\nNFS based storage/volume support\nExpanding the mounted volume (resizefs)\nKubernetes volumes enable complex applications, with data persistence and Pod volume sharing requirements, to be deployed on\nKubernetes. Management of persistent volumes associated with a specific storage back-end or protocol includes actions such as\nprovisioning/de-provisioning/resizing of volumes, attaching/detaching a volume to/from a Kubernetes node and\nmounting/dismounting a volume to/from individual containers in a pod that needs to persist data.\nVolume management components are shipped as Kubernetes volume plugin. The following broad classes of Kubernetes volume\nplugins are supported on Windows:\nFlexVolume plugins\n\nPlease note that FlexVolumes have been deprecated as of 1.23\nCSI Plugins\n\nIn-tree volume plugins\nThe following in-tree plugins support persistent storage on Windows nodes:\nazureFile\nvsphereVolume\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n368/684\n\n11/7/25, 4:37 PM\n\n7 - Configuration\n\nConcepts | Kubernetes\n\nResources that Kubernetes provides for configuring Pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0376", "text": "7.1 - Configuration Best Practices\nThis document highlights and consolidates configuration best practices that are introduced throughout the user guide, Getting\nStarted documentation, and examples.\nThis is a living document. If you think of something that is not on this list but might be useful to others, please don't hesitate to file\nan issue or submit a PR.\n\nGeneral Configuration Tips\nWhen defining configurations, specify the latest stable API version.\nConfiguration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a\nconfiguration change if necessary. It also aids cluster re-creation and restoration.\nWrite your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all\nscenarios, YAML tends to be more user-friendly.\nGroup related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the\nguestbook-all-in-one.yaml file as an example of this syntax.\nNote also that many kubectl commands can be called on a directory. For example, you can call kubectl apply on a directory\nof config files.\nDon't specify default values unnecessarily: simple, minimal configuration will make errors less likely.\nPut object descriptions in annotations, to allow better introspection.\nNote:\nThere is a breaking change introduced in the YAML 1.2 boolean values specification with respect to YAML 1.1. This is a known\nissue in Kubernetes. YAML 1.2 only recognizes true and false as valid booleans, while YAML 1.1 also accepts yes, no, on, and\noff as booleans. However, Kubernetes uses YAML parsers that are mostly compatible with YAML 1.1, which means that using\nyes or no instead of true or false in a YAML manifest may cause unexpected errors or behaviors. To avoid this issue, it is\nrecommended to always use true or false for boolean values in YAML manifests, and to quote any strings that may be\nconfused with booleans, such as \"yes\" or \"no\".\nBesides booleans, there are additional specifications changes between YAML versions. Please refer to the YAML Specification\nChanges documentation for a comprehensive list."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0377", "text": "\"Naked\" Pods versus ReplicaSets, Deployments, and Jobs\nDon't use naked Pods (that is, Pods not bound to a ReplicaSet or Deployment) if you can avoid it. Naked Pods will not be\nrescheduled in the event of a node failure.\nA Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a\nstrategy to replace Pods (such as RollingUpdate), is almost always preferable to creating Pods directly, except for some explicit\nrestartPolicy: Never scenarios. A Job may also be appropriate.\n\nServices\nCreate a Service before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that\nneed to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were\nrunning when the container was started. For example, if a Service named foo exists, all containers will get the following\nvariables in their initial environment:\nhttps://kubernetes.io/docs/concepts/_print/\n\n369/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFOO_SERVICE_HOST=<the host the Service is running on>\nFOO_SERVICE_PORT=<the port the Service is running on>"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0378", "text": "This does imply an ordering requirement - any Service that a Pod wants to access must be created before the Pod itself, or\nelse the environment variables will not be populated. DNS does not have this restriction.\nAn optional (though strongly recommended) cluster add-on is a DNS server. The DNS server watches the Kubernetes API for\nnew Services and creates a set of DNS records for each. If DNS has been enabled throughout the cluster then all Pods\nshould be able to do name resolution of Services automatically.\nDon't specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort , it limits the number\nof places the Pod can be scheduled, because each < hostIP , hostPort , protocol > combination must be unique. If you don't\nspecify the hostIP and protocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as the default\nprotocol .\nIf you only need access to the port for debugging purposes, you can use the apiserver proxy or kubectl port-forward .\nIf you explicitly need to expose a Pod's port on the node, consider using a NodePort Service before resorting to hostPort .\nAvoid using hostNetwork , for the same reasons as hostPort .\nUse headless Services (which have a ClusterIP of None ) for service discovery when you don't need kube-proxy load\nbalancing."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0379", "text": "Using Labels\nDefine and use labels that identify semantic attributes of your application or Deployment, such as {\napp.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 } . You can use these labels to select the\nappropriate Pods for other resources; for example, a Service that selects all tier: frontend Pods, or all phase: test\ncomponents of app.kubernetes.io/name: MyApp . See the guestbook app for examples of this approach.\nA Service can be made to span multiple Deployments by omitting release-specific labels from its selector. When you need to\nupdate a running service without downtime, use a Deployment.\nA desired state of an object is described by a Deployment, and if changes to that spec are applied, the deployment controller\nchanges the actual state to the desired state at a controlled rate.\nUse the Kubernetes common labels for common use cases. These standardized labels enrich the metadata in a way that allows\ntools, including kubectl and dashboard, to work in an interoperable way.\nYou can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods\nusing selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being\nserved traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This\nis a useful way to debug a previously \"live\" Pod in a \"quarantine\" environment. To interactively remove or add labels, use\nkubectl label .\n\nUsing kubectl\nUse kubectl apply -f <directory> . This looks for Kubernetes configuration in all .yaml , .yml , and .json files in\n<directory> and passes it to apply .\nUse label selectors for get and delete operations instead of specific object names. See the sections on label selectors and\nusing labels effectively.\nUse kubectl create deployment and kubectl expose to quickly create single-container Deployments and Services. See Use a\nService to Access an Application in a Cluster for an example.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n370/684\n\n11/7/25, 4:37 PM\n\n7.2 - ConfigMaps\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0380", "text": "A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment\nvariables, command-line arguments, or as configuration files in a volume.\nA ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are\neasily portable.\nCaution:\nConfigMap does not provide secrecy or encryption. If the data you want to store are confidential, use a Secret rather than a\nConfigMap, or use additional (third party) tools to keep your data private.\n\nMotivation\nUse a ConfigMap for setting configuration data separately from application code.\nFor example, imagine that you are developing an application that you can run on your own computer (for development) and in the\ncloud (to handle real traffic). You write the code to look in an environment variable named DATABASE_HOST . Locally, you set that\nvariable to localhost . In the cloud, you set it to refer to a Kubernetes Service that exposes the database component to your cluster.\nThis lets you fetch a container image running in the cloud and debug the exact same code locally if needed.\nNote:\nA ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you need to\nstore settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file\nservice.\n\nConfigMap object\nA ConfigMap is an API object that lets you store configuration for other objects to use. Unlike most Kubernetes objects that have a\nspec , a ConfigMap has data and binaryData fields. These fields accept key-value pairs as their values. Both the data field and\nthe binaryData are optional. The data field is designed to contain UTF-8 strings while the binaryData field is designed to contain\nbinary data as base64-encoded strings.\nThe name of a ConfigMap must be a valid DNS subdomain name.\nEach key under the data or the binaryData field must consist of alphanumeric characters, - , _ or . . The keys stored in data\nmust not overlap with the keys in the binaryData field.\nStarting from v1.19, you can add an immutable field to a ConfigMap definition to create an immutable ConfigMap."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0381", "text": "ConfigMaps and Pods\nYou can write a Pod spec that refers to a ConfigMap and configures the container(s) in that Pod based on the data in the\nConfigMap. The Pod and the ConfigMap must be in the same namespace.\nNote:\nThe spec of a static Pod cannot refer to a ConfigMap or any other API objects.\nHere's an example ConfigMap that has some keys with single values, and other keys where the value looks like a fragment of a\nconfiguration format.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n371/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: game-demo\ndata:\n# property-like keys; each key maps to a simple value\nplayer_initial_lives: \"3\"\nui_properties_file_name: \"user-interface.properties\"\n# file-like keys\ngame.properties: |\nenemy.types=aliens,monsters\nplayer.maximum-lives=5\nuser-interface.properties: |\ncolor.good=purple\ncolor.bad=yellow\nallow.textmode=true\n\nThere are four different ways that you can use a ConfigMap to configure a container inside a Pod:\n1. Inside a container command and args\n2. Environment variables for a container\n3. Add a file in read-only volume, for the application to read\n4. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap\nThese different methods lend themselves to different ways of modeling the data being consumed. For the first three methods, the\nkubelet uses the data from the ConfigMap when it launches container(s) for a Pod.\nThe fourth method means you have to write code to read the ConfigMap and its data. However, because you're using the\nKubernetes API directly, your application can subscribe to get updates whenever the ConfigMap changes, and react when that\nhappens. By accessing the Kubernetes API directly, this technique also lets you access a ConfigMap in a different namespace.\nHere's an example Pod that uses values from game-demo to configure a Pod:\nconfigmap/configure-pod.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n372/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: configmap-demo-pod\nspec:\ncontainers:\n- name: demo\nimage: alpine\ncommand: [\"sleep\", \"3600\"]\nenv:\n# Define the environment variable\n- name: PLAYER_INITIAL_LIVES # Notice that the case is different here\n# from the key name in the ConfigMap.\nvalueFrom:\nconfigMapKeyRef:\nname: game-demo\n\n# The ConfigMap this value comes from."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0382", "text": "key: player_initial_lives # The key to fetch.\n- name: UI_PROPERTIES_FILE_NAME\nvalueFrom:\nconfigMapKeyRef:\nname: game-demo\nkey: ui_properties_file_name\nvolumeMounts:\n- name: config\nmountPath: \"/config\"\nreadOnly: true\nvolumes:\n# You set volumes at the Pod level, then mount them into containers inside that Pod\n- name: config\nconfigMap:\n# Provide the name of the ConfigMap you want to mount.\nname: game-demo\n# An array of keys from the ConfigMap to create as files\nitems:\n- key: \"game.properties\"\npath: \"game.properties\"\n- key: \"user-interface.properties\"\npath: \"user-interface.properties\"\n\nA ConfigMap doesn't differentiate between single line property values and multi-line file-like values. What matters is how Pods and\nother objects consume those values.\nFor this example, defining a volume and mounting it inside the demo container as /config creates two files,\n/config/game.properties and /config/user-interface.properties , even though there are four keys in the ConfigMap. This is\nbecause the Pod definition specifies an items array in the volumes section. If you omit the items array entirely, every key in the\nConfigMap becomes a file with the same name as the key, and you get 4 files.\n\nUsing ConfigMaps\nConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other parts of the system, without being directly\nexposed to the Pod. For example, ConfigMaps can hold data that other parts of the system should use for configuration.\nThe most common way to use ConfigMaps is to configure settings for containers running in a Pod in the same namespace. You can\nalso use a ConfigMap separately.\nFor example, you might encounter addons or operators that adjust their behavior based on a ConfigMap.\n\nUsing ConfigMaps as files from a Pod\nTo consume a ConfigMap in a volume in a Pod:\n1. Create a ConfigMap or use an existing one. Multiple Pods can reference the same ConfigMap.\nhttps://kubernetes.io/docs/concepts/_print/\n\n373/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0383", "text": "2. Modify your Pod definition to add a volume under .spec.volumes[] . Name the volume anything, and have a\n.spec.volumes[].configMap.name field set to reference your ConfigMap object.\n3. Add a .spec.containers[].volumeMounts[] to each container that needs the ConfigMap. Specify\n.spec.containers[].volumeMounts[].readOnly = true and .spec.containers[].volumeMounts[].mountPath to an unused\ndirectory name where you would like the ConfigMap to appear.\n4. Modify your image or command line so that the program looks for files in that directory. Each key in the ConfigMap data map\nbecomes the filename under mountPath .\nThis is an example of a Pod that mounts a ConfigMap in a volume:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: mypod\nspec:\ncontainers:\n- name: mypod\nimage: redis\nvolumeMounts:\n- name: foo\nmountPath: \"/etc/foo\"\nreadOnly: true\nvolumes:\n- name: foo\nconfigMap:\nname: myconfigmap\n\nEach ConfigMap you want to use needs to be referred to in .spec.volumes .\nIf there are multiple containers in the Pod, then each container needs its own volumeMounts block, but only one .spec.volumes is\nneeded per ConfigMap.\n\nMounted ConfigMaps are updated automatically\nWhen a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well. The kubelet checks\nwhether the mounted ConfigMap is fresh on every periodic sync. However, the kubelet uses its local cache for getting the current\nvalue of the ConfigMap. The type of the cache is configurable using the configMapAndSecretChangeDetectionStrategy field in the\nKubeletConfiguration struct. A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting all requests\ndirectly to the API server. As a result, the total delay from the moment when the ConfigMap is updated to the moment when new\nkeys are projected to the Pod can be as long as the kubelet sync period + cache propagation delay, where the cache propagation\ndelay depends on the chosen cache type (it equals to watch propagation delay, ttl of cache, or zero correspondingly).\nConfigMaps consumed as environment variables are not updated automatically and require a pod restart.\nNote:\nA container using a ConfigMap as a subPath volume mount will not receive ConfigMap updates."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0384", "text": "Using Configmaps as environment variables\nTo use a Configmap in an environment variable in a Pod:\n1. For each container in your Pod specification, add an environment variable for each Configmap key that you want to use to the\nenv[].valueFrom.configMapKeyRef field.\n2. Modify your image and/or command line so that the program looks for values in the specified environment variables.\nThis is an example of defining a ConfigMap as a pod environment variable:\nThe following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n374/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: myconfigmap\ndata:\nusername: k8s-admin\naccess_level: \"1\"\n\nThe following command will create the ConfigMap object:\n\nkubectl apply -f myconfigmap.yaml\n\nThe following Pod consumes the content of the ConfigMap as environment variables:\nconfigmap/env-configmap.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: env-configmap\nspec:\ncontainers:\n- name: app\ncommand: [\"/bin/sh\", \"-c\", \"printenv\"]\nimage: busybox:latest\nenvFrom:\n- configMapRef:\nname: myconfigmap\n\nThe envFrom field instructs Kubernetes to create environment variables from the sources nested within it. The inner configMapRef\nrefers to a ConfigMap by its name and selects all its key-value pairs. Add the Pod to your cluster, then retrieve its logs to see the\noutput from the printenv command. This should confirm that the two key-value pairs from the ConfigMap have been set as\nenvironment variables:\n\nkubectl apply -f env-configmap.yaml\n\nkubectl logs pod/ env-configmap\n\nThe output is similar to this:\n\n...\nusername: \"k8s-admin\"\naccess_level: \"1\"\n...\n\nSometimes a Pod won't require access to all the values in a ConfigMap. For example, you could have another Pod which only uses\nthe username value from the ConfigMap. For this use case, you can use the env.valueFrom syntax instead, which lets you select\nindividual keys in a ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap. For\nexample:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n375/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: env-configmap\nspec:\ncontainers:\n- name: envars-test-container\nimage: nginx\nenv:\n- name: CONFIGMAP_USERNAME\nvalueFrom:\nconfigMapKeyRef:\nname: myconfigmap\nkey: username"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0385", "text": "In the Pod created from this manifest, you will see that the environment variable CONFIGMAP_USERNAME is set to the value of the\nusername value from the ConfigMap. Other keys from the ConfigMap data are not copied into the environment.\nIt's important to note that the range of characters allowed for environment variable names in pods is restricted. If any keys do not\nmeet the rules, those keys are not made available to your container, though the Pod is allowed to start.\n\nImmutable ConfigMaps\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]\n\nThe Kubernetes feature Immutable Secrets and ConfigMaps provides an option to set individual Secrets and ConfigMaps as\nimmutable. For clusters that extensively use ConfigMaps (at least tens of thousands of unique ConfigMap to Pod mounts),\npreventing changes to their data has the following advantages:\nprotects you from accidental (or unwanted) updates that could cause applications outages\nimproves performance of your cluster by significantly reducing load on kube-apiserver, by closing watches for ConfigMaps\nmarked as immutable.\nYou can create an immutable ConfigMap by setting the immutable field to true . For example:\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n...\ndata:\n...\nimmutable: true\n\nOnce a ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data or the\nbinaryData field. You can only delete and recreate the ConfigMap. Because existing Pods maintain a mount point to the deleted\nConfigMap, it is recommended to recreate these pods.\n\nWhat's next\nRead about Secrets.\nRead Configure a Pod to Use a ConfigMap.\nRead about changing a ConfigMap (or any other Kubernetes object)\nRead The Twelve-Factor App to understand the motivation for separating code from configuration.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n376/684\n\n11/7/25, 4:37 PM\n\n7.3 - Secrets\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0386", "text": "A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might\notherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential\ndata in your application code.\nBecause Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed\nduring the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take\nadditional precautions with Secrets, such as avoiding writing sensitive data to nonvolatile storage.\nSecrets are similar to ConfigMaps but are specifically intended to hold confidential data.\nCaution:\nKubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). Anyone with API access\ncan retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod\nin a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to\ncreate a Deployment.\nIn order to safely use Secrets, take at least the following steps:\n1. Enable Encryption at Rest for Secrets.\n2. Enable or configure RBAC rules with least-privilege access to Secrets.\n3. Restrict Secret access to specific containers.\n4. Consider using external Secret store providers.\nFor more guidelines to manage and improve the security of your Secrets, refer to Good practices for Kubernetes Secrets.\n\nSee Information security for Secrets for more details.\n\nUses for Secrets\nYou can use Secrets for purposes such as the following:\nSet environment variables for a container.\nProvide credentials such as SSH keys or passwords to Pods.\nAllow the kubelet to pull container images from private registries.\nThe Kubernetes control plane also uses Secrets; for example, bootstrap token Secrets are a mechanism to help automate node\nregistration."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0387", "text": "Use case: dotfiles in a secret volume\nYou can make your data \"hidden\" by defining a key that begins with a dot. This key represents a dotfile or \"hidden\" file. For example,\nwhen the following Secret is mounted into a volume, secret-volume , the volume will contain a single file, called .secret-file , and\nthe dotfile-test-container will have this file present at the path /etc/secret-volume/.secret-file .\nNote:\nFiles beginning with dot characters are hidden from the output of ls -l; you must use ls -la to see them when listing\ndirectory contents.\n\nsecret/dotfile-secret.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n377/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Secret\nmetadata:\nname: dotfile-secret\ndata:\n.secret-file: dmFsdWUtMg0KDQo=\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: secret-dotfiles-pod\nspec:\nvolumes:\n- name: secret-volume\nsecret:\nsecretName: dotfile-secret\ncontainers:\n- name: dotfile-test-container\nimage: registry.k8s.io/busybox\ncommand:\n- ls\n- \"-l\"\n- \"/etc/secret-volume\"\nvolumeMounts:\n- name: secret-volume\nreadOnly: true\nmountPath: \"/etc/secret-volume\"\n\nUse case: Secret visible to one container in a Pod\nConsider a program that needs to handle HTTP requests, do some complex business logic, and then sign some messages with an\nHMAC. Because it has complex application logic, there might be an unnoticed remote file reading exploit in the server, which could\nexpose the private key to an attacker.\nThis could be divided into two processes in two containers: a frontend container which handles user interaction and business logic,\nbut which cannot see the private key; and a signer container that can see the private key, and responds to simple signing requests\nfrom the frontend (for example, over localhost networking).\nWith this partitioned approach, an attacker now has to trick the application server into doing something rather arbitrary, which may\nbe harder than getting it to read a file."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0388", "text": "Alternatives to Secrets\nRather than using a Secret to protect confidential data, you can pick from alternatives.\nHere are some of your options:\nIf your cloud-native component needs to authenticate to another application that you know is running within the same\nKubernetes cluster, you can use a ServiceAccount and its tokens to identify your client.\nThere are third-party tools that you can run, either within or outside your cluster, that manage sensitive data. For example, a\nservice that Pods access over HTTPS, that reveals a Secret if the client correctly authenticates (for example, with a\nServiceAccount token).\nFor authentication, you can implement a custom signer for X.509 certificates, and use CertificateSigningRequests to let that\ncustom signer issue certificates to Pods that need them.\nYou can use a device plugin to expose node-local encryption hardware to a specific Pod. For example, you can schedule trusted\nPods onto nodes that provide a Trusted Platform Module, configured out-of-band.\nYou can also combine two or more of those options, including the option to use Secret objects themselves.\nFor example: implement (or deploy) an operator that fetches short-lived session tokens from an external service, and then creates\nSecrets based on those short-lived session tokens. Pods running in your cluster can make use of the session tokens, and operator\nensures they are valid. This separation means that you can run Pods that are unaware of the exact mechanisms for issuing and\nrefreshing those session tokens.\nhttps://kubernetes.io/docs/concepts/_print/\n\n378/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTypes of Secret\n\nWhen creating a Secret, you can specify its type using the type field of the Secret resource, or certain equivalent kubectl\ncommand line flags (if available). The Secret type is used to facilitate programmatic handling of the Secret data.\nKubernetes provides several built-in types for some common usage scenarios. These types vary in terms of the validations\nperformed and the constraints Kubernetes imposes on them.\nBuilt-in Type\n\nUsage\n\nOpaque\n\narbitrary user-defined data\n\nkubernetes.io/service-account-token\n\nServiceAccount token\n\nkubernetes.io/dockercfg\n\nserialized ~/.dockercfg file\n\nkubernetes.io/dockerconfigjson\n\nserialized ~/.docker/config.json file\n\nkubernetes.io/basic-auth\n\ncredentials for basic authentication\n\nkubernetes.io/ssh-auth\n\ncredentials for SSH authentication\n\nkubernetes.io/tls\n\ndata for a TLS client or server\n\nbootstrap.kubernetes.io/token\n\nbootstrap token data"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0389", "text": "You can define and use your own Secret type by assigning a non-empty string as the type value for a Secret object (an empty string\nis treated as an Opaque type).\nKubernetes doesn't impose any constraints on the type name. However, if you are using one of the built-in types, you must meet all\nthe requirements defined for that type.\nIf you are defining a type of Secret that's for public use, follow the convention and structure the Secret type to have your domain\nname before the name, separated by a / . For example: cloud-hosting.example.net/cloud-api-credentials .\n\nOpaque Secrets\nis the default Secret type if you don't explicitly specify a type in a Secret manifest. When you create a Secret using kubectl ,\nyou must use the generic subcommand to indicate an Opaque Secret type. For example, the following command creates an empty\nSecret of type Opaque :\nOpaque\n\nkubectl create secret generic empty-secret\nkubectl get secret empty-secret\n\nThe output looks like:\nNAME\nempty-secret\n\nTYPE\nOpaque\n\nDATA\n0\n\nAGE\n2m6s\n\nThe DATA column shows the number of data items stored in the Secret. In this case, 0 means you have created an empty Secret.\n\nServiceAccount token Secrets\nA kubernetes.io/service-account-token type of Secret is used to store a token credential that identifies a ServiceAccount. This is a\nlegacy mechanism that provides long-lived ServiceAccount credentials to Pods.\nIn Kubernetes v1.22 and later, the recommended approach is to obtain a short-lived, automatically rotating ServiceAccount token by\nusing the TokenRequest API instead. You can get these short-lived tokens using the following methods:\nhttps://kubernetes.io/docs/concepts/_print/\n\n379/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0390", "text": "Call the TokenRequest API either directly or by using an API client like kubectl . For example, you can use the kubectl create\ntoken command.\nRequest a mounted token in a projected volume in your Pod manifest. Kubernetes creates the token and mounts it in the Pod.\nThe token is automatically invalidated when the Pod that it's mounted in is deleted. For details, see Launch a Pod using service\naccount token projection.\nNote:\nYou should only create a ServiceAccount token Secret if you can't use the TokenRequest API to obtain a token, and the security\nexposure of persisting a non-expiring token credential in a readable API object is acceptable to you. For instructions, see\nManually create a long-lived API token for a ServiceAccount.\nWhen using this Secret type, you need to ensure that the kubernetes.io/service-account.name annotation is set to an existing\nServiceAccount name. If you are creating both the ServiceAccount and the Secret objects, you should create the ServiceAccount\nobject first.\nAfter the Secret is created, a Kubernetes controller fills in some other fields such as the kubernetes.io/service-account.uid\nannotation, and the token key in the data field, which is populated with an authentication token.\nThe following example configuration declares a ServiceAccount token Secret:\nsecret/serviceaccount-token-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: secret-sa-sample\nannotations:\nkubernetes.io/service-account.name: \"sa-name\"\ntype: kubernetes.io/service-account-token\ndata:\nextra: YmFyCg==\n\nAfter creating the Secret, wait for Kubernetes to populate the token key in the data field.\nSee the ServiceAccount documentation for more information on how ServiceAccounts work. You can also check the\nautomountServiceAccountToken field and the serviceAccountName field of the Pod for information on referencing ServiceAccount\ncredentials from within Pods.\n\nDocker config Secrets\nIf you are creating a Secret to store credentials for accessing a container image registry, you must use one of the following type\nvalues for that Secret:\nkubernetes.io/dockercfg : store a serialized ~/.dockercfg\n\nwhich is the legacy format for configuring Docker command line.\nThe Secret data field contains a .dockercfg key whose value is the content of a base64 encoded ~/.dockercfg file.\nkubernetes.io/dockerconfigjson : store a serialized JSON that follows the same format rules as the ~/.docker/config.json\n\nfile, which is a new format for ~/.dockercfg . The Secret data field must contain a .dockerconfigjson key for which the value\nis the content of a base64 encoded ~/.docker/config.json file.\nBelow is an example for a kubernetes.io/dockercfg type of Secret:\nsecret/dockercfg-secret.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n380/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0391", "text": "apiVersion: v1\nkind: Secret\nmetadata:\nname: secret-dockercfg\ntype: kubernetes.io/dockercfg\ndata:\n.dockercfg: |\neyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo=\n\nNote:\nIf you do not want to perform the base64 encoding, you can choose to use the stringData field instead.\nWhen you create Docker config Secrets using a manifest, the API server checks whether the expected key exists in the data field,\nand it verifies if the value provided can be parsed as a valid JSON. The API server doesn't validate if the JSON actually is a Docker\nconfig file.\nYou can also use kubectl to create a Secret for accessing a container registry, such as when you don't have a Docker configuration\nfile:\n\nkubectl create secret docker-registry secret-tiger-docker \\\n--docker-email=tiger@acme.example \\\n--docker-username=tiger \\\n--docker-password=pass1234 \\\n--docker-server=my-registry.example:5000\n\nThis command creates a Secret of type kubernetes.io/dockerconfigjson .\nRetrieve the .data.dockerconfigjson field from that new Secret and decode the data:\n\nkubectl get secret secret-tiger-docker -o jsonpath='{.data.*}' | base64 -d\n\nThe output is equivalent to the following JSON document (which is also a valid Docker configuration file):\n\n{\n\"auths\": {\n\"my-registry.example:5000\": {\n\"username\": \"tiger\",\n\"password\": \"pass1234\",\n\"email\": \"tiger@acme.example\",\n\"auth\": \"dGlnZXI6cGFzczEyMzQ=\"\n}\n}\n}\n\nCaution:\nThe auth value there is base64 encoded; it is obscured but not secret. Anyone who can read that Secret can learn the registry\naccess bearer token.\nIt is suggested to use credential providers to dynamically and securely provide pull secrets on-demand.\n\nBasic authentication Secret\nThe kubernetes.io/basic-auth type is provided for storing credentials needed for basic authentication. When using this Secret\ntype, the data field of the Secret must contain one of the following two keys:\nhttps://kubernetes.io/docs/concepts/_print/\n\n381/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nusername : the user name for authentication\npassword : the password or token for authentication\n\nBoth values for the above two keys are base64 encoded strings. You can alternatively provide the clear text content using the\nstringData field in the Secret manifest.\nThe following manifest is an example of a basic authentication Secret:\nsecret/basicauth-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: secret-basic-auth\ntype: kubernetes.io/basic-auth\nstringData:\nusername: admin # required field for kubernetes.io/basic-auth\npassword: t0p-Secret # required field for kubernetes.io/basic-auth"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0392", "text": "Note:\nThe stringData field for a Secret does not work well with server-side apply.\nThe basic authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for basic\nauthentication. However, using the defined and public Secret type ( kubernetes.io/basic-auth ) helps other people to understand\nthe purpose of your Secret, and sets a convention for what key names to expect.\n\nSSH authentication Secrets\nThe builtin type kubernetes.io/ssh-auth is provided for storing data used in SSH authentication. When using this Secret type, you\nwill have to specify a ssh-privatekey key-value pair in the data (or stringData ) field as the SSH credential to use.\nThe following manifest is an example of a Secret used for SSH public/private key authentication:\nsecret/ssh-auth-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: secret-ssh-auth\ntype: kubernetes.io/ssh-auth\ndata:\n# the data is abbreviated in this example\nssh-privatekey: |\nUG91cmluZzYlRW1vdGljb24lU2N1YmE=\n\nThe SSH authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for SSH\nauthentication. However, using the defined and public Secret type ( kubernetes.io/ssh-auth ) helps other people to understand the\npurpose of your Secret, and sets a convention for what key names to expect. The Kubernetes API verifies that the required keys are\nset for a Secret of this type.\nCaution:\nSSH private keys do not establish trusted communication between an SSH client and host server on their own. A secondary\nmeans of establishing trust is needed to mitigate \"man in the middle\" attacks, such as a known_hosts file added to a\nConfigMap.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n382/684\n\n11/7/25, 4:37 PM\n\nTLS Secrets\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0393", "text": "The kubernetes.io/tls Secret type is for storing a certificate and its associated key that are typically used for TLS.\nOne common use for TLS Secrets is to configure encryption in transit for an Ingress, but you can also use it with other resources or\ndirectly in your workload. When using this type of Secret, the tls.key and the tls.crt key must be provided in the data (or\nstringData ) field of the Secret configuration, although the API server doesn't actually validate the values for each key.\nAs an alternative to using stringData , you can use the data field to provide the base64 encoded certificate and private key. For\ndetails, see Constraints on Secret names and data.\nThe following YAML contains an example config for a TLS Secret:\nsecret/tls-auth-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\nname: secret-tls\ntype: kubernetes.io/tls\ndata:\n# values are base64 encoded, which obscures them but does NOT provide\n# any useful level of confidentiality\n# Replace the following values with your own base64-encoded certificate and key.\ntls.crt: \"REPLACE_WITH_BASE64_CERT\"\ntls.key: \"REPLACE_WITH_BASE64_KEY\"\n\nThe TLS Secret type is provided only for convenience. You can create an Opaque type for credentials used for TLS authentication.\nHowever, using the defined and public Secret type ( kubernetes.io/tls ) helps ensure the consistency of Secret format in your\nproject. The API server verifies if the required keys are set for a Secret of this type.\nTo create a TLS Secret using kubectl , use the tls subcommand:\n\nkubectl create secret tls my-tls-secret \\\n--cert=path/to/cert/file \\\n--key=path/to/key/file\n\nThe public/private key pair must exist before hand. The public key certificate for --cert must be .PEM encoded and must match the\ngiven private key for --key .\n\nBootstrap token Secrets\nThe bootstrap.kubernetes.io/token Secret type is for tokens used during the node bootstrap process. It stores tokens used to sign\nwell-known ConfigMaps.\nA bootstrap token Secret is usually created in the kube-system namespace and named in the form bootstrap-token-<token-id>\nwhere <token-id> is a 6 character string of the token ID.\nAs a Kubernetes manifest, a bootstrap token Secret might look like the following:\nsecret/bootstrap-token-secret-base64.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n383/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Secret\nmetadata:\nname: bootstrap-token-5emitj\nnamespace: kube-system\ntype: bootstrap.kubernetes.io/token\ndata:\nauth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=\nexpiration: MjAyMC0wOS0xM1QwNDozOToxMFo=\ntoken-id: NWVtaXRq\ntoken-secret: a3E0Z2lodnN6emduMXAwcg==\nusage-bootstrap-authentication: dHJ1ZQ==\nusage-bootstrap-signing: dHJ1ZQ=="}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0394", "text": "A bootstrap token Secret has the following keys specified under data :\ntoken-id : A random 6 character string as the token identifier. Required.\ntoken-secret : A random 16 character string as the actual token Secret. Required.\ndescription : A human-readable string that describes what the token is used for. Optional.\nexpiration : An absolute UTC time using RFC3339 specifying when the token should be expired. Optional.\nusage-bootstrap-<usage> : A boolean flag indicating additional usage for the bootstrap token.\nauth-extra-groups : A comma-separated list of group names that will be authenticated as in addition to the\nsystem:bootstrappers\n\ngroup.\n\nYou can alternatively provide the values in the stringData field of the Secret without base64 encoding them:\nsecret/bootstrap-token-secret-literal.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n# Note how the Secret is named\nname: bootstrap-token-5emitj\n# A bootstrap token Secret usually resides in the kube-system namespace\nnamespace: kube-system\ntype: bootstrap.kubernetes.io/token\nstringData:\nauth-extra-groups: \"system:bootstrappers:kubeadm:default-node-token\"\nexpiration: \"2020-09-13T04:39:10Z\"\n# This token ID is used in the name\ntoken-id: \"5emitj\"\ntoken-secret: \"kq4gihvszzgn1p0r\"\n# This token can be used for authentication\nusage-bootstrap-authentication: \"true\"\n# and it can be used for signing\nusage-bootstrap-signing: \"true\"\n\nNote:\nThe stringData field for a Secret does not work well with server-side apply.\n\nWorking with Secrets\nCreating a Secret\nThere are several options to create a Secret:\nUse kubectl\nUse a configuration file\nhttps://kubernetes.io/docs/concepts/_print/\n\n384/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nUse the Kustomize tool\n\nConstraints on Secret names and data\nThe name of a Secret object must be a valid DNS subdomain name.\nYou can specify the data and/or the stringData field when creating a configuration file for a Secret. The data and the\nstringData fields are optional. The values for all keys in the data field have to be base64-encoded strings. If the conversion to\nbase64 string is not desirable, you can choose to specify the stringData field instead, which accepts arbitrary strings as values.\nThe keys of data and stringData must consist of alphanumeric characters, - , _ or . . All key-value pairs in the stringData\nfield are internally merged into the data field. If a key appears in both the data and the stringData field, the value specified in\nthe stringData field takes precedence."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0395", "text": "Size limit\nIndividual Secrets are limited to 1MiB in size. This is to discourage creation of very large Secrets that could exhaust the API server\nand kubelet memory. However, creation of many smaller Secrets could also exhaust memory. You can use a resource quota to limit\nthe number of Secrets (or other resources) in a namespace.\n\nEditing a Secret\nYou can edit an existing Secret unless it is immutable. To edit a Secret, use one of the following methods:\nUse kubectl\nUse a configuration file\nYou can also edit the data in a Secret using the Kustomize tool. However, this method creates a new Secret object with the edited\ndata.\nDepending on how you created the Secret, as well as how the Secret is used in your Pods, updates to existing Secret objects are\npropagated automatically to Pods that use the data. For more information, refer to Using Secrets as files from a Pod section.\n\nUsing a Secret\nSecrets can be mounted as data volumes or exposed as environment variables to be used by a container in a Pod. Secrets can also\nbe used by other parts of the system, without being directly exposed to the Pod. For example, Secrets can hold credentials that\nother parts of the system should use to interact with external systems on your behalf.\nSecret volume sources are validated to ensure that the specified object reference actually points to an object of type Secret.\nTherefore, a Secret needs to be created before any Pods that depend on it.\nIf the Secret cannot be fetched (perhaps because it does not exist, or due to a temporary lack of connection to the API server) the\nkubelet periodically retries running that Pod. The kubelet also reports an Event for that Pod, including details of the problem fetching\nthe Secret.\n\nOptional Secrets\nWhen you reference a Secret in a Pod, you can mark the Secret as optional, such as in the following example. If an optional Secret\ndoesn't exist, Kubernetes ignores it.\nsecret/optional-secret.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n385/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: mypod\nspec:\ncontainers:\n- name: mypod\nimage: redis\nvolumeMounts:\n- name: foo\nmountPath: \"/etc/foo\"\nreadOnly: true\nvolumes:\n- name: foo\nsecret:\nsecretName: mysecret\noptional: true"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0396", "text": "By default, Secrets are required. None of a Pod's containers will start until all non-optional Secrets are available.\nIf a Pod references a specific key in a non-optional Secret and that Secret does exist, but is missing the named key, the Pod fails\nduring startup.\n\nUsing Secrets as files from a Pod\nIf you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes make the value of that Secret be\navailable as a file inside the filesystem of one or more of the Pod's containers.\nFor instructions, refer to Create a Pod that has access to the secret data through a Volume.\nWhen a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks this and updates the data in the volume,\nusing an eventually-consistent approach.\nNote:\nA container using a Secret as a subPath volume mount does not receive automated Secret updates.\nThe kubelet keeps a cache of the current keys and values for the Secrets that are used in volumes for pods on that node. You can\nconfigure the way that the kubelet detects changes from the cached values. The configMapAndSecretChangeDetectionStrategy field\nin the kubelet configuration controls which strategy the kubelet uses. The default strategy is Watch .\nUpdates to Secrets can be either propagated by an API watch mechanism (the default), based on a cache with a defined time-to-live,\nor polled from the cluster API server on each kubelet synchronisation loop.\nAs a result, the total delay from the moment when the Secret is updated to the moment when new keys are projected to the Pod can\nbe as long as the kubelet sync period + cache propagation delay, where the cache propagation delay depends on the chosen cache\ntype (following the same order listed in the previous paragraph, these are: watch propagation delay, the configured cache TTL, or\nzero for direct polling)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0397", "text": "Using Secrets as environment variables\nTo use a Secret in an environment variable in a Pod:\n1. For each container in your Pod specification, add an environment variable for each Secret key that you want to use to the\nenv[].valueFrom.secretKeyRef field.\n2. Modify your image and/or command line so that the program looks for values in the specified environment variables.\nFor instructions, refer to Define container environment variables using Secret data.\nIt's important to note that the range of characters allowed for environment variable names in pods is restricted. If any keys do not\nmeet the rules, those keys are not made available to your container, though the Pod is allowed to start.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n386/684\n\n11/7/25, 4:37 PM\n\nContainer image pull Secrets\n\nConcepts | Kubernetes\n\nIf you want to fetch container images from a private repository, you need a way for the kubelet on each node to authenticate to that\nrepository. You can configure image pull Secrets to make this possible. These Secrets are configured at the Pod level.\n\nUsing imagePullSecrets\nThe imagePullSecrets field is a list of references to Secrets in the same namespace. You can use an imagePullSecrets to pass a\nSecret that contains a Docker (or other) image registry password to the kubelet. The kubelet uses this information to pull a private\nimage on behalf of your Pod. See the PodSpec API for more information about the imagePullSecrets field.\n\nManually specifying an imagePullSecret\nYou can learn how to specify imagePullSecrets from the container images documentation.\n\nArranging for imagePullSecrets to be automatically attached\nYou can manually create imagePullSecrets , and reference these from a ServiceAccount. Any Pods created with that ServiceAccount\nor created with that ServiceAccount by default, will get their imagePullSecrets field set to that of the service account. See Add\nImagePullSecrets to a service account for a detailed explanation of that process.\n\nUsing Secrets with static Pods\nYou cannot use ConfigMaps or Secrets with static Pods.\n\nImmutable Secrets\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0398", "text": "Kubernetes lets you mark specific Secrets (and ConfigMaps) as immutable. Preventing changes to the data of an existing Secret has\nthe following benefits:\nprotects you from accidental (or unwanted) updates that could cause applications outages\n(for clusters that extensively use Secrets - at least tens of thousands of unique Secret to Pod mounts), switching to immutable\nSecrets improves the performance of your cluster by significantly reducing load on kube-apiserver. The kubelet does not need\nto maintain a [watch] on any Secrets that are marked as immutable.\n\nMarking a Secret as immutable\nYou can create an immutable Secret by setting the immutable field to true . For example,\n\napiVersion: v1\nkind: Secret\nmetadata: ...\ndata: ...\nimmutable: true\n\nYou can also update any existing mutable Secret to make it immutable.\nNote:\nOnce a Secret or ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the\ndata field. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is\nrecommended to recreate these pods.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n387/684\n\n11/7/25, 4:37 PM\n\nInformation security for Secrets\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0399", "text": "Although ConfigMap and Secret work similarly, Kubernetes applies some additional protection for Secret objects.\nSecrets often hold values that span a spectrum of importance, many of which can cause escalations within Kubernetes (e.g. service\naccount tokens) and to external systems. Even if an individual app can reason about the power of the Secrets it expects to interact\nwith, other apps within the same namespace can render those assumptions invalid.\nA Secret is only sent to a node if a Pod on that node requires it. For mounting Secrets into Pods, the kubelet stores a copy of the data\ninto a tmpfs so that the confidential data is not written to durable storage. Once the Pod that depends on the Secret is deleted, the\nkubelet deletes its local copy of the confidential data from the Secret.\nThere may be several containers in a Pod. By default, containers you define only have access to the default ServiceAccount and its\nrelated Secret. You must explicitly define environment variables or map a volume into a container in order to provide access to any\nother Secret.\nThere may be Secrets for several Pods on the same node. However, only the Secrets that a Pod requests are potentially visible within\nits containers. Therefore, one Pod does not have access to the Secrets of another Pod.\n\nConfigure least-privilege access to Secrets\nTo enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets.\n\nWarning:\nAny containers that run with privileged: true on a node can access all Secrets used on that node.\n\nWhat's next\nFor guidelines to manage and improve the security of your Secrets, refer to Good practices for Kubernetes Secrets.\nLearn how to manage Secrets using kubectl\nLearn how to manage Secrets using config file\nLearn how to manage Secrets using kustomize\nRead the API reference for Secret\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n388/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n7.4 - Liveness, Readiness, and Startup Probes\nKubernetes has various types of probes:\nLiveness probe\nReadiness probe\nStartup probe"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0400", "text": "Liveness probe\nLiveness probes determine when to restart a container. For example, liveness probes could catch a deadlock when an application is\nrunning but unable to make progress.\nIf a container fails its liveness probe repeatedly, the kubelet restarts the container.\nLiveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe, you can either\ndefine initialDelaySeconds or use a startup probe.\n\nReadiness probe\nReadiness probes determine when a container is ready to accept traffic. This is useful when waiting for an application to perform\ntime-consuming initial tasks that depend on its backing services; for example: establishing network connections, loading files, and\nwarming caches. Readiness probes can also be useful later in the containerâ€™s lifecycle, for example, when recovering from temporary\nfaults or overloads.\nIf the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.\nReadiness probes run on the container during its whole lifecycle.\n\nStartup probe\nA startup probe verifies whether the application within a container is started. This can be used to adopt liveness checks on slow\nstarting containers, avoiding them getting killed by the kubelet before they are up and running.\nIf such a probe is configured, it disables liveness and readiness checks until it succeeds.\nThis type of probe is only executed at startup, unlike liveness and readiness probes, which are run periodically.\nRead more about the Configure Liveness, Readiness and Startup Probes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n389/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n7.5 - Resource Management for Pods and Containers\nWhen you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to\nspecify are CPU and memory (RAM); there are others.\nWhen you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to\nplace the Pod on. When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is\nnot allowed to use more of that resource than the limit you set. The kubelet also reserves at least the request amount of that system\nresource specifically for that container to use."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0401", "text": "Requests and limits\nIf the node where a Pod is running has enough of a resource available, it's possible (and allowed) for a container to use more\nresource than its request for that resource specifies.\nFor example, if you set a memory request of 256 MiB for a container, and that container is in a Pod scheduled to a Node with 8GiB of\nmemory and no other Pods, then the container can try to use more RAM.\nLimits are a different story. Both cpu and memory limits are applied by the kubelet (and container runtime), and are ultimately\nenforced by the kernel. On Linux nodes, the Linux kernel enforces limits with cgroups. The behavior of cpu and memory limit\nenforcement is slightly different.\nlimits are enforced by CPU throttling. When a container approaches its cpu limit, the kernel will restrict access to the CPU\ncorresponding to the container's limit. Thus, a cpu limit is a hard limit the kernel enforces. Containers may not use more CPU than\nis specified in their cpu limit.\ncpu\n\nlimits are enforced by the kernel with out of memory (OOM) kills. When a container uses more than its memory limit, the\nkernel may terminate it. However, terminations only happen when the kernel detects memory pressure. Thus, a container that over\nallocates memory may not be immediately killed. This means memory limits are enforced reactively. A container may use more\nmemory than its memory limit, but if it does, it may get killed.\nmemory\n\nNote:\nThere is an alpha feature MemoryQoS which attempts to add more preemptive limit enforcement for memory (as opposed to\nreactive enforcement by the OOM killer). However, this effort is stalled due to a potential livelock situation a memory hungry\ncan cause.\n\nNote:\nIf you specify a limit for a resource, but do not specify any request, and no admission-time mechanism has applied a default\nrequest for that resource, then Kubernetes copies the limit you specified and uses it as the requested value for the resource."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0402", "text": "Resource types\nCPU and memory are each a resource type. A resource type has a base unit. CPU represents compute processing and is specified in\nunits of Kubernetes CPUs. Memory is specified in units of bytes. For Linux workloads, you can specify huge page resources. Huge\npages are a Linux-specific feature where the node kernel allocates blocks of memory that are much larger than the default page size.\nFor example, on a system where the default page size is 4KiB, you could specify a limit, hugepages-2Mi: 80Mi . If the container tries\nallocating over 40 2MiB huge pages (a total of 80 MiB), that allocation fails.\nNote:\nYou cannot overcommit hugepages-* resources. This is different from the memory and cpu resources.\nCPU and memory are collectively referred to as compute resources, or resources. Compute resources are measurable quantities that\ncan be requested, allocated, and consumed. They are distinct from API resources. API resources, such as Pods and Services are\nobjects that can be read and modified through the Kubernetes API server.\nhttps://kubernetes.io/docs/concepts/_print/\n\n390/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nResource requests and limits of Pod and container\nFor each container, you can specify resource limits and requests, including the following:\nspec.containers[].resources.limits.cpu\nspec.containers[].resources.limits.memory\nspec.containers[].resources.limits.hugepages-<size>\nspec.containers[].resources.requests.cpu\nspec.containers[].resources.requests.memory\nspec.containers[].resources.requests.hugepages-<size>\n\nAlthough you can only specify requests and limits for individual containers, it is also useful to think about the overall resource\nrequests and limits for a Pod. For a particular resource, a Pod resource request/limit is the sum of the resource requests/limits of that\ntype for each container in the Pod.\n\nPod-level resource specification\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0403", "text": "Provided your cluster has the PodLevelResources feature gate enabled, you can specify resource requests and limits at the Pod\nlevel. At the Pod level, Kubernetes 1.34 only supports resource requests or limits for specific resource types: cpu and / or memory\nand / or hugepages . With this feature, Kubernetes allows you to declare an overall resource budget for the Pod, which is especially\nhelpful when dealing with a large number of containers where it can be difficult to accurately gauge individual resource needs.\nAdditionally, it enables containers within a Pod to share idle resources with each other, improving resource utilization.\nFor a Pod, you can specify resource limits and requests for CPU and memory by including the following:\nspec.resources.limits.cpu\nspec.resources.limits.memory\nspec.resources.limits.hugepages-<size>\nspec.resources.requests.cpu\nspec.resources.requests.memory\nspec.resources.requests.hugepages-<size>\n\nResource units in Kubernetes\nCPU resource units\nLimits and requests for CPU resources are measured in cpu units. In Kubernetes, 1 CPU unit is equivalent to 1 physical CPU core, or\n1 virtual core, depending on whether the node is a physical host or a virtual machine running inside a physical machine.\nFractional requests are allowed. When you define a container with spec.containers[].resources.requests.cpu set to 0.5 , you are\nrequesting half as much CPU time compared to if you asked for 1.0 CPU. For CPU resource units, the quantity expression 0.1 is\nequivalent to the expression 100m , which can be read as \"one hundred millicpu\". Some people say \"one hundred millicores\", and\nthis is understood to mean the same thing.\nCPU resource is always specified as an absolute amount of resource, never as a relative amount. For example, 500m CPU represents\nthe roughly same amount of computing power whether that container runs on a single-core, dual-core, or 48-core machine.\nNote:\nKubernetes doesn't allow you to specify CPU resources with a precision finer than 1m or 0.001 CPU. To avoid accidentally\nusing an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form instead of the decimal form when using\nless than 1 CPU unit.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n391/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, you have a Pod that uses 5m or 0.005 CPU and would like to decrease its CPU resources. By using the decimal\nform, it's harder to spot that 0.0005 CPU is an invalid value, while by using the milliCPU form, it's easier to spot that 0.5m is\nan invalid value."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0404", "text": "Memory resource units\nLimits and requests for memory are measured in bytes. You can express memory as a plain integer or as a fixed-point number using\none of these quantity suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the\nfollowing represent roughly the same value:\n\n128974848, 129e6, 129M,\n\n128974848000m, 123Mi\n\nPay attention to the case of the suffixes. If you request 400m of memory, this is a request for 0.4 bytes. Someone who types that\nprobably meant to ask for 400 mebibytes ( 400Mi ) or 400 megabytes ( 400M ).\n\nContainer resources example\nThe following Pod has two containers. Both containers are defined with a request for 0.25 CPU and 64MiB (226 bytes) of memory.\nEach container has a limit of 0.5 CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128 MiB of memory,\nand a limit of 1 CPU and 256MiB of memory.\n\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: frontend\nspec:\ncontainers:\n- name: app\nimage: images.my-company.example/app:v4\nresources:\nrequests:\nmemory: \"64Mi\"\ncpu: \"250m\"\nlimits:\nmemory: \"128Mi\"\ncpu: \"500m\"\n- name: log-aggregator\nimage: images.my-company.example/log-aggregator:v6\nresources:\nrequests:\nmemory: \"64Mi\"\ncpu: \"250m\"\nlimits:\nmemory: \"128Mi\"\ncpu: \"500m\"\n\nPod resources example\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)\n\nThis feature can be enabled by setting the PodLevelResources feature gate. The following Pod has an explicit request of 1 CPU and\n100 MiB of memory, and an explicit limit of 1 CPU and 200 MiB of memory. The pod-resources-demo-ctr-1 container has explicit\nrequests and limits set. However, the pod-resources-demo-ctr-2 container will simply share the resources available within the Pod\nresource boundaries, as it does not have explicit requests and limits set.\nhttps://kubernetes.io/docs/concepts/_print/\n\n392/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\npods/resource/pod-level-resources.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod-resources-demo\nnamespace: pod-resources-example\nspec:\nresources:\nlimits:\ncpu: \"1\"\nmemory: \"200Mi\"\nrequests:\ncpu: \"1\"\nmemory: \"100Mi\"\ncontainers:\n- name: pod-resources-demo-ctr-1\nimage: nginx\nresources:\nlimits:\ncpu: \"0.5\"\nmemory: \"100Mi\"\nrequests:\ncpu: \"0.5\"\nmemory: \"50Mi\"\n- name: pod-resources-demo-ctr-2\nimage: fedora\ncommand:\n- sleep\n- inf"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0405", "text": "How Pods with resource requests are scheduled\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each\nof the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type,\nthe sum of the resource requests of the scheduled containers is less than the capacity of the node. Note that although actual\nmemory or CPU resource usage on nodes is very low, the scheduler still refuses to place a Pod on a node if the capacity check fails.\nThis protects against a resource shortage on a node when resource usage later increases, for example, during a daily peak in\nrequest rate."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0406", "text": "How Kubernetes applies resource requests and limits\nWhen the kubelet starts a container as part of a Pod, the kubelet passes that container's requests and limits for memory and CPU to\nthe container runtime.\nOn Linux, the container runtime typically configures kernel cgroups that apply and enforce the limits you defined.\nThe CPU limit defines a hard ceiling on how much CPU time the container can use. During each scheduling interval (time slice),\nthe Linux kernel checks to see if this limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.\nThe CPU request typically defines a weighting. If several different containers (cgroups) want to run on a contended system,\nworkloads with larger CPU requests are allocated more CPU time than workloads with small requests.\nThe memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses cgroups v2, the container\nruntime might use the memory request as a hint to set memory.min and memory.low .\nThe memory limit defines a memory limit for that cgroup. If the container tries to allocate more memory than this limit, the\nLinux kernel out-of-memory subsystem activates and, typically, intervenes by stopping one of the processes in the container\nthat tried to allocate memory. If that process is the container's PID 1, and the container is marked as restartable, Kubernetes\nrestarts the container.\nThe memory limit for the Pod or container can also apply to pages in memory backed volumes, such as an emptyDir . The\nkubelet tracks tmpfs emptyDir volumes as container memory use, rather than as local ephemeral storage. When using\nmemory backed emptyDir , be sure to check the notes below.\nhttps://kubernetes.io/docs/concepts/_print/\n\n393/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf a container exceeds its memory request and the node that it runs on becomes short of memory overall, it is likely that the Pod the\ncontainer belongs to will be evicted.\nA container might or might not be allowed to exceed its CPU limit for extended periods of time. However, container runtimes don't\nterminate Pods or containers for excessive CPU usage.\nTo determine whether a container cannot be scheduled or is being killed due to resource limits, see the Troubleshooting section."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0407", "text": "Monitoring compute & memory resource usage\nThe kubelet reports the resource usage of a Pod as part of the Pod status .\nIf optional tools for monitoring are available in your cluster, then Pod resource usage can be retrieved either from the Metrics API\ndirectly or from your monitoring tools.\n\nConsiderations for memory backed emptyDir volumes\nCaution:\nIf you do not specify a sizeLimit for an emptyDir volume, that volume may consume up to that pod's memory limit\n(Pod.spec.containers[].resources.limits.memory). If you do not set a memory limit, the pod has no upper bound on\nmemory consumption, and can consume all available memory on the node. Kubernetes schedules pods based on resource\nrequests (Pod.spec.containers[].resources.requests) and will not consider memory usage above the request when deciding\nif another pod can fit on a given node. This can result in a denial of service and cause the OS to do out-of-memory (OOM)\nhandling. It is possible to create any number of emptyDirs that could potentially consume all available memory on the node,\nmaking OOM more likely.\nFrom the perspective of memory management, there are some similarities between when a process uses memory as a work area\nand when using memory-backed emptyDir . But when using memory as a volume, like memory-backed emptyDir , there are\nadditional points below that you should be careful of:\nFiles stored on a memory-backed volume are almost entirely managed by the user application. Unlike when used as a work\narea for a process, you can not rely on things like language-level garbage collection.\nThe purpose of writing files to a volume is to save data or pass it between applications. Neither Kubernetes nor the OS may\nautomatically delete files from a volume, so memory used by those files can not be reclaimed when the system or the pod are\nunder memory pressure.\nA memory-backed emptyDir is useful because of its performance, but memory is generally much smaller in size and much\nhigher in cost than other storage media, such as disks or SSDs. Using large amounts of memory for emptyDir volumes may\naffect the normal operation of your pod or of the whole node, so should be used carefully.\nIf you are administering a cluster or namespace, you can also set ResourceQuota that limits memory use; you may also want to\ndefine a LimitRange for additional enforcement. If you specify a spec.containers[].resources.limits.memory for each Pod, then\nthe maximum size of an emptyDir volume will be the pod's memory limit.\nAs an alternative, a cluster administrator can enforce size limits for emptyDir volumes in new Pods using a policy mechanism such\nas ValidationAdmissionPolicy."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0408", "text": "Local ephemeral storage\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nNodes have local ephemeral storage, backed by locally-attached writeable devices or, sometimes, by RAM. \"Ephemeral\" means that\nthere is no long-term guarantee about durability.\nPods use ephemeral local storage for scratch space, caching, and for logs. The kubelet can provide scratch space to Pods using local\nephemeral storage to mount emptyDir volumes into containers.\nThe kubelet also uses this kind of storage to hold node-level container logs, container images, and the writable layers of running\ncontainers.\nhttps://kubernetes.io/docs/concepts/_print/\n\n394/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nCaution:\nIf a node fails, the data in its ephemeral storage can be lost. Your applications cannot expect any performance SLAs (disk IOPS\nfor example) from local ephemeral storage.\n\nNote:\nTo make the resource quota work on ephemeral-storage, two things need to be done:\nAn admin sets the resource quota for ephemeral-storage in a namespace.\nA user needs to specify limits for the ephemeral-storage resource in the Pod spec.\nIf the user doesn't specify the ephemeral-storage resource limit in the Pod spec, the resource quota is not enforced on\nephemeral-storage.\n\nKubernetes lets you track, reserve and limit the amount of ephemeral local storage a Pod can consume.\n\nConfigurations for local ephemeral storage\nKubernetes supports two ways to configure local ephemeral storage on a node:\nSingle filesystem\n\nTwo filesystems\n\nIn this configuration, you place all different kinds of ephemeral local data ( emptyDir volumes,\nwriteable layers, container images, logs) into one filesystem. The most effective way to configure the\nkubelet means dedicating this filesystem to Kubernetes (kubelet) data.\nThe kubelet also writes node-level container logs and treats these similarly to ephemeral local\nstorage.\nThe kubelet writes logs to files inside its configured log directory ( /var/log by default); and has a\nbase directory for other locally stored data ( /var/lib/kubelet by default).\nTypically, both /var/lib/kubelet and /var/log are on the system root filesystem, and the kubelet\nis designed with that layout in mind.\nYour node can have as many other filesystems, not used for Kubernetes, as you like."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0409", "text": "The kubelet can measure how much local storage it is using. It does this provided that you have set up the node using one of the\nsupported configurations for local ephemeral storage.\nIf you have a different configuration, then the kubelet does not apply resource limits for ephemeral local storage.\nNote:\nThe kubelet tracks tmpfs emptyDir volumes as container memory use, rather than as local ephemeral storage.\n\nNote:\nThe kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to\n/var/lib/kubelet or /var/lib/containers will not report ephemeral storage correctly.\n\nSetting requests and limits for local ephemeral storage\nYou can specify ephemeral-storage for managing local ephemeral storage. Each container of a Pod can specify either or both of the\nfollowing:\nspec.containers[].resources.limits.ephemeral-storage\nspec.containers[].resources.requests.ephemeral-storage\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n395/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nLimits and requests for ephemeral-storage are measured in byte quantities. You can express storage as a plain integer or as a\nfixed-point number using one of these suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For\nexample, the following quantities all represent roughly the same value:\n128974848\n129e6\n129M\n123Mi\n\nPay attention to the case of the suffixes. If you request 400m of ephemeral-storage, this is a request for 0.4 bytes. Someone who\ntypes that probably meant to ask for 400 mebibytes ( 400Mi ) or 400 megabytes ( 400M ).\nIn the following example, the Pod has two containers. Each container has a request of 2GiB of local ephemeral storage. Each\ncontainer has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a\nlimit of 8GiB of local ephemeral storage. 500Mi of that limit could be consumed by the emptyDir volume.\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: frontend\nspec:\ncontainers:\n- name: app\nimage: images.my-company.example/app:v4\nresources:\nrequests:\nephemeral-storage: \"2Gi\"\nlimits:\nephemeral-storage: \"4Gi\"\nvolumeMounts:\n- name: ephemeral\nmountPath: \"/tmp\"\n- name: log-aggregator\nimage: images.my-company.example/log-aggregator:v6\nresources:\nrequests:\nephemeral-storage: \"2Gi\"\nlimits:\nephemeral-storage: \"4Gi\"\nvolumeMounts:\n- name: ephemeral\nmountPath: \"/tmp\"\nvolumes:\n- name: ephemeral\nemptyDir:\nsizeLimit: 500Mi"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0410", "text": "How Pods with ephemeral-storage requests are scheduled\nWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum amount of local\nephemeral storage it can provide for Pods. For more information, see Node Allocatable.\nThe scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.\n\nEphemeral storage consumption management\nIf the kubelet is managing local ephemeral storage as a resource, then the kubelet measures storage use in:\nemptyDir\n\nvolumes, except tmpfs emptyDir volumes\n\ndirectories holding node-level logs\nwriteable container layers\nIf a Pod is using more ephemeral storage than you allow it to, the kubelet sets an eviction signal that triggers Pod eviction.\nhttps://kubernetes.io/docs/concepts/_print/\n\n396/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor container-level isolation, if a container's writable layer and log usage exceeds its storage limit, the kubelet marks the Pod for\neviction.\nFor pod-level isolation the kubelet works out an overall Pod storage limit by summing the limits for the containers in that Pod. In this\ncase, if the sum of the local ephemeral storage usage from all containers and also the Pod's emptyDir volumes exceeds the overall\nPod storage limit, then the kubelet also marks the Pod for eviction.\nCaution:\nIf the kubelet is not measuring local ephemeral storage, then a Pod that exceeds its local storage limit will not be evicted for\nbreaching local storage resource limits.\nHowever, if the filesystem space for writeable container layers, node-level logs, or emptyDir volumes falls low, the node taints\nitself as short on local storage and this taint triggers eviction for any Pods that don't specifically tolerate the taint.\nSee the supported configurations for ephemeral local storage.\n\nThe kubelet supports different ways to measure Pod storage use:\nPeriodic scanning\n\nFilesystem project quota\n\nThe kubelet performs regular, scheduled checks that scan each emptyDir volume, container log\ndirectory, and writeable container layer.\nThe scan measures how much space is used.\nNote:\nIn this mode, the kubelet does not track open file descriptors for deleted files.\nIf you (or a container) create a file inside an emptyDir volume, something then opens that file,\nand you delete the file while it is still open, then the inode for the deleted file stays until you\nclose that file but the kubelet does not categorize the space as in use."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0411", "text": "Extended resources\nExtended resources are fully-qualified resource names outside the kubernetes.io domain. They allow cluster operators to advertise\nand users to consume the non-Kubernetes-built-in resources.\nThere are two steps required to use Extended Resources. First, the cluster operator must advertise an Extended Resource. Second,\nusers must request the Extended Resource in Pods.\n\nManaging extended resources\nNode-level extended resources\nNode-level extended resources are tied to nodes.\n\nDevice plugin managed resources\nSee Device Plugin for how to advertise device plugin managed resources on each node.\n\nOther resources\nTo advertise a new node-level extended resource, the cluster operator can submit a PATCH HTTP request to the API server to specify\nthe available quantity in the status.capacity for a node in the cluster. After this operation, the node's status.capacity will\ninclude a new resource. The status.allocatable field is updated automatically with the new resource asynchronously by the\nhttps://kubernetes.io/docs/concepts/_print/\n\n397/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubelet.\nBecause the scheduler uses the node's status.allocatable value when evaluating Pod fitness, the scheduler only takes account of\nthe new value after that asynchronous update. There may be a short delay between patching the node capacity with a new resource\nand the time when the first Pod that requests the resource can be scheduled on that node.\nExample:\nHere is an example showing how to use curl to form an HTTP request that advertises five \"example.com/foo\" resources on node\nk8s-node-1 whose master is k8s-master .\n\ncurl --header \"Content-Type: application/json-patch+json\" \\\n--request PATCH \\\n--data '[{\"op\": \"add\", \"path\": \"/status/capacity/example.com~1foo\", \"value\": \"5\"}]' \\\nhttp://k8s-master:8080/api/v1/nodes/k8s-node-1/status\n\nNote:\nIn the preceding request, ~1 is the encoding for the character / in the patch path. The operation path value in JSON-Patch is\ninterpreted as a JSON-Pointer. For more details, see IETF RFC 6901, section 3.\n\nCluster-level extended resources\nCluster-level extended resources are not tied to nodes. They are usually managed by scheduler extenders, which handle the\nresource consumption and resource quota.\nYou can specify the extended resources that are handled by scheduler extenders in scheduler configuration\nExample:\nThe following configuration for a scheduler policy indicates that the cluster-level extended resource \"example.com/foo\" is handled\nby the scheduler extender.\nThe scheduler sends a Pod to the scheduler extender only if the Pod requests \"example.com/foo\".\nThe ignoredByScheduler field specifies that the scheduler does not check the \"example.com/foo\" resource in its\nPodFitsResources predicate."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0412", "text": "{\n\"kind\": \"Policy\",\n\"apiVersion\": \"v1\",\n\"extenders\": [\n{\n\"urlPrefix\":\"<extender-endpoint>\",\n\"bindVerb\": \"bind\",\n\"managedResources\": [\n{\n\"name\": \"example.com/foo\",\n\"ignoredByScheduler\": true\n}\n]\n}\n]\n}\n\nExtended resources allocation by DRA\nExtended resources allocation by DRA allows cluster administrators to specify an extendedResourceName in DeviceClass, then the\ndevices matching the DeviceClass can be requested from a pod's extended resource requests. Read more about Extended Resource\nallocation by DRA.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n398/684\n\n11/7/25, 4:37 PM\n\nConsuming extended resources\n\nConcepts | Kubernetes\n\nUsers can consume extended resources in Pod specs like CPU and memory. The scheduler takes care of the resource accounting so\nthat no more than the available amount is simultaneously allocated to Pods.\nThe API server restricts quantities of extended resources to whole numbers. Examples of valid quantities are 3 , 3000m and 3Ki .\nExamples of invalid quantities are 0.5 and 1500m (because 1500m would result in 1.5 ).\nNote:\nExtended resources replace Opaque Integer Resources. Users can use any domain name prefix other than kubernetes.io\nwhich is reserved.\nTo consume an extended resource in a Pod, include the resource name as a key in the spec.containers[].resources.limits map\nin the container spec.\nNote:\nExtended resources cannot be overcommitted, so request and limit must be equal if both are present in a container spec.\nA Pod is scheduled only if all of the resource requests are satisfied, including CPU, memory and any extended resources. The Pod\nremains in the PENDING state as long as the resource request cannot be satisfied.\nExample:\nThe Pod below requests 2 CPUs and 1 \"example.com/foo\" (an extended resource).\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: my-pod\nspec:\ncontainers:\n- name: my-container\nimage: myimage\nresources:\nrequests:\ncpu: 2\nexample.com/foo: 1\nlimits:\nexample.com/foo: 1\n\nPID limiting\nProcess ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that a given Pod can consume. See PID\nLimiting for information.\n\nTroubleshooting\nMy Pods are pending with event message FailedScheduling\nIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. An Event is\nproduced each time the scheduler fails to find a place for the Pod. You can use kubectl to view the events for a Pod; for example:\n\nkubectl describe pod frontend | grep -A 9999999999 Events\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n399/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nEvents:\nType\n\nReason\n\nAge\n\nFrom\n\nMessage\n\n---Warning\n\n-----FailedScheduling\n\n---23s\n\n---default-scheduler"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0413", "text": "------0/42 nodes available: insufficient cpu\n\nIn the preceding example, the Pod named \"frontend\" fails to be scheduled due to insufficient CPU resource on any node. Similar\nerror messages can also suggest failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod is pending with a\nmessage of this type, there are several things to try:\nAdd more nodes to the cluster.\nTerminate unneeded Pods to make room for pending Pods.\nCheck that the Pod is not larger than all the nodes. For example, if all the nodes have a capacity of cpu: 1 , then a Pod with a\nrequest of cpu: 1.1 will never be scheduled.\nCheck for node taints. If most of your nodes are tainted, and the new Pod does not tolerate that taint, the scheduler only\nconsiders placements onto the remaining nodes that don't have that taint.\nYou can check node capacities and amounts allocated with the kubectl describe nodes command. For example:\n\nkubectl describe nodes e2e-test-node-pool-4lw4\n\nName:\n\ne2e-test-node-pool-4lw4\n\n[ ... lines removed for clarity ...]\nCapacity:\ncpu:\n\n2\n\nmemory:\n\n7679792Ki\n\npods:\n\n110\n\nAllocatable:\ncpu:\n\n1800m\n\nmemory:\n\n7474992Ki\n\npods:\n110\n[ ... lines removed for clarity ...]\nNon-terminated Pods:\n\n(5 in total)\n\nNamespace\n\nName\n\nCPU Requests\n\nCPU Limits\n\nMemory Requests\n\nMemory Limits\n\n--------kube-system\n\n---fluentd-gcp-v1.38-28bv1\n\n-----------100m (5%)\n\n---------0 (0%)\n\n--------------200Mi (2%)\n\n------------200Mi (2%)\n\nkube-system\n\nkube-dns-3297075139-61lj3\n\n260m (13%)\n\n0 (0%)\n\n100Mi (1%)\n\n170Mi (2%)\n\nkube-system\nkube-system\n\nkube-proxy-e2e-test-...\nmonitoring-influxdb-grafana-v4-z1m12\n\n100m (5%)\n200m (10%)\n\n0 (0%)\n200m (10%)\n\n0 (0%)\n600Mi (8%)\n\n0 (0%)\n600Mi (8%)\n\nkube-system\n\nnode-problem-detector-v0.1-fj7m3\n\n20m (1%)\n\n200m (10%)\n\n20Mi (0%)\n\n100Mi (1%)\n\nAllocated resources:\n(Total limits may be over 100 percent, i.e., overcommitted.)\nCPU Requests\nCPU Limits\nMemory Requests\nMemory Limits\n------------\n\n----------\n\n---------------\n\n-------------\n\n680m (34%)\n\n400m (20%)\n\n920Mi (11%)\n\n1070Mi (13%)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0414", "text": "In the preceding output, you can see that if a Pod requests more than 1.120 CPUs or more than 6.23Gi of memory, that Pod will not\nfit on the node.\nBy looking at the â€œPodsâ€ section, you can see which Pods are taking up space on the node.\nThe amount of resources available to Pods is less than the node capacity because system daemons use a portion of the available\nresources. Within the Kubernetes API, each Node has a .status.allocatable field (see NodeStatus for details).\nThe .status.allocatable field describes the amount of resources that are available to Pods on that node (for example: 15 virtual\nCPUs and 7538 MiB of memory). For more information on node allocatable resources in Kubernetes, see Reserve Compute\nResources for System Daemons.\nYou can configure resource quotas to limit the total amount of resources that a namespace can consume. Kubernetes enforces\nquotas for objects in particular namespace when there is a ResourceQuota in that namespace. For example, if you assign specific\nnamespaces to different teams, you can add ResourceQuotas into those namespaces. Setting resource quotas helps to prevent one\nteam from using so much of any resource that this over-use affects other teams.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n400/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou should also consider what access you grant to that namespace: full write access to a namespace allows someone with that\naccess to remove any resource, including a configured ResourceQuota.\n\nMy container is terminated\nYour container might get terminated because it is resource-starved. To check whether a container is being killed because it is hitting\na resource limit, call kubectl describe pod on the Pod of interest:\n\nkubectl describe pod simmemleak-hra99"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0415", "text": "The output is similar to:\nName:\nsimmemleak-hra99\nNamespace:\ndefault\nImage(s):\nsaadali/simmemleak\nNode:\nkubernetes-node-tf0f/10.240.216.66\nLabels:\nname=simmemleak\nStatus:\nRunning\nReason:\nMessage:\nIP:\n10.244.2.75\nContainers:\nsimmemleak:\nImage: saadali/simmemleak:latest\nLimits:\ncpu:\n100m\nmemory:\n50Mi\nState:\nRunning\nStarted:\nTue, 07 Jul 2019 12:54:41 -0700\nLast State:\nTerminated\nReason:\nOOMKilled\nExit Code:\n137\nStarted:\nFri, 07 Jul 2019 12:54:30 -0700\nFinished:\nFri, 07 Jul 2019 12:54:33 -0700\nReady:\nFalse\nRestart Count: 5\nConditions:\nType\nStatus\nReady\nFalse\nEvents:\nType\nReason\nAge\nFrom\nMessage\n------------ ---------Normal Scheduled 42s\ndefault-scheduler Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f\nNormal Pulled\n41s\nkubelet\nContainer image \"saadali/simmemleak:latest\" already present on machine\nNormal Created\n41s\nkubelet\nCreated container simmemleak\nNormal Started\n40s\nkubelet\nStarted container simmemleak\nNormal Killing\n32s\nkubelet\nKilling container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need t\n\nIn the preceding example, the Restart Count: 5 indicates that the simmemleak container in the Pod was terminated and restarted\nfive times (so far). The OOMKilled reason shows that the container tried to use more memory than its limit.\nYour next step might be to check the application code for a memory leak. If you find that the application is behaving how you expect,\nconsider setting a higher memory limit (and possibly request) for that container.\n\nWhat's next\nGet hands-on experience assigning Memory resources to containers and Pods.\nGet hands-on experience assigning CPU resources to containers and Pods.\nRead how the API reference defines a container and its resource requirements\nRead about project quotas in XFS\nRead more about the kube-scheduler configuration reference (v1)\nRead more about Quality of Service classes for Pods\nhttps://kubernetes.io/docs/concepts/_print/\n\n401/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nRead more about Extended Resource allocation by DRA\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n402/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n7.6 - Organizing Cluster Access Using kubeconfig Files\nUse kubeconfig files to organize information about clusters, users, namespaces, and authentication mechanisms. The kubectl\ncommand-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of\na cluster.\nNote:\nA file that is used to configure access to clusters is called a kubeconfig file. This is a generic way of referring to configuration\nfiles. It does not mean that there is a file named kubeconfig."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0416", "text": "Warning:\nOnly use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code\nexecution or file exposure. If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell\nscript.\nBy default, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting\nthe KUBECONFIG environment variable or by setting the --kubeconfig flag.\nFor step-by-step instructions on creating and specifying kubeconfig files, see Configure Access to Multiple Clusters.\n\nSupporting multiple clusters, users, and authentication\nmechanisms\nSuppose you have several clusters, and your users and components authenticate in a variety of ways. For example:\nA running kubelet might authenticate using certificates.\nA user might authenticate using tokens.\nAdministrators might have sets of certificates that they provide to individual users.\nWith kubeconfig files, you can organize your clusters, users, and namespaces. You can also define contexts to quickly and easily\nswitch between clusters and namespaces.\n\nContext\nA context element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three\nparameters: cluster, namespace, and user. By default, the kubectl command-line tool uses parameters from the current context to\ncommunicate with the cluster.\nTo choose the current context:\nkubectl config use-context\n\nThe KUBECONFIG environment variable\nThe KUBECONFIG environment variable holds a list of kubeconfig files. For Linux and Mac, the list is colon-delimited. For Windows,\nthe list is semicolon-delimited. The KUBECONFIG environment variable is not required. If the KUBECONFIG environment variable\ndoesn't exist, kubectl uses the default kubeconfig file, $HOME/.kube/config .\nIf the KUBECONFIG environment variable does exist, kubectl uses an effective configuration that is the result of merging the files\nlisted in the KUBECONFIG environment variable.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n403/684\n\n11/7/25, 4:37 PM\n\nMerging kubeconfig files\n\nConcepts | Kubernetes\n\nTo see your configuration, enter this command:\n\nkubectl config view"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0417", "text": "As described previously, the output might be from a single kubeconfig file, or it might be the result of merging several kubeconfig\nfiles.\nHere are the rules that kubectl uses when it merges kubeconfig files:\n1. If the --kubeconfig flag is set, use only the specified file. Do not merge. Only one instance of this flag is allowed.\nOtherwise, if the KUBECONFIG environment variable is set, use it as a list of files that should be merged. Merge the files listed in\nthe KUBECONFIG environment variable according to these rules:\nIgnore empty filenames.\nProduce errors for files with content that cannot be deserialized.\nThe first file to set a particular value or map key wins.\nNever change the value or map key. Example: Preserve the context of the first file to set current-context . Example: If\ntwo files specify a red-user , use only values from the first file's red-user . Even if the second file has non-conflicting\nentries under red-user , discard them.\nFor an example of setting the KUBECONFIG environment variable, see Setting the KUBECONFIG environment variable.\nOtherwise, use the default kubeconfig file, $HOME/.kube/config , with no merging.\n2. Determine the context to use based on the first hit in this chain:\n1. Use the --context command-line flag if it exists.\n2. Use the current-context from the merged kubeconfig files.\nAn empty context is allowed at this point.\n3. Determine the cluster and user. At this point, there might or might not be a context. Determine the cluster and user based on\nthe first hit in this chain, which is run twice: once for user and once for cluster:\n1. Use a command-line flag if it exists: --user or --cluster .\n2. If the context is non-empty, take the user or cluster from the context.\nThe user and cluster can be empty at this point.\n4. Determine the actual cluster information to use. At this point, there might or might not be cluster information. Build each piece\nof the cluster information based on this chain; the first hit wins:\n1. Use command line flags if they exist: --server , --certificate-authority , --insecure-skip-tls-verify .\n2. If any cluster information attributes exist from the merged kubeconfig files, use them.\n3. If there is no server location, fail.\n5. Determine the actual user information to use. Build user information using the same rules as cluster information, except allow\nonly one authentication technique per user:\n1. Use command line flags if they exist: --client-certificate , --client-key , --username , --password , --token .\n2. Use the user fields from the merged kubeconfig files.\n3. If there are two conflicting techniques, fail.\n6. For any information still missing, use default values and potentially prompt for authentication information."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0418", "text": "File references\nFile and path references in a kubeconfig file are relative to the location of the kubeconfig file. File references on the command line\nare relative to the current working directory. In $HOME/.kube/config , relative paths are stored relatively, and absolute paths are\nstored absolutely.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n404/684\n\n11/7/25, 4:37 PM\n\nProxy\n\nConcepts | Kubernetes\n\nYou can configure kubectl to use a proxy per cluster using proxy-url in your kubeconfig file, like this:\n\napiVersion: v1\nkind: Config\nclusters:\n- cluster:\nproxy-url: http://proxy.example.org:3128\nserver: https://k8s.example.org/k8s/clusters/c-xxyyzz\nname: development\nusers:\n- name: developer\ncontexts:\n- context:\nname: development\n\nWhat's next\nConfigure Access to Multiple Clusters\nkubectl config\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n405/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n7.7 - Resource Management for Windows nodes\nThis page outlines the differences in how resources are managed between Linux and Windows.\nOn Linux nodes, cgroups are used as a pod boundary for resource control. Containers are created within that boundary for network,\nprocess and file system isolation. The Linux cgroup APIs can be used to gather CPU, I/O, and memory use statistics.\nIn contrast, Windows uses a job object per container with a system namespace filter to contain all processes in a container and\nprovide logical isolation from the host. (Job objects are a Windows process isolation mechanism and are different from what\nKubernetes refers to as a Job).\nThere is no way to run a Windows container without the namespace filtering in place. This means that system privileges cannot be\nasserted in the context of the host, and thus privileged containers are not available on Windows. Containers cannot assume an\nidentity from the host because the Security Account Manager (SAM) is separate.\n\nMemory management\nWindows does not have an out-of-memory process killer as Linux does. Windows always treats all user-mode memory allocations as\nvirtual, and pagefiles are mandatory.\nWindows nodes do not overcommit memory for processes. The net effect is that Windows won't reach out of memory conditions the\nsame way Linux does, and processes page to disk instead of being subject to out of memory (OOM) termination. If memory is overprovisioned and all physical memory is exhausted, then paging can slow down performance."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0419", "text": "CPU management\nWindows can limit the amount of CPU time allocated for different processes but cannot guarantee a minimum amount of CPU time.\nOn Windows, the kubelet supports a command-line flag to set the scheduling priority of the kubelet process: --windowspriorityclass . This flag allows the kubelet process to get more CPU time slices when compared to other processes running on the\nWindows host. More information on the allowable values and their meaning is available at Windows Priority Classes. To ensure that\nrunning Pods do not starve the kubelet of CPU cycles, set this flag to ABOVE_NORMAL_PRIORITY_CLASS or above.\n\nResource reservation\nTo account for memory and CPU used by the operating system, the container runtime, and by Kubernetes host processes such as\nthe kubelet, you can (and should) reserve memory and CPU resources with the --kube-reserved and/or --system-reserved\nkubelet flags. On Windows these values are only used to calculate the node's allocatable resources.\nCaution:\nAs you deploy workloads, set resource memory and CPU limits on containers. This also subtracts from NodeAllocatable and\nhelps the cluster-wide scheduler in determining which pods to place on which nodes.\nScheduling pods without limits may over-provision the Windows nodes and in extreme cases can cause the nodes to become\nunhealthy.\n\nOn Windows, a good practice is to reserve at least 2GiB of memory.\nTo determine how much CPU to reserve, identify the maximum pod density for each node and monitor the CPU usage of the system\nservices running there, then choose a value that meets your workload needs.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n406/684\n\n11/7/25, 4:37 PM\n\n8 - Security\n\nConcepts | Kubernetes\n\nConcepts for keeping your cloud-native workload secure.\nThis section of the Kubernetes documentation aims to help you learn to run workloads more securely, and about the essential\naspects of keeping a Kubernetes cluster secure.\nKubernetes is based on a cloud-native architecture, and draws on advice from the CNCF about good practice for cloud native\ninformation security.\nRead Cloud Native Security and Kubernetes for the broader context about how to secure your cluster and the applications that\nyou're running on it.\n\nKubernetes security mechanisms\nKubernetes includes several APIs and security controls, as well as ways to define policies that can form part of how you manage\ninformation security."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0420", "text": "Control plane protection\nA key security mechanism for any Kubernetes cluster is to control access to the Kubernetes API.\nKubernetes expects you to configure and use TLS to provide data encryption in transit within the control plane, and between the\ncontrol plane and its clients. You can also enable encryption at rest for the data stored within Kubernetes control plane; this is\nseparate from using encryption at rest for your own workloads' data, which might also be a good idea.\n\nSecrets\nThe Secret API provides basic protection for configuration values that require confidentiality.\n\nWorkload protection\nEnforce Pod security standards to ensure that Pods and their containers are isolated appropriately. You can also use RuntimeClasses\nto define custom isolation if you need it.\nNetwork policies let you control network traffic between Pods, or between Pods and the network outside your cluster.\nYou can deploy security controls from the wider ecosystem to implement preventative or detective controls around Pods, their\ncontainers, and the images that run in them.\n\nAdmission control\nAdmission controllers are plugins that intercept Kubernetes API requests and can validate or mutate the requests based on specific\nfields in the request. Thoughtfully designing these controllers helps to avoid unintended disruptions as Kubernetes APIs change\nacross version updates. For design considerations, see Admission Webhook Good Practices.\n\nAuditing\nKubernetes audit logging provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.\nThe cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.\n\nCloud provider security\nNote: Items on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those\nthird-party products or projects. To add a vendor, product or project to this list, read the content guide before submitting a\nchange. More information.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n407/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security\nbest practices. Here are links to some of the popular cloud providers' security documentation:\nIaaS Provider\n\nLink\n\nAlibaba Cloud\n\nhttps://www.alibabacloud.com/trust-center\n\nAmazon Web Services\n\nhttps://aws.amazon.com/security\n\nGoogle Cloud Platform\n\nhttps://cloud.google.com/security\n\nHuawei Cloud\n\nhttps://www.huaweicloud.com/intl/en-us/securecenter/overallsafety\n\nIBM Cloud\n\nhttps://www.ibm.com/cloud/security\n\nMicrosoft Azure\n\nhttps://docs.microsoft.com/en-us/azure/security/azure-security\n\nOracle Cloud Infrastructure\n\nhttps://www.oracle.com/security\n\nTencent Cloud"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0421", "text": "https://www.tencentcloud.com/solutions/data-security-and-information-protection\n\nVMware vSphere\n\nhttps://www.vmware.com/solutions/security/hardening-guides\n\nPolicies\nYou can define security policies using Kubernetes-native mechanisms, such as NetworkPolicy (declarative control over network\npacket filtering) or ValidatingAdmissionPolicy (declarative restrictions on what changes someone can make using the Kubernetes\nAPI).\nHowever, you can also rely on policy implementations from the wider ecosystem around Kubernetes. Kubernetes provides extension\nmechanisms to let those ecosystem projects implement their own policy controls on source code review, container image approval,\nAPI access controls, networking, and more.\nFor more information about policy mechanisms and Kubernetes, read Policies.\n\nWhat's next\nLearn about related Kubernetes security topics:\nSecuring your cluster\nKnown vulnerabilities in Kubernetes (and links to further information)\nData encryption in transit for the control plane\nData encryption at rest\nControlling Access to the Kubernetes API\nNetwork policies for Pods\nSecrets in Kubernetes\nPod security standards\nRuntimeClasses\nLearn the context:\nCloud Native Security and Kubernetes\nGet certified:\nCertified Kubernetes Security Specialist certification and official training course.\nRead more in this section:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n408/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.1 - Cloud Native Security and Kubernetes\nConcepts for keeping your cloud-native workload secure.\nKubernetes is based on a cloud-native architecture, and draws on advice from the CNCF about good practice for cloud native\ninformation security.\nRead on through this page for an overview of how Kubernetes is designed to help you deploy a secure cloud native platform.\n\nCloud native information security\nThe CNCF white paper on cloud native security defines security controls and practices that are appropriate to different lifecycle\nphases.\n\nDevelop lifecycle phase\nEnsure the integrity of development environments.\nDesign applications following good practice for information security, appropriate for your context.\nConsider end user security as part of solution design.\nTo achieve this, you can:\n1. Adopt an architecture, such as zero trust, that minimizes attack surfaces, even for internal threats.\n2. Define a code review process that considers security concerns.\n3. Build a threat model of your system or application that identifies trust boundaries. Use that to model to identify risks and to\nhelp find ways to treat those risks.\n4. Incorporate advanced security automation, such as fuzzing and security chaos engineering, where it's justified."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0422", "text": "Distribute lifecycle phase\nEnsure the security of the supply chain for container images you execute.\nEnsure the security of the supply chain for the cluster and other components that execute your application. An example of\nanother component might be an external database that your cloud-native application uses for persistence.\nTo achieve this, you can:\n1. Scan container images and other artifacts for known vulnerabilities.\n2. Ensure that software distribution uses encryption in transit, with a chain of trust for the software source.\n3. Adopt and follow processes to update dependencies when updates are available, especially in response to security\nannouncements.\n4. Use validation mechanisms such as digital certificates for supply chain assurance.\n5. Subscribe to feeds and other mechanisms to alert you to security risks.\n6. Restrict access to artifacts. Place container images in a private registry that only allows authorized clients to pull images.\n\nDeploy lifecycle phase\nEnsure appropriate restrictions on what can be deployed, who can deploy it, and where it can be deployed to. You can enforce\nmeasures from the distribute phase, such as verifying the cryptographic identity of container image artifacts.\nYou can deploy different applications and cluster components into different namespaces. Containers themselves, and namespaces,\nboth provide isolation mechanisms that are relevant to information security.\nWhen you deploy Kubernetes, you also set the foundation for your applications' runtime environment: a Kubernetes cluster (or\nmultiple clusters). That IT infrastructure must provide the security guarantees that higher layers expect.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n409/684\n\n11/7/25, 4:37 PM\n\nRuntime lifecycle phase\n\nConcepts | Kubernetes\n\nThe Runtime phase comprises three critical areas: access, compute, and storage.\n\nRuntime protection: access\nThe Kubernetes API is what makes your cluster work. Protecting this API is key to providing effective cluster security.\nOther pages in the Kubernetes documentation have more detail about how to set up specific aspects of access control. The security\nchecklist has a set of suggested basic checks for your cluster.\nBeyond that, securing your cluster means implementing effective authentication and authorization for API access. Use\nServiceAccounts to provide and manage security identities for workloads and cluster components.\nKubernetes uses TLS to protect API traffic; make sure to deploy the cluster using TLS (including for traffic between nodes and the\ncontrol plane), and protect the encryption keys. If you use Kubernetes' own API for CertificateSigningRequests, pay special attention\nto restricting misuse there."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0423", "text": "Runtime protection: compute\nContainers provide two things: isolation between different applications, and a mechanism to combine those isolated applications to\nrun on the same host computer. Those two aspects, isolation and aggregation, mean that runtime security involves identifying tradeoffs and finding an appropriate balance.\nKubernetes relies on a container runtime to actually set up and run containers. The Kubernetes project does not recommend a\nspecific container runtime and you should make sure that the runtime(s) that you choose meet your information security needs.\nTo protect your compute at runtime, you can:\n1. Enforce Pod security standards for applications, to help ensure they run with only the necessary privileges.\n2. Run a specialized operating system on your nodes that is designed specifically for running containerized workloads. This is\ntypically based on a read-only operating system (immutable image) that provides only the services essential for running\ncontainers.\nContainer-specific operating systems help to isolate system components and present a reduced attack surface in case of a\ncontainer escape.\n3. Define ResourceQuotas to fairly allocate shared resources, and use mechanisms such as LimitRanges to ensure that Pods\nspecify their resource requirements.\n4. Partition workloads across different nodes. Use node isolation mechanisms, either from Kubernetes itself or from the\necosystem, to ensure that Pods with different trust contexts are run on separate sets of nodes.\n5. Use a container runtime that provides security restrictions.\n6. On Linux nodes, use a Linux security module such as AppArmor or seccomp.\n\nRuntime protection: storage\nTo protect storage for your cluster and the applications that run there, you can:\n1. Integrate your cluster with an external storage plugin that provides encryption at rest for volumes.\n2. Enable encryption at rest for API objects.\n3. Protect data durability using backups. Verify that you can restore these, whenever you need to.\n4. Authenticate connections between cluster nodes and any network storage they rely upon.\n5. Implement data encryption within your own application.\nFor encryption keys, generating these within specialized hardware provides the best protection against disclosure risks. A hardware\nsecurity module can let you perform cryptographic operations without allowing the security key to be copied elsewhere.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n410/684\n\n11/7/25, 4:37 PM\n\nNetworking and security\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0424", "text": "You should also consider network security measures, such as NetworkPolicy or a service mesh. Some network plugins for\nKubernetes provide encryption for your cluster network, using technologies such as a virtual private network (VPN) overlay. By\ndesign, Kubernetes lets you use your own networking plugin for your cluster (if you use managed Kubernetes, the person or\norganization managing your cluster may have chosen a network plugin for you).\nThe network plugin you choose and the way you integrate it can have a strong impact on the security of information in transit.\n\nObservability and runtime security\nKubernetes lets you extend your cluster with extra tooling. You can set up third party solutions to help you monitor or troubleshoot\nyour applications and the clusters they are running. You also get some basic observability features built in to Kubernetes itself. Your\ncode running in containers can generate logs, publish metrics or provide other observability data; at deploy time, you need to make\nsure your cluster provides an appropriate level of protection there.\nIf you set up a metrics dashboard or something similar, review the chain of components that populate data into that dashboard, as\nwell as the dashboard itself. Make sure that the whole chain is designed with enough resilience and enough integrity protection that\nyou can rely on it even during an incident where your cluster might be degraded.\nWhere appropriate, deploy security measures below the level of Kubernetes itself, such as cryptographically measured boot, or\nauthenticated distribution of time (which helps ensure the fidelity of logs and audit records).\nFor a high assurance environment, deploy cryptographic protections to ensure that logs are both tamper-proof and confidential.\n\nWhat's next\nCloud native security\nCNCF white paper on cloud native security.\nCNCF white paper on good practices for securing a software supply chain.\nFixing the Kubernetes clusterf**k: Understanding security from the kernel up (FOSDEM 2020)\nKubernetes Security Best Practices (Kubernetes Forum Seoul 2019)\nTowards Measured Boot Out of the Box (Linux Security Summit 2016)\n\nKubernetes and information security\nKubernetes security\nSecuring your cluster\nData encryption in transit for the control plane\nData encryption at rest\nSecrets in Kubernetes\nControlling Access to the Kubernetes API\nNetwork policies for Pods\nPod security standards\nRuntimeClasses\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n411/684\n\n11/7/25, 4:37 PM\n\n8.2 - Pod Security Standards\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0425", "text": "A detailed look at the different policy levels defined in the Pod Security Standards.\nThe Pod Security Standards define three different policies to broadly cover the security spectrum. These policies are cumulative and\nrange from highly-permissive to highly-restrictive. This guide outlines the requirements of each policy.\nProfile\n\nDescription\n\nPrivileged\n\nUnrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege\nescalations.\n\nBaseline\n\nMinimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod\nconfiguration.\n\nRestricted\n\nHeavily restricted policy, following current Pod hardening best practices.\n\nProfile Details\nPrivileged\nThe Privileged policy is purposely-open, and entirely unrestricted. This type of policy is typically aimed at system- and\ninfrastructure-level workloads managed by privileged, trusted users.\nThe Privileged policy is defined by an absence of restrictions. If you define a Pod where the Privileged security policy applies, the Pod\nyou define is able to bypass typical container isolation mechanisms. For example, you can define a Pod that has access to the node's\nhost network.\n\nBaseline\nThe Baseline policy is aimed at ease of adoption for common containerized workloads while preventing known privilege\nescalations. This policy is targeted at application operators and developers of non-critical applications. The following listed controls\nshould be enforced/disallowed:\nNote:\nIn this table, wildcards (*) indicate all elements in a list. For example, spec.containers[*].securityContext refers to the\nSecurity Context object for all defined containers. If any of the listed containers fails to meet the requirements, the entire pod\nwill fail validation.\n\nControl\n\nhttps://kubernetes.io/docs/concepts/_print/\n\nPolicy\n\n412/684\n\n11/7/25, 4:37 PM\n\nHostProcess\n\nConcepts | Kubernetes\n\nWindows Pods offer the ability to run HostProcess containers which enables privileged access to the\nWindows host machine. Privileged access to the host is disallowed in the Baseline policy.\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]\n\nRestricted Fields\nspec.securityContext.windowsOptions.hostProcess\nspec.containers[*].securityContext.windowsOptions.hostProcess\nspec.initContainers[*].securityContext.windowsOptions.hostProcess\nspec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess\n\nAllowed Values\nUndefined/nil\nfalse\n\nHost Namespaces\n\nSharing the host namespaces must be disallowed.\nRestricted Fields\nspec.hostNetwork\nspec.hostPID\nspec.hostIPC\n\nAllowed Values\nUndefined/nil\nfalse\n\nPrivileged Containers"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0426", "text": "Privileged Pods disable most security mechanisms and must be disallowed.\nRestricted Fields\nspec.containers[*].securityContext.privileged\nspec.initContainers[*].securityContext.privileged\nspec.ephemeralContainers[*].securityContext.privileged\n\nAllowed Values\nUndefined/nil\nfalse\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n413/684\n\n11/7/25, 4:37 PM\n\nCapabilities\n\nConcepts | Kubernetes\n\nAdding additional capabilities beyond those listed below must be disallowed.\nRestricted Fields\nspec.containers[*].securityContext.capabilities.add\nspec.initContainers[*].securityContext.capabilities.add\nspec.ephemeralContainers[*].securityContext.capabilities.add\n\nAllowed Values\nUndefined/nil\nAUDIT_WRITE\nCHOWN\nDAC_OVERRIDE\nFOWNER\nFSETID\nKILL\nMKNOD\nNET_BIND_SERVICE\nSETFCAP\nSETGID\nSETPCAP\nSETUID\nSYS_CHROOT\n\nHostPath Volumes\n\nHostPath volumes must be forbidden.\nRestricted Fields\nspec.volumes[*].hostPath\n\nAllowed Values\nUndefined/nil\nHost Ports\n\nHostPorts should be disallowed entirely (recommended) or restricted to a known list\nRestricted Fields\nspec.containers[*].ports[*].hostPort\nspec.initContainers[*].ports[*].hostPort\nspec.ephemeralContainers[*].ports[*].hostPort\n\nAllowed Values\nUndefined/nil\nKnown list (not supported by the built-in Pod Security Admission controller)\n0\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n414/684\n\n11/7/25, 4:37 PM\n\nHost Probes /\nLifecycle Hooks\n(v1.34+)\n\nConcepts | Kubernetes\n\nThe Host field in probes and lifecycle hooks must be disallowed.\nRestricted Fields\nspec.containers[*].livenessProbe.httpGet.host\nspec.containers[*].readinessProbe.httpGet.host\nspec.containers[*].startupProbe.httpGet.host\nspec.containers[*].livenessProbe.tcpSocket.host\nspec.containers[*].readinessProbe.tcpSocket.host\nspec.containers[*].startupProbe.tcpSocket.host\nspec.containers[*].lifecycle.postStart.tcpSocket.host\nspec.containers[*].lifecycle.preStop.tcpSocket.host\nspec.containers[*].lifecycle.postStart.httpGet.host\nspec.containers[*].lifecycle.preStop.httpGet.host\nspec.initContainers[*].livenessProbe.httpGet.host\nspec.initContainers[*].readinessProbe.httpGet.host\nspec.initContainers[*].startupProbe.httpGet.host\nspec.initContainers[*].livenessProbe.tcpSocket.host\nspec.initContainers[*].readinessProbe.tcpSocket.host\nspec.initContainers[*].startupProbe.tcpSocket.host\nspec.initContainers[*].lifecycle.postStart.tcpSocket.host\nspec.initContainers[*].lifecycle.preStop.tcpSocket.host\nspec.initContainers[*].lifecycle.postStart.httpGet.host\nspec.initContainers[*].lifecycle.preStop.httpGet.host\n\nAllowed Values\nUndefined/nil\n\"\"\nAppArmor"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0427", "text": "On supported hosts, the RuntimeDefault AppArmor profile is applied by default. The baseline policy\nshould prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed\nset of profiles.\nRestricted Fields\nspec.securityContext.appArmorProfile.type\nspec.containers[*].securityContext.appArmorProfile.type\nspec.initContainers[*].securityContext.appArmorProfile.type\nspec.ephemeralContainers[*].securityContext.appArmorProfile.type\n\nAllowed Values\nUndefined/nil\nRuntimeDefault\nLocalhost\n\nmetadata.annotations[\"container.apparmor.security.beta.kubernetes.io/*\"]\n\nAllowed Values\nUndefined/nil\nruntime/default\nlocalhost/*\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n415/684\n\n11/7/25, 4:37 PM\n\nSELinux\n\nConcepts | Kubernetes\n\nSetting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.\nRestricted Fields\nspec.securityContext.seLinuxOptions.type\nspec.containers[*].securityContext.seLinuxOptions.type\nspec.initContainers[*].securityContext.seLinuxOptions.type\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.type\n\nAllowed Values\nUndefined/\"\"\ncontainer_t\ncontainer_init_t\ncontainer_kvm_t\ncontainer_engine_t (since Kubernetes 1.31)\n\nRestricted Fields\nspec.securityContext.seLinuxOptions.user\nspec.containers[*].securityContext.seLinuxOptions.user\nspec.initContainers[*].securityContext.seLinuxOptions.user\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.user\nspec.securityContext.seLinuxOptions.role\nspec.containers[*].securityContext.seLinuxOptions.role\nspec.initContainers[*].securityContext.seLinuxOptions.role\nspec.ephemeralContainers[*].securityContext.seLinuxOptions.role\n\nAllowed Values\nUndefined/\"\"\n/proc Mount Type\n\nThe default /proc masks are set up to reduce attack surface, and should be required.\nRestricted Fields\nspec.containers[*].securityContext.procMount\nspec.initContainers[*].securityContext.procMount\nspec.ephemeralContainers[*].securityContext.procMount\n\nAllowed Values\nUndefined/nil\nDefault\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n416/684\n\n11/7/25, 4:37 PM\n\nSeccomp\n\nConcepts | Kubernetes\n\nSeccomp profile must not be explicitly set to Unconfined .\nRestricted Fields\nspec.securityContext.seccompProfile.type\nspec.containers[*].securityContext.seccompProfile.type\nspec.initContainers[*].securityContext.seccompProfile.type\nspec.ephemeralContainers[*].securityContext.seccompProfile.type\n\nAllowed Values\nUndefined/nil\nRuntimeDefault\nLocalhost\n\nSysctls\n\nSysctls can disable security mechanisms or affect all containers on a host, and should be disallowed\nexcept for an allowed \"safe\" subset. A sysctl is considered safe if it is namespaced in the container or the\nPod, and it is isolated from other Pods or processes on the same Node.\nRestricted Fields\nspec.securityContext.sysctls[*].name"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0428", "text": "Allowed Values\nUndefined/nil\nkernel.shm_rmid_forced\nnet.ipv4.ip_local_port_range\nnet.ipv4.ip_unprivileged_port_start\nnet.ipv4.tcp_syncookies\nnet.ipv4.ping_group_range\nnet.ipv4.ip_local_reserved_ports (since Kubernetes 1.27)\nnet.ipv4.tcp_keepalive_time (since Kubernetes 1.29)\nnet.ipv4.tcp_fin_timeout (since Kubernetes 1.29)\nnet.ipv4.tcp_keepalive_intvl (since Kubernetes 1.29)\nnet.ipv4.tcp_keepalive_probes (since Kubernetes 1.29)\n\nRestricted\nThe Restricted policy is aimed at enforcing current Pod hardening best practices, at the expense of some compatibility. It is\ntargeted at operators and developers of security-critical applications, as well as lower-trust users. The following listed controls\nshould be enforced/disallowed:\nNote:\nIn this table, wildcards (*) indicate all elements in a list. For example, spec.containers[*].securityContext refers to the\nSecurity Context object for all defined containers. If any of the listed containers fails to meet the requirements, the entire pod\nwill fail validation.\n\nControl\n\nPolicy\n\nEverything from the Baseline policy\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n417/684\n\n11/7/25, 4:37 PM\n\nVolume Types\n\nConcepts | Kubernetes\n\nThe Restricted policy only permits the following volume types.\nRestricted Fields\nspec.volumes[*]\n\nAllowed Values\nEvery item in the spec.volumes[*] list must set one of the following fields to a non-null\nvalue:\nspec.volumes[*].configMap\nspec.volumes[*].csi\nspec.volumes[*].downwardAPI\nspec.volumes[*].emptyDir\nspec.volumes[*].ephemeral\nspec.volumes[*].persistentVolumeClaim\nspec.volumes[*].projected\nspec.volumes[*].secret\n\nPrivilege Escalation (v1.8+)\n\nPrivilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed.\nThis is Linux only policy in v1.25+ (spec.os.name != windows)\nRestricted Fields\nspec.containers[*].securityContext.allowPrivilegeEscalation\nspec.initContainers[*].securityContext.allowPrivilegeEscalation\nspec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation\n\nAllowed Values\nfalse\n\nRunning as Non-root\n\nContainers must be required to run as non-root users.\nRestricted Fields\nspec.securityContext.runAsNonRoot\nspec.containers[*].securityContext.runAsNonRoot\nspec.initContainers[*].securityContext.runAsNonRoot\nspec.ephemeralContainers[*].securityContext.runAsNonRoot\n\nAllowed Values\ntrue\n\nThe container fields may be undefined/ nil if the pod-level spec.securityContext.runAsNonRoot is\nset to true .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n418/684\n\n11/7/25, 4:37 PM\n\nRunning as Non-root user (v1.23+)\n\nConcepts | Kubernetes\n\nContainers must not set runAsUser to 0\nRestricted Fields\nspec.securityContext.runAsUser\nspec.containers[*].securityContext.runAsUser\nspec.initContainers[*].securityContext.runAsUser\nspec.ephemeralContainers[*].securityContext.runAsUser"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0429", "text": "Allowed Values\nany non-zero value\nundefined/null\n\nSeccomp (v1.19+)\n\nSeccomp profile must be explicitly set to one of the allowed values. Both the Unconfined\nprofile and the absence of a profile are prohibited. This is Linux only policy in v1.25+\n(spec.os.name != windows)\n\nRestricted Fields\nspec.securityContext.seccompProfile.type\nspec.containers[*].securityContext.seccompProfile.type\nspec.initContainers[*].securityContext.seccompProfile.type\nspec.ephemeralContainers[*].securityContext.seccompProfile.type\n\nAllowed Values\nRuntimeDefault\nLocalhost\n\nThe container fields may be undefined/ nil if the pod-level\nspec.securityContext.seccompProfile.type field is set appropriately. Conversely, the pod-level\nfield may be undefined/ nil if _all_ container- level fields are set.\n\nCapabilities (v1.22+)\n\nContainers must drop ALL capabilities, and are only permitted to add back the\nNET_BIND_SERVICE capability. This is Linux only policy in v1.25+ (.spec.os.name !=\n\"windows\")\n\nRestricted Fields\nspec.containers[*].securityContext.capabilities.drop\nspec.initContainers[*].securityContext.capabilities.drop\nspec.ephemeralContainers[*].securityContext.capabilities.drop\n\nAllowed Values\nAny list of capabilities that includes ALL\nRestricted Fields\nspec.containers[*].securityContext.capabilities.add\nspec.initContainers[*].securityContext.capabilities.add\nspec.ephemeralContainers[*].securityContext.capabilities.add\n\nAllowed Values\nUndefined/nil\nNET_BIND_SERVICE\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n419/684\n\n11/7/25, 4:37 PM\n\nPolicy Instantiation\n\nConcepts | Kubernetes\n\nDecoupling policy definition from policy instantiation allows for a common understanding and consistent language of policies across\nclusters, independent of the underlying enforcement mechanism.\nAs mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement of individual policies are not\ndefined here.\nPod Security Admission Controller\nPrivileged namespace\nBaseline namespace\nRestricted namespace\n\nAlternatives\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nOther alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:\nKubewarden\nKyverno\nOPA Gatekeeper"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0430", "text": "Pod OS field\nKubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of node in one cluster. Windows in\nKubernetes has some limitations and differentiators from Linux-based workloads. Specifically, many of the Pod securityContext\nfields have no effect on Windows.\nNote:\nKubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the Restricted\npolicies should be pinned to a version prior to v1.25.\n\nRestricted Pod Security Standard changes\nAnother important change, made in Kubernetes v1.25 is that the Restricted policy has been updated to use the pod.spec.os.name\nfield. Based on the OS name, certain policies that are specific to a particular OS can be relaxed for the other OS.\n\nOS-specific policy controls\nRestrictions on the following controls are only required if .spec.os.name is not windows :\nPrivilege Escalation\nSeccomp\nLinux Capabilities\n\nUser namespaces\nUser Namespaces are a Linux-only feature to run workloads with increased isolation. How they work together with Pod Security\nStandards is described in the documentation for Pods that use user namespaces.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n420/684\n\n11/7/25, 4:37 PM\n\nFAQ\n\nConcepts | Kubernetes\n\nWhy isn't there a profile between Privileged and Baseline?\nThe three profiles defined here have a clear linear progression from most secure (Restricted) to least secure (Privileged), and cover a\nbroad set of workloads. Privileges required above the Baseline policy are typically very application specific, so we do not offer a\nstandard profile in this niche. This is not to say that the privileged profile should always be used in this case, but that policies in this\nspace need to be defined on a case-by-case basis.\nSIG Auth may reconsider this position in the future, should a clear need for other profiles arise.\n\nWhat's the difference between a security profile and a security context?\nSecurity Contexts configure Pods and Containers at runtime. Security contexts are defined as part of the Pod and container\nspecifications in the Pod manifest, and represent parameters to the container runtime.\nSecurity profiles are control plane mechanisms to enforce specific settings in the Security Context, as well as other related\nparameters outside the Security Context. As of July 2021, Pod Security Policies are deprecated in favor of the built-in Pod Security\nAdmission Controller."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0431", "text": "What about sandboxed Pods?\nThere is currently no API standard that controls whether a Pod is considered sandboxed or not. Sandbox Pods may be identified by\nthe use of a sandboxed runtime (such as gVisor or Kata Containers), but there is no standard definition of what a sandboxed\nruntime is.\nThe protections necessary for sandboxed workloads can differ from others. For example, the need to restrict privileged permissions\nis lessened when the workload is isolated from the underlying kernel. This allows for workloads requiring heightened permissions to\nstill be isolated.\nAdditionally, the protection of sandboxed workloads is highly dependent on the method of sandboxing. As such, no single\nrecommended profile is recommended for all sandboxed workloads.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n421/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.3 - Pod Security Admission\n\nAn overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.\nâ“˜ FEATURE STATE: Kubernetes v1.25 [stable]\n\nThe Kubernetes Pod Security Standards define different isolation levels for Pods. These standards let you define how you want to\nrestrict the behavior of pods in a clear, consistent fashion.\nKubernetes offers a built-in Pod Security admission controller to enforce the Pod Security Standards. Pod security restrictions are\napplied at the namespace level when pods are created.\n\nBuilt-in Pod Security admission enforcement\nThis page is part of the documentation for Kubernetes v1.34. If you are running a different version of Kubernetes, consult the\ndocumentation for that release.\n\nPod Security levels\nPod Security admission places requirements on a Pod's Security Context and other related fields according to the three levels\ndefined by the Pod Security Standards: privileged , baseline , and restricted . Refer to the Pod Security Standards page for an\nin-depth look at those requirements.\n\nPod Security Admission labels for namespaces\nOnce the feature is enabled or the webhook is installed, you can configure namespaces to define the admission control mode you\nwant to use for pod security in each namespace. Kubernetes defines a set of labels that you can set to define which of the\npredefined Pod Security Standard levels you want to use for a namespace. The label you select defines what action the control plane\ntakes if a potential violation is detected:\nMode\n\nDescription\n\nenforce\n\nPolicy violations will cause the pod to be rejected.\n\naudit"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0432", "text": "Policy violations will trigger the addition of an audit annotation to the event recorded in the audit log, but are\notherwise allowed.\n\nwarn\n\nPolicy violations will trigger a user-facing warning, but are otherwise allowed.\n\nA namespace can configure any or all modes, or even set a different level for different modes.\nFor each mode, there are two labels that determine the policy used:\n\n# The per-mode level label indicates which policy level to apply for the mode.\n#\n# MODE must be one of `enforce`, `audit`, or `warn`.\n# LEVEL must be one of `privileged`, `baseline`, or `restricted`.\npod-security.kubernetes.io/<MODE>: <LEVEL>\n# Optional: per-mode version label that can be used to pin the policy to the\n# version that shipped with a given Kubernetes minor version (for example v1.34).\n#\n# MODE must be one of `enforce`, `audit`, or `warn`.\n# VERSION must be a valid Kubernetes minor version, or `latest`.\npod-security.kubernetes.io/<MODE>-version: <VERSION>\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n422/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nCheck out Enforce Pod Security Standards with Namespace Labels to see example usage.\n\nWorkload resources and Pod templates\nPods are often created indirectly, by creating a workload object such as a Deployment or Job. The workload object defines a Pod\ntemplate and a controller for the workload resource creates Pods based on that template. To help catch violations early, both the\naudit and warning modes are applied to the workload resources. However, enforce mode is not applied to workload resources, only\nto the resulting pod objects."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0433", "text": "Exemptions\nYou can define exemptions from pod security enforcement in order to allow the creation of pods that would have otherwise been\nprohibited due to the policy associated with a given namespace. Exemptions can be statically configured in the Admission Controller\nconfiguration.\nExemptions must be explicitly enumerated. Requests meeting exemption criteria are ignored by the Admission Controller (all\nenforce , audit and warn behaviors are skipped). Exemption dimensions include:\nUsernames: requests from users with an exempt authenticated (or impersonated) username are ignored.\nRuntimeClassNames: pods and workload resources specifying an exempt runtime class name are ignored.\nNamespaces: pods and workload resources in an exempt namespace are ignored.\nCaution:\nMost pods are created by a controller in response to a workload resource, meaning that exempting an end user will only\nexempt them from enforcement when creating pods directly, but not when creating a workload resource. Controller service\naccounts (such as system:serviceaccount:kube-system:replicaset-controller) should generally not be exempted, as doing\nso would implicitly exempt any user that can create the corresponding workload resource.\nUpdates to the following pod fields are exempt from policy checks, meaning that if a pod update request only changes these fields, it\nwill not be denied even if the pod is in violation of the current policy level:\nAny metadata updates except changes to the seccomp or AppArmor annotations:\nseccomp.security.alpha.kubernetes.io/pod (deprecated)\ncontainer.seccomp.security.alpha.kubernetes.io/*\n\n(deprecated)\n\ncontainer.apparmor.security.beta.kubernetes.io/*\n\n(deprecated)\n\nValid updates to .spec.activeDeadlineSeconds\nValid updates to .spec.tolerations\n\nMetrics\nHere are the Prometheus metrics exposed by kube-apiserver:\npod_security_errors_total : This metric indicates the number of errors preventing normal evaluation. Non-fatal errors may\n\nresult in the latest restricted profile being used for enforcement.\npod_security_evaluations_total : This metric indicates the number of policy evaluations that have occurred, not counting\nignored or exempt requests during exporting.\npod_security_exemptions_total : This metric indicates the number of exempt requests, not counting ignored or out of scope\nrequests.\n\nWhat's next\nPod Security Standards\nEnforcing Pod Security Standards\nEnforce Pod Security Standards by Configuring the Built-in Admission Controller\nhttps://kubernetes.io/docs/concepts/_print/\n\n423/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0434", "text": "Enforce Pod Security Standards with Namespace Labels\nIf you are running an older version of Kubernetes and want to upgrade to a version of Kubernetes that does not include\nPodSecurityPolicies, read migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n424/684\n\n11/7/25, 4:37 PM\n\n8.4 - Service Accounts\n\nConcepts | Kubernetes\n\nLearn about ServiceAccount objects in Kubernetes.\nThis page introduces the ServiceAccount object in Kubernetes, providing information about how service accounts work, use cases,\nlimitations, alternatives, and links to resources for additional guidance.\n\nWhat are service accounts?\nA service account is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster. Application\nPods, system components, and entities inside and outside the cluster can use a specific ServiceAccount's credentials to identify as\nthat ServiceAccount. This identity is useful in various situations, including authenticating to the API server or implementing identitybased security policies.\nService accounts exist as ServiceAccount objects in the API server. Service accounts have the following properties:\nNamespaced: Each service account is bound to a Kubernetes namespace. Every namespace gets a default ServiceAccount\nupon creation.\nLightweight: Service accounts exist in the cluster and are defined in the Kubernetes API. You can quickly create service\naccounts to enable specific tasks.\nPortable: A configuration bundle for a complex containerized workload might include service account definitions for the\nsystem's components. The lightweight nature of service accounts and the namespaced identities make the configurations\nportable.\nService accounts are different from user accounts, which are authenticated human users in the cluster. By default, user accounts\ndon't exist in the Kubernetes API server; instead, the API server treats user identities as opaque data. You can authenticate as a user\naccount using multiple methods. Some Kubernetes distributions might add custom extension APIs to represent user accounts in the\nAPI server.\nDescription\n\nServiceAccount\n\nUser or group\n\nLocation\n\nKubernetes API (ServiceAccount object)\n\nExternal\n\nAccess\ncontrol\n\nKubernetes RBAC or other authorization\nmechanisms\n\nKubernetes RBAC or other identity and access management\nmechanisms\n\nIntended use\n\nWorkloads, automation\n\nPeople"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0435", "text": "Default service accounts\nWhen you create a cluster, Kubernetes automatically creates a ServiceAccount object named default for every namespace in your\ncluster. The default service accounts in each namespace get no permissions by default other than the default API discovery\npermissions that Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled. If you delete the\ndefault ServiceAccount object in a namespace, the control plane replaces it with a new one.\nIf you deploy a Pod in a namespace, and you don't manually assign a ServiceAccount to the Pod, Kubernetes assigns the default\nServiceAccount for that namespace to the Pod.\n\nUse cases for Kubernetes service accounts\nAs a general guideline, you can use service accounts to provide identities in the following scenarios:\nYour Pods need to communicate with the Kubernetes API server, for example in situations such as the following:\nProviding read-only access to sensitive information stored in Secrets.\nGranting cross-namespace access, such as allowing a Pod in namespace example to read, list, and watch for Lease\nobjects in the kube-node-lease namespace.\nYour Pods need to communicate with an external service. For example, a workload Pod requires an identity for a commercially\navailable cloud API, and the commercial provider allows configuring a suitable trust relationship.\nhttps://kubernetes.io/docs/concepts/_print/\n\n425/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAuthenticating to a private image registry using an imagePullSecret.\nAn external service needs to communicate with the Kubernetes API server. For example, authenticating to the cluster as part of\na CI/CD pipeline.\nYou use third-party security software in your cluster that relies on the ServiceAccount identity of different Pods to group those\nPods into different contexts.\n\nHow to use service accounts\nTo use a Kubernetes service account, you do the following:\n1. Create a ServiceAccount object using a Kubernetes client like kubectl or a manifest that defines the object.\n2. Grant permissions to the ServiceAccount object using an authorization mechanism such as RBAC.\n3. Assign the ServiceAccount object to Pods during Pod creation.\nIf you're using the identity from an external service, retrieve the ServiceAccount token and use it from that service instead.\nFor instructions, refer to Configure Service Accounts for Pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0436", "text": "Grant permissions to a ServiceAccount\nYou can use the built-in Kubernetes role-based access control (RBAC) mechanism to grant the minimum permissions required by\neach service account. You create a role, which grants access, and then bind the role to your ServiceAccount. RBAC lets you define a\nminimum set of permissions so that the service account permissions follow the principle of least privilege. Pods that use that service\naccount don't get more permissions than are required to function correctly.\nFor instructions, refer to ServiceAccount permissions.\n\nCross-namespace access using a ServiceAccount\nYou can use RBAC to allow service accounts in one namespace to perform actions on resources in a different namespace in the\ncluster. For example, consider a scenario where you have a service account and Pod in the dev namespace and you want your Pod\nto see Jobs running in the maintenance namespace. You could create a Role object that grants permissions to list Job objects. Then,\nyou'd create a RoleBinding object in the maintenance namespace to bind the Role to the ServiceAccount object. Now, Pods in the\ndev namespace can list Job objects in the maintenance namespace using that service account.\n\nAssign a ServiceAccount to a Pod\nTo assign a ServiceAccount to a Pod, you set the spec.serviceAccountName field in the Pod specification. Kubernetes then\nautomatically provides the credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes gets a short-lived,\nautomatically rotating token using the TokenRequest API and mounts the token as a projected volume.\nBy default, Kubernetes provides the Pod with the credentials for an assigned ServiceAccount, whether that is the default\nServiceAccount or a custom ServiceAccount that you specify.\nTo prevent Kubernetes from automatically injecting credentials for a specified ServiceAccount or the default ServiceAccount, set\nthe automountServiceAccountToken field in your Pod specification to false .\nIn versions earlier than 1.22, Kubernetes provides a long-lived, static token to the Pod as a Secret."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0437", "text": "Manually retrieve ServiceAccount credentials\nIf you need the credentials for a ServiceAccount to mount in a non-standard location, or for an audience that isn't the API server, use\none of the following methods:\nTokenRequest API (recommended): Request a short-lived service account token from within your own application code. The\ntoken expires automatically and can rotate upon expiration. If you have a legacy application that is not aware of Kubernetes,\nyou could use a sidecar container within the same pod to fetch these tokens and make them available to the application\nworkload.\nToken Volume Projection (also recommended): In Kubernetes v1.20 and later, use the Pod specification to tell the kubelet to\nadd the service account token to the Pod as a projected volume. Projected tokens expire automatically, and the kubelet rotates\nthe token before it expires.\nhttps://kubernetes.io/docs/concepts/_print/\n\n426/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nService Account Token Secrets (not recommended): You can mount service account tokens as Kubernetes Secrets in Pods.\nThese tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each\nservice account. This method is not recommended anymore, especially at scale, because of the risks associated with static,\nlong-lived credentials. The LegacyServiceAccountTokenNoAutoGeneration feature gate (which was enabled by default from\nKubernetes v1.24 to v1.26), prevented Kubernetes from automatically creating these tokens for ServiceAccounts. The feature\ngate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually,\nbut should take into account the security implications.\nNote:\nFor applications running outside your Kubernetes cluster, you might be considering creating a long-lived ServiceAccount token\nthat is stored in a Secret. This allows authentication, but the Kubernetes project recommends you avoid this approach. Longlived bearer tokens represent a security risk as, once disclosed, the token can be misused. Instead, consider using an\nalternative. For example, your external application can authenticate using a well-protected private key and a certificate, or\nusing a custom mechanism such as an authentication webhook that you implement yourself.\nYou can also use TokenRequest to obtain short-lived tokens for your external application.\n\nRestricting access to Secrets (deprecated)\nâ“˜ FEATURE STATE: Kubernetes v1.32 [deprecated]\n\nNote:\n\nkubernetes.io/enforce-mountable-secrets is deprecated since Kubernetes v1.32. Use separate namespaces to isolate access"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0438", "text": "to mounted secrets.\nKubernetes provides an annotation called kubernetes.io/enforce-mountable-secrets that you can add to your ServiceAccounts.\nWhen this annotation is applied, the ServiceAccount's secrets can only be mounted on specified types of resources, enhancing the\nsecurity posture of your cluster.\nYou can add the annotation to a ServiceAccount using a manifest:\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nannotations:\nkubernetes.io/enforce-mountable-secrets: \"true\"\nname: my-serviceaccount\nnamespace: my-namespace\n\nWhen this annotation is set to \"true\", the Kubernetes control plane ensures that the Secrets from this ServiceAccount are subject to\ncertain mounting restrictions.\n1. The name of each Secret that is mounted as a volume in a Pod must appear in the secrets field of the Pod's ServiceAccount.\n2. The name of each Secret referenced using envFrom in a Pod must also appear in the secrets field of the Pod's\nServiceAccount.\n3. The name of each Secret referenced using imagePullSecrets in a Pod must also appear in the secrets field of the Pod's\nServiceAccount.\nBy understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that\nsecrets are accessed only by the appropriate resources.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n427/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0439", "text": "Authenticating service account credentials\nServiceAccounts use signed JSON Web Tokens (JWTs) to authenticate to the Kubernetes API server, and to any other system where a\ntrust relationship exists. Depending on how the token was issued (either time-limited using a TokenRequest or using a legacy\nmechanism with a Secret), a ServiceAccount token might also have an expiry time, an audience, and a time after which the token\nstarts being valid. When a client that is acting as a ServiceAccount tries to communicate with the Kubernetes API server, the client\nincludes an Authorization: Bearer <token> header with the HTTP request. The API server checks the validity of that bearer token\nas follows:\n1. Checks the token signature.\n2. Checks whether the token has expired.\n3. Checks whether object references in the token claims are currently valid.\n4. Checks whether the token is currently valid.\n5. Checks the audience claims.\nThe TokenRequest API produces bound tokens for a ServiceAccount. This binding is linked to the lifetime of the client, such as a Pod,\nthat is acting as that ServiceAccount. See Token Volume Projection for an example of a bound pod service account token's JWT\nschema and payload.\nFor tokens issued using the TokenRequest API, the API server also checks that the specific object reference that is using the\nServiceAccount still exists, matching by the unique ID of that object. For legacy tokens that are mounted as Secrets in Pods, the API\nserver checks the token against the Secret.\nFor more information about the authentication process, refer to Authentication."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0440", "text": "Authenticating service account credentials in your own code\nIf you have services of your own that need to validate Kubernetes service account credentials, you can use the following methods:\nTokenReview API (recommended)\nOIDC discovery\nThe Kubernetes project recommends that you use the TokenReview API, because this method invalidates tokens that are bound to\nAPI objects such as Secrets, ServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you delete the Pod that\ncontains a projected ServiceAccount token, the cluster invalidates that token immediately and a TokenReview immediately fails. If\nyou use OIDC validation instead, your clients continue to treat the token as valid until the token reaches its expiration timestamp.\nYour application should always define the audience that it accepts, and should check that the token's audiences match the\naudiences that the application expects. This helps to minimize the scope of the token so that it can only be used in your application\nand nowhere else.\n\nAlternatives\nIssue your own tokens using another mechanism, and then use Webhook Token Authentication to validate bearer tokens using\nyour own validation service.\nProvide your own identities to Pods.\nUse the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods.\nðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. More information\n\nUse a service mesh such as Istio to provide certificates to Pods.\nAuthenticate from outside the cluster to the API server without using service account tokens:\nConfigure the API server to accept OpenID Connect (OIDC) tokens from your identity provider.\nUse service accounts or user accounts created using an external Identity and Access Management (IAM) service, such as\nfrom a cloud provider, to authenticate to your cluster.\nUse the CertificateSigningRequest API with client certificates.\nConfigure the kubelet to retrieve credentials from an image registry.\nUse a Device Plugin to access a virtual Trusted Platform Module (TPM), which then allows authentication using a private key.\nhttps://kubernetes.io/docs/concepts/_print/\n\n428/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nLearn how to manage your ServiceAccounts as a cluster administrator.\nLearn how to assign a ServiceAccount to a Pod.\nRead the ServiceAccount API reference.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n429/684\n\n11/7/25, 4:37 PM\n\n8.5 - Pod Security Policies\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0441", "text": "Removed feature\nPodSecurityPolicy was deprecated in Kubernetes v1.21, and removed from Kubernetes in v1.25.\nInstead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using either or both:\nPod Security Admission\na 3rd party admission plugin, that you deploy and configure yourself\nFor a migration guide, see Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller. For more information on\nthe removal of this API, see PodSecurityPolicy Deprecation: Past, Present, and Future.\nIf you are not running Kubernetes v1.34, check the documentation for your version of Kubernetes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n430/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.6 - Security For Linux Nodes\n\nThis page describes security considerations and best practices specific to the Linux operating system.\n\nProtection for Secret data on nodes\nOn Linux nodes, memory-backed volumes (such as secret volume mounts, or emptyDir with medium: Memory ) are implemented\nwith a tmpfs filesystem.\nIf you have swap configured and use an older Linux kernel (or a current kernel and an unsupported configuration of Kubernetes),\nmemory backed volumes can have data written to persistent storage.\nThe Linux kernel officially supports the noswap option from version 6.3, therefore it is recommended the used kernel version is 6.3\nor later, or supports the noswap option via a backport, if swap is enabled on the node.\nRead swap memory management for more info.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n431/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.7 - Security For Windows Nodes\n\nThis page describes security considerations and best practices specific to the Windows operating system.\n\nProtection for Secret data on nodes\nOn Windows, data from Secrets are written out in clear text onto the node's local storage (as compared to using tmpfs / in-memory\nfilesystems on Linux). As a cluster operator, you should take both of the following additional measures:\n1. Use file ACLs to secure the Secrets' file location.\n2. Apply volume-level encryption using BitLocker."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0442", "text": "Container users\nRunAsUsername can be specified for Windows Pods or containers to execute the container processes as specific user. This is roughly\nequivalent to RunAsUser.\nWindows containers offer two default user accounts, ContainerUser and ContainerAdministrator. The differences between these two\nuser accounts are covered in When to use ContainerAdmin and ContainerUser user accounts within Microsoft's Secure Windows\ncontainers documentation.\nLocal users can be added to container images during the container build process.\nNote:\nNano Server based images run as ContainerUser by default\nServer Core based images run as ContainerAdministrator by default\n\nWindows containers can also run as Active Directory identities by utilizing Group Managed Service Accounts\n\nPod-level security isolation\nLinux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom POSIX capabilities) are not\nsupported on Windows nodes.\nPrivileged containers are not supported on Windows. Instead HostProcess containers can be used on Windows to perform many of\nthe tasks performed by privileged containers on Linux.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n432/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.8 - Controlling Access to the Kubernetes API\nThis page provides an overview of controlling access to the Kubernetes API.\nUsers access the Kubernetes API using kubectl , client libraries, or by making REST requests. Both human users and Kubernetes\nservice accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the\nfollowing diagram:\n\nTransport security\nBy default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical\nproduction Kubernetes cluster, the API serves on port 443. The port can be changed with the --secure-port , and the listening IP\naddress with the --bind-address flag.\nThe API server presents a certificate. This certificate may be signed using a private certificate authority (CA), or based on a public key\ninfrastructure linked to a generally recognized CA. The certificate and corresponding private key can be set by using the --tls-certfile and --tls-private-key-file flags.\nIf your cluster uses a private certificate authority, you need a copy of that CA certificate configured into your ~/.kube/config on the\nclient, so that you can trust the connection and be confident it was not intercepted.\nYour client can present a TLS client certificate at this stage."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0443", "text": "Authentication\nOnce TLS is established, the HTTP request moves to the Authentication step. This is shown as step 1 in the diagram. The cluster\ncreation script or cluster admin configures the API server to run one or more Authenticator modules. Authenticators are described in\nmore detail in Authentication.\nThe input to the authentication step is the entire HTTP request; however, it typically examines the headers and/or client certificate.\nAuthentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for\nservice accounts).\nMultiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n433/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific\nusername , and the user name is available to subsequent steps to use in their decisions. Some authenticators also provide the group\nmemberships of the user, while other authenticators do not.\nWhile Kubernetes uses usernames for access control decisions and in request logging, it does not have a User object nor does it\nstore usernames or other information about users in its API.\n\nAuthorization\nAfter the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step 2 in the\ndiagram.\nA request must include the username of the requester, the requested action, and the object affected by the action. The request is\nauthorized if an existing policy declares that the user has permissions to complete the requested action.\nFor example, if Bob has the policy below, then he can read pods only in the namespace projectCaribou :\n\n{\n\"apiVersion\": \"abac.authorization.kubernetes.io/v1beta1\",\n\"kind\": \"Policy\",\n\"spec\": {\n\"user\": \"bob\",\n\"namespace\": \"projectCaribou\",\n\"resource\": \"pods\",\n\"readonly\": true\n}\n}\n\nIf Bob makes the following request, the request is authorized because he is allowed to read objects in the projectCaribou\nnamespace:\n\n{\n\"apiVersion\": \"authorization.k8s.io/v1beta1\",\n\"kind\": \"SubjectAccessReview\",\n\"spec\": {\n\"resourceAttributes\": {\n\"namespace\": \"projectCaribou\",\n\"verb\": \"get\",\n\"group\": \"unicorn.example.org\",\n\"resource\": \"pods\"\n}\n}\n}"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0444", "text": "If Bob makes a request to write ( create or update ) to the objects in the projectCaribou namespace, his authorization is denied. If\nBob makes a request to read ( get ) objects in a different namespace such as projectFish , then his authorization is denied.\nKubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloudprovider-wide access control systems. It is important to use REST formatting because these control systems might interact with other\nAPIs besides the Kubernetes API.\nKubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator\ncreates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization\nmodules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed.\nIf all of the modules deny the request, then the request is denied (HTTP status code 403).\nTo learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules,\nsee Authorization.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n434/684\n\n11/7/25, 4:37 PM\n\nAdmission control\n\nConcepts | Kubernetes\n\nAdmission Control modules are software modules that can modify or reject requests. In addition to the attributes available to\nAuthorization modules, Admission Control modules can access the contents of the object that is being created or modified.\nAdmission controllers act on requests that create, modify, delete, or connect to (proxy) an object. Admission controllers do not act\non requests that merely read objects. When multiple admission controllers are configured, they are called in order.\nThis is shown as step 3 in the diagram.\nUnlike Authentication and Authorization modules, if any admission controller module rejects, then the request is immediately\nrejected.\nIn addition to rejecting objects, admission controllers can also set complex defaults for fields.\nThe available Admission Control modules are described in Admission Controllers.\nOnce a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and\nthen written to the object store (shown as step 4).\n\nAuditing\nKubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster. The\ncluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.\nFor more information, see Auditing."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0445", "text": "What's next\nRead more documentation on authentication, authorization and API access control:\nAuthenticating\nAuthenticating with Bootstrap Tokens\nAdmission Controllers\nDynamic Admission Control\nAuthorization\nRole Based Access Control\nAttribute Based Access Control\nNode Authorization\nWebhook Authorization\nCertificate Signing Requests\nincluding CSR approval and certificate signing\nService accounts\nDeveloper guide\nAdministration\nYou can learn about:\nhow Pods can use Secrets to obtain API credentials.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n435/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.9 - Role Based Access Control Good Practices\nPrinciples and practices for good RBAC design for cluster operators.\nKubernetes RBAC is a key security control to ensure that cluster users and workloads have only the access to resources required to\nexecute their roles. It is important to ensure that, when designing permissions for cluster users, the cluster administrator\nunderstands the areas where privilege escalation could occur, to reduce the risk of excessive access leading to security incidents.\nThe good practices laid out here should be read in conjunction with the general RBAC documentation.\n\nGeneral good practice\nLeast privilege\nIdeally, minimal RBAC rights should be assigned to users and service accounts. Only permissions explicitly required for their\noperation should be used. While each cluster will be different, some general rules that can be applied are :\nAssign permissions at the namespace level where possible. Use RoleBindings as opposed to ClusterRoleBindings to give users\nrights only within a specific namespace.\nAvoid providing wildcard permissions when possible, especially to all resources. As Kubernetes is an extensible system,\nproviding wildcard access gives rights not just to all object types that currently exist in the cluster, but also to all object types\nwhich are created in the future.\nAdministrators should not use cluster-admin accounts except where specifically needed. Providing a low privileged account\nwith impersonation rights can avoid accidental modification of cluster resources.\nAvoid adding users to the system:masters group. Any user who is a member of this group bypasses all RBAC rights checks and\nwill always have unrestricted superuser access, which cannot be revoked by removing RoleBindings or ClusterRoleBindings. As\nan aside, if a cluster is using an authorization webhook, membership of this group also bypasses that webhook (requests from\nusers who are members of that group are never sent to the webhook)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0446", "text": "Minimize distribution of privileged tokens\nIdeally, pods shouldn't be assigned service accounts that have been granted powerful permissions (for example, any of the rights\nlisted under privilege escalation risks). In cases where a workload requires powerful permissions, consider the following practices:\nLimit the number of nodes running powerful pods. Ensure that any DaemonSets you run are necessary and are run with least\nprivilege to limit the blast radius of container escapes.\nAvoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using Taints and Toleration, NodeAffinity,\nor PodAntiAffinity to ensure pods don't run alongside untrusted or less-trusted Pods. Pay special attention to situations where\nless-trustworthy Pods are not meeting the Restricted Pod Security Standard.\n\nHardening\nKubernetes defaults to providing access which may not be required in every cluster. Reviewing the RBAC rights provided by default\ncan provide opportunities for security hardening. In general, changes should not be made to rights provided to system: accounts\nsome options to harden cluster rights exist:\nReview bindings for the system:unauthenticated group and remove them where possible, as this gives access to anyone who\ncan contact the API server at a network level.\nAvoid the default auto-mounting of service account tokens by setting automountServiceAccountToken: false . For more details,\nsee using default service account token. Setting this value for a Pod will overwrite the service account setting, workloads which\nrequire service account tokens can still mount them.\n\nPeriodic review\nIt is vital to periodically review the Kubernetes RBAC settings for redundant entries and possible privilege escalations. If an attacker\nis able to create a user account with the same name as a deleted user, they can automatically inherit all the rights of the deleted\nuser, especially the rights assigned to that user.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n436/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nKubernetes RBAC - privilege escalation risks\nWithin Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account to escalate their\nprivileges in the cluster or affect systems outside the cluster.\nThis section is intended to provide visibility of the areas where cluster operators should take care, to ensure that they do not\ninadvertently allow for more access to clusters than intended."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0447", "text": "Listing secrets\nIt is generally clear that allowing get access on Secrets will allow a user to read their contents. It is also important to note that\nlist and watch access also effectively allow for users to reveal the Secret contents. For example, when a List response is returned\n(for example, via kubectl get secrets -A -o yaml ), the response includes the contents of all Secrets.\n\nWorkload creation\nPermission to create workloads (either Pods, or workload resources that manage Pods) in a namespace implicitly grants access to\nmany other resources in that namespace, such as Secrets, ConfigMaps, and PersistentVolumes that can be mounted in Pods.\nAdditionally, since Pods can run as any ServiceAccount, granting permission to create workloads also implicitly grants the API access\nlevels of any service account in that namespace.\nUsers who can run privileged Pods can use that access to gain node access and potentially to further elevate their privileges. Where\nyou do not fully trust a user or other principal with the ability to create suitably secure and isolated Pods, you should enforce either\nthe Baseline or Restricted Pod Security Standard. You can use Pod Security admission or other (third party) mechanisms to\nimplement that enforcement.\nFor these reasons, namespaces should be used to separate resources requiring different levels of trust or tenancy. It is still\nconsidered best practice to follow least privilege principles and assign the minimum set of permissions, but boundaries within a\nnamespace should be considered weak."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0448", "text": "Persistent volume creation\nIf someone - or some application - is allowed to create arbitrary PersistentVolumes, that access includes the creation of hostPath\nvolumes, which then means that a Pod would get access to the underlying host filesystem(s) on the associated node. Granting that\nability is a security risk.\nThere are many ways a container with unrestricted access to the host filesystem can escalate privileges, including reading data from\nother containers, and abusing the credentials of system services, such as Kubelet.\nYou should only allow access to create PersistentVolume objects for:\nUsers (cluster operators) that need this access for their work, and who you trust.\nThe Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaims that are\nconfigured for automatic provisioning. This is usually setup by the Kubernetes provider or by the operator when installing a CSI\ndriver.\nWhere access to persistent storage is required trusted administrators should create PersistentVolumes, and constrained users\nshould use PersistentVolumeClaims to access that storage.\n\nAccess to proxy subresource of Nodes\nUsers with access to the proxy sub-resource of node objects have rights to the Kubelet API, which allows for command execution on\nevery pod on the node(s) to which they have rights. This access bypasses audit logging and admission control, so care should be\ntaken before granting rights to this resource.\n\nEscalate verb\nGenerally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses. The exception to\nthis is the escalate verb. As noted in the RBAC documentation, users with this right can effectively escalate their privileges.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n437/684\n\n11/7/25, 4:37 PM\n\nBind verb\n\nConcepts | Kubernetes\n\nSimilar to the escalate verb, granting users this right allows for the bypass of Kubernetes in-built protections against privilege\nescalation, allowing users to create bindings to roles with rights they do not already have.\n\nImpersonate verb\nThis verb allows users to impersonate and gain the rights of other users in the cluster. Care should be taken when granting it, to\nensure that excessive permissions cannot be gained via one of the impersonated accounts."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0449", "text": "CSRs and certificate issuing\nThe CSR API allows for users with create rights to CSRs and update rights on certificatesigningrequests/approval where the\nsigner is kubernetes.io/kube-apiserver-client to create new client certificates which allow users to authenticate to the cluster.\nThose client certificates can have arbitrary names including duplicates of Kubernetes system components. This will effectively allow\nfor privilege escalation.\n\nToken request\nUsers with create rights on serviceaccounts/token can create TokenRequests to issue tokens for existing service accounts.\n\nControl admission webhooks\nUsers with control over validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read\nany object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects.\n\nNamespace modification\nUsers who can perform patch operations on Namespace objects (through a namespaced RoleBinding to a Role with that access) can\nmodify labels on that namespace. In clusters where Pod Security Admission is used, this may allow a user to configure the\nnamespace for a more permissive policy than intended by the administrators. For clusters where NetworkPolicy is used, users may\nbe set labels that indirectly allow access to services that an administrator did not intend to allow.\n\nKubernetes RBAC - denial of service risks\nObject creation denial-of-service\nUsers who have rights to create objects in a cluster may be able to create sufficient large objects to create a denial of service\ncondition either based on the size or number of objects, as discussed in etcd used by Kubernetes is vulnerable to OOM attack. This\nmay be specifically relevant in multi-tenant clusters if semi-trusted or untrusted users are allowed limited access to a system.\nOne option for mitigation of this issue would be to use resource quotas to limit the quantity of objects which can be created.\n\nWhat's next\nTo learn more about RBAC, see the RBAC documentation.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n438/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0450", "text": "8.10 - Good practices for Kubernetes Secrets\nPrinciples and practices for good Secret management for cluster administrators and application developers.\nIn Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.\nSecrets give you more control over how sensitive information is used and reduces the risk of accidental exposure. Secret values are\nencoded as base64 strings and are stored unencrypted by default, but can be configured to be encrypted at rest.\nA Pod can reference the Secret in a variety of ways, such as in a volume mount or as an environment variable. Secrets are designed\nfor confidential data and ConfigMaps are designed for non-confidential data.\nThe following good practices are intended for both cluster administrators and application developers. Use these guidelines to\nimprove the security of your sensitive information in Secret objects, as well as to more effectively manage your Secrets.\n\nCluster administrators\nThis section provides good practices that cluster administrators can use to improve the security of confidential information in the\ncluster.\n\nConfigure encryption at rest\nBy default, Secret objects are stored unencrypted in etcd. You should configure encryption of your Secret data in etcd . For\ninstructions, refer to Encrypt Secret Data at Rest."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0451", "text": "Configure least-privilege access to Secrets\nWhen planning your access control mechanism, such as Kubernetes Role-based Access Control (RBAC), consider the following\nguidelines for access to Secret objects. You should also follow the other guidelines in RBAC good practices.\nComponents: Restrict watch or list access to only the most privileged, system-level components. Only grant get access for\nSecrets if the component's normal behavior requires it.\nHumans: Restrict get , watch , or list access to Secrets. Only allow cluster administrators to access etcd . This includes\nread-only access. For more complex access control, such as restricting access to Secrets with specific annotations, consider\nusing third-party authorization mechanisms.\nCaution:\nGranting list access to Secrets implicitly lets the subject fetch the contents of the Secrets.\nA user who can create a Pod that uses a Secret can also see the value of that Secret. Even if cluster policies do not allow a user to\nread the Secret directly, the same user could have access to run a Pod that then exposes the Secret. You can detect or limit the\nimpact caused by Secret data being exposed, either intentionally or unintentionally, by a user with this access. Some\nrecommendations include:\nUse short-lived Secrets\nImplement audit rules that alert on specific events, such as concurrent reading of multiple Secrets by a single user\n\nRestrict Access for Secrets\nUse separate namespaces to isolate access to mounted secrets.\n\nImprove etcd management policies\nConsider wiping or shredding the durable storage used by etcd once it is no longer in use.\nIf there are multiple etcd instances, configure encrypted SSL/TLS communication between the instances to protect the Secret data\nin transit.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n439/684\n\n11/7/25, 4:37 PM\n\nConfigure access to external Secrets\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0452", "text": "Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nYou can use third-party Secrets store providers to keep your confidential data outside your cluster and then configure Pods to\naccess that information. The Kubernetes Secrets Store CSI Driver is a DaemonSet that lets the kubelet retrieve Secrets from external\nstores, and mount the Secrets as a volume into specific Pods that you authorize to access the data.\nFor a list of supported providers, refer to Providers for the Secret Store CSI Driver.\n\nGood practices for using swap memory\nFor best practices for setting swap memory for Linux nodes, please refer to swap memory management.\n\nDevelopers\nThis section provides good practices for developers to use to improve the security of confidential data when building and deploying\nKubernetes resources.\n\nRestrict Secret access to specific containers\nIf you are defining multiple containers in a Pod, and only one of those containers needs access to a Secret, define the volume mount\nor environment variable configuration so that the other containers do not have access to that Secret.\n\nProtect Secret data after reading\nApplications still need to protect the value of confidential information after reading it from an environment variable or volume. For\nexample, your application must avoid logging the secret data in the clear or transmitting it to an untrusted party.\n\nAvoid sharing Secret manifests\nIf you configure a Secret through a manifest, with the secret data encoded as base64, sharing this file or checking it in to a source\nrepository means the secret is available to everyone who can read the manifest.\nCaution:\nBase64 encoding is not an encryption method, it provides no additional confidentiality over plain text.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n440/684\n\n11/7/25, 4:37 PM\n\n8.11 - Multi-tenancy\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0453", "text": "This page provides an overview of available configuration options and best practices for cluster multi-tenancy.\nSharing clusters saves costs and simplifies administration. However, sharing clusters also presents challenges such as security,\nfairness, and managing noisy neighbors.\nClusters can be shared in many ways. In some cases, different applications may run in the same cluster. In other cases, multiple\ninstances of the same application may run in the same cluster, one for each end user. All these types of sharing are frequently\ndescribed using the umbrella term multi-tenancy.\nWhile Kubernetes does not have first-class concepts of end users or tenants, it provides several features to help manage different\ntenancy requirements. These are discussed below.\n\nUse cases\nThe first step to determining how to share your cluster is understanding your use case, so you can evaluate the patterns and tools\navailable. In general, multi-tenancy in Kubernetes clusters falls into two broad categories, though many variations and hybrids are\nalso possible.\n\nMultiple teams\nA common form of multi-tenancy is to share a cluster between multiple teams within an organization, each of whom may operate\none or more workloads. These workloads frequently need to communicate with each other, and with other workloads located on the\nsame or different clusters.\nIn this scenario, members of the teams often have direct access to Kubernetes resources via tools such as kubectl , or indirect\naccess through GitOps controllers or other types of release automation tools. There is often some level of trust between members of\ndifferent teams, but Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly share clusters.\n\nMultiple customers\nThe other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor running multiple instances of a\nworkload for customers. This business model is so strongly associated with this deployment style that many people call it \"SaaS\ntenancy.\" However, a better term might be \"multi-customer tenancy,\" since SaaS vendors may also use other deployment models,\nand this deployment model can also be used outside of SaaS.\nIn this scenario, the customers do not have access to the cluster; Kubernetes is invisible from their perspective and is only used by\nthe vendor to manage the workloads. Cost optimization is frequently a critical concern, and Kubernetes policies are used to ensure\nthat the workloads are strongly isolated from each other."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0454", "text": "Terminology\nTenants\nWhen discussing multi-tenancy in Kubernetes, there is no single definition for a \"tenant\". Rather, the definition of a tenant will vary\ndepending on whether multi-team or multi-customer tenancy is being discussed.\nIn multi-team usage, a tenant is typically a team, where each team typically deploys a small number of workloads that scales with the\ncomplexity of the service. However, the definition of \"team\" may itself be fuzzy, as teams may be organized into higher-level\ndivisions or subdivided into smaller teams.\nBy contrast, if each team deploys dedicated workloads for each new client, they are using a multi-customer model of tenancy. In this\ncase, a \"tenant\" is simply a group of users who share a single workload. This may be as large as an entire company, or as small as a\nsingle team at that company.\nIn many cases, the same organization may use both definitions of \"tenants\" in different contexts. For example, a platform team may\noffer shared services such as security tools and databases to multiple internal â€œcustomersâ€ and a SaaS vendor may also have\nmultiple teams sharing a development cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a\ncombination of per-customer workloads for sensitive data, combined with multi-tenant shared services.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n441/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA cluster showing coexisting tenancy models"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0455", "text": "Isolation\nThere are several ways to design and build multi-tenant solutions with Kubernetes. Each of these methods comes with its own set of\ntradeoffs that impact the isolation level, implementation effort, operational complexity, and cost of service.\nA Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane consisting of worker nodes where\ntenant workloads are executed as pods. Tenant isolation can be applied in both the control plane and the data plane based on\norganizational requirements.\nThe level of isolation offered is sometimes described using terms like â€œhardâ€ multi-tenancy, which implies strong isolation, and â€œsoftâ€\nmulti-tenancy, which implies weaker isolation. In particular, \"hard\" multi-tenancy is often used to describe cases where the tenants\ndo not trust each other, often from security and resource sharing perspectives (e.g. guarding against attacks such as data exfiltration\nor DoS). Since data planes typically have much larger attack surfaces, \"hard\" multi-tenancy often requires extra attention to isolating\nthe data-plane, though control plane isolation also remains critical.\nHowever, the terms \"hard\" and \"soft\" can often be confusing, as there is no single definition that will apply to all users. Rather,\n\"hardness\" or \"softness\" is better understood as a broad spectrum, with many different techniques that can be used to maintain\ndifferent types of isolation in your clusters, based on your requirements.\nIn more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and assign each tenant their dedicated\ncluster, possibly even running on dedicated hardware if VMs are not considered an adequate security boundary. This may be easier\nwith managed Kubernetes clusters, where the overhead of creating and operating clusters is at least somewhat taken on by a cloud\nprovider. The benefit of stronger tenant isolation must be evaluated against the cost and complexity of managing multiple clusters.\nThe Multi-cluster SIG is responsible for addressing these types of use cases.\nThe remainder of this page focuses on isolation techniques used for shared Kubernetes clusters. However, even if you are\nconsidering dedicated clusters, it may be valuable to review these recommendations, as it will give you the flexibility to shift to\nshared clusters in the future if your needs or capabilities change.\n\nControl plane isolation\nControl plane isolation ensures that different tenants cannot access or affect each others' Kubernetes API resources.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n442/684\n\n11/7/25, 4:37 PM\n\nNamespaces\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0456", "text": "In Kubernetes, a Namespace provides a mechanism for isolating groups of API resources within a single cluster. This isolation has\ntwo key dimensions:\n1. Object names within a namespace can overlap with names in other namespaces, similar to files in folders. This allows tenants\nto name their resources without having to consider what other tenants are doing.\n2. Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network Policies are namespacescoped resources. Using RBAC, Users and Service Accounts can be restricted to a namespace.\nIn a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and distinct management unit. In fact,\na common practice is to isolate every workload in its own namespace, even if multiple workloads are operated by the same tenant.\nThis ensures that each workload has its own identity and can be configured with an appropriate security policy.\nThe namespace isolation model requires configuration of several other Kubernetes resources, networking plugins, and adherence to\nsecurity best practices to properly isolate tenant workloads. These considerations are discussed below."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0457", "text": "Access controls\nThe most important type of isolation for the control plane is authorization. If teams or their workloads can access or modify each\nothers' API resources, they can change or disable all other types of policies thereby negating any protection those policies may offer.\nAs a result, it is critical to ensure that each tenant has the appropriate access to only the namespaces they need, and no more. This\nis known as the \"Principle of Least Privilege.\"\nRole-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes control plane, for both users and\nworkloads (service accounts). Roles and RoleBindings are Kubernetes objects that are used at a namespace level to enforce access\ncontrol in your application; similar objects exist for authorizing access to cluster-level objects, though these are less useful for multitenant clusters.\nIn a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate namespaces, and ensure that clusterwide resources can only be accessed or modified by privileged users such as cluster administrators.\nIf a policy ends up granting a user more permissions than they need, this is likely a signal that the namespace containing the\naffected resources should be refactored into finer-grained namespaces. Namespace management tools may simplify the\nmanagement of these finer-grained namespaces by applying common RBAC policies to different namespaces, while still allowing\nfine-grained policies where necessary."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0458", "text": "Quotas\nKubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment, you can use Resource\nQuotas to manage resource usage of tenant workloads. For the multiple teams use case, where tenants have access to the\nKubernetes API, you can use resource quotas to limit the number of API resources (for example: the number of Pods, or the number\nof ConfigMaps) that a tenant can create. Limits on object count ensure fairness and aim to avoid noisy neighbor issues from affecting\nother tenants that share a control plane.\nResource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use quotas to ensure that a\ntenant cannot monopolize a cluster's resources or overwhelm its control plane. Namespace management tools simplify the\nadministration of quotas. In addition, while Kubernetes quotas only apply within a single namespace, some namespace\nmanagement tools allow groups of namespaces to share quotas, giving administrators far more flexibility with less effort than builtin quotas.\nQuotas prevent a single tenant from consuming greater than their allocated share of resources hence minimizing the â€œnoisy\nneighborâ€ issue, where one tenant negatively impacts the performance of other tenants' workloads.\nWhen you apply a quota to namespace, Kubernetes requires you to also specify resource requests and limits for each container.\nLimits are the upper bound for the amount of resources that a container can consume. Containers that attempt to consume\nresources that exceed the configured limits will either be throttled or killed, based on the resource type. When resource requests are\nset lower than limits, each container is guaranteed the requested amount but there may still be some potential for impact across\nworkloads.\nQuotas cannot protect against all kinds of resource sharing, such as network traffic. Node isolation (described below) may be a\nbetter solution for this problem.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n443/684\n\n11/7/25, 4:37 PM\n\nData Plane Isolation\n\nConcepts | Kubernetes\n\nData plane isolation ensures that pods and workloads for different tenants are sufficiently isolated."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0459", "text": "Network isolation\nBy default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all network traffic is unencrypted. This\ncan lead to security vulnerabilities where traffic is accidentally or maliciously sent to an unintended destination, or is intercepted by\na workload on a compromised node.\nPod-to-pod communication can be controlled using Network Policies, which restrict communication between pods using namespace\nlabels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required, starting with a\ndefault policy that denies communication between pods is recommended with another rule that allows all pods to query the DNS\nserver for name resolution. With such a default policy in place, you can begin adding more permissive rules that allow for\ncommunication within a namespace. It is also recommended not to use empty label selector '{}' for namespaceSelector field in\nnetwork policy definition, in case traffic need to be allowed between namespaces. This scheme can be further refined as required.\nNote that this only applies to pods within a single control plane; pods that belong to different virtual control planes cannot talk to\neach other via Kubernetes networking.\nNamespace management tools may simplify the creation of default or common network policies. In addition, some of these tools\nallow you to enforce a consistent set of namespace labels across your cluster, ensuring that they are a trusted basis for your policies.\n\nWarning:\nNetwork policies require a CNI plugin that supports the implementation of network policies. Otherwise, NetworkPolicy\nresources will be ignored.\nMore advanced network isolation may be provided by service meshes, which provide OSI Layer 7 policies based on workload\nidentity, in addition to namespaces. These higher-level policies can make it easier to manage namespace-based multi-tenancy,\nespecially when multiple namespaces are dedicated to a single tenant. They frequently also offer encryption using mutual TLS,\nprotecting your data even in the presence of a compromised node, and work across dedicated or virtual clusters. However, they can\nbe significantly more complex to manage and may not be appropriate for all users."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0460", "text": "Storage isolation\nKubernetes offers several types of volumes that can be used as persistent storage for workloads. For security and data-isolation,\ndynamic volume provisioning is recommended and volume types that use node resources should be avoided.\nStorageClasses allow you to describe custom \"classes\" of storage offered by your cluster, based on quality-of-service levels, backup\npolicies, or custom policies determined by the cluster administrators.\nPods can request storage using a PersistentVolumeClaim. A PersistentVolumeClaim is a namespaced resource, which enables\nisolating portions of the storage system and dedicating it to tenants within the shared Kubernetes cluster. However, it is important\nto note that a PersistentVolume is a cluster-wide resource and has a lifecycle independent of workloads and namespaces.\nFor example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation. If a StorageClass is\nshared, you should set a reclaim policy of Delete to ensure that a PersistentVolume cannot be reused across different namespaces.\n\nSandboxing containers\nKubernetes pods are composed of one or more containers that execute on worker nodes. Containers utilize OS-level virtualization\nand hence offer a weaker isolation boundary than virtual machines that utilize hardware-based virtualization.\nIn a shared environment, unpatched vulnerabilities in the application and system layers can be exploited by attackers for container\nbreakouts and remote code execution that allow access to host resources. In some applications, like a Content Management System\n(CMS), customers may be allowed the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further\nisolate and protect workloads using strong isolation are desirable.\nSandboxing provides a way to isolate workloads running in a shared cluster. It typically involves running each pod in a separate\nexecution environment such as a virtual machine or a userspace kernel. Sandboxing is often recommended when you are running\nuntrusted code, where workloads are assumed to be malicious. Part of the reason this type of isolation is necessary is because\ncontainers are processes running on a shared kernel; they mount file systems like /sys and /proc from the underlying host,\nmaking them less secure than an application that runs on a virtual machine which has its own kernel. While controls such as\nhttps://kubernetes.io/docs/concepts/_print/\n\n444/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0461", "text": "seccomp, AppArmor, and SELinux can be used to strengthen the security of containers, it is hard to apply a universal set of rules to\nall workloads running in a shared cluster. Running workloads in a sandbox environment helps to insulate the host from container\nescapes, where an attacker exploits a vulnerability to gain access to the host system and all the processes/files running on that host.\nVirtual machines and userspace kernels are two popular approaches to sandboxing.\n\nNode Isolation\nNode isolation is another technique that you can use to isolate tenant workloads from each other. With node isolation, a set of\nnodes is dedicated to running pods from a particular tenant and co-mingling of tenant pods is prohibited. This configuration reduces\nthe noisy tenant issue, as all pods running on a node will belong to a single tenant. The risk of information disclosure is slightly lower\nwith node isolation because an attacker that manages to escape from a container will only have access to the containers and\nvolumes mounted to that node.\nAlthough workloads from different tenants are running on different nodes, it is important to be aware that the kubelet and (unless\nusing virtual control planes) the API service are still shared services. A skilled attacker could use the permissions assigned to the\nkubelet or other pods running on the node to move laterally within the cluster and gain access to tenant workloads running on other\nnodes. If this is a major concern, consider implementing compensating controls such as seccomp, AppArmor or SELinux or explore\nusing sandboxed containers or creating separate clusters for each tenant.\nNode isolation is a little easier to reason about from a billing standpoint than sandboxing containers since you can charge back per\nnode rather than per pod. It also has fewer compatibility and performance issues and may be easier to implement than sandboxing\ncontainers. For example, nodes for each tenant can be configured with taints so that only pods with the corresponding toleration\ncan run on them. A mutating webhook could then be used to automatically add tolerations and node affinities to pods deployed into\ntenant namespaces so that they run on a specific set of nodes designated for that tenant.\nNode isolation can be implemented using pod node selectors.\n\nAdditional Considerations\nThis section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0462", "text": "API Priority and Fairness\nAPI priority and fairness is a Kubernetes feature that allows you to assign a priority to certain pods running within the cluster. When\nan application calls the Kubernetes API, the API server evaluates the priority assigned to pod. Calls from pods with higher priority are\nfulfilled before those with a lower priority. When contention is high, lower priority calls can be queued until the server is less busy or\nyou can reject the requests.\nUsing API priority and fairness will not be very common in SaaS environments unless you are allowing customers to run applications\nthat interface with the Kubernetes API, for example, a controller.\n\nQuality-of-Service (QoS)\nWhen youâ€™re running a SaaS application, you may want the ability to offer different Quality-of-Service (QoS) tiers of service to\ndifferent tenants. For example, you may have freemium service that comes with fewer performance guarantees and features and a\nfor-fee service tier with specific performance guarantees. Fortunately, there are several Kubernetes constructs that can help you\naccomplish this within a shared cluster, including network QoS, storage classes, and pod priority and preemption. The idea with each\nof these is to provide tenants with the quality of service that they paid for. Letâ€™s start by looking at networking QoS.\nTypically, all pods on a node share a network interface. Without network QoS, some pods may consume an unfair share of the\navailable bandwidth at the expense of other pods. The Kubernetes bandwidth plugin creates an extended resource for networking\nthat allows you to use Kubernetes resources constructs, i.e. requests/limits, to apply rate limits to pods by using Linux tc queues. Be\naware that the plugin is considered experimental as per the Network Plugins documentation and should be thoroughly tested\nbefore use in production environments.\nFor storage QoS, you will likely want to create different storage classes or profiles with different performance characteristics. Each\nstorage profile can be associated with a different tier of service that is optimized for different workloads such IO, redundancy, or\nthroughput. Additional logic might be necessary to allow the tenant to associate the appropriate storage profile with their workload.\nFinally, thereâ€™s pod priority and preemption where you can assign priority values to pods. When scheduling pods, the scheduler will\ntry evicting pods with lower priority when there are insufficient resources to schedule pods that are assigned a higher priority. If you\nhave a use case where tenants have different service tiers in a shared cluster e.g. free and paid, you may want to give higher priority\nhttps://kubernetes.io/docs/concepts/_print/"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0463", "text": "445/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nto certain tiers using this feature.\n\nDNS\nKubernetes clusters include a Domain Name System (DNS) service to provide translations from names to IP addresses, for all\nServices and Pods. By default, the Kubernetes DNS service allows lookups across all namespaces in the cluster.\nIn multi-tenant environments where tenants can access pods and other Kubernetes resources, or where stronger isolation is\nrequired, it may be necessary to prevent pods from looking up services in other Namespaces. You can restrict cross-namespace DNS\nlookups by configuring security rules for the DNS service. For example, CoreDNS (the default DNS service for Kubernetes) can\nleverage Kubernetes metadata to restrict queries to Pods and Services within a namespace. For more information, read an example\nof configuring this within the CoreDNS documentation.\nWhen a Virtual Control Plane per tenant model is used, a DNS service must be configured per tenant or a multi-tenant DNS service\nmust be used. Here is an example of a customized version of CoreDNS that supports multiple tenants.\n\nOperators\nOperators are Kubernetes controllers that manage applications. Operators can simplify the management of multiple instances of an\napplication, like a database service, which makes them a common building block in the multi-consumer (SaaS) multi-tenancy use\ncase.\nOperators used in a multi-tenant environment should follow a stricter set of guidelines. Specifically, the Operator should:\nSupport creating resources within different tenant namespaces, rather than just in the namespace in which the Operator is\ndeployed.\nEnsure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.\nSupport configuration of Pods for data-plane isolation techniques such as node isolation and sandboxed containers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0464", "text": "Implementations\nThere are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces (that is, a Namespace per tenant) or\nby virtualizing the control plane (that is, virtual control plane per tenant).\nIn both cases, data plane isolation, and management of additional considerations such as API Priority and Fairness, is also\nrecommended.\nNamespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides mechanisms to allow tenants to\ninteract appropriately, such as by allowing service-to-service communication. However, it can be difficult to configure, and doesn't\napply to Kubernetes resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.\nControl plane virtualization allows for isolation of non-namespaced resources at the cost of somewhat higher resource usage and\nmore difficult cross-tenant sharing. It is a good option when namespace isolation is insufficient but dedicated clusters are\nundesirable, due to the high cost of maintaining them (especially on-prem) or due to their higher overhead and lack of resource\nsharing. However, even within a virtualized control plane, you will likely see benefits by using namespaces as well.\nThe two options are discussed in more detail in the following sections.\n\nNamespace per tenant\nAs previously mentioned, you should consider isolating each workload in its own namespace, even if you are using dedicated\nclusters or virtualized control planes. This ensures that each workload only has access to its own resources, such as ConfigMaps and\nSecrets, and allows you to tailor dedicated security policies for each workload. In addition, it is a best practice to give each\nnamespace names that are unique across your entire fleet (that is, even if they are in separate clusters), as this gives you the\nflexibility to switch between dedicated and shared clusters in the future, or to use multi-cluster tooling such as service meshes.\nConversely, there are also advantages to assigning namespaces at the tenant level, not just the workload level, since there are often\npolicies that apply to all workloads owned by a single tenant. However, this raises its own problems. Firstly, this makes it difficult or\nimpossible to customize policies to individual workloads, and secondly, it may be challenging to come up with a single level of\n\"tenancy\" that should be given a namespace. For example, an organization may have divisions, teams, and subteams - which should\nbe assigned a namespace?\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n446/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0465", "text": "One possible approach is to organize your namespaces into hierarchies, and share certain policies and resources between them.\nThis could include managing namespace labels, namespace lifecycles, delegated access, and shared resource quotas across related\nnamespaces. These capabilities can be useful in both multi-team and multi-customer scenarios.\n\nVirtual control plane per tenant\nAnother form of control-plane isolation is to use Kubernetes extensions to provide each tenant a virtual control-plane that enables\nsegmentation of cluster-wide API resources. Data plane isolation techniques can be used with this model to securely manage worker\nnodes across tenants.\nThe virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by providing each tenant with\ndedicated control plane components, and hence complete control over cluster-wide resources and add-on services. Worker nodes\nare shared across all tenants, and are managed by a Kubernetes cluster that is normally inaccessible to tenants. This cluster is often\nreferred to as a super-cluster (or sometimes as a host-cluster). Since a tenantâ€™s control-plane is not directly associated with underlying\ncompute resources it is referred to as a virtual control plane.\nA virtual control plane typically consists of the Kubernetes API server, the controller manager, and the etcd data store. It interacts\nwith the super cluster via a metadata synchronization controller which coordinates changes across tenant control planes and the\ncontrol plane of the super-cluster.\nBy using per-tenant dedicated control planes, most of the isolation problems due to sharing one API server among all tenants are\nsolved. Examples include noisy neighbors in the control plane, uncontrollable blast radius of policy misconfigurations, and conflicts\nbetween cluster scope objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly suitable for cases\nwhere each tenant requires access to a Kubernetes API server and expects the full cluster manageability.\nThe improved isolation comes at the cost of running and maintaining an individual virtual control plane per tenant. In addition, pertenant control planes do not solve isolation problems in the data plane, such as node-level noisy neighbors or security threats. These\nmust still be addressed separately.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n447/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0466", "text": "8.12 - Hardening Guide - Authentication Mechanisms\nInformation on authentication options in Kubernetes and their security properties.\nSelecting the appropriate authentication mechanism(s) is a crucial aspect of securing your cluster. Kubernetes provides several builtin mechanisms, each with its own strengths and weaknesses that should be carefully considered when choosing the best\nauthentication mechanism for your cluster.\nIn general, it is recommended to enable as few authentication mechanisms as possible to simplify user management and prevent\ncases where users retain access to a cluster that is no longer required.\nIt is important to note that Kubernetes does not have an in-built user database within the cluster. Instead, it takes user information\nfrom the configured authentication system and uses that to make authorization decisions. Therefore, to audit user access, you need\nto review credentials from every configured authentication source.\nFor production clusters with multiple users directly accessing the Kubernetes API, it is recommended to use external authentication\nsources such as OIDC. The internal authentication mechanisms, such as client certificates and service account tokens, described\nbelow, are not suitable for this use case.\n\nX.509 client certificate authentication\nKubernetes leverages X.509 client certificate authentication for system components, such as when the kubelet authenticates to the\nAPI Server. While this mechanism can also be used for user authentication, it might not be suitable for production use due to several\nrestrictions:\nClient certificates cannot be individually revoked. Once compromised, a certificate can be used by an attacker until it expires.\nTo mitigate this risk, it is recommended to configure short lifetimes for user authentication credentials created using client\ncertificates.\nIf a certificate needs to be invalidated, the certificate authority must be re-keyed, which can introduce availability risks to the\ncluster.\nThere is no permanent record of client certificates created in the cluster. Therefore, all issued certificates must be recorded if\nyou need to keep track of them.\nPrivate keys used for client certificate authentication cannot be password-protected. Anyone who can read the file containing\nthe key will be able to make use of it.\nUsing client certificate authentication requires a direct connection from the client to the API server without any intervening TLS\ntermination points, which can complicate network architectures.\nGroup data is embedded in the O value of the client certificate, which means the user's group memberships cannot be\nchanged for the lifetime of the certificate."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0467", "text": "Static token file\nAlthough Kubernetes allows you to load credentials from a static token file located on the control plane node disks, this approach is\nnot recommended for production servers due to several reasons:\nCredentials are stored in clear text on control plane node disks, which can be a security risk.\nChanging any credential requires a restart of the API server process to take effect, which can impact availability.\nThere is no mechanism available to allow users to rotate their credentials. To rotate a credential, a cluster administrator must\nmodify the token on disk and distribute it to the users.\nThere is no lockout mechanism available to prevent brute-force attacks.\n\nBootstrap tokens\nBootstrap tokens are used for joining nodes to clusters and are not recommended for user authentication due to several reasons:\nThey have hard-coded group memberships that are not suitable for general use, making them unsuitable for authentication\npurposes.\nManually generating bootstrap tokens can lead to weak tokens that can be guessed by an attacker, which can be a security risk.\nhttps://kubernetes.io/docs/concepts/_print/\n\n448/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThere is no lockout mechanism available to prevent brute-force attacks, making it easier for attackers to guess or crack the\ntoken.\n\nServiceAccount secret tokens\nService account secrets are available as an option to allow workloads running in the cluster to authenticate to the API server. In\nKubernetes < 1.23, these were the default option, however, they are being replaced with TokenRequest API tokens. While these\nsecrets could be used for user authentication, they are generally unsuitable for a number of reasons:\nThey cannot be set with an expiry and will remain valid until the associated service account is deleted.\nThe authentication tokens are visible to any cluster user who can read secrets in the namespace that they are defined in.\nService accounts cannot be added to arbitrary groups complicating RBAC management where they are used.\n\nTokenRequest API tokens\nThe TokenRequest API is a useful tool for generating short-lived credentials for service authentication to the API server or third-party\nsystems. However, it is not generally recommended for user authentication as there is no revocation method available, and\ndistributing credentials to users in a secure manner can be challenging.\nWhen using TokenRequest tokens for service authentication, it is recommended to implement a short lifespan to reduce the impact\nof compromised tokens."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0468", "text": "OpenID Connect token authentication\nKubernetes supports integrating external authentication services with the Kubernetes API using OpenID Connect (OIDC). There is a\nwide variety of software that can be used to integrate Kubernetes with an identity provider. However, when using OIDC\nauthentication in Kubernetes, it is important to consider the following hardening measures:\nThe software installed in the cluster to support OIDC authentication should be isolated from general workloads as it will run\nwith high privileges.\nSome Kubernetes managed services are limited in the OIDC providers that can be used.\nAs with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the impact of compromised tokens.\n\nWebhook token authentication\nWebhook token authentication is another option for integrating external authentication providers into Kubernetes. This mechanism\nallows for an authentication service, either running inside the cluster or externally, to be contacted for an authentication decision\nover a webhook. It is important to note that the suitability of this mechanism will likely depend on the software used for the\nauthentication service, and there are some Kubernetes-specific considerations to take into account.\nTo configure Webhook authentication, access to control plane server filesystems is required. This means that it will not be possible\nwith Managed Kubernetes unless the provider specifically makes it available. Additionally, any software installed in the cluster to\nsupport this access should be isolated from general workloads, as it will run with high privileges.\n\nAuthenticating proxy\nAnother option for integrating external authentication systems into Kubernetes is to use an authenticating proxy. With this\nmechanism, Kubernetes expects to receive requests from the proxy with specific header values set, indicating the username and\ngroup memberships to assign for authorization purposes. It is important to note that there are specific considerations to take into\naccount when using this mechanism.\nFirstly, securely configured TLS must be used between the proxy and Kubernetes API server to mitigate the risk of traffic interception\nor sniffing attacks. This ensures that the communication between the proxy and Kubernetes API server is secure.\nSecondly, it is important to be aware that an attacker who is able to modify the headers of the request may be able to gain\nunauthorized access to Kubernetes resources. As such, it is important to ensure that the headers are properly secured and cannot\nbe tampered with.\nhttps://kubernetes.io/docs/concepts/_print/\n\n449/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0469", "text": "User Authentication\nAuthenticating with Bootstrap Tokens\nkubelet Authentication\nAuthenticating with Service Account Tokens\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n450/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.13 - Hardening Guide - Scheduler Configuration\nInformation about how to make the Kubernetes scheduler more secure.\nThe Kubernetes scheduler is one of the critical components of the control plane.\nThis document covers how to improve the security posture of the Scheduler.\nA misconfigured scheduler can have security implications. Such a scheduler can target specific nodes and evict the workloads or\napplications that are sharing the node and its resources. This can aid an attacker with a Yo-Yo attack: an attack on a vulnerable\nautoscaler.\n\nkube-scheduler configuration\nScheduler authentication & authorization command line options\nWhen setting up authentication configuration, it should be made sure that kube-scheduler's authentication remains consistent with\nkube-api-server's authentication. If any request has missing authentication headers, the authentication should happen through the\nkube-api-server allowing all authentication to be consistent in the cluster.\nauthentication-kubeconfig : Make sure to provide a proper kubeconfig so that the scheduler can retrieve authentication\n\nconfiguration options from the API Server. This kubeconfig file should be protected with strict file permissions.\nauthentication-tolerate-lookup-failure : Set this to false to make sure the scheduler always looks up its authentication\nconfiguration from the API server.\nauthentication-skip-lookup : Set this to false to make sure the scheduler always looks up its authentication configuration\nfrom the API server.\nauthorization-always-allow-paths : These paths should respond with data that is appropriate for anonymous authorization.\nDefaults to /healthz,/readyz,/livez .\nprofiling : Set to false\n\nto disable the profiling endpoints which are provide debugging information but which should not be\nenabled on production clusters as they present a risk of denial of service or information leakage. The --profiling argument is\ndeprecated and can now be provided through the KubeScheduler DebuggingConfiguration. Profiling can be disabled through\nthe kube-scheduler config by setting enableProfiling to false .\nrequestheader-client-ca-file : Avoid passing this argument.\n\nScheduler networking command line options\nbind-address : In most cases, the kube-scheduler does not need to be externally accessible. Setting the bind address to\nlocalhost\n\nis a secure practice.\n\npermit-address-sharing : Set this to false"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0470", "text": "to disable connection sharing through SO_REUSEADDR . SO_REUSEADDR can lead to\nreuse of terminated connections that are in TIME_WAIT state.\npermit-port-sharing : Default false . Use the default unless you are confident you understand the security implications.\n\nScheduler TLS command line options\ntls-cipher-suites : Always provide a list of preferred cipher suites. This ensures encryption never happens with insecure\n\ncipher suites.\n\nScheduling configurations for custom schedulers\nWhen using custom schedulers based on the Kubernetes scheduling code, cluster administrators need to be careful with plugins\nthat use the queueSort , prefilter , filter , or permit extension points. These extension points control various stages of a\nscheduling process, and the wrong configuration can impact the kube-scheduler's behavior in your cluster.\n\nKey considerations\nExactly one plugin that uses the queueSort extension point can be enabled at a time. Any plugins that use queueSort should\nbe scrutinized.\nhttps://kubernetes.io/docs/concepts/_print/\n\n451/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPlugins that implement the prefilter or filter extension point can potentially mark all nodes as unschedulable. This can\nbring scheduling of new pods to a halt.\nPlugins that implement the permit extension point can prevent or delay the binding of a Pod. Such plugins should be\nthoroughly reviewed by the cluster administrator.\nWhen using a plugin that is not one of the default plugins, consider disabling the queueSort , filter and permit extension points\nas follows:\n\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: my-scheduler\nplugins:\n# Disable specific plugins for different extension points\n# You can disable all plugins for an extension point using \"*\"\nqueueSort:\ndisabled:\n- name: \"*\"\n# - name: \"PrioritySort\"\n\n# Disable all queueSort plugins\n# Disable specific queueSort plugin\n\nfilter:\ndisabled:\n- name: \"*\"\n# - name: \"NodeResourcesFit\"\n\n# Disable all filter plugins\n# Disable specific filter plugin\n\npermit:\ndisabled:\n- name: \"*\"\n\n# Disables all permit plugins\n\n# - name: \"TaintToleration\" # Disable specific permit plugin"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0471", "text": "This creates a scheduler profile my-custom-scheduler . Whenever the .spec of a Pod does not have a value for\n.spec.schedulerName , the kube-scheduler runs for that Pod, using its main configuration, and default plugins. If you define a Pod\nwith .spec.schedulerName set to my-custom-scheduler , the kube-scheduler runs but with a custom configuration; in that custom\nconfiguration, the queueSort , filter and permit extension points are disabled. If you use this KubeSchedulerConfiguration, and\ndon't run any custom scheduler, and you then define a Pod with .spec.schedulerName set to nonexistent-scheduler (or any other\nscheduler name that doesn't exist in your cluster), no events would be generated for a pod.\n\nDisallow labeling nodes\nA cluster administrator should ensure that cluster users cannot label the nodes. A malicious actor can use nodeSelector to\nschedule workloads on nodes where those workloads should not be present.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n452/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.14 - Kubernetes API Server Bypass Risks\nSecurity architecture information relating to the API server and other components\nThe Kubernetes API server is the main point of entry to a cluster for external parties (users and services) interacting with it.\nAs part of this role, the API server has several key built-in security controls, such as audit logging and admission controllers.\nHowever, there are ways to modify the configuration or content of the cluster that bypass these controls.\nThis page describes the ways in which the security controls built into the Kubernetes API server can be bypassed, so that cluster\noperators and security architects can ensure that these bypasses are appropriately restricted."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0472", "text": "Static Pods\nThe kubelet on each node loads and directly manages any manifests that are stored in a named directory or fetched from a specific\nURL as static Pods in your cluster. The API server doesn't manage these static Pods. An attacker with write access to this location\ncould modify the configuration of static pods loaded from that source, or could introduce new static Pods.\nStatic Pods are restricted from accessing other objects in the Kubernetes API. For example, you can't configure a static Pod to mount\na Secret from the cluster. However, these Pods can take other security sensitive actions, such as using hostPath mounts from the\nunderlying node.\nBy default, the kubelet creates a mirror pod so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an\ninvalid namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only be discovered by tooling\nthat has access to the affected host(s).\nIf a static Pod fails admission control, the kubelet won't register the Pod with the API server. However, the Pod still runs on the node.\nFor more information, refer to kubeadm issue #1541.\n\nMitigations\nOnly enable the kubelet static Pod manifest functionality if required by the node.\nIf a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory or URL to users who\nneed the access.\nRestrict access to kubelet configuration parameters and files to prevent an attacker setting a static Pod path or URL.\nRegularly audit and centrally report all access to directories or web storage locations that host static Pod manifests and kubelet\nconfiguration files."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0473", "text": "The kubelet API\nThe kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster worker nodes. The API might also be\nexposed on control plane nodes depending on the Kubernetes distribution in use. Direct access to the API allows for disclosure of\ninformation about the pods running on a node, the logs from those pods, and execution of commands in every container running on\nthe node.\nWhen Kubernetes cluster users have RBAC access to Node object sub-resources, that access serves as authorization to interact with\nthe kubelet API. The exact access depends on which sub-resource access has been granted, as detailed in kubelet authorization.\nDirect access to the kubelet API is not subject to admission control and is not logged by Kubernetes audit logging. An attacker with\ndirect access to this API may be able to bypass controls that detect or prevent certain actions.\nThe kubelet API can be configured to authenticate requests in a number of ways. By default, the kubelet configuration allows\nanonymous access. Most Kubernetes providers change the default to use webhook and certificate authentication. This lets the\ncontrol plane ensure that the caller is authorized to access the nodes API resource or sub-resources. The default anonymous access\ndoesn't make this assertion with the control plane.\n\nMitigations\nRestrict access to sub-resources of the nodes API object using mechanisms such as RBAC. Only grant this access when\nrequired, such as by monitoring services.\nhttps://kubernetes.io/docs/concepts/_print/\n\n453/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nRestrict access to the kubelet port. Only allow specified and trusted IP address ranges to access the port.\nEnsure that kubelet authentication. is set to webhook or certificate mode.\nEnsure that the unauthenticated \"read-only\" Kubelet port is not enabled on the cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0474", "text": "The etcd API\nKubernetes clusters use etcd as a datastore. The etcd service listens on TCP port 2379. The only clients that need access are the\nKubernetes API server and any backup tooling that you use. Direct access to this API allows for disclosure or modification of any data\nheld in the cluster.\nAccess to the etcd API is typically managed by client certificate authentication. Any certificate issued by a certificate authority that\netcd trusts allows full access to the data stored inside etcd.\nDirect access to etcd is not subject to Kubernetes admission control and is not logged by Kubernetes audit logging. An attacker who\nhas read access to the API server's etcd client certificate private key (or can create a new trusted client certificate) can gain cluster\nadmin rights by accessing cluster secrets or modifying access rules. Even without elevating their Kubernetes RBAC privileges, an\nattacker who can modify etcd can retrieve any API object or create new workloads inside the cluster.\nMany Kubernetes providers configure etcd to use mutual TLS (both client and server verify each other's certificate for\nauthentication). There is no widely accepted implementation of authorization for the etcd API, although the feature exists. Since\nthere is no authorization model, any certificate with client access to etcd can be used to gain full access to etcd. Typically, etcd client\ncertificates that are only used for health checking can also grant full read and write access.\n\nMitigations\nEnsure that the certificate authority trusted by etcd is used only for the purposes of authentication to that service.\nControl access to the private key for the etcd server certificate, and to the API server's client certificate and key.\nConsider restricting access to the etcd port at a network level, to only allow access from specified and trusted IP address\nranges."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0475", "text": "Container runtime socket\nOn each node in a Kubernetes cluster, access to interact with containers is controlled by the container runtime (or runtimes, if you\nhave configured more than one). Typically, the container runtime exposes a Unix socket that the kubelet can access. An attacker with\naccess to this socket can launch new containers or interact with running containers.\nAt the cluster level, the impact of this access depends on whether the containers that run on the compromised node have access to\nSecrets or other confidential data that an attacker could use to escalate privileges to other worker nodes or to control plane\ncomponents.\n\nMitigations\nEnsure that you tightly control filesystem access to container runtime sockets. When possible, restrict this access to the root\nuser.\nIsolate the kubelet from other components running on the node, using mechanisms such as Linux kernel namespaces.\nEnsure that you restrict or forbid the use of hostPath mounts that include the container runtime socket, either directly or by\nmounting a parent directory. Also hostPath mounts must be set as read-only to mitigate risks of attackers bypassing directory\nrestrictions.\nRestrict user access to nodes, and especially restrict superuser access to nodes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n454/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.15 - Linux kernel security constraints for Pods and\ncontainers\nOverview of Linux kernel security modules and constraints that you can use to harden your Pods and\ncontainers.\nThis page describes some of the security features that are built into the Linux kernel that you can use in your Kubernetes workloads.\nTo learn how to apply these features to your Pods and containers, refer to Configure a SecurityContext for a Pod or Container. You\nshould already be familiar with Linux and with the basics of Kubernetes workloads."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0476", "text": "Run workloads without root privileges\nWhen you deploy a workload in Kubernetes, use the Pod specification to restrict that workload from running as the root user on the\nnode. You can use the Pod securityContext to define the specific Linux user and group for the processes in the Pod, and explicitly\nrestrict containers from running as root users. Setting these values in the Pod manifest takes precedence over similar values in the\ncontainer image, which is especially useful if you're running images that you don't own.\nCaution:\nEnsure that the user or group that you assign to the workload has the permissions required for the application to function\ncorrectly. Changing the user or group to one that doesn't have the correct permissions could lead to file access issues or failed\noperations.\nConfiguring the kernel security features on this page provides fine-grained control over the actions that processes in your cluster can\ntake, but managing these configurations can be challenging at scale. Running containers as non-root, or in user namespaces if you\nneed root privileges, helps to reduce the chance that you'll need to enforce your configured kernel security capabilities.\n\nSecurity features in the Linux kernel\nKubernetes lets you configure and use Linux kernel features to improve isolation and harden your containerized workloads.\nCommon features include the following:\nSecure computing mode (seccomp): Filter which system calls a process can make\nAppArmor: Restrict the access privileges of individual programs\nSecurity Enhanced Linux (SELinux): Assign security labels to objects for more manageable security policy enforcement\nTo configure settings for one of these features, the operating system that you choose for your nodes must enable the feature in the\nkernel. For example, Ubuntu 7.10 and later enable AppArmor by default. To learn whether your OS enables a specific feature,\nconsult the OS documentation.\nYou use the securityContext field in your Pod specification to define the constraints that apply to those processes. The\nsecurityContext field also supports other security settings, such as specific Linux capabilities or file access permissions using UIDs\nand GIDs. To learn more, refer to Configure a SecurityContext for a Pod or Container."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0477", "text": "seccomp\nSome of your workloads might need privileges to perform specific actions as the root user on your node's host machine. Linux uses\ncapabilities to divide the available privileges into categories, so that processes can get the privileges required to perform specific\nactions without being granted all privileges. Each capability has a set of system calls (syscalls) that a process can make. seccomp lets\nyou restrict these individual syscalls. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from\nuserspace into the kernel.\nIn Kubernetes, you use a container runtime on each node to run your containers. Example runtimes include CRI-O, Docker, or\ncontainerd. Each runtime allows only a subset of Linux capabilities by default. You can further limit the allowed syscalls individually\nby using a seccomp profile. Container runtimes usually include a default seccomp profile. Kubernetes lets you automatically apply\nseccomp profiles loaded onto a node to your Pods and containers.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n455/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNote:\nKubernetes also has the allowPrivilegeEscalation setting for Pods and containers. When set to false, this prevents\nprocesses from gaining new capabilities and restricts unprivileged users from changing the applied seccomp profile to a more\npermissive profile.\nTo learn how to implement seccomp in Kubernetes, refer to Restrict a Container's Syscalls with seccomp or the Seccomp node\nreference\nTo learn more about seccomp, see Seccomp BPF in the Linux kernel documentation.\n\nConsiderations for seccomp\nseccomp is a low-level security configuration that you should only configure yourself if you require fine-grained control over Linux\nsyscalls. Using seccomp, especially at scale, has the following risks:\nConfigurations might break during application updates\nAttackers can still use allowed syscalls to exploit vulnerabilities\nProfile management for individual applications becomes challenging at scale\nRecommendation: Use the default seccomp profile that's bundled with your container runtime. If you need a more isolated\nenvironment, consider using a sandbox, such as gVisor. Sandboxes solve the preceding risks with custom seccomp profiles, but\nrequire more compute resources on your nodes and might have compatibility issues with GPUs and other specialized hardware.\n\nAppArmor and SELinux: policy-based mandatory access control\nYou can use Linux policy-based mandatory access control (MAC) mechanisms, such as AppArmor and SELinux, to harden your\nKubernetes workloads."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0478", "text": "AppArmor\nAppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine\nprograms to a limited set of resources. AppArmor can be configured for any application to reduce its potential attack surface and\nprovide greater in-depth defense. It is configured through profiles tuned to allow the access needed by a specific program or\ncontainer, such as Linux capabilities, network access, and file permissions. Each profile can be run in either enforcing mode, which\nblocks access to disallowed resources, or complain mode, which only reports violations.\nAppArmor can help you to run a more secure deployment by restricting what containers are allowed to do, and/or provide better\nauditing through system logs. The container runtime that you use might ship with a default AppArmor profile, or you can use a\ncustom profile.\nTo learn how to use AppArmor in Kubernetes, refer to Restrict a Container's Access to Resources with AppArmor.\n\nSELinux\nSELinux is a Linux kernel security module that lets you restrict the access that a specific subject, such as a process, has to the files on\nyour system. You define security policies that apply to subjects that have specific SELinux labels. When a process that has an SELinux\nlabel attempts to access a file, the SELinux server checks whether that process' security policy allows the access and makes an\nauthorization decision.\nIn Kubernetes, you can set an SELinux label in the securityContext field of your manifest. The specified labels are assigned to those\nprocesses. If you have configured security policies that affect those labels, the host OS kernel enforces these policies.\nTo learn how to use SELinux in Kubernetes, refer to Assign SELinux labels to a container.\n\nDifferences between AppArmor and SELinux\nThe operating system on your Linux nodes usually includes one of either AppArmor or SELinux. Both mechanisms provide similar\ntypes of protection, but have differences such as the following:\nConfiguration: AppArmor uses profiles to define access to resources. SELinux uses policies that apply to specific labels.\nPolicy application: In AppArmor, you define resources using file paths. SELinux uses the index node (inode) of a resource to\nidentify the resource.\nhttps://kubernetes.io/docs/concepts/_print/\n\n456/684\n\n11/7/25, 4:37 PM\n\nSummary of features\n\nConcepts | Kubernetes\n\nThe following table describes the use cases and scope of each security control. You can use all of these controls together to build a\nmore hardened system.\nSecurity\nfeature\n\nDescription"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0479", "text": "How to use\n\nExample\n\nseccomp\n\nRestrict individual kernel calls in the\nuserspace. Reduces the likelihood that a\nvulnerability that uses a restricted syscall\nwould compromise the system.\n\nSpecify a loaded seccomp profile in the\nPod or container specification to apply\nits constraints to the processes in the\nPod.\n\nReject the unshare\nsyscall, which was used\nin CVE-2022-0185.\n\nAppArmor\n\nRestrict program access to specific\nresources. Reduces the attack surface of\nthe program. Improves audit logging.\n\nSpecify a loaded AppArmor profile in\nthe container specification.\n\nRestrict a read-only\nprogram from writing to\nany file path in the\nsystem.\n\nSELinux\n\nRestrict access to resources such as files,\napplications, ports, and processes using\nlabels and security policies.\n\nSpecify access restrictions for specific\nlabels. Tag processes with those labels\nto enforce the access restrictions\nrelated to the label.\n\nRestrict a container\nfrom accessing files\noutside its own\nfilesystem.\n\nSummary of Linux kernel security features\n\nNote:\nMechanisms like AppArmor and SELinux can provide protection that extends beyond the container. For example, you can use\nSELinux to help mitigate CVE-2019-5736.\n\nConsiderations for managing custom configurations\nseccomp, AppArmor, and SELinux usually have a default configuration that offers basic protections. You can also create custom\nprofiles and policies that meet the requirements of your workloads. Managing and distributing these custom configurations at scale\nmight be challenging, especially if you use all three features together. To help you to manage these configurations at scale, use a tool\nlike the Kubernetes Security Profiles Operator.\n\nKernel-level security features and privileged containers\nKubernetes lets you specify that some trusted containers can run in privileged mode. Any container in a Pod can run in privileged\nmode to use operating system administrative capabilities that would otherwise be inaccessible. This is available for both Windows\nand Linux.\nPrivileged containers explicitly override some of the Linux kernel constraints that you might use in your workloads, as follows:\nseccomp: Privileged containers run as the Unconfined seccomp profile, overriding any seccomp profile that you specified in\nyour manifest.\nAppArmor: Privileged containers ignore any applied AppArmor profiles.\nSELinux: Privileged containers run as the unconfined_t domain."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0480", "text": "Privileged containers\nAny container in a Pod can enable Privileged mode if you set the privileged: true field in the securityContext field for the\ncontainer. Privileged containers override or undo many other hardening settings such as the applied seccomp profile, AppArmor\nprofile, or SELinux constraints. Privileged containers are given all Linux capabilities, including capabilities that they don't require. For\nexample, a root user in a privileged container might be able to use the CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities on the node,\nbypassing the runtime seccomp configuration and other restrictions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n457/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIn most cases, you should avoid using privileged containers, and instead grant the specific capabilities required by your container\nusing the capabilities field in the securityContext field. Only use privileged mode if you have a capability that you can't grant\nwith the securityContext. This is useful for containers that want to use operating system administrative capabilities such as\nmanipulating the network stack or accessing hardware devices.\nIn Kubernetes version 1.26 and later, you can also run Windows containers in a similarly privileged mode by setting the\nwindowsOptions.hostProcess flag on the security context of the Pod spec. For details and instructions, see Create a Windows\nHostProcess Pod.\n\nRecommendations and best practices\nBefore configuring kernel-level security capabilities, you should consider implementing network-level isolation. For more\ninformation, read the Security Checklist.\nUnless necessary, run Linux workloads as non-root by setting specific user and group IDs in your Pod manifest and by\nspecifying runAsNonRoot: true .\nAdditionally, you can run workloads in user namespaces by setting hostUsers: false in your Pod manifest. This lets you run\ncontainers as root users in the user namespace, but as non-root users in the host namespace on the node. This is still in early stages\nof development and might not have the level of support that you need. For instructions, refer to Use a User Namespace With a Pod.\n\nWhat's next\nLearn how to use AppArmor\nLearn how to use seccomp\nLearn how to use SELinux\nSeccomp Node Reference\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n458/684\n\n11/7/25, 4:37 PM\n\n8.16 - Security Checklist\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0481", "text": "Baseline checklist for ensuring security in Kubernetes clusters.\nThis checklist aims at providing a basic list of guidance with links to more comprehensive documentation on each topic. It does not\nclaim to be exhaustive and is meant to evolve.\nOn how to read and use this document:\nThe order of topics does not reflect an order of priority.\nSome checklist items are detailed in the paragraph below the list of each section.\nCaution:\nChecklists are not sufficient for attaining a good security posture on their own. A good security posture requires constant\nattention and improvement, but a checklist can be the first step on the never-ending journey towards security preparedness.\nSome of the recommendations in this checklist may be too restrictive or too lax for your specific security needs. Since\nKubernetes security is not \"one size fits all\", each category of checklist items should be evaluated on its merits.\n\nAuthentication & Authorization\nsystem:masters\n\ngroup is not used for user or component authentication after bootstrapping.\n\nThe kube-controller-manager is running with --use-service-account-credentials enabled.\nThe root certificate is protected (either an offline CA, or a managed online CA with effective access controls).\nIntermediate and leaf certificates have an expiry date no more than 3 years in the future.\nA process exists for periodic access review, and reviews occur no more than 24 months apart.\nThe Role Based Access Control Good Practices are followed for guidance related to authentication and authorization.\nAfter bootstrapping, neither users nor components should authenticate to the Kubernetes API as system:masters . Similarly,\nrunning all of kube-controller-manager as system:masters should be avoided. In fact, system:masters should only be used as a\nbreak-glass mechanism, as opposed to an admin user."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0482", "text": "Network security\nCNI plugins in use support network policies.\nIngress and egress network policies are applied to all workloads in the cluster.\nDefault network policies within each namespace, selecting all pods, denying everything, are in place.\nIf appropriate, a service mesh is used to encrypt all communications inside of the cluster.\nThe Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.\nAccess from the workloads to the cloud metadata API is filtered.\nUse of LoadBalancer and ExternalIPs is restricted.\nA number of Container Network Interface (CNI) plugins plugins provide the functionality to restrict network resources that pods may\ncommunicate with. This is most commonly done through Network Policies which provide a namespaced resource to define rules.\nDefault network policies that block all egress and ingress, in each namespace, selecting all pods, can be useful to adopt an allow list\napproach to ensure that no workloads are missed.\nNot all CNI plugins provide encryption in transit. If the chosen plugin lacks this feature, an alternative solution could be to use a\nservice mesh to provide that functionality.\nThe etcd datastore of the control plane should have controls to limit access and not be publicly exposed on the Internet.\nFurthermore, mutual TLS (mTLS) should be used to communicate securely with it. The certificate authority for this should be unique\nto etcd.\nExternal Internet access to the Kubernetes API server should be restricted to not expose the API publicly. Be careful, as many\nmanaged Kubernetes distributions are publicly exposing the API server by default. You can then use a bastion host to access the\nserver.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n459/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe kubelet API access should be restricted and not exposed publicly, the default authentication and authorization settings, when no\nconfiguration file specified with the --config flag, are overly permissive.\nIf a cloud provider is used for hosting Kubernetes, the access from pods to the cloud metadata API 169.254.169.254 should also be\nrestricted or blocked if not needed because it may leak information.\nFor restricted LoadBalancer and ExternalIPs use, see CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs and the\nDenyServiceExternalIPs admission controller for further information."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0483", "text": "Pod security\nRBAC rights to create , update , patch , delete workloads is only granted if necessary.\nAppropriate Pod Security Standards policy is applied for all namespaces and enforced.\nMemory limit is set for the workloads with a limit equal or inferior to the request.\nCPU limit might be set on sensitive workloads.\nFor nodes that support it, Seccomp is enabled with appropriate syscalls profile for programs.\nFor nodes that support it, AppArmor or SELinux is enabled with appropriate profile for programs.\nRBAC authorization is crucial but cannot be granular enough to have authorization on the Pods' resources (or on any resource that\nmanages Pods). The only granularity is the API verbs on the resource itself, for example, create on Pods. Without additional\nadmission, the authorization to create these resources allows direct unrestricted access to the schedulable nodes of a cluster.\nThe Pod Security Standards define three different policies, privileged, baseline and restricted that limit how fields can be set in the\nPodSpec regarding security. These standards can be enforced at the namespace level with the new Pod Security admission, enabled\nby default, or by third-party admission webhook. Please note that, contrary to the removed PodSecurityPolicy admission it replaces,\nPod Security admission can be easily combined with admission webhooks and external services.\nPod Security admission restricted policy, the most restrictive policy of the Pod Security Standards set, can operate in several\nmodes, warn , audit or enforce to gradually apply the most appropriate security context according to security best practices.\nNevertheless, pods' security context should be separately investigated to limit the privileges and access pods may have on top of the\npredefined security standards, for specific use cases.\nFor a hands-on tutorial on Pod Security, see the blog post Kubernetes 1.23: Pod Security Graduates to Beta.\nMemory and CPU limits should be set in order to restrict the memory and CPU resources a pod can consume on a node, and\ntherefore prevent potential DoS attacks from malicious or breached workloads. Such policy can be enforced by an admission\ncontroller. Please note that CPU limits will throttle usage and thus can have unintended effects on auto-scaling features or efficiency\ni.e. running the process in best effort with the CPU resource available.\nCaution:\nMemory limit superior to request can expose the whole node to OOM issues."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0484", "text": "Enabling Seccomp\nSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to\nsandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel. Kubernetes lets you\nautomatically apply seccomp profiles loaded onto a node to your Pods and containers.\nSeccomp can improve the security of your workloads by reducing the Linux kernel syscall attack surface available inside containers.\nThe seccomp filter mode leverages BPF to create an allow or deny list of specific syscalls, named profiles.\nSince Kubernetes 1.27, you can enable the use of RuntimeDefault as the default seccomp profile for all workloads. A security\ntutorial is available on this topic. In addition, the Kubernetes Security Profiles Operator is a project that facilitates the management\nand use of seccomp in clusters.\nNote:\nSeccomp is only available on Linux nodes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n460/684\n\n11/7/25, 4:37 PM\n\nEnabling AppArmor or SELinux\n\nConcepts | Kubernetes\n\nAppArmor\nAppArmor is a Linux kernel security module that can provide an easy way to implement Mandatory Access Control (MAC) and better\nauditing through system logs. A default AppArmor profile is enforced on nodes that support it, or a custom profile can be\nconfigured. Like seccomp, AppArmor is also configured through profiles, where each profile is either running in enforcing mode,\nwhich blocks access to disallowed resources or complain mode, which only reports violations. AppArmor profiles are enforced on a\nper-container basis, with an annotation, allowing for processes to gain just the right privileges.\nNote:\nAppArmor is only available on Linux nodes, and enabled in some Linux distributions.\n\nSELinux\nSELinux is also a Linux kernel security module that can provide a mechanism for supporting access control security policies,\nincluding Mandatory Access Controls (MAC). SELinux labels can be assigned to containers or pods via their securityContext section.\nNote:\nSELinux is only available on Linux nodes, and enabled in some Linux distributions.\n\nLogs and auditing\nAudit logs, if enabled, are protected from general access."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0485", "text": "Pod placement\nPod placement is done in accordance with the tiers of sensitivity of the application.\nSensitive applications are running isolated on nodes or with specific sandboxed runtimes.\nPods that are on different tiers of sensitivity, for example, an application pod and the Kubernetes API server, should be deployed\nonto separate nodes. The purpose of node isolation is to prevent an application container breakout to directly providing access to\napplications with higher level of sensitivity to easily pivot within the cluster. This separation should be enforced to prevent pods\naccidentally being deployed onto the same node. This could be enforced with the following features:\nNode Selectors\nKey-value pairs, as part of the pod specification, that specify which nodes to deploy onto. These can be enforced at the\nnamespace and cluster level with the PodNodeSelector admission controller.\nPodTolerationRestriction\nAn admission controller that allows administrators to restrict permitted tolerations within a namespace. Pods within a\nnamespace may only utilize the tolerations specified on the namespace object annotation keys that provide a set of default and\nallowed tolerations.\nRuntimeClass\nRuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a\nPod's containers and can provide more or less isolation from the host at the cost of performance overhead.\n\nSecrets\nConfigMaps are not used to hold confidential data.\nEncryption at rest is configured for the Secret API.\nIf appropriate, a mechanism to inject secrets stored in third-party storage is deployed and available.\nService account tokens are not mounted in pods that don't require them.\nhttps://kubernetes.io/docs/concepts/_print/\n\n461/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0486", "text": "Bound service account token volume is in-use instead of non-expiring tokens.\nSecrets required for pods should be stored within Kubernetes Secrets as opposed to alternatives such as ConfigMap. Secret\nresources stored within etcd should be encrypted at rest.\nPods needing secrets should have these automatically mounted through volumes, preferably stored in memory like with the\nemptyDir.medium option. Mechanism can be used to also inject secrets from third-party storages as volume, like the Secrets Store\nCSI Driver. This should be done preferentially as compared to providing the pods service account RBAC access to secrets. This would\nallow adding secrets into the pod as environment variables or files. Please note that the environment variable method might be\nmore prone to leakage due to crash dumps in logs and the non-confidential nature of environment variable in Linux, as opposed to\nthe permission mechanism on files.\nService account tokens should not be mounted into pods that do not require them. This can be configured by setting\nautomountServiceAccountToken to false either within the service account to apply throughout the namespace or specifically for a\npod. For Kubernetes v1.22 and above, use Bound Service Accounts for time-bound service account credentials."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0487", "text": "Images\nMinimize unnecessary content in container images.\nContainer images are configured to be run as unprivileged user.\nReferences to container images are made by sha256 digests (rather than tags) or the provenance of the image is validated by\nverifying the image's digital signature at deploy time via admission control.\nContainer images are regularly scanned during creation and in deployment, and known vulnerable software is patched.\nContainer image should contain the bare minimum to run the program they package. Preferably, only the program and its\ndependencies, building the image from the minimal possible base. In particular, image used in production should not contain shells\nor debugging utilities, as an ephemeral debug container can be used for troubleshooting.\nBuild images to directly start with an unprivileged user by using the USER instruction in Dockerfile. The Security Context allows a\ncontainer image to be started with a specific user and group with runAsUser and runAsGroup , even if not specified in the image\nmanifest. However, the file permissions in the image layers might make it impossible to just start the process with a new\nunprivileged user without image modification.\nAvoid using image tags to reference an image, especially the latest tag, the image behind a tag can be easily modified in a registry.\nPrefer using the complete sha256 digest which is unique to the image manifest. This policy can be enforced via an\nImagePolicyWebhook. Image signatures can also be automatically verified with an admission controller at deploy time to validate\ntheir authenticity and integrity.\nScanning a container image can prevent critical vulnerabilities from being deployed to the cluster alongside the container image.\nImage scanning should be completed before deploying a container image to a cluster and is usually done as part of the deployment\nprocess in a CI/CD pipeline. The purpose of an image scan is to obtain information about possible vulnerabilities and their\nprevention in the container image, such as a Common Vulnerability Scoring System (CVSS) score. If the result of the image scans is\ncombined with the pipeline compliance rules, only properly patched container images will end up in Production."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0488", "text": "Admission controllers\nAn appropriate selection of admission controllers is enabled.\nA pod security policy is enforced by the Pod Security Admission or/and a webhook admission controller.\nThe admission chain plugins and webhooks are securely configured.\nAdmission controllers can help improve the security of the cluster. However, they can present risks themselves as they extend the\nAPI server and should be properly secured.\nThe following lists present a number of admission controllers that could be considered to enhance the security posture of your\ncluster and application. It includes controllers that may be referenced in other parts of this document.\nThis first group of admission controllers includes plugins enabled by default, consider to leave them enabled unless you know what\nyou are doing:\nCertificateApproval\n\nPerforms additional authorization checks to ensure the approving user has permission to approve certificate request.\nhttps://kubernetes.io/docs/concepts/_print/\n\n462/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nCertificateSigning\n\nPerforms additional authorization checks to ensure the signing user has permission to sign certificate requests.\nCertificateSubjectRestriction\n\nRejects any certificate request that specifies a 'group' (or 'organization attribute') of system:masters.\nLimitRanger\n\nEnforces the LimitRange API constraints.\nMutatingAdmissionWebhook\n\nAllows the use of custom controllers through webhooks, these controllers may mutate requests that they review.\nPodSecurity\n\nReplacement for Pod Security Policy, restricts security contexts of deployed Pods.\nResourceQuota\n\nEnforces resource quotas to prevent over-usage of resources.\nValidatingAdmissionWebhook\n\nAllows the use of custom controllers through webhooks, these controllers do not mutate requests that it reviews.\nThe second group includes plugins that are not enabled by default but are in general availability state and are recommended to\nimprove your security posture:\nDenyServiceExternalIPs\n\nRejects all net-new usage of the Service.spec.externalIPs field. This is a mitigation for CVE-2020-8554: Man in the middle using\nLoadBalancer or ExternalIPs.\nNodeRestriction\n\nRestricts kubelet's permissions to only modify the pods API resources they own or the node API resource that represent\nthemselves. It also prevents kubelet from using the node-restriction.kubernetes.io/ annotation, which can be used by an\nattacker with access to the kubelet's credentials to influence pod placement to the controlled node.\nThe third group includes plugins that are not enabled by default but could be considered for certain use cases:\nAlwaysPullImages\n\nEnforces the usage of the latest version of a tagged image and ensures that the deployer has permissions to use the image.\nImagePolicyWebhook"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0489", "text": "Allows enforcing additional controls for images through webhooks.\n\nWhat's next\nPrivilege escalation via Pod creation warns you about a specific access control risk; check how you're managing that threat.\nIf you use Kubernetes RBAC, read RBAC Good Practices for further information on authorization.\nSecuring a Cluster for information on protecting a cluster from accidental or malicious access.\nCluster Multi-tenancy guide for configuration options recommendations and best practices on multi-tenancy.\nBlog post \"A Closer Look at NSA/CISA Kubernetes Hardening Guidance\" for complementary resource on hardening Kubernetes\nclusters.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n463/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n8.17 - Application Security Checklist\nBaseline guidelines around ensuring application security on Kubernetes, aimed at application developers\nThis checklist aims to provide basic guidelines on securing applications running in Kubernetes from a developer's perspective. This\nlist is not meant to be exhaustive and is intended to evolve over time.\nOn how to read and use this document:\nThe order of topics does not reflect an order of priority.\nSome checklist items are detailed in the paragraph below the list of each section.\nThis checklist assumes that a developer is a Kubernetes cluster user who interacts with namespaced scope objects.\nCaution:\nChecklists are not sufficient for attaining a good security posture on their own. A good security posture requires constant\nattention and improvement, but a checklist can be the first step on the never-ending journey towards security preparedness.\nSome recommendations in this checklist may be too restrictive or too lax for your specific security needs. Since Kubernetes\nsecurity is not \"one size fits all\", each category of checklist items should be evaluated on its merits.\n\nBase security hardening\nThe following checklist provides base security hardening recommendations that would apply to most applications deploying to\nKubernetes.\n\nApplication design\nFollow the right security principles when designing applications.\nApplication configured with appropriate QoS class through resource request and limits.\nMemory limit is set for the workloads with a limit equal to or greater than the request.\nCPU limit might be set on sensitive workloads.\n\nService account\nAvoid using the default ServiceAccount. Instead, create ServiceAccounts for each workload or microservice.\nautomountServiceAccountToken\n\nshould be set to false unless the pod specifically requires access to the Kubernetes API to\n\noperate."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0490", "text": "Pod-level securityContext recommendations\nSet runAsNonRoot: true .\nConfigure the container to execute as a less privileged user (for example, using runAsUser and runAsGroup ), and configure\nappropriate permissions on files or directories inside the container image.\nOptionally add a supplementary group with fsGroup to access persistent volumes.\nThe application deploys into a namespace that enforces an appropriate Pod security standard. If you cannot control this\nenforcement for the cluster(s) where the application is deployed, take this into account either through documentation or\nadditional defense in depth.\n\nContainer-level securityContext recommendations\nDisable privilege escalations using allowPrivilegeEscalation: false .\nConfigure the root filesystem to be read-only with readOnlyRootFilesystem: true .\nAvoid running privileged containers (set privileged: false ).\nDrop all capabilities from the containers and add back only specific ones that are needed for operation of the container.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n464/684\n\n11/7/25, 4:37 PM\n\nRole Based Access Control (RBAC)\n\nConcepts | Kubernetes\n\nPermissions such as create, patch, update and delete should be only granted if necessary.\nAvoid creating RBAC permissions to create or update roles which can lead to privilege escalation.\nReview bindings for the system:unauthenticated group and remove them where possible, as this gives access to anyone\nwho can contact the API server at a network level.\nThe create, update and delete verbs should be permitted judiciously. The patch verb if allowed on a Namespace can allow users to\nupdate labels on the namespace or deployments which can increase the attack surface.\nFor sensitive workloads, consider providing a recommended ValidatingAdmissionPolicy that further restricts the permitted write\nactions.\n\nImage security\nUsing an image scanning tool to scan an image before deploying containers in the Kubernetes cluster.\nUse container signing to validate the container image signature before deploying to the Kubernetes cluster.\n\nNetwork policies\nConfigure NetworkPolicies to only allow expected ingress and egress traffic from the pods.\nMake sure that your cluster provides and enforces NetworkPolicy. If you are writing an application that users will deploy to different\nclusters, consider whether you can assume that NetworkPolicy is available and enforced.\n\nAdvanced security hardening\nThis section of this guide covers some advanced security hardening points which might be valuable based on different Kubernetes\nenvironment setup.\n\nLinux container security\nConfigure Security Context for the pod-container.\nSet the Seccomp Profile for a Container.\nRestrict a Container's Access to Resources with AppArmor.\nAssign SELinux Labels to a Container."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0491", "text": "Runtime classes\nConfigure appropriate runtime classes for containers.\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nSome containers may require a different isolation level from what is provided by the default runtime of the cluster.\nruntimeClassName can be used in a podspec to define a different runtime class.\nFor sensitive workloads consider using kernel emulation tools like gVisor, or virtualized isolation using a mechanism such as katacontainers.\nIn high trust environments, consider using confidential virtual machines to improve cluster security even further.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n465/684\n\n11/7/25, 4:37 PM\n\n9 - Policies\n\nConcepts | Kubernetes\n\nManage security and best-practices with policies.\nKubernetes policies are configurations that manage other configurations or runtime behaviors. Kubernetes offers various forms of\npolicies, described below:\n\nApply policies using API objects\nSome API objects act as policies. Here are some examples:\nNetworkPolicies can be used to restrict ingress and egress traffic for a workload.\nLimitRanges manage resource allocation constraints across different object kinds.\nResourceQuotas limit resource consumption for a namespace.\n\nApply policies using admission controllers\nAn admission controller runs in the API server and can validate or mutate API requests. Some admission controllers act to apply\npolicies. For example, the AlwaysPullImages admission controller modifies a new Pod to set the image pull policy to Always .\nKubernetes has several built-in admission controllers that are configurable via the API server --enable-admission-plugins flag.\nDetails on admission controllers, with the complete list of available admission controllers, are documented in a dedicated section:\nAdmission Controllers\n\nApply policies using ValidatingAdmissionPolicy\nValidating admission policies allow configurable validation checks to be executed in the API server using the Common Expression\nLanguage (CEL). For example, a ValidatingAdmissionPolicy can be used to disallow use of the latest image tag.\nA ValidatingAdmissionPolicy operates on an API request and can be used to block, audit, and warn users about non-compliant\nconfigurations.\nDetails on the ValidatingAdmissionPolicy API, with examples, are documented in a dedicated section:\nValidating Admission Policy"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0492", "text": "Apply policies using dynamic admission control\nDynamic admission controllers (or admission webhooks) run outside the API server as separate applications that register to receive\nwebhooks requests to perform validation or mutation of API requests.\nDynamic admission controllers can be used to apply policies on API requests and trigger other policy-based workflows. A dynamic\nadmission controller can perform complex checks including those that require retrieval of other cluster resources and external data.\nFor example, an image verification check can lookup data from OCI registries to validate the container image signatures and\nattestations.\nDetails on dynamic admission control are documented in a dedicated section:\nDynamic Admission Control\n\nImplementations\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nhttps://kubernetes.io/docs/concepts/_print/\n\n466/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDynamic Admission Controllers that act as flexible policy engines are being developed in the Kubernetes ecosystem, such as:\nKubewarden\nKyverno\nOPA Gatekeeper\nPolaris\n\nApply policies using Kubelet configurations\nKubernetes allows configuring the Kubelet on each worker node. Some Kubelet configurations act as policies:\nProcess ID limits and reservations are used to limit and reserve allocatable PIDs.\nNode Resource Managers can manage compute, memory, and device resources for latency-critical and high-throughput\nworkloads.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n467/684\n\n11/7/25, 4:37 PM\n\n9.1 - Limit Ranges\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0493", "text": "By default, containers run with unbounded compute resources on a Kubernetes cluster. Using Kubernetes resource quotas,\nadministrators (also termed cluster operators) can restrict consumption and creation of cluster resources (such as CPU time,\nmemory, and persistent storage) within a specified namespace. Within a namespace, a Pod can consume as much CPU and memory\nas is allowed by the ResourceQuotas that apply to that namespace. As a cluster operator, or as a namespace-level administrator, you\nmight also be concerned about making sure that a single object cannot monopolize all available resources within a namespace.\nA LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object\nkind (such as Pod or PersistentVolumeClaim) in a namespace.\nA LimitRange provides constraints that can:\nEnforce minimum and maximum compute resources usage per Pod or Container in a namespace.\nEnforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.\nEnforce a ratio between request and limit for a resource in a namespace.\nSet default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.\nKubernetes constrains resource allocations to Pods in a particular namespace whenever there is at least one LimitRange object in\nthat namespace.\nThe name of a LimitRange object must be a valid DNS subdomain name."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0494", "text": "Constraints on resource limits and requests\nThe administrator creates a LimitRange in a namespace.\nUsers create (or try to create) objects in that namespace, such as Pods or PersistentVolumeClaims.\nFirst, the LimitRange admission controller applies default request and limit values for all Pods (and their containers) that do not\nset compute resource requirements.\nSecond, the LimitRange tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any\nLimitRange present in the namespace.\nIf you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates a LimitRange constraint, your request\nto the API server will fail with anHTTP status code 403 Forbidden and a message explaining the constraint that has been\nviolated.\nIf you add a LimitRange in a namespace that applies to compute-related resources such as cpu and memory , you must specify\nrequests or limits for those values. Otherwise, the system may reject Pod creation.\nLimitRange validations occur only at Pod admission stage, not on running Pods. If you add or modify a LimitRange, the Pods\nthat already exist in that namespace continue unchanged.\nIf two or more LimitRange objects exist in the namespace, it is not deterministic which default value will be applied.\n\nLimitRange and admission checks for Pods\nA LimitRange does not check the consistency of the default values it applies. This means that a default value for the limit that is set\nby LimitRange may be less than the request value specified for the container in the spec that a client submits to the API server. If that\nhappens, the final Pod will not be schedulable.\nFor example, you define a LimitRange with below manifest:\nNote:\nThe following examples operate within the default namespace of your cluster, as the namespace parameter is undefined and\nthe LimitRange scope is limited to the namespace level. This implies that any references or operations within these examples\nwill interact with elements within the default namespace of your cluster. You can override the operating namespace by\nconfiguring namespace in the metadata.namespace field.\n\nconcepts/policy/limit-range/problematic-limit-range.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n468/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: LimitRange\nmetadata:\nname: cpu-resource-constraint\nspec:\nlimits:\n- default: # this section defines default limits\ncpu: 500m\ndefaultRequest: # this section defines default requests\ncpu: 500m\nmax: # max and min define the limit range\ncpu: \"1\"\nmin:\ncpu: 100m\ntype: Container"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0495", "text": "along with a Pod that declares a CPU resource request of 700m , but not a limit:\nconcepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: example-conflict-with-limitrange-cpu\nspec:\ncontainers:\n- name: demo\nimage: registry.k8s.io/pause:3.8\nresources:\nrequests:\ncpu: 700m\n\nthen that Pod will not be scheduled, failing with an error similar to:\nPod \"example-conflict-with-limitrange-cpu\" is invalid: spec.containers[0].resources.requests: Invalid value: \"700m\":\n\nIf you set both request and limit , then that new Pod will be scheduled successfully even with the same LimitRange in place:\nconcepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: example-no-conflict-with-limitrange-cpu\nspec:\ncontainers:\n- name: demo\nimage: registry.k8s.io/pause:3.8\nresources:\nrequests:\ncpu: 700m\nlimits:\ncpu: 700m\n\nExample resource constraints\nExamples of policies that could be created using LimitRange are:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n469/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIn a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a\nmax limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.\nDefine default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and\nmemory requests in their specs.\nIn the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers, there may be\ncontention for resources. In this case, the Containers or Pods will not be created.\nNeither contention nor changes to a LimitRange will affect already created resources.\n\nWhat's next\nFor examples on using limits, see:\nhow to configure minimum and maximum CPU constraints per namespace.\nhow to configure minimum and maximum Memory constraints per namespace.\nhow to configure default CPU Requests and Limits per namespace.\nhow to configure default Memory Requests and Limits per namespace.\nhow to configure minimum and maximum Storage consumption per namespace.\na detailed example on configuring quota per namespace.\nRefer to the LimitRanger design document for context and historical information.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n470/684\n\n11/7/25, 4:37 PM\n\n9.2 - Resource Quotas\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0496", "text": "When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its\nfair share of resources.\nResource quotas are a tool for administrators to address this concern.\nA resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per\nnamespace. A ResourceQuota can also limit the quantity of objects that can be created in a namespace by API kind, as well as the\ntotal amount of infrastructure resources that may be consumed by API objects found in that namespace.\nCaution:\nNeither contention nor changes to quota will affect already created resources.\n\nHow Kubernetes ResourceQuotas work\nResourceQuotas work like this:\nDifferent teams work in different namespaces. This separation can be enforced with RBAC or any other authorization\nmechanism.\nA cluster administrator creates at least one ResourceQuota for each namespace.\nTo make sure the enforcement stays enforced, the cluster administrator should also restrict access to delete or update\nthat ResourceQuota; for example, by defining a ValidatingAdmissionPolicy.\nUsers create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed\nhard resource limits defined in a ResourceQuota.\nYou can apply a scope to a ResourceQuota to limit where it applies,\nIf creating or updating a resource violates a quota constraint, the control plane rejects that request with HTTP status code 403\nForbidden . The error includes a message explaining the constraint that would have been violated.\nIf quotas are enabled in a namespace for resource such as cpu and memory , users must specify requests or limits for those\nvalues when they define a Pod; otherwise, the quota system may reject pod creation.\nThe resource quota walkthrough shows an example of how to avoid this problem.\nNote:\nYou can define a LimitRange to force defaults on pods that make no compute resource requirements (so that users don't\nhave to remember to do that)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0497", "text": "You often do not create Pods directly; for example, you more usually create a workload management object such as a Deployment. If\nyou create a Deployment that tries to use more resources than are available, the creation of the Deployment (or other workload\nmanagement object) succeeds, but the Deployment may not be able to get all of the Pods it manages to exist. In that case you can\ncheck the status of the Deployment, for example with kubectl describe , to see what has happened.\nFor cpu and memory resources, ResourceQuotas enforce that every (new) pod in that namespace sets a limit for that\nresource. If you enforce a resource quota in a namespace for either cpu or memory , you and other clients, must specify either\nrequests or limits for that resource, for every new Pod you submit. If you don't, the control plane may reject admission for\nthat Pod.\nFor other resources: ResourceQuota works and will ignore pods in the namespace without setting a limit or request for that\nresource. It means that you can create a new pod without limit/request for ephemeral storage if the resource quota limits the\nephemeral storage of this namespace.\nYou can use a LimitRange to automatically set a default request for these resources.\nThe name of a ResourceQuota object must be a valid DNS subdomain name.\nExamples of policies that could be created using namespaces and quotas are:\nhttps://kubernetes.io/docs/concepts/_print/\n\n471/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIn a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores, let B use 10GiB and 4 cores, and\nhold 2GiB and 2 cores in reserve for future allocation.\nLimit the \"testing\" namespace to using 1 core and 1GiB RAM. Let the \"production\" namespace use any amount.\nIn the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces, there may be contention for\nresources. This is handled on a first-come-first-served basis.\n\nEnabling Resource Quota\nResourceQuota support is enabled by default for many Kubernetes distributions. It is enabled when the API server --enableadmission-plugins= flag has ResourceQuota as one of its arguments.\nA resource quota is enforced in a particular namespace when there is a ResourceQuota in that namespace."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0498", "text": "Types of resource quota\nThe ResourceQuota mechanism lets you enforce different kinds of limits. This section describes the types of limit that you can\nenforce.\n\nQuota for infrastructure resources\nYou can limit the total sum of compute resources that can be requested in a given namespace.\nThe following resource types are supported:\nResource Name\n\nDescription\n\nlimits.cpu\n\nAcross all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.\n\nlimits.memory\n\nAcross all pods in a non-terminal state, the sum of memory limits cannot exceed this value.\n\nrequests.cpu\n\nAcross all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.\n\nrequests.memory\n\nAcross all pods in a non-terminal state, the sum of memory requests cannot exceed this value.\n\nhugepages<size>\n\nAcross all pods in a non-terminal state, the number of huge page requests of the specified size cannot\nexceed this value.\n\ncpu\n\nSame as requests.cpu\n\nmemory\n\nSame as requests.memory\n\nQuota for extended resources\nIn addition to the resources mentioned above, in release 1.10, quota support for extended resources is added.\nAs overcommit is not allowed for extended resources, it makes no sense to specify both requests and limits for the same\nextended resource in a quota. So for extended resources, only quota items with prefix requests. are allowed.\nTake the GPU resource as an example, if the resource name is nvidia.com/gpu , and you want to limit the total number of GPUs\nrequested in a namespace to 4, you can define a quota as follows:\nrequests.nvidia.com/gpu: 4\n\nSee Viewing and Setting Quotas for more details.\n\nQuota for storage\nYou can limit the total sum of storage for volumes that can be requested in a given namespace.\nhttps://kubernetes.io/docs/concepts/_print/\n\n472/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIn addition, you can limit consumption of storage resources based on associated StorageClass.\nResource Name\n\nDescription\n\nrequests.storage\n\nAcross all persistent volume claims, the sum of\nstorage requests cannot exceed this value.\n\npersistentvolumeclaims\n\nThe total number of PersistentVolumeClaims that\ncan exist in the namespace.\n\n<storage-class-\n\nAcross all persistent volume claims associated\nwith the <storage-class-name> , the sum of\nstorage requests cannot exceed this value.\n\nname>.storageclass.storage.k8s.io/requests.storage\n\n<storage-classname>.storageclass.storage.k8s.io/persistentvolumeclaims\n\nAcross all persistent volume claims associated\nwith the <storage-class-name> , the total\nnumber of persistent volume claims that can\nexist in the namespace."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0499", "text": "For example, if you want to quota storage with gold StorageClass separate from a bronze StorageClass, you can define a quota as\nfollows:\ngold.storageclass.storage.k8s.io/requests.storage: 500Gi\nbronze.storageclass.storage.k8s.io/requests.storage: 100Gi\n\nQuota for local ephemeral storage\nâ“˜ FEATURE STATE: Kubernetes v1.8 [alpha]\n\nResource Name\n\nDescription\n\nrequests.ephemeral-\n\nAcross all pods in the namespace, the sum of local ephemeral storage requests cannot\nexceed this value.\n\nstorage\nlimits.ephemeral-storage\n\nAcross all pods in the namespace, the sum of local ephemeral storage limits cannot exceed\nthis value.\n\nephemeral-storage\n\nSame as requests.ephemeral-storage .\n\nNote:\nWhen using a CRI container runtime, container logs will count against the ephemeral storage quota. This can result in the\nunexpected eviction of pods that have exhausted their storage quotas.\nRefer to Logging Architecture for details.\n\nQuota on object count\nYou can set quota for the total number of one particular resource kind in the Kubernetes API, using the following syntax:\ncount/<resource>.<group>\ncount/<resource>\n\nfor resources from non-core API groups\n\nfor resources from the core API group\n\nFor example, the PodTemplate API is in the core API group and so if you want to limit the number of PodTemplate objects in a\nnamespace, you use count/podtemplates .\nThese types of quotas are useful to protect against exhaustion of control plane storage. For example, you may want to limit the\nnumber of Secrets in a server given their large size. Too many Secrets in a cluster can actually prevent servers and controllers from\nstarting. You can set a quota for Jobs to protect against a poorly configured CronJob. CronJobs that create too many Jobs in a\nhttps://kubernetes.io/docs/concepts/_print/\n\n473/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nnamespace can lead to a denial of service.\nIf you define a quota this way, it applies to Kubernetes' APIs that are part of the API server, and to any custom resources backed by a\nCustomResourceDefinition. For example, to create a quota on a widgets custom resource in the example.com API group, use\ncount/widgets.example.com . If you use API aggregation to add additional, custom APIs that are not defined as\nCustomResourceDefinitions, the core Kubernetes control plane does not enforce quota for the aggregated API. The extension API\nserver is expected to provide quota enforcement if that's appropriate for the custom API."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0500", "text": "Generic syntax\nThis is a list of common examples of object kinds that you may want to put under object count quota, listed by the configuration\nstring that you would use.\ncount/pods\ncount/persistentvolumeclaims\ncount/services\ncount/secrets\ncount/configmaps\ncount/deployments.apps\ncount/replicasets.apps\ncount/statefulsets.apps\ncount/jobs.batch\ncount/cronjobs.batch\n\nSpecialized syntax\nThere is another syntax only to set the same type of quota, that only works for certain API kinds. The following types are supported:\nResource Name\n\nDescription\n\nconfigmaps\n\nThe total number of ConfigMaps that can exist in the namespace.\n\npersistentvolumeclaims\n\nThe total number of PersistentVolumeClaims that can exist in the namespace.\n\npods\n\nThe total number of Pods in a non-terminal state that can exist in the namespace. A pod is in a\nterminal state if .status.phase in (Failed, Succeeded) is true.\n\nreplicationcontrollers\n\nThe total number of ReplicationControllers that can exist in the namespace.\n\nresourcequotas\n\nThe total number of ResourceQuotas that can exist in the namespace.\n\nservices\n\nThe total number of Services that can exist in the namespace.\n\nservices.loadbalancers\n\nThe total number of Services of type LoadBalancer that can exist in the namespace.\n\nservices.nodeports\n\nThe total number of NodePorts allocated to Services of type NodePort or LoadBalancer\nthat can exist in the namespace.\n\nsecrets\n\nThe total number of Secrets that can exist in the namespace.\n\nFor example, pods quota counts and enforces a maximum on the number of pods created in a single namespace that are not\nterminal. You might want to set a pods quota on a namespace to avoid the case where a user creates many small pods and\nexhausts the cluster's supply of Pod IPs.\nYou can find more examples on Viewing and Setting Quotas.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n474/684\n\n11/7/25, 4:37 PM\n\nViewing and Setting Quotas\n\nConcepts | Kubernetes\n\nkubectl supports creating, updating, and viewing quotas:\n\nkubectl create namespace myspace\n\ncat <<EOF > compute-resources.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: compute-resources\nspec:\nhard:\nrequests.cpu: \"1\"\nrequests.memory: \"1Gi\"\nlimits.cpu: \"2\"\nlimits.memory: \"2Gi\"\nrequests.nvidia.com/gpu: 4\nEOF\n\nkubectl create -f ./compute-resources.yaml --namespace=myspace\n\ncat <<EOF > object-counts.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: object-counts\nspec:\nhard:\nconfigmaps: \"10\"\npersistentvolumeclaims: \"4\"\npods: \"4\"\nreplicationcontrollers: \"20\"\nsecrets: \"10\"\nservices: \"10\"\nservices.loadbalancers: \"2\"\nEOF\n\nkubectl create -f ./object-counts.yaml --namespace=myspace\n\nkubectl get quota --namespace=myspace\n\nNAME\n\nAGE\n\ncompute-resources\n\n30s\n\nobject-counts\n\n32s\n\nkubectl describe quota compute-resources --namespace=myspace\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n475/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nName:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0501", "text": "compute-resources\n\nNamespace:\n\nmyspace\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\nlimits.cpu\nlimits.memory\n\n0\n0\n\n2\n2Gi\n\nrequests.cpu\n\n0\n\n1\n\nrequests.memory\n\n0\n\n1Gi\n\nrequests.nvidia.com/gpu\n\n0\n\n4\n\nkubectl describe quota object-counts --namespace=myspace\n\nName:\n\nobject-counts\n\nNamespace:\n\nmyspace\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\nconfigmaps\n\n0\n\n10\n\npersistentvolumeclaims\n\n0\n\n4\n\npods\nreplicationcontrollers\n\n0\n0\n\n4\n20\n\nsecrets\n\n1\n\n10\n\nservices\n\n0\n\n10\n\nservices.loadbalancers\n\n0\n\n2\n\nkubectl also supports object count quota for all standard namespaced resources using the syntax count/<resource>.<group> :\n\nkubectl create namespace myspace\n\nkubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --na\n\nkubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2\n\nkubectl describe quota --namespace=myspace\n\nName:\n\ntest\n\nNamespace:\n\nmyspace\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\ncount/deployments.apps\ncount/pods\n\n1\n2\n\n2\n3\n\ncount/replicasets.apps\n\n1\n\n4\n\ncount/secrets\n\n1\n\n4\n\nQuota and Cluster Capacity\nResourceQuotas are independent of the cluster capacity. They are expressed in absolute units. So, if you add nodes to your cluster,\nthis does not automatically give each namespace the ability to consume more resources.\nSometimes more complex policies may be desired, such as:\nProportionally divide total cluster resources among several teams.\nAllow each tenant to grow resource usage as needed, but have a generous limit to prevent accidental resource exhaustion.\nhttps://kubernetes.io/docs/concepts/_print/\n\n476/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDetect demand from one namespace, add nodes, and increase quota.\nSuch policies could be implemented using ResourceQuotas as building blocks, by writing a \"controller\" that watches the quota usage\nand adjusts the quota hard limits of each namespace according to other signals.\nNote that resource quota divides up aggregate cluster resources, but it creates no restrictions around nodes: pods from several\nnamespaces may run on the same node.\n\nQuota scopes\nEach quota can have an associated set of scopes . A quota will only measure usage for a resource if it matches the intersection of\nenumerated scopes.\nWhen a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope. Resources\nspecified on the quota outside of the allowed set results in a validation error.\nKubernetes 1.34 supports the following scopes:\nScope\n\nDescription\n\nBestEffort\n\nMatch pods that have best effort quality of service.\n\nCrossNamespacePodAffinity\n\nMatch pods that have cross-namespace pod (anti)affinity terms.\n\nNotBestEffort\n\nMatch pods that do not have best effort quality of service.\n\nNotTerminating\n\nMatch pods where .spec.activeDeadlineSeconds ]() is nil ]()\n\nPriorityClass\n\nMatch pods that references the specified priority class.\n\nTerminating"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0502", "text": "Match pods where .spec.activeDeadlineSeconds ]() >= 0 ]()\n\nVolumeAttributesClass\n\nMatch PersistentVolumeClaims that reference the specified volume attributes class.\n\nResourceQuotas with a scope set can also have a optional scopeSelector field. You define one or more match expressions that\nspecify an operators and, if relevant, a set of values to match. For example:\n\nscopeSelector:\nmatchExpressions:\n- scopeName: BestEffort # Match pods that have best effort quality of service\noperator: Exists # optional; \"Exists\" is implied for BestEffort scope\n\nThe scopeSelector supports the following values in the operator field:\nIn\nNotIn\nExists\nDoesNotExist\n\nIf the operator is In or NotIn , the values field must have at least one value. For example:\n\nscopeSelector:\nmatchExpressions:\n- scopeName: PriorityClass\noperator: In\nvalues:\n- middle\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n477/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the operator is Exists or DoesNotExist , the values field must NOT be specified.\n\nBest effort Pods scope\nThis scope only tracks quota consumed by Pods. It only matches pods that have the best effort QoS class.\nThe operator for a scopeSelector must be Exists .\n\nNot-best-effort Pods scope\nThis scope only tracks quota consumed by Pods. It only matches pods that have the Guaranteed or Burstable QoS class.\nThe operator for a scopeSelector must be Exists .\n\nNon-terminating Pods scope\nThis scope only tracks quota consumed by Pods that are not terminating. The operator for a scopeSelector must be Exists .\nA Pod is not terminating if the .spec.activeDeadlineSeconds field is unset.\nYou can use a ResourceQuota with this scope to manage the following resources:\ncount.pods\npods\ncpu\nmemory\nrequests.cpu\nrequests.memory\nlimits.cpu\nlimits.memory\n\nTerminating Pods scope\nThis scope only tracks quota consumed by Pods that are terminating. The operator for a scopeSelector must be Exists .\nA Pod is considered as terminating if the .spec.activeDeadlineSeconds field is set to any number.\nYou can use a ResourceQuota with this scope to manage the following resources:\ncount.pods\npods\ncpu\nmemory\nrequests.cpu\nrequests.memory\nlimits.cpu\nlimits.memory\n\nCross-namespace pod affinity scope\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0503", "text": "You can use CrossNamespacePodAffinity quota scope to limit which namespaces are allowed to have pods with affinity terms that\ncross namespaces. Specifically, it controls which pods are allowed to set namespaces or namespaceSelector fields in pod\n(anti)affinity terms.\nPreventing users from using cross-namespace affinity terms might be desired since a pod with anti-affinity constraints can block\npods from all other namespaces from getting scheduled in a failure domain.\nhttps://kubernetes.io/docs/concepts/_print/\n\n478/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nUsing this scope, you (as a cluster administrator) can prevent certain namespaces - such as foo-ns in the example below - from\nhaving pods that use cross-namespace pod affinity. You configure this creating a ResourceQuota object in that namespace with\nCrossNamespacePodAffinity scope and hard limit of 0:\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: disable-cross-namespace-affinity\nnamespace: foo-ns\nspec:\nhard:\npods: \"0\"\nscopeSelector:\nmatchExpressions:\n- scopeName: CrossNamespacePodAffinity\noperator: Exists\n\nIf you want to disallow using namespaces and namespaceSelector by default, and only allow it for specific namespaces, you could\nconfigure CrossNamespacePodAffinity as a limited resource by setting the kube-apiserver flag --admission-control-config-file\nto the path of the following configuration file:\n\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: \"ResourceQuota\"\nconfiguration:\napiVersion: apiserver.config.k8s.io/v1\nkind: ResourceQuotaConfiguration\nlimitedResources:\n- resource: pods\nmatchScopes:\n- scopeName: CrossNamespacePodAffinity\noperator: Exists\n\nWith the above configuration, pods can use namespaces and namespaceSelector in pod affinity only if the namespace where they\nare created have a resource quota object with CrossNamespacePodAffinity scope and a hard limit greater than or equal to the\nnumber of pods using those fields.\n\nPriorityClass scope\nâ“˜ FEATURE STATE: Kubernetes v1.17 [stable]\n\nA ResourceQuota with a PriorityClass scope only matches Pods that have a particular priority class, and only if any scopeSelector in\nthe quota spec selects a particular Pod.\nPods can be created at a specific priority. You can control a pod's consumption of system resources based on a pod's priority, by\nusing the scopeSelector field in the quota spec.\nWhen quota is scoped for PriorityClass using the scopeSelector field, the ResourceQuota can only track (and limit) the following\nresources:\npods\ncpu\nmemory\nephemeral-storage\nlimits.cpu\nlimits.memory\nhttps://kubernetes.io/docs/concepts/_print/\n\n479/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nlimits.ephemeral-storage\nrequests.cpu\nrequests.memory\nrequests.ephemeral-storage"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0504", "text": "Example\nThis example creates a ResourceQuota matches it with pods at specific priorities. The example works as follows:\nPods in the cluster have one of the three PriorityClasses, \"low\", \"medium\", \"high\".\nIf you want to try this out, use a testing cluster and set up those three PriorityClasses before you continue.\nOne quota object is created for each priority.\nInspect this set of ResourceQuotas:\npolicy/quota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: pods-high\nspec:\nhard:\ncpu: \"1000\"\nmemory: \"200Gi\"\npods: \"10\"\nscopeSelector:\nmatchExpressions:\n- operator: In\nscopeName: PriorityClass\nvalues: [\"high\"]\n--apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: pods-medium\nspec:\nhard:\ncpu: \"10\"\nmemory: \"20Gi\"\npods: \"10\"\nscopeSelector:\nmatchExpressions:\n- operator: In\nscopeName: PriorityClass\nvalues: [\"medium\"]\n--apiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: pods-low\nspec:\nhard:\ncpu: \"5\"\nmemory: \"10Gi\"\npods: \"10\"\nscopeSelector:\nmatchExpressions:\n- operator: In\nscopeName: PriorityClass\nvalues: [\"low\"]\n\nApply the YAML using kubectl create .\nhttps://kubernetes.io/docs/concepts/_print/\n\n480/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl create -f https://k8s.io/examples/policy/quota.yaml\n\nresourcequota/pods-high created\nresourcequota/pods-medium created\nresourcequota/pods-low created\n\nVerify that Used quota is 0 using kubectl describe quota .\n\nkubectl describe quota\n\nName:\nNamespace:\n\npods-high\ndefault\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\ncpu\n\n0\n\n1k\n\nmemory\n\n0\n\n200Gi\n\npods\n\n0\n\n10\n\nName:\n\npods-low\n\nNamespace:\n\ndefault\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\ncpu\n\n0\n\n5\n\nmemory\npods\n\n0\n0\n\n10Gi\n10\n\nName:\n\npods-medium\n\nNamespace:\n\ndefault\n\nResource\n\nUsed\n\nHard\n\n-------cpu\n\n---0\n\n---10\n\nmemory\n\n0\n\n20Gi\n\npods\n\n0\n\n10\n\nCreate a pod with priority \"high\".\npolicy/high-priority-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: high-priority\nspec:\ncontainers:\n- name: high-priority\nimage: ubuntu\ncommand: [\"/bin/sh\"]\nargs: [\"-c\", \"while true; do echo hello; sleep 10;done\"]\nresources:\nrequests:\nmemory: \"10Gi\"\ncpu: \"500m\"\nlimits:\nmemory: \"10Gi\"\ncpu: \"500m\"\npriorityClassName: high\n\nTo create the Pod:\nhttps://kubernetes.io/docs/concepts/_print/\n\n481/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl create -f https://k8s.io/examples/policy/high-priority-pod.yaml\n\nVerify that \"Used\" stats for \"high\" priority quota, pods-high , has changed and that the other two quotas are unchanged.\n\nkubectl describe quota\n\nName:\n\npods-high\n\nNamespace:\n\ndefault\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\ncpu\n\n500m\n\n1k\n\nmemory\n\n10Gi\n\n200Gi\n\npods\n\n1\n\n10\n\nName:\n\npods-low\n\nNamespace:\n\ndefault\n\nResource\n\nUsed\n\nHard\n\n--------\n\n----\n\n----\n\ncpu\nmemory\n\n0\n0\n\n5\n10Gi\n\npods\n\n0\n\n10\n\nName:\n\npods-medium\n\nNamespace:\n\ndefault\n\nResource\n--------\n\nUsed\n----\n\nHard\n----\n\ncpu\n\n0\n\n10\n\nmemory\n\n0\n\n20Gi\n\npods\n\n0\n\n10"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0505", "text": "Limiting PriorityClass consumption by default\nIt may be desired that pods at a particular priority, such as \"cluster-services\", should be allowed in a namespace, if and only if, a\nmatching quota object exists.\nWith this mechanism, operators are able to restrict usage of certain high priority classes to a limited number of namespaces and not\nevery namespace will be able to consume these priority classes by default.\nTo enforce this, kube-apiserver flag --admission-control-config-file should be used to pass path to the following configuration\nfile:\n\napiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: \"ResourceQuota\"\nconfiguration:\napiVersion: apiserver.config.k8s.io/v1\nkind: ResourceQuotaConfiguration\nlimitedResources:\n- resource: pods\nmatchScopes:\n- scopeName: PriorityClass\noperator: In\nvalues: [\"cluster-services\"]\n\nThen, create a resource quota object in the kube-system namespace:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n482/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\npolicy/priority-class-resourcequota.yaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\nname: pods-cluster-services\nspec:\nscopeSelector:\nmatchExpressions:\n- operator : In\nscopeName: PriorityClass\nvalues: [\"cluster-services\"]\n\nkubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system\n\nresourcequota/pods-cluster-services created\n\nIn this case, a pod creation will be allowed if:\n1. the Pod's priorityClassName is not specified.\n2. the Pod's priorityClassName is specified to a value other than cluster-services .\n3. the Pod's priorityClassName is set to cluster-services , it is to be created in the kube-system namespace, and it has passed\nthe resource quota check.\nA Pod creation request is rejected if its priorityClassName is set to cluster-services and it is to be created in a namespace other\nthan kube-system .\n\nVolumeAttributesClass scope\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nThis scope only tracks quota consumed by PersistentVolumeClaims.\nPersistentVolumeClaims can be created with a specific VolumeAttributesClass, and might be modified after creation. You can control\na PVC's consumption of storage resources based on the associated VolumeAttributesClasses, by using the scopeSelector field in\nthe quota spec.\nThe PVC references the associated VolumeAttributesClass by the following fields:\nspec.volumeAttributesClassName\nstatus.currentVolumeAttributesClassName\nstatus.modifyVolumeStatus.targetVolumeAttributesClassName"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0506", "text": "A relevant ResourceQuota is matched and consumed only if the ResourceQuota has a scopeSelector that selects the PVC.\nWhen the quota is scoped for the volume attributes class using the scopeSelector field, the quota object is restricted to track only\nthe following resources:\npersistentvolumeclaims\nrequests.storage\n\nRead Limit Storage Consumption to learn more about this.\n\nWhat's next\nSee a detailed example for how to use resource quota.\nRead the ResourceQuota API reference\nLearn about LimitRanges\nhttps://kubernetes.io/docs/concepts/_print/\n\n483/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can read the historical ResourceQuota design document for more information.\nYou can also read the Quota support for priority class design document.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n484/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n9.3 - Process ID Limits And Reservations\nâ“˜ FEATURE STATE: Kubernetes v1.20 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0507", "text": "Kubernetes allow you to limit the number of process IDs (PIDs) that a Pod can use. You can also reserve a number of allocatable PIDs\nfor each node for use by the operating system and daemons (rather than by Pods).\nProcess IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the task limit without hitting any other resource limits,\nwhich can then cause instability to a host machine.\nCluster administrators require mechanisms to ensure that Pods running in the cluster cannot induce PID exhaustion that prevents\nhost daemons (such as the kubelet or kube-proxy, and potentially also the container runtime) from running. In addition, it is\nimportant to ensure that PIDs are limited among Pods in order to ensure they have limited impact on other workloads on the same\nnode.\nNote:\nOn certain Linux installations, the operating system sets the PIDs limit to a low default, such as 32768. Consider raising the\nvalue of /proc/sys/kernel/pid_max.\nYou can configure a kubelet to limit the number of PIDs a given Pod can consume. For example, if your node's host OS is set to use a\nmaximum of 262144 PIDs and expect to host less than 250 Pods, one can give each Pod a budget of 1000 PIDs to prevent using up\nthat node's overall number of available PIDs. If the admin wants to overcommit PIDs similar to CPU or memory, they may do so as\nwell with some additional risks. Either way, a single Pod will not be able to bring the whole machine down. This kind of resource\nlimiting helps to prevent simple fork bombs from affecting operation of an entire cluster.\nPer-Pod PID limiting allows administrators to protect one Pod from another, but does not ensure that all Pods scheduled onto that\nhost are unable to impact the node overall. Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.\nYou can also reserve an amount of PIDs for node overhead, separate from the allocation to Pods. This is similar to how you can\nreserve CPU, memory, or other resources for use by the operating system and other facilities outside of Pods and their containers.\nPID limiting is an important sibling to compute resource requests and limits. However, you specify it in a different way: rather than\ndefining a Pod's resource limit in the .spec for a Pod, you configure the limit as a setting on the kubelet. Pod-defined PID limits are\nnot currently supported.\nCaution:\nThis means that the limit that applies to a Pod may be different depending on where the Pod is scheduled. To make things\nsimple, it's easiest if all Nodes use the same PID resource limits and reservations."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0508", "text": "Node PID limits\nKubernetes allows you to reserve a number of process IDs for the system use. To configure the reservation, use the parameter pid=\n<number> in the --system-reserved and --kube-reserved command line options to the kubelet. The value you specified declares\nthat the specified number of process IDs will be reserved for the system as a whole and for Kubernetes system daemons\nrespectively.\n\nPod PID limits\nKubernetes allows you to limit the number of processes running in a Pod. You specify this limit at the node level, rather than\nconfiguring it as a resource limit for a particular Pod. Each Node can have a different PID limit.\nTo configure the limit, you can specify the command line parameter --pod-max-pids to the kubelet, or set PodPidsLimit in the\nkubelet configuration file.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n485/684\n\n11/7/25, 4:37 PM\n\nPID based eviction\n\nConcepts | Kubernetes\n\nYou can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources. This\nfeature is called eviction. You can Configure Out of Resource Handling for various eviction signals. Use pid.available eviction signal\nto configure the threshold for number of PIDs used by Pod. You can set soft and hard eviction policies. However, even with the hard\neviction policy, if the number of PIDs growing very fast, node can still get into unstable state by hitting the node PIDs limit. Eviction\nsignal value is calculated periodically and does NOT enforce the limit.\nPID limiting - per Pod and per Node sets the hard limit. Once the limit is hit, workload will start experiencing failures when trying to\nget a new PID. It may or may not lead to rescheduling of a Pod, depending on how workload reacts on these failures and how\nliveness and readiness probes are configured for the Pod. However, if limits were set correctly, you can guarantee that other Pods\nworkload and system processes will not run out of PIDs when one Pod is misbehaving.\n\nWhat's next\nRefer to the PID Limiting enhancement document for more information.\nFor historical context, read Process ID Limiting for Stability Improvements in Kubernetes 1.14.\nRead Managing Resources for Containers.\nLearn how to Configure Out of Resource Handling.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n486/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n9.4 - Node Resource Managers"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0509", "text": "In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers\naim to co-ordinate and optimise the alignment of node's resources for pods configured with a specific requirement for CPUs,\ndevices, and memory (hugepages) resources.\n\nHardware topology alignment policies\nTopology Manager is a kubelet component that aims to coordinate the set of components that are responsible for these\noptimizations. The overall resource management process is governed using the policy you specify. To learn more, read Control\nTopology Management Policies on a Node.\n\nPolicies for assigning CPUs to Pods\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable] (enabled by default: true)\n\nOnce a Pod is bound to a Node, the kubelet on that node may need to either multiplex the existing hardware (for example, sharing\nCPUs across multiple Pods) or allocate hardware by dedicating some resource (for example, assigning one of more CPUs for a Pod's\nexclusive use).\nBy default, the kubelet uses CFS quota to enforce pod CPU limits. When the node runs many CPU-bound pods, the workload can\nmove to different CPU cores depending on whether the pod is throttled and which CPU cores are available at scheduling time. Many\nworkloads are not sensitive to this migration and thus work fine without any intervention.\nHowever, in workloads where CPU cache affinity and scheduling latency significantly affect workload performance, the kubelet\nallows alternative CPU management policies to determine some placement preferences on the node. This is implemented using the\nCPU Manager and its policy. There are two available policies:\nnone : the none\n\npolicy explicitly enables the existing default CPU affinity scheme, providing no affinity beyond what the OS\nscheduler does automatically. Limits on CPU usage for Guaranteed pods and Burstable pods are enforced using CFS quota.\nstatic : the static policy allows containers in Guaranteed pods with integer CPU requests access to exclusive CPUs on the\nnode. This exclusivity is enforced using the cpuset cgroup controller.\nNote:\nSystem services such as the container runtime and the kubelet itself can continue to run on these exclusive CPUs. The\nexclusivity only extends to other pods.\nCPU Manager doesn't support offlining and onlining of CPUs at runtime."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0510", "text": "Static policy\nThe static policy enables finer-grained CPU management and exclusive CPU assignment. This policy manages a shared pool of CPUs\nthat initially contains all CPUs in the node. The amount of exclusively allocatable CPUs is equal to the total number of CPUs in the\nnode minus any CPU reservations set by the kubelet configuration. CPUs reserved by these options are taken, in integer quantity,\nfrom the initial shared pool in ascending order by physical core ID. This shared pool is the set of CPUs on which any containers in\nBestEffort and Burstable pods run. Containers in Guaranteed pods with fractional CPU requests also run on CPUs in the\nshared pool. Only containers that are part of a Guaranteed pod and have integer CPU requests are assigned exclusive CPUs.\nNote:\nThe kubelet requires a CPU reservation greater than zero when the static policy is enabled. This is because a zero CPU\nreservation would allow the shared pool to become empty.\nAs Guaranteed pods whose containers fit the requirements for being statically assigned are scheduled to the node, CPUs are\nremoved from the shared pool and placed in the cpuset for the container. CFS quota is not used to bound the CPU usage of these\ncontainers as their usage is bound by the scheduling domain itself. In others words, the number of CPUs in the container cpuset is\nhttps://kubernetes.io/docs/concepts/_print/\n\n487/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nequal to the integer CPU limit specified in the pod spec. This static assignment increases CPU affinity and decreases context\nswitches due to throttling for the CPU-bound workload.\nConsider the containers in the following pod specs:\n\nspec:\ncontainers:\n- name: nginx\nimage: nginx\n\nThe pod above runs in the BestEffort QoS class because no resource requests or limits are specified. It runs in the shared\npool.\n\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nresources:\nlimits:\nmemory: \"200Mi\"\nrequests:\nmemory: \"100Mi\"\n\nThe pod above runs in the Burstable QoS class because resource requests do not equal limits and the cpu quantity is not\nspecified. It runs in the shared pool.\n\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nresources:\nlimits:\nmemory: \"200Mi\"\ncpu: \"2\"\nrequests:\nmemory: \"100Mi\"\ncpu: \"1\"\n\nThe pod above runs in the Burstable QoS class because resource requests do not equal limits . It runs in the shared pool."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0511", "text": "spec:\ncontainers:\n- name: nginx\nimage: nginx\nresources:\nlimits:\nmemory: \"200Mi\"\ncpu: \"2\"\nrequests:\nmemory: \"200Mi\"\ncpu: \"2\"\n\nThe pod above runs in the Guaranteed QoS class because requests are equal to limits . And the container's resource limit for the\nCPU resource is an integer greater than or equal to one. The nginx container is granted 2 exclusive CPUs.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n488/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nresources:\nlimits:\nmemory: \"200Mi\"\ncpu: \"1.5\"\nrequests:\nmemory: \"200Mi\"\ncpu: \"1.5\"\n\nThe pod above runs in the Guaranteed QoS class because requests are equal to limits . But the container's resource limit for the\nCPU resource is a fraction. It runs in the shared pool.\n\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nresources:\nlimits:\nmemory: \"200Mi\"\ncpu: \"2\"\n\nThe pod above runs in the Guaranteed QoS class because only limits are specified and requests are set equal to limits when\nnot explicitly specified. And the container's resource limit for the CPU resource is an integer greater than or equal to one. The nginx\ncontainer is granted 2 exclusive CPUs.\n\nStatic policy options\nHere are the available policy options for the static CPU management policy, listed in alphabetical order:\nalign-by-socket (alpha, hidden by default)\n\nAlign CPUs by physical package / socket boundary, rather than logical NUMA boundaries (available since Kubernetes v1.25)\ndistribute-cpus-across-cores (alpha, hidden by default)\n\nAllocate virtual cores, sometimes called hardware threads, across different physical cores (available since Kubernetes v1.31)\ndistribute-cpus-across-numa (beta, visible by default)\n\nSpread CPUs across different NUMA domains, aiming for an even balance between the selected domains (available since\nKubernetes v1.23)\nfull-pcpus-only (GA, visible by default)\n\nAlways allocate full physical cores (available since Kubernetes v1.22, GA since Kubernetes v1.33)\nstrict-cpu-reservation (beta, visible by default)\n\nPrevent all the pods regardless of their Quality of Service class to run on reserved CPUs (available since Kubernetes v1.32)\nprefer-align-cpus-by-uncorecache (beta, visible by default)\n\nAlign CPUs by uncore (Last-Level) cache boundary on a best-effort way (available since Kubernetes v1.32)\nYou can toggle groups of options on and off based upon their maturity level using the following feature gates:\nCPUManagerPolicyBetaOptions\n\n(default enabled). Disable to hide beta-level options.\n\nCPUManagerPolicyAlphaOptions\n\n(default disabled). Enable to show alpha-level options.\n\nYou will still have to enable each option using the cpuManagerPolicyOptions field in the kubelet configuration file.\nhttps://kubernetes.io/docs/concepts/_print/\n\n489/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0512", "text": "For more detail about the individual options you can configure, read on.\nfull-pcpus-only\n\nIf the full-pcpus-only policy option is specified, the static policy will always allocate full physical cores. By default, without this\noption, the static policy allocates CPUs using a topology-aware best-fit allocation. On SMT enabled systems, the policy can allocate\nindividual virtual cores, which correspond to hardware threads. This can lead to different containers sharing the same physical\ncores; this behaviour in turn contributes to the noisy neighbours problem. With the option enabled, the pod will be admitted by the\nkubelet only if the CPU request of all its containers can be fulfilled by allocating full physical cores. If the pod does not pass the\nadmission, it will be put in Failed state with the message SMTAlignmentError .\ndistribute-cpus-across-numa\n\nIf the distribute-cpus-across-numa policy option is specified, the static policy will evenly distribute CPUs across NUMA nodes in\ncases where more than one NUMA node is required to satisfy the allocation. By default, the CPUManager will pack CPUs onto one\nNUMA node until it is filled, with any remaining CPUs simply spilling over to the next NUMA node. This can cause undesired\nbottlenecks in parallel code relying on barriers (and similar synchronization primitives), as this type of code tends to run only as fast\nas its slowest worker (which is slowed down by the fact that fewer CPUs are available on at least one NUMA node). By distributing\nCPUs evenly across NUMA nodes, application developers can more easily ensure that no single worker suffers from NUMA effects\nmore than any other, improving the overall performance of these types of applications.\nalign-by-socket"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0513", "text": "If the align-by-socket policy option is specified, CPUs will be considered aligned at the socket boundary when deciding how to\nallocate CPUs to a container. By default, the CPUManager aligns CPU allocations at the NUMA boundary, which could result in\nperformance degradation if CPUs need to be pulled from more than one NUMA node to satisfy the allocation. Although it tries to\nensure that all CPUs are allocated from the minimum number of NUMA nodes, there is no guarantee that those NUMA nodes will be\non the same socket. By directing the CPUManager to explicitly align CPUs at the socket boundary rather than the NUMA boundary,\nwe are able to avoid such issues. Note, this policy option is not compatible with TopologyManager single-numa-node policy and\ndoes not apply to hardware where the number of sockets is greater than number of NUMA nodes.\ndistribute-cpus-across-cores\n\nIf the distribute-cpus-across-cores policy option is specified, the static policy will attempt to allocate virtual cores (hardware\nthreads) across different physical cores. By default, the CPUManager tends to pack CPUs onto as few physical cores as possible,\nwhich can lead to contention among CPUs on the same physical core and result in performance bottlenecks. By enabling the\ndistribute-cpus-across-cores policy, the static policy ensures that CPUs are distributed across as many physical cores as possible,\nreducing the contention on the same physical core and thereby improving overall performance. However, it is important to note that\nthis strategy might be less effective when the system is heavily loaded. Under such conditions, the benefit of reducing contention\ndiminishes. Conversely, default behavior can help in reducing inter-core communication overhead, potentially providing better\nperformance under high load conditions.\nstrict-cpu-reservation"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0514", "text": "The reservedSystemCPUs parameter in KubeletConfiguration, or the deprecated kubelet command line option --reserved-cpus ,\ndefines an explicit CPU set for OS system daemons and kubernetes system daemons. More details of this parameter can be found\non the Explicitly Reserved CPU List page. By default, this isolation is implemented only for guaranteed pods with integer CPU\nrequests not for burstable and best-effort pods (and guaranteed pods with fractional CPU requests). Admission is only comparing\nthe CPU requests against the allocatable CPUs. Since the CPU limit is higher than the request, the default behaviour allows burstable\nand best-effort pods to use up the capacity of reservedSystemCPUs and cause host OS services to starve in real life deployments. If\nthe strict-cpu-reservation policy option is enabled, the static policy will not allow any workload to use the CPU cores specified in\nreservedSystemCPUs .\nprefer-align-cpus-by-uncorecache\n\nIf the prefer-align-cpus-by-uncorecache policy is specified, the static policy will allocate CPU resources for individual containers\nsuch that all CPUs assigned to a container share the same uncore cache block (also known as the Last-Level Cache or LLC). By\ndefault, the CPUManager will tightly pack CPU assignments which can result in containers being assigned CPUs from multiple uncore\ncaches. This option enables the CPUManager to allocate CPUs in a way that maximizes the efficient use of the uncore cache.\nAllocation is performed on a best-effort basis, aiming to affine as many CPUs as possible within the same uncore cache. If the\ncontainer's CPU requirement exceeds the CPU capacity of a single uncore cache, the CPUManager minimizes the number of uncore\nhttps://kubernetes.io/docs/concepts/_print/\n\n490/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ncaches used in order to maintain optimal uncore cache alignment. Specific workloads can benefit in performance from the reduction\nof inter-cache latency and noisy neighbors at the cache level. If the CPUManager cannot align optimally while the node has sufficient\nresources, the container will still be admitted using the default packed behavior.\n\nMemory Management Policies\nâ“˜ FEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0515", "text": "The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed\nQoS class.\nThe Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod. The Memory Manager\nfeeds the central manager (Topology Manager) with these affinity hints. Based on both the hints and Topology Manager policy, the\npod is rejected or admitted to the node.\nMoreover, the Memory Manager ensures that the memory which a pod requests is allocated from a minimum number of NUMA\nnodes.\n\nOther resource managers\nThe configuration of individual managers is elaborated in dedicated documents:\nDevice Manager\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n491/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10 - Scheduling, Preemption and Eviction\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is\nthe process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process\nof terminating one or more Pods on Nodes.\n\nScheduling\nKubernetes Scheduler\nAssigning Pods to Nodes\nPod Overhead\nPod Topology Spread Constraints\nTaints and Tolerations\nScheduling Framework\nDynamic Resource Allocation\nScheduler Performance Tuning\nResource Bin Packing for Extended Resources\nPod Scheduling Readiness\nDescheduler\n\nPod Disruption\nPod disruption is the process by which Pods on Nodes are terminated either voluntarily or involuntarily.\nVoluntary disruptions are started intentionally by application owners or cluster administrators. Involuntary disruptions are\nunintentional and can be triggered by unavoidable issues like Nodes running out of resources, or by accidental deletions.\nPod Priority and Preemption\nNode-pressure Eviction\nAPI-initiated Eviction\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n492/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.1 - Kubernetes Scheduler\n\nIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that Kubelet can run them.\n\nScheduling overview\nA scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, the scheduler\nbecomes responsible for finding the best Node for that Pod to run on. The scheduler reaches this placement decision taking into\naccount the scheduling principles described below.\nIf you want to understand why Pods are placed onto a particular Node, or if you're planning to implement a custom scheduler\nyourself, this page will help you learn about scheduling."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0516", "text": "kube-scheduler\nkube-scheduler is the default scheduler for Kubernetes and runs as part of the control plane. kube-scheduler is designed so that, if\nyou want and need to, you can write your own scheduling component and use that instead.\nKube-scheduler selects an optimal node to run newly created or not yet scheduled (unscheduled) pods. Since containers in pods and pods themselves - can have different requirements, the scheduler filters out any nodes that don't meet a Pod's specific\nscheduling needs. Alternatively, the API lets you specify a node for a Pod when you create it, but this is unusual and is only done in\nspecial cases.\nIn a cluster, Nodes that meet the scheduling requirements for a Pod are called feasible nodes. If none of the nodes are suitable, the\npod remains unscheduled until the scheduler is able to place it.\nThe scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the feasible Nodes and picks a Node with the\nhighest score among the feasible ones to run the Pod. The scheduler then notifies the API server about this decision in a process\ncalled binding.\nFactors that need to be taken into account for scheduling decisions include individual and collective resource requirements,\nhardware / software / policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and so on."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0517", "text": "Node selection in kube-scheduler\nkube-scheduler selects a node for the pod in a 2-step operation:\n1. Filtering\n2. Scoring\nThe filtering step finds the set of Nodes where it's feasible to schedule the Pod. For example, the PodFitsResources filter checks\nwhether a candidate Node has enough available resources to meet a Pod's specific resource requests. After this step, the node list\ncontains any suitable Nodes; often, there will be more than one. If the list is empty, that Pod isn't (yet) schedulable.\nIn the scoring step, the scheduler ranks the remaining nodes to choose the most suitable Pod placement. The scheduler assigns a\nscore to each Node that survived filtering, basing this score on the active scoring rules.\nFinally, kube-scheduler assigns the Pod to the Node with the highest ranking. If there is more than one node with equal scores, kubescheduler selects one of these at random.\nThere are two supported ways to configure the filtering and scoring behavior of the scheduler:\n1. Scheduling Policies allow you to configure Predicates for filtering and Priorities for scoring.\n2. Scheduling Profiles allow you to configure Plugins that implement different scheduling stages, including: QueueSort , Filter ,\nScore , Bind , Reserve , Permit , and others. You can also configure the kube-scheduler to run different profiles.\n\nWhat's next\nRead about scheduler performance tuning\nRead about Pod topology spread constraints\nRead the reference documentation for kube-scheduler\nhttps://kubernetes.io/docs/concepts/_print/\n\n493/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nRead the kube-scheduler config (v1) reference\nLearn about configuring multiple schedulers\nLearn about topology management policies\nLearn about Pod Overhead\nLearn about scheduling of Pods that use volumes in:\nVolume Topology Support\nStorage Capacity Tracking\nNode-specific Volume Limits\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n494/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.2 - Assigning Pods to Nodes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0518", "text": "You can constrain a Pod so that it is restricted to run on particular node(s), or to prefer to run on particular nodes. There are several\nways to do this and the recommended approaches all use label selectors to facilitate the selection. Often, you do not need to set any\nsuch constraints; the scheduler will automatically do a reasonable placement (for example, spreading your Pods across nodes so as\nnot place Pods on a node with insufficient free resources). However, there are some circumstances where you may want to control\nwhich node the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate Pods\nfrom two different services that communicate a lot into the same availability zone.\nYou can use any of the following methods to choose where Kubernetes schedules specific Pods:\nnodeSelector field matching against node labels\nAffinity and anti-affinity\nnodeName field\nPod topology spread constraints\n\nNode labels\nLike many other Kubernetes objects, nodes have labels. You can attach labels manually. Kubernetes also populates a standard set of\nlabels on all nodes in a cluster.\nNote:\nThe value of these labels is cloud provider specific and is not guaranteed to be reliable. For example, the value of\nkubernetes.io/hostname may be the same as the node name in some environments and a different value in other\nenvironments.\n\nNode isolation/restriction\nAdding labels to nodes allows you to target Pods for scheduling on specific nodes or groups of nodes. You can use this functionality\nto ensure that specific Pods only run on nodes with certain isolation, security, or regulatory properties.\nIf you use labels for node isolation, choose label keys that the kubelet cannot modify. This prevents a compromised node from\nsetting those labels on itself so that the scheduler schedules workloads onto the compromised node.\nThe NodeRestriction admission plugin prevents the kubelet from setting or modifying labels with a noderestriction.kubernetes.io/ prefix.\nTo make use of that label prefix for node isolation:\n1. Ensure you are using the Node authorizer and have enabled the NodeRestriction admission plugin.\n2. Add labels with the node-restriction.kubernetes.io/ prefix to your nodes, and use those labels in your node selectors. For\nexample, example.com.node-restriction.kubernetes.io/fips=true or example.com.node-restriction.kubernetes.io/pcidss=true ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0519", "text": "nodeSelector\nis the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod\nspecification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have\neach of the labels you specify.\nnodeSelector\n\nSee Assign Pods to Nodes for more information.\n\nAffinity and anti-affinity\nis the simplest way to constrain Pods to nodes with specific labels. Affinity and anti-affinity expand the types of\nconstraints you can define. Some of the benefits of affinity and anti-affinity include:\nnodeSelector\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n495/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe affinity/anti-affinity language is more expressive. nodeSelector only selects nodes with all the specified labels.\nAffinity/anti-affinity gives you more control over the selection logic.\nYou can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a matching\nnode.\nYou can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node\nlabels, which allows you to define rules for which Pods can be co-located on a node.\nThe affinity feature consists of two types of affinity:\nNode affinity functions like the nodeSelector field but is more expressive and allows you to specify soft rules.\nInter-pod affinity/anti-affinity allows you to constrain Pods against labels on other Pods.\n\nNode affinity\nNode affinity is conceptually similar to nodeSelector , allowing you to constrain which nodes your Pod can be scheduled on based\non node labels. There are two types of node affinity:\nrequiredDuringSchedulingIgnoredDuringExecution : The scheduler can't schedule the Pod unless the rule is met. This functions\n\nlike nodeSelector , but with a more expressive syntax.\npreferredDuringSchedulingIgnoredDuringExecution : The scheduler tries to find a node that meets the rule. If a matching node"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0520", "text": "is not available, the scheduler still schedules the Pod.\nNote:\nIn the preceding types, IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the\nPod continues to run.\nYou can specify node affinities using the .spec.affinity.nodeAffinity field in your Pod spec.\nFor example, consider the following Pod spec:\npods/pod-with-node-affinity.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: with-node-affinity\nspec:\naffinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: topology.kubernetes.io/zone\noperator: In\nvalues:\n- antarctica-east1\n- antarctica-west1\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 1\npreference:\nmatchExpressions:\n- key: another-node-label-key\noperator: In\nvalues:\n- another-node-label-value\ncontainers:\n- name: with-node-affinity\nimage: registry.k8s.io/pause:3.8\n\nIn this example, the following rules apply:\nhttps://kubernetes.io/docs/concepts/_print/\n\n496/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe node must have a label with the key topology.kubernetes.io/zone and the value of that label must be either antarcticaeast1 or antarctica-west1 .\nThe node preferably has a label with the key another-node-label-key and the value another-node-label-value .\nYou can use the operator field to specify a logical operator for Kubernetes to use when interpreting the rules. You can use In ,\nNotIn , Exists , DoesNotExist , Gt and Lt .\nRead Operators to learn more about how these work.\nand DoesNotExist allow you to define node anti-affinity behavior. Alternatively, you can use node taints to repel Pods from\nspecific nodes.\nNotIn\n\nNote:\nIf you specify both nodeSelector and nodeAffinity , both must be satisfied for the Pod to be scheduled onto a node.\nIf you specify multiple terms in nodeSelectorTerms associated with nodeAffinity types, then the Pod can be scheduled onto\na node if one of the specified terms can be satisfied (terms are ORed).\nIf you specify multiple expressions in a single matchExpressions field associated with a term in nodeSelectorTerms , then the\nPod can be scheduled onto a node only if all the expressions are satisfied (expressions are ANDed).\n\nSee Assign Pods to Nodes using Node Affinity for more information."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0521", "text": "Node affinity weight\nYou can specify a weight between 1 and 100 for each instance of the preferredDuringSchedulingIgnoredDuringExecution affinity\ntype. When the scheduler finds nodes that meet all the other scheduling requirements of the Pod, the scheduler iterates through\nevery preferred rule that the node satisfies and adds the value of the weight for that expression to a sum.\nThe final sum is added to the score of other priority functions for the node. Nodes with the highest total score are prioritized when\nthe scheduler makes a scheduling decision for the Pod.\nFor example, consider the following Pod spec:\npods/pod-with-affinity-preferred-weight.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n497/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: with-affinity-preferred-weight\nspec:\naffinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: kubernetes.io/os\noperator: In\nvalues:\n- linux\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 1\npreference:\nmatchExpressions:\n- key: label-1\noperator: In\nvalues:\n- key-1\n- weight: 50\npreference:\nmatchExpressions:\n- key: label-2\noperator: In\nvalues:\n- key-2\ncontainers:\n- name: with-node-affinity\nimage: registry.k8s.io/pause:3.8\n\nIf there are two possible nodes that match the preferredDuringSchedulingIgnoredDuringExecution rule, one with the label-1:key1 label and another with the label-2:key-2 label, the scheduler considers the weight of each node and adds the weight to the\nother scores for that node, and schedules the Pod onto the node with the highest final score.\nNote:\nIf you want Kubernetes to successfully schedule the Pods in this example, you must have existing nodes with the\nkubernetes.io/os=linux label.\n\nNode affinity per scheduling profile\nâ“˜ FEATURE STATE: Kubernetes v1.20 [beta]\n\nWhen configuring multiple scheduling profiles, you can associate a profile with a node affinity, which is useful if a profile only applies\nto a specific set of nodes. To do so, add an addedAffinity to the args field of the NodeAffinity plugin in the scheduler\nconfiguration. For example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n498/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: default-scheduler\n- schedulerName: foo-scheduler\npluginConfig:\n- name: NodeAffinity\nargs:\naddedAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: scheduler-profile\noperator: In\nvalues:\n- foo"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0522", "text": "The addedAffinity is applied to all Pods that set .spec.schedulerName to foo-scheduler , in addition to the NodeAffinity specified\nin the PodSpec. That is, in order to match the Pod, nodes need to satisfy addedAffinity and the Pod's .spec.NodeAffinity .\nSince the addedAffinity is not visible to end users, its behavior might be unexpected to them. Use node labels that have a clear\ncorrelation to the scheduler profile name.\nNote:\nThe DaemonSet controller, which creates Pods for DaemonSets, does not support scheduling profiles. When the DaemonSet\ncontroller creates Pods, the default Kubernetes scheduler places those Pods and honors any nodeAffinity rules in the\nDaemonSet controller.\n\nInter-pod affinity and anti-affinity\nInter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods\nalready running on that node, instead of the node labels.\n\nTypes of Inter-pod Affinity and Anti-affinity\nInter-pod affinity and anti-affinity take the form \"this Pod should (or, in the case of anti-affinity, should not) run in an X if that X is\nalready running one or more Pods that meet rule Y\", where X is a topology domain like node, rack, cloud provider zone or region, or\nsimilar and Y is the rule Kubernetes tries to satisfy.\nYou express these rules (Y) as label selectors with an optional associated list of namespaces. Pods are namespaced objects in\nKubernetes, so Pod labels also implicitly have namespaces. Any label selectors for Pod labels should specify the namespaces in\nwhich Kubernetes should look for those labels.\nYou express the topology domain (X) using a topologyKey , which is the key for the node label that the system uses to denote the\ndomain. For examples, see Well-Known Labels, Annotations and Taints.\nNote:\nInter-pod affinity and anti-affinity require substantial amounts of processing which can slow down scheduling in large clusters\nsignificantly. We do not recommend using them in clusters larger than several hundred nodes.\n\nNote:\nPod anti-affinity requires nodes to be consistently labeled, in other words, every node in the cluster must have an appropriate\nlabel matching topologyKey. If some or all nodes are missing the specified topologyKey label, it can lead to unintended\nbehavior.\nSimilar to node affinity are two types of Pod affinity and anti-affinity as follows:\nhttps://kubernetes.io/docs/concepts/_print/\n\n499/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nrequiredDuringSchedulingIgnoredDuringExecution\npreferredDuringSchedulingIgnoredDuringExecution"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0523", "text": "For example, you could use requiredDuringSchedulingIgnoredDuringExecution affinity to tell the scheduler to co-locate Pods of two\nservices in the same cloud provider zone because they communicate with each other a lot. Similarly, you could use\npreferredDuringSchedulingIgnoredDuringExecution anti-affinity to spread Pods from a service across multiple cloud provider\nzones.\nTo use inter-pod affinity, use the affinity.podAffinity field in the Pod spec. For inter-pod anti-affinity, use the\naffinity.podAntiAffinity field in the Pod spec.\n\nScheduling Behavior\nWhen scheduling a new Pod, the Kubernetes scheduler evaluates the Pod's affinity/anti-affinity rules in the context of the current\ncluster state:\n1. Hard Constraints (Node Filtering):\nand\npodAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution :\nThe scheduler ensures the new Pod is assigned to nodes that satisfy these required affinity and anti-affinity rules\nbased on existing Pods.\n2. Soft Constraints (Scoring):\npodAffinity.requiredDuringSchedulingIgnoredDuringExecution\n\nand\npodAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution :\nThe scheduler scores nodes based on how well they meet these preferred affinity and anti-affinity rules to optimize\nPod placement.\n3. Ignored Fields:\npodAffinity.preferredDuringSchedulingIgnoredDuringExecution\n\nExisting Pods' podAffinity.preferredDuringSchedulingIgnoredDuringExecution :\nThese preferred affinity rules are not considered during the scheduling decision for new Pods.\nExisting Pods' podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution :\nSimilarly, preferred anti-affinity rules of existing Pods are ignored during scheduling.\n\nScheduling a Group of Pods with Inter-pod Affinity to Themselves\nIf the current Pod being scheduled is the first in a series that have affinity to themselves, it is allowed to be scheduled if it passes all\nother affinity checks. This is determined by verifying that no other Pod in the cluster matches the namespace and selector of this\nPod, that the Pod matches its own terms, and the chosen node matches all requested topologies. This ensures that there will not be\na deadlock even if all the Pods have inter-pod affinity specified.\n\nPod Affinity Example\nConsider the following Pod spec:\npods/pod-with-pod-affinity.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n500/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0524", "text": "apiVersion: v1\nkind: Pod\nmetadata:\nname: with-pod-affinity\nspec:\naffinity:\npodAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: security\noperator: In\nvalues:\n- S1\ntopologyKey: topology.kubernetes.io/zone\npodAntiAffinity:\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 100\npodAffinityTerm:\nlabelSelector:\nmatchExpressions:\n- key: security\noperator: In\nvalues:\n- S2\ntopologyKey: topology.kubernetes.io/zone\ncontainers:\n- name: with-pod-affinity\nimage: registry.k8s.io/pause:3.8\n\nThis example defines one Pod affinity rule and one Pod anti-affinity rule. The Pod affinity rule uses the \"hard\"\nrequiredDuringSchedulingIgnoredDuringExecution , while the anti-affinity rule uses the \"soft\"\npreferredDuringSchedulingIgnoredDuringExecution .\nThe affinity rule specifies that the scheduler is allowed to place the example Pod on a node only if that node belongs to a specific\nzone where other Pods have been labeled with security=S1 . For instance, if we have a cluster with a designated zone, let's call it\n\"Zone V,\" consisting of nodes labeled with topology.kubernetes.io/zone=V , the scheduler can assign the Pod to any node within\nZone V, as long as there is at least one Pod within Zone V already labeled with security=S1 . Conversely, if there are no Pods with\nsecurity=S1 labels in Zone V, the scheduler will not assign the example Pod to any node in that zone.\nThe anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod on a node if that node belongs to a specific\nzone where other Pods have been labeled with security=S2 . For instance, if we have a cluster with a designated zone, let's call it\n\"Zone R,\" consisting of nodes labeled with topology.kubernetes.io/zone=R , the scheduler should avoid assigning the Pod to any\nnode within Zone R, as long as there is at least one Pod within Zone R already labeled with security=S2 . Conversely, the anti-affinity\nrule does not impact scheduling into Zone R if there are no Pods with security=S2 labels.\nTo get yourself more familiar with the examples of Pod affinity and anti-affinity, refer to the design proposal.\nYou can use the In , NotIn , Exists and DoesNotExist values in the operator field for Pod affinity and anti-affinity.\nRead Operators to learn more about how these work.\nIn principle, the topologyKey can be any allowed label key with the following exceptions for performance and security reasons:\nFor Pod affinity and anti-affinity, an empty topologyKey field is not allowed in both\nrequiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution .\nFor requiredDuringSchedulingIgnoredDuringExecution Pod anti-affinity rules, the admission controller\nLimitPodHardAntiAffinityTopology limits topologyKey to kubernetes.io/hostname . You can modify or disable the admission\ncontroller if you want to allow custom topologies.\nIn addition to labelSelector and topologyKey , you can optionally specify a list of namespaces which the labelSelector should\nmatch against using the namespaces field at the same level as labelSelector and topologyKey . If omitted or empty, namespaces\ndefaults to the namespace of the Pod where the affinity/anti-affinity definition appears.\nhttps://kubernetes.io/docs/concepts/_print/"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0525", "text": "501/684\n\n11/7/25, 4:37 PM\n\nNamespace Selector\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nYou can also select matching namespaces using namespaceSelector , which is a label query over the set of namespaces. The affinity\nterm is applied to namespaces selected by both namespaceSelector and the namespaces field. Note that an empty\nnamespaceSelector ({}) matches all namespaces, while a null or empty namespaces list and null namespaceSelector matches the\nnamespace of the Pod where the rule is defined.\n\nmatchLabelKeys\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)\n\nNote:\nThe matchLabelKeys field is a beta-level field and is enabled by default in Kubernetes 1.34. When you want to disable it, you\nhave to disable it explicitly via the MatchLabelKeysInPodAffinity feature gate.\n\nKubernetes includes an optional matchLabelKeys field for Pod affinity or anti-affinity. The field specifies keys for the labels that\nshould match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.\nThe keys are used to look up values from the Pod labels; those key-value labels are combined (using AND ) with the match\nrestrictions defined using the labelSelector field. The combined filtering selects the set of existing Pods that will be taken into Pod\n(anti)affinity calculation.\nCaution:\nIt's not recommended to use matchLabelKeys with labels that might be updated directly on pods. Even if you edit the pod's\nlabel that is specified at matchLabelKeys directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label\nupdate onto the merged labelSelector.\nA common use case is to use matchLabelKeys with pod-template-hash (set on Pods managed as part of a Deployment, where the\nvalue is unique for each revision). Using pod-template-hash in matchLabelKeys allows you to target the Pods that belong to the\nsame revision as the incoming Pod, so that a rolling upgrade won't break affinity.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n502/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: application-server\n...\nspec:\ntemplate:\nspec:\naffinity:\npodAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- database\ntopologyKey: topology.kubernetes.io/zone\n# Only Pods from a given rollout are taken into consideration when calculating pod affinity.\n# If you update the Deployment, the replacement Pods follow their own affinity rules\n# (if there are any defined in the new Pod template)\nmatchLabelKeys:\n- pod-template-hash\n\nmismatchLabelKeys\nâ“˜ FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0526", "text": "Note:\nThe mismatchLabelKeys field is a beta-level field and is enabled by default in Kubernetes 1.34. When you want to disable it,\nyou have to disable it explicitly via the MatchLabelKeysInPodAffinity feature gate.\n\nKubernetes includes an optional mismatchLabelKeys field for Pod affinity or anti-affinity. The field specifies keys for the labels that\nshould not match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.\nCaution:\nIt's not recommended to use mismatchLabelKeys with labels that might be updated directly on pods. Even if you edit the pod's\nlabel that is specified at mismatchLabelKeys directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label\nupdate onto the merged labelSelector.\nOne example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team\nare scheduled in. In other words, you want to avoid running Pods from two different tenants on the same topology domain at the\nsame time.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n503/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nlabels:\n# Assume that all relevant Pods have a \"tenant\" label set\ntenant: tenant-a\n...\nspec:\naffinity:\npodAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n# ensure that Pods associated with this tenant land on the correct node pool\n- matchLabelKeys:\n- tenant\ntopologyKey: node-pool\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n# ensure that Pods associated with this tenant can't schedule to nodes used for another tenant\n- mismatchLabelKeys:\n- tenant # whatever the value of the \"tenant\" label for this Pod, prevent\n# scheduling to nodes in any pool where any Pod from a different\n# tenant is running.\nlabelSelector:\n# We have to have the labelSelector which selects only Pods with the tenant label,\n# otherwise this Pod would have anti-affinity against Pods from daemonsets as well, for example,\n# which aren't supposed to have the tenant label.\nmatchExpressions:\n- key: tenant\noperator: Exists\ntopologyKey: node-pool"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0527", "text": "More practical use-cases\nInter-pod affinity and anti-affinity can be even more useful when they are used with higher level collections such as ReplicaSets,\nStatefulSets, Deployments, etc. These rules allow you to configure that a set of workloads should be co-located in the same defined\ntopology; for example, preferring to place two related Pods onto the same node.\nFor example: imagine a three-node cluster. You use the cluster to run a web application and also an in-memory cache (such as\nRedis). For this example, also assume that latency between the web application and the memory cache should be as low as is\npractical. You could use inter-pod affinity and anti-affinity to co-locate the web servers with the cache as much as possible.\nIn the following example Deployment for the Redis cache, the replicas get the label app=store . The podAntiAffinity rule tells the\nscheduler to avoid placing multiple replicas with the app=store label on a single node. This creates each cache in a separate node.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n504/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: redis-cache\nspec:\nselector:\nmatchLabels:\napp: store\nreplicas: 3\ntemplate:\nmetadata:\nlabels:\napp: store\nspec:\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- store\ntopologyKey: \"kubernetes.io/hostname\"\ncontainers:\n- name: redis-server\nimage: redis:3.2-alpine\n\nThe following example Deployment for the web servers creates replicas with the label app=web-store . The Pod affinity rule tells the\nscheduler to place each replica on a node that has a Pod with the label app=store . The Pod anti-affinity rule tells the scheduler\nnever to place multiple app=web-store servers on a single node.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n505/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: web-server\nspec:\nselector:\nmatchLabels:\napp: web-store\nreplicas: 3\ntemplate:\nmetadata:\nlabels:\napp: web-store\nspec:\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- web-store\ntopologyKey: \"kubernetes.io/hostname\"\npodAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- labelSelector:\nmatchExpressions:\n- key: app\noperator: In\nvalues:\n- store\ntopologyKey: \"kubernetes.io/hostname\"\ncontainers:\n- name: web-app\nimage: nginx:1.16-alpine\n\nCreating the two preceding Deployments results in the following cluster layout, where each web server is co-located with a cache, on\nthree separate nodes.\nnode-1\n\nnode-2\n\nnode-3\n\nwebserver-1\n\nwebserver-2\n\nwebserver-3\n\ncache-1\n\ncache-2\n\ncache-3"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0528", "text": "The overall effect is that each cache instance is likely to be accessed by a single client that is running on the same node. This\napproach aims to minimize both skew (imbalanced load) and latency.\nYou might have other reasons to use Pod anti-affinity. See the ZooKeeper tutorial for an example of a StatefulSet configured with\nanti-affinity for high availability, using the same technique as this example.\n\nnodeName\nis a more direct form of node selection than affinity or nodeSelector . nodeName is a field in the Pod spec. If the\nnodeName field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node.\nUsing nodeName overrules using nodeSelector or affinity and anti-affinity rules.\nnodeName\n\nSome of the limitations of using nodeName to select nodes are:\nIf the named node does not exist, the Pod will not run, and in some cases may be automatically deleted.\nhttps://kubernetes.io/docs/concepts/_print/\n\n506/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate why, for\nexample OutOfmemory or OutOfcpu.\nNode names in cloud environments are not always predictable or stable.\n\nWarning:\nnodeName is intended for use by custom schedulers or advanced use cases where you need to bypass any configured\n\nschedulers. Bypassing the schedulers might lead to failed Pods if the assigned Nodes get oversubscribed. You can use node\naffinity or the nodeSelector field to assign a Pod to a specific Node without bypassing the schedulers.\nHere is an example of a Pod spec using the nodeName field:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nnodeName: kube-01\n\nThe above Pod will only run on the node kube-01 .\n\nnominatedNodeName\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\ncan be used for external components to nominate node for a pending pod. This nomination is best effort: it\nmight be ignored if the scheduler determines the pod cannot go to a nominated node.\nnominatedNodeName"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0529", "text": "Also, this field can be (over)written by the scheduler:\nIf the scheduler finds a node to nominate via the preemption.\nIf the scheduler decides where the pod is going, and move it to the binding cycle.\nNote that, in this case, nominatedNodeName is put only when the pod has to go through WaitOnPermit or PreBind\nextension points.\nHere is an example of a Pod status using the nominatedNodeName field:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\n...\nstatus:\nnominatedNodeName: kube-01\n\nPod topology spread constraints\nYou can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions,\nzones, nodes, or among any other topology domains that you define. You might do this to improve performance, expected\navailability, or overall utilization.\nRead Pod topology spread constraints to learn more about how these work.\nhttps://kubernetes.io/docs/concepts/_print/\n\n507/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOperators\n\nThe following are all the logical operators that you can use in the operator field for nodeAffinity and podAffinity mentioned\nabove.\nOperator\n\nBehavior\n\nIn\n\nThe label value is present in the supplied set of strings\n\nNotIn\n\nThe label value is not contained in the supplied set of strings\n\nExists\n\nA label with this key exists on the object\n\nDoesNotExist\n\nNo label with this key exists on the object\n\nThe following operators can only be used with nodeAffinity .\nOperator\n\nBehavior\n\nGt\n\nThe field value will be parsed as an integer, and that integer is less than the integer that results from parsing the\nvalue of a label named by this selector\n\nLt\n\nThe field value will be parsed as an integer, and that integer is greater than the integer that results from parsing the\nvalue of a label named by this selector\n\nNote:\nGt and Lt operators will not work with non-integer values. If the given value doesn't parse as an integer, the Pod will fail to get\nscheduled. Also, Gt and Lt are not available for podAffinity.\n\nWhat's next\nRead more about taints and tolerations.\nRead the design docs for node affinity and for inter-pod affinity/anti-affinity.\nLearn about how the topology manager takes part in node-level resource allocation decisions.\nLearn how to use nodeSelector.\nLearn how to use affinity and anti-affinity.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n508/684\n\n11/7/25, 4:37 PM\n\n10.3 - Pod Overhead\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0530", "text": "When you run a Pod on a Node, the Pod itself takes an amount of system resources. These resources are additional to the resources\nneeded to run the container(s) inside the Pod. In Kubernetes, Pod Overhead is a way to account for the resources consumed by the\nPod infrastructure on top of the container requests & limits.\nIn Kubernetes, the Pod's overhead is set at admission time according to the overhead associated with the Pod's RuntimeClass.\nA pod's overhead is considered in addition to the sum of container resource requests when scheduling a Pod. Similarly, the kubelet\nwill include the Pod overhead when sizing the Pod cgroup, and when carrying out Pod eviction ranking.\n\nConfiguring Pod overhead\nYou need to make sure a RuntimeClass is utilized which defines the overhead field.\n\nUsage example\nTo work with Pod overhead, you need a RuntimeClass that defines the overhead field. As an example, you could use the following\nRuntimeClass definition with a virtualization container runtime (in this example, Kata Containers combined with the Firecracker\nvirtual machine monitor) that uses around 120MiB per Pod for the virtual machine and the guest OS:\n\n# You need to change this example to match the actual runtime name, and per-Pod\n# resource overhead, that the container runtime is adding in your cluster.\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\nname: kata-fc\nhandler: kata-fc\noverhead:\npodFixed:\nmemory: \"120Mi\"\ncpu: \"250m\"\n\nWorkloads which are created which specify the kata-fc RuntimeClass handler will take the memory and cpu overheads into\naccount for resource quota calculations, node scheduling, as well as Pod cgroup sizing.\nConsider running the given example workload, test-pod:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n509/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-pod\nspec:\nruntimeClassName: kata-fc\ncontainers:\n- name: busybox-ctr\nimage: busybox:1.28\nstdin: true\ntty: true\nresources:\nlimits:\ncpu: 500m\nmemory: 100Mi\n- name: nginx-ctr\nimage: nginx\nresources:\nlimits:\ncpu: 1500m\nmemory: 100Mi"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0531", "text": "Note:\nIf only limits are specified in the pod definition, kubelet will deduce requests from those limits and set them to be the same\nas the defined limits.\nAt admission time the RuntimeClass admission controller updates the workload's PodSpec to include the overhead as described in\nthe RuntimeClass. If the PodSpec already has this field defined, the Pod will be rejected. In the given example, since only the\nRuntimeClass name is specified, the admission controller mutates the Pod to include an overhead .\nAfter the RuntimeClass admission controller has made modifications, you can check the updated Pod overhead value:\n\nkubectl get pod test-pod -o jsonpath='{.spec.overhead}'\n\nThe output is:\nmap[cpu:250m memory:120Mi]\n\nIf a ResourceQuota is defined, the sum of container requests as well as the overhead field are counted.\nWhen the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's overhead as well as the\nsum of container requests for that Pod. For this example, the scheduler adds the requests and the overhead, then looks for a node\nthat has 2.25 CPU and 320 MiB of memory available.\nOnce a Pod is scheduled to a node, the kubelet on that node creates a new cgroup for the Pod. It is within this pod that the\nunderlying container runtime will create containers.\nIf the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined), the kubelet will set an\nupper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU and memory.limit_in_bytes memory). This\nupper limit is based on the sum of the container limits plus the overhead defined in the PodSpec.\nFor CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set cpu.shares based on the sum of container requests plus\nthe overhead defined in the PodSpec.\nLooking at our example, verify the container requests for the workload:\n\nkubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}'\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n510/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe total container requests are 2000m CPU and 200MiB of memory:\nmap[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]\n\nCheck this against what is observed by the node:\n\nkubectl describe node | grep test-pod -B2\n\nThe output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:\nNamespace\n\nName\n\nCPU Requests\n\nCPU Limits\n\nMemory Requests\n\nMemory Limits\n\nAGE\n\n---------\n\n----\n\n------------\n\n----------\n\n---------------\n\n-------------\n\n---\n\ndefault\n\ntest-pod\n\n2250m (56%)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0532", "text": "2250m (56%)\n\n320Mi (1%)\n\n320Mi (1%)\n\n36m\n\nVerify Pod cgroup limits\nCheck the Pod's memory cgroups on the node where the workload is running. In the following example, crictl is used on the\nnode, which provides a CLI for CRI-compatible container runtimes. This is an advanced example to show Pod overhead behavior, and\nit is not expected that users should need to check cgroups directly on the node.\nFirst, on the particular node, determine the Pod identifier:\n\n# Run this on the node where the Pod is scheduled\nPOD_ID=\"$(sudo crictl pods --name test-pod -q)\"\n\nFrom this, you can determine the cgroup path for the Pod:\n\n# Run this on the node where the Pod is scheduled\nsudo crictl inspectp -o=json $POD_ID | grep cgroupsPath\n\nThe resulting cgroup path includes the Pod's pause container. The Pod level cgroup is one directory above.\n\"cgroupsPath\": \"/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf0\n\nIn this specific case, the pod cgroup path is kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2 . Verify the Pod level cgroup setting\nfor memory:\n\n# Run this on the node where the Pod is scheduled.\n# Also, change the name of the cgroup to match the cgroup allocated for your pod.\ncat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes\n\nThis is 320 MiB, as expected:\n335544320\n\nObservability\nSome kube_pod_overhead_* metrics are available in kube-state-metrics to help identify when Pod overhead is being utilized and to\nhelp observe stability of workloads running with a defined overhead.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n511/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nLearn more about RuntimeClass\nRead the PodOverhead Design enhancement proposal for extra context\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n512/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.4 - Pod Scheduling Readiness\nâ“˜ FEATURE STATE: Kubernetes v1.30 [stable]\n\nPods were considered ready for scheduling once created. Kubernetes scheduler does its due diligence to find nodes to place all\npending Pods. However, in a real-world case, some Pods may stay in a \"miss-essential-resources\" state for a long period. These Pods\nactually churn the scheduler (and downstream integrators like Cluster AutoScaler) in an unnecessary manner.\nBy specifying/removing a Pod's .spec.schedulingGates , you can control when a Pod is ready to be considered for scheduling."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0533", "text": "Configuring Pod schedulingGates\nThe schedulingGates field contains a list of strings, and each string literal is perceived as a criteria that Pod should be satisfied\nbefore considered schedulable. This field can be initialized only when a Pod is created (either by the client, or mutated during\nadmission). After creation, each schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is\ndisallowed.\n\npod created\n\nempty scheduling gates?\nno\n\nscheduling gate removed\n\npod scheduling gated\n\nyes\npod scheduling ready\n\npod running\n\nFigure. Pod SchedulingGates\n\nUsage example\nTo mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this:\npods/pod-with-scheduling-gates.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n513/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-pod\nspec:\nschedulingGates:\n- name: example.com/foo\n- name: example.com/bar\ncontainers:\n- name: pause\nimage: registry.k8s.io/pause:3.6\n\nAfter the Pod's creation, you can check its state using:\n\nkubectl get pod test-pod\n\nThe output reveals it's in SchedulingGated state:\nNAME\ntest-pod\n\nREADY\n0/1\n\nSTATUS\nSchedulingGated\n\nRESTARTS\n0\n\nAGE\n7s\n\nYou can also check its schedulingGates field by running:\n\nkubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}'\n\nThe output is:\n[{\"name\":\"example.com/foo\"},{\"name\":\"example.com/bar\"}]\n\nTo inform scheduler this Pod is ready for scheduling, you can remove its schedulingGates entirely by reapplying a modified\nmanifest:\npods/pod-without-scheduling-gates.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: test-pod\nspec:\ncontainers:\n- name: pause\nimage: registry.k8s.io/pause:3.6\n\nYou can check if the schedulingGates is cleared by running:\n\nkubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}'\n\nThe output is expected to be empty. And you can check its latest status by running:\n\nkubectl get pod test-pod -o wide\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n514/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nGiven the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state get transited from previous\nSchedulingGated to Running :\nNAME\n\nREADY\n\nSTATUS\n\nRESTARTS\n\nAGE\n\nIP\n\nNODE\n\ntest-pod\n\n1/1\n\nRunning\n\n0\n\n15s\n\n10.0.0.4\n\nnode-2\n\nObservability\nThe metric scheduler_pending_pods comes with a new label \"gated\" to distinguish whether a Pod has been tried scheduling but\nclaimed as unschedulable, or explicitly marked as not ready for scheduling. You can use scheduler_pending_pods{queue=\"gated\"} to\ncheck the metric result."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0534", "text": "Mutable Pod scheduling directives\nYou can mutate scheduling directives of Pods while they have scheduling gates, with certain constraints. At a high level, you can only\ntighten the scheduling directives of a Pod. In other words, the updated directives would cause the Pods to only be able to be\nscheduled on a subset of the nodes that it would previously match. More concretely, the rules for updating a Pod's scheduling\ndirectives are as follows:\n1. For .spec.nodeSelector , only additions are allowed. If absent, it will be allowed to be set.\n2. For spec.affinity.nodeAffinity , if nil, then setting anything is allowed.\n3. If NodeSelectorTerms was empty, it will be allowed to be set. If not empty, then only additions of NodeSelectorRequirements\nto matchExpressions or fieldExpressions are allowed, and no changes to existing matchExpressions and fieldExpressions\nwill be allowed. This is because the terms in .requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms , are ORed\nwhile the expressions in nodeSelectorTerms[].matchExpressions and nodeSelectorTerms[].fieldExpressions are ANDed.\n4. For .preferredDuringSchedulingIgnoredDuringExecution , all updates are allowed. This is because preferred terms are not\nauthoritative, and so policy controllers don't validate those terms.\n\nWhat's next\nRead the PodSchedulingReadiness KEP for more details\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n515/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.5 - Pod Topology Spread Constraints\nYou can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions,\nzones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource\nutilization.\nYou can set cluster-level constraints as a default, or configure topology spread constraints for individual workloads."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0535", "text": "Motivation\nImagine that you have a cluster of up to twenty nodes, and you want to run a workload that automatically scales how many replicas\nit uses. There could be as few as two Pods or as many as fifteen. When there are only two Pods, you'd prefer not to have both of\nthose Pods run on the same node: you would run the risk that a single node failure takes your workload offline.\nIn addition to this basic usage, there are some advanced usage examples that enable your workloads to benefit on high availability\nand cluster utilization.\nAs you scale up and run more Pods, a different concern becomes important. Imagine that you have three nodes running five Pods\neach. The nodes have enough capacity to run that many replicas; however, the clients that interact with this workload are split\nacross three different datacenters (or infrastructure zones). Now you have less concern about a single node failure, but you notice\nthat latency is higher than you'd like, and you are paying for network costs associated with sending network traffic between the\ndifferent zones.\nYou decide that under normal operation you'd prefer to have a similar number of replicas scheduled into each infrastructure zone,\nand you'd like the cluster to self-heal in the case that there is a problem.\nPod topology spread constraints offer you a declarative way to configure that.\n\ntopologySpreadConstraints field\nThe Pod API includes a field, spec.topologySpreadConstraints . The usage of this field looks like the following:\n\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: example-pod\nspec:\n# Configure a topology spread constraint\ntopologySpreadConstraints:\n- maxSkew: <integer>\nminDomains: <integer> # optional\ntopologyKey: <string>\nwhenUnsatisfiable: <string>\nlabelSelector: <object>\nmatchLabelKeys: <list> # optional; beta since v1.27\nnodeAffinityPolicy: [Honor|Ignore] # optional; beta since v1.26\nnodeTaintsPolicy: [Honor|Ignore] # optional; beta since v1.26\n### other Pod fields go here\n\nNote:\nThere can only be one topologySpreadConstraint for a given topologyKey and whenUnsatisfiable value. For example, if you\nhave defined a topologySpreadConstraint that uses the topologyKey \"kubernetes.io/hostname\" and whenUnsatisfiable value\n\"DoNotSchedule\", you can only add another topologySpreadConstraint for the topologyKey \"kubernetes.io/hostname\" if you\nuse a different whenUnsatisfiable value.\nYou can read more about this field by running kubectl explain Pod.spec.topologySpreadConstraints or refer to the scheduling\nsection of the API reference for Pod.\nhttps://kubernetes.io/docs/concepts/_print/\n\n516/684\n\n11/7/25, 4:37 PM\n\nSpread constraint definition\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0536", "text": "You can define one or multiple topologySpreadConstraints entries to instruct the kube-scheduler how to place each incoming Pod\nin relation to the existing Pods across your cluster. Those fields are:\nmaxSkew describes the degree to which Pods may be unevenly distributed. You must specify this field and the number must\nbe greater than zero. Its semantics differ according to the value of whenUnsatisfiable :\nif you select whenUnsatisfiable: DoNotSchedule , then maxSkew defines the maximum permitted difference between the\nnumber of matching pods in the target topology and the global minimum (the minimum number of matching pods in an\neligible domain or zero if the number of eligible domains is less than MinDomains). For example, if you have 3 zones with\n2, 2 and 1 matching pods respectively, MaxSkew is set to 1 then the global minimum is 1.\nif you select whenUnsatisfiable: ScheduleAnyway , the scheduler gives higher precedence to topologies that would help\nreduce the skew.\nminDomains indicates a minimum number of eligible domains. This field is optional. A domain is a particular instance of a\ntopology. An eligible domain is a domain whose nodes match the node selector.\nNote:\nBefore Kubernetes v1.30, the minDomains field was only available if the MinDomainsInPodTopologySpread feature gate was\nenabled (default since v1.28). In older Kubernetes clusters it might be explicitly disabled or the field might not be\navailable.\nThe value of minDomains must be greater than 0, when specified. You can only specify minDomains in conjunction with\nwhenUnsatisfiable: DoNotSchedule .\nWhen the number of eligible domains with match topology keys is less than minDomains , Pod topology spread treats\nglobal minimum as 0, and then the calculation of skew is performed. The global minimum is the minimum number of\nmatching Pods in an eligible domain, or zero if the number of eligible domains is less than minDomains .\nWhen the number of eligible domains with matching topology keys equals or is greater than minDomains , this value has\nno effect on scheduling.\nIf you do not specify minDomains , the constraint behaves as if minDomains is 1.\ntopologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the\nsame topology. We call each instance of a topology (in other words, a <key, value> pair) a domain. The scheduler will try to put\na balanced number of pods into each domain. Also, we define an eligible domain as a domain whose nodes meet the\nrequirements of nodeAffinityPolicy and nodeTaintsPolicy.\nwhenUnsatisfiable indicates how to deal with a Pod if it doesn't satisfy the spread constraint:\nDoNotSchedule"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0537", "text": "(default) tells the scheduler not to schedule it.\n\nScheduleAnyway\n\ntells the scheduler to still schedule it while prioritizing nodes that minimize the skew.\n\nlabelSelector is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods\nin their corresponding topology domain. See Label Selectors for more details.\nmatchLabelKeys is a list of pod label keys to select the group of pods over which the spreading skew will be calculated. At a\npod creation, the kube-apiserver uses those keys to lookup values from the incoming pod labels, and those key-value labels will\nbe merged with any existing labelSelector . The same key is forbidden to exist in both matchLabelKeys and labelSelector .\nmatchLabelKeys cannot be set when labelSelector isn't set. Keys that don't exist in the pod labels will be ignored. A null or\nempty list means only match against the labelSelector .\nCaution:\nIt's not recommended to use matchLabelKeys with labels that might be updated directly on pods. Even if you edit the\npod's label that is specified at matchLabelKeys directly, (that is, you edit the Pod and not a Deployment), kube-apiserver\ndoesn't reflect the label update onto the merged labelSelector.\nWith matchLabelKeys , you don't need to update the pod.spec between different revisions. The controller/operator just needs\nto set different values to the same label key for different revisions. For example, if you are configuring a Deployment, you can\nuse the label keyed with pod-template-hash, which is added automatically by the Deployment controller, to distinguish\nbetween different revisions in a single Deployment.\nhttps://kubernetes.io/docs/concepts/_print/\n\n517/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ntopologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: kubernetes.io/hostname\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\napp: foo\nmatchLabelKeys:\n- pod-template-hash"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0538", "text": "Note:\nThe matchLabelKeys field is a beta-level field and enabled by default in 1.27. You can disable it by disabling the\nMatchLabelKeysInPodTopologySpread feature gate.\nBefore v1.34, matchLabelKeys was handled implicitly. Since v1.34, key-value labels corresponding to matchLabelKeys are\nexplicitly merged into labelSelector . You can disable it and revert to the previous behavior by disabling the\nMatchLabelKeysInPodTopologySpreadSelectorMerge feature gate of kube-apiserver.\nnodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew.\nOptions are:\nHonor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.\nIgnore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.\nIf this value is null, the behavior is equivalent to the Honor policy.\nNote:\nThe nodeAffinityPolicy became beta in 1.26 and graduated to GA in 1.33. It's enabled by default in beta, you can disable\nit by disabling the NodeInclusionPolicyInPodTopologySpread feature gate.\nnodeTaintsPolicy indicates how we will treat node taints when calculating pod topology spread skew. Options are:\nHonor: nodes without taints, along with tainted nodes for which the incoming pod has a toleration, are included.\nIgnore: node taints are ignored. All nodes are included.\nIf this value is null, the behavior is equivalent to the Ignore policy.\nNote:\nThe nodeTaintsPolicy became beta in 1.26 and graduated to GA in 1.33. It's enabled by default in beta, you can disable it\nby disabling the NodeInclusionPolicyInPodTopologySpread feature gate.\nWhen a Pod defines more than one topologySpreadConstraint , those constraints are combined using a logical AND operation: the\nkube-scheduler looks for a node for the incoming Pod that satisfies all the configured constraints.\n\nNode labels\nTopology spread constraints rely on node labels to identify the topology domain(s) that each node is in. For example, a node might\nhave labels:\n\nregion: us-east-1\nzone: us-east-1a\n\nNote:\nFor brevity, this example doesn't use the well-known label keys topology.kubernetes.io/zone and\ntopology.kubernetes.io/region . However, those registered label keys are nonetheless recommended rather than the private\n(unqualified) label keys region and zone that are used here.\nhttps://kubernetes.io/docs/concepts/_print/\n\n518/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can't make a reliable assumption about the meaning of a private label key between different contexts.\n\nSuppose you have a 4-node cluster with the following labels:\nNAME\n\nSTATUS\n\nROLES\n\nAGE\n\nVERSION\n\nLABELS\n\nnode1\nnode2\n\nReady\nReady\n\n<none>\n<none>\n\n4m26s\n3m58s\n\nv1.16.0\nv1.16.0\n\nnode=node1,zone=zoneA\nnode=node2,zone=zoneA\n\nnode3\n\nReady\n\n<none>\n\n3m17s\n\nv1.16.0\n\nnode=node3,zone=zoneB\n\nnode4\n\nReady\n\n<none>\n\n2m43s\n\nv1.16.0\n\nnode=node4,zone=zoneB"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0539", "text": "Then the cluster is logically viewed as below:\ngraph TB subgraph \"zoneB\" n3(Node3) n4(Node4) end subgraph \"zoneA\" n1(Node1) n2(Node2) end\nclassDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s\nfill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,strokewidth:2px,color:#326ce5; class n1,n2,n3,n4 k8s; class zoneA,zoneB cluster;\n\nConsistency\nYou should set the same Pod topology spread constraints on all pods in a group.\nUsually, if you are using a workload controller such as a Deployment, the pod template takes care of this for you. If you mix different\nspread constraints then Kubernetes follows the API definition of the field; however, the behavior is more likely to become confusing\nand troubleshooting is less straightforward.\nYou need a mechanism to ensure that all the nodes in a topology domain (such as a cloud provider region) are labeled consistently.\nTo avoid you needing to manually label nodes, most clusters automatically populate well-known labels such as\nkubernetes.io/hostname . Check whether your cluster supports this.\n\nTopology spread constraint examples\nExample: one topology spread constraint\nSuppose you have a 4-node cluster where 3 Pods labeled foo: bar are located in node1, node2 and node3 respectively:\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph \"zoneA\" p1(Pod) -->\nn1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class zoneA,zoneB\ncluster;\nIf you want an incoming Pod to be evenly spread with existing Pods across zones, you can use a manifest similar to:\npods/topology-spread-constraints/one-constraint.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n519/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkind: Pod\napiVersion: v1\nmetadata:\nname: mypod\nlabels:\nfoo: bar\nspec:\ntopologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: zone\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\nfoo: bar\ncontainers:\n- name: pause\nimage: registry.k8s.io/pause:3.1"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0540", "text": "From that manifest, topologyKey: zone implies the even distribution will only be applied to nodes that are labeled zone: <any\nvalue> (nodes that don't have a zone label are skipped). The field whenUnsatisfiable: DoNotSchedule tells the scheduler to let the\nincoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.\nIf the scheduler placed this incoming Pod into zone A , the distribution of Pods would become [3, 1] . That means the actual skew\nis then 2 (calculated as 3 - 1 ), which violates maxSkew: 1 . To satisfy the constraints and context for this example, the incoming\nPod can only be placed onto a node in zone B :\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) p4(mypod) --> n4(Node4) end subgraph \"zoneA\"\np1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,strokewidth:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class\nzoneA,zoneB cluster;\nOR\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) p4(mypod) --> n3 n4(Node4) end subgraph \"zoneA\"\np1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,strokewidth:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class\nzoneA,zoneB cluster;\nYou can tweak the Pod spec to meet various kinds of requirements:\nChange maxSkew to a bigger value - such as 2 - so that the incoming Pod can be placed into zone A as well.\nChange topologyKey to node so as to distribute the Pods evenly across nodes instead of zones. In the above example, if\nmaxSkew remains 1 , the incoming Pod can only be placed onto the node node4 .\nChange whenUnsatisfiable: DoNotSchedule to whenUnsatisfiable: ScheduleAnyway to ensure the incoming Pod to be always\nschedulable (suppose other scheduling APIs are satisfied). However, it's preferred to be placed into the topology domain which\nhas fewer matching Pods. (Be aware that this preference is jointly normalized with other internal scheduling priorities such as\nresource usage ratio).\n\nExample: multiple topology spread constraints\nThis builds upon the previous example. Suppose you have a 4-node cluster where 3 existing Pods labeled foo: bar are located on\nnode1, node2 and node3 respectively:\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph \"zoneA\" p1(Pod) -->\nn1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n520/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0541", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class\nzoneA,zoneB cluster;\nYou can combine two topology spread constraints to control the spread of Pods both by node and by zone:\npods/topology-spread-constraints/two-constraints.yaml\nkind: Pod\napiVersion: v1\nmetadata:\nname: mypod\nlabels:\nfoo: bar\nspec:\ntopologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: zone\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\nfoo: bar\n- maxSkew: 1\ntopologyKey: node\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\nfoo: bar\ncontainers:\n- name: pause\nimage: registry.k8s.io/pause:3.1\n\nIn this case, to match the first constraint, the incoming Pod can only be placed onto nodes in zone B ; while in terms of the second\nconstraint, the incoming Pod can only be scheduled to the node node4 . The scheduler only considers options that satisfy all defined\nconstraints, so the only valid placement is onto node node4 .\n\nExample: conflicting topology spread constraints\nMultiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:\ngraph BT subgraph \"zoneB\" p4(Pod) --> n3(Node3) p5(Pod) --> n3 end subgraph \"zoneA\" p1(Pod) -->\nn1(Node1) p2(Pod) --> n1 p3(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,strokewidth:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s; class\nzoneA,zoneB cluster;\nIf you were to apply two-constraints.yaml (the manifest from the previous example) to this cluster, you would see that the Pod\nmypod stays in the Pending state. This happens because: to satisfy the first constraint, the Pod mypod can only be placed into zone\nB ; while in terms of the second constraint, the Pod mypod can only schedule to node node2 . The intersection of the two\nconstraints returns an empty set, and the scheduler cannot place the Pod.\nTo overcome this situation, you can either increase the value of maxSkew or modify one of the constraints to use\nwhenUnsatisfiable: ScheduleAnyway . Depending on circumstances, you might also decide to delete an existing Pod manually - for\nexample, if you are troubleshooting why a bug-fix rollout is not making progress.\n\nInteraction with node affinity and node selectors\nThe scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has spec.nodeSelector or\nspec.affinity.nodeAffinity defined.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n521/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0542", "text": "Example: topology spread constraints with node affinity\nSuppose you have a 5-node cluster ranging across zones A to C:\ngraph BT subgraph \"zoneB\" p3(Pod) --> n3(Node3) n4(Node4) end subgraph \"zoneA\" p1(Pod) -->\nn1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;\nclassDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class\nzoneA,zoneB cluster;\n\ngraph BT subgraph \"zoneC\" n5(Node5) end classDef plain fill:#ddd,stroke:#fff,strokewidth:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster\nfill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n5 k8s; class zoneC cluster;\nand you know that zone C must be excluded. In this case, you can compose a manifest as below, so that Pod mypod will be placed\ninto zone B instead of zone C . Similarly, Kubernetes also respects spec.nodeSelector .\npods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml\nkind: Pod\napiVersion: v1\nmetadata:\nname: mypod\nlabels:\nfoo: bar\nspec:\ntopologySpreadConstraints:\n- maxSkew: 1\ntopologyKey: zone\nwhenUnsatisfiable: DoNotSchedule\nlabelSelector:\nmatchLabels:\nfoo: bar\naffinity:\nnodeAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\nnodeSelectorTerms:\n- matchExpressions:\n- key: zone\noperator: NotIn\nvalues:\n- zoneC\ncontainers:\n- name: pause\nimage: registry.k8s.io/pause:3.1\n\nImplicit conventions\nThere are some implicit conventions worth noting here:\nOnly the Pods holding the same namespace as the incoming Pod can be matching candidates.\nThe scheduler only considers nodes that have all topologySpreadConstraints[*].topologyKey present at the same time.\nNodes missing any of these topologyKeys are bypassed. This implies that:\n1. any Pods located on those bypassed nodes do not impact maxSkew calculation - in the above example, suppose the node\nnode1 does not have a label \"zone\", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into\nzone A .\n2. the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a node node5\nhas the mistyped label zone-typo: zoneC (and no zone label set). After node node5 joins the cluster, it will be\nhttps://kubernetes.io/docs/concepts/_print/\n\n522/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0543", "text": "bypassed and Pods for this workload aren't scheduled there.\nBe aware of what will happen if the incoming Pod's topologySpreadConstraints[*].labelSelector doesn't match its own\nlabels. In the above example, if you remove the incoming Pod's labels, it can still be placed onto nodes in zone B , since the\nconstraints are still satisfied. However, after that placement, the degree of imbalance of the cluster remains unchanged - it's\nstill zone A having 2 Pods labeled as foo: bar , and zone B having 1 Pod labeled as foo: bar . If this is not what you expect,\nupdate the workload's topologySpreadConstraints[*].labelSelector to match the labels in the pod template.\n\nCluster-level default constraints\nIt is possible to set default topology spread constraints for a cluster. Default topology spread constraints are applied to a Pod if, and\nonly if:\nIt doesn't define any constraints in its .spec.topologySpreadConstraints .\nIt belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.\nDefault constraints can be set as part of the PodTopologySpread plugin arguments in a scheduling profile. The constraints are\nspecified with the same API above, except that labelSelector must be empty. The selectors are calculated from the Services,\nReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.\nAn example configuration might look like follows:\n\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: default-scheduler\npluginConfig:\n- name: PodTopologySpread\nargs:\ndefaultConstraints:\n- maxSkew: 1\ntopologyKey: topology.kubernetes.io/zone\nwhenUnsatisfiable: ScheduleAnyway\ndefaultingType: List\n\nBuilt-in default constraints\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]\n\nIf you don't configure any cluster-level default constraints for pod topology spreading, then kube-scheduler acts as if you specified\nthe following default topology constraints:\n\ndefaultConstraints:\n- maxSkew: 3\ntopologyKey: \"kubernetes.io/hostname\"\nwhenUnsatisfiable: ScheduleAnyway\n- maxSkew: 5\ntopologyKey: \"topology.kubernetes.io/zone\"\nwhenUnsatisfiable: ScheduleAnyway\n\nAlso, the legacy SelectorSpread plugin, which provides an equivalent behavior, is disabled by default.\nNote:\nThe PodTopologySpread plugin does not score the nodes that don't have the topology keys specified in the spreading\nconstraints. This might result in a different default behavior compared to the legacy SelectorSpread plugin when using the\ndefault topology constraints.\nhttps://kubernetes.io/docs/concepts/_print/\n\n523/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf your nodes are not expected to have both kubernetes.io/hostname and topology.kubernetes.io/zone labels set, define\nyour own constraints instead of using the Kubernetes defaults."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0544", "text": "If you don't want to use the default Pod spreading constraints for your cluster, you can disable those defaults by setting\ndefaultingType to List and leaving empty defaultConstraints in the PodTopologySpread plugin configuration:\n\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- schedulerName: default-scheduler\npluginConfig:\n- name: PodTopologySpread\nargs:\ndefaultConstraints: []\ndefaultingType: List\n\nComparison with podAffinity and podAntiAffinity\nIn Kubernetes, inter-Pod affinity and anti-affinity control how Pods are scheduled in relation to one another - either more packed or\nmore scattered.\npodAffinity\n\nattracts Pods; you can try to pack any number of Pods into qualifying topology domain(s).\npodAntiAffinity\n\nrepels Pods. If you set this to requiredDuringSchedulingIgnoredDuringExecution mode then only a single Pod can be scheduled\ninto a single topology domain; if you choose preferredDuringSchedulingIgnoredDuringExecution then you lose the ability to\nenforce the constraint.\nFor finer control, you can specify topology spread constraints to distribute Pods across different topology domains - to achieve either\nhigh availability or cost-saving. This can also help on rolling update workloads and scaling out replicas smoothly.\nFor more context, see the Motivation section of the enhancement proposal about Pod topology spread constraints."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0545", "text": "Known limitations\nThere's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment\nmay result in imbalanced Pods distribution.\nYou can use a tool such as the Descheduler to rebalance the Pods distribution.\nPods matched on tainted nodes are respected. See Issue 80921.\nThe scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined\nfrom the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is\nscaled to zero nodes, and you're expecting the cluster to scale up, because, in this case, those topology domains won't be\nconsidered until there is at least one node in them.\nYou can work around this by using a Node autoscaler that is aware of Pod topology spread constraints and is also aware of the\noverall set of topology domains.\nPods that don't match their own labelSelector create \"ghost pods\". If a pod's labels don't match the labelSelector in its\ntopology spread constraint, the pod won't count itself in spread calculations. This means:\nMultiple such pods can just accumulate on the same topology (until matching pods are newly created/deleted) because\nthose pod's schedule don't change a spreading calculation result.\nThe spreading constraint works in an unintended way, most likely not matching your expectations\nhttps://kubernetes.io/docs/concepts/_print/\n\n524/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nEnsure your pod's labels match the labelSelector in your spread constraints. Typically, a pod should match its own topology\nspread constraint selector.\n\nWhat's next\nThe blog article Introducing PodTopologySpread explains maxSkew in some detail, as well as covering some advanced usage\nexamples.\nRead the scheduling section of the API reference for Pod.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n525/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.6 - Taints and Tolerations"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0546", "text": "Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the\nopposite -- they allow a node to repel a set of pods.\nTolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling\nbut don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.\nTaints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are\napplied to a node; this marks that the node should not accept any pods that do not tolerate the taints.\n\nConcepts\nYou add a taint to a node using kubectl taint. For example,\n\nkubectl taint nodes node1 key1=value1:NoSchedule\n\nplaces a taint on node node1 . The taint has key key1 , value value1 , and taint effect NoSchedule . This means that no pod will be\nable to schedule onto node1 unless it has a matching toleration.\nTo remove the taint added by the command above, you can run:\n\nkubectl taint nodes node1 key1=value1:NoSchedule-\n\nYou specify a toleration for a pod in the PodSpec. Both of the following tolerations \"match\" the taint created by the kubectl taint\nline above, and thus a pod with either toleration would be able to schedule onto node1 :\n\ntolerations:\n- key: \"key1\"\noperator: \"Equal\"\nvalue: \"value1\"\neffect: \"NoSchedule\"\n\ntolerations:\n- key: \"key1\"\noperator: \"Exists\"\neffect: \"NoSchedule\"\n\nThe default Kubernetes scheduler takes taints and tolerations into account when selecting a node to run a particular Pod. However,\nif you manually specify the .spec.nodeName for a Pod, that action bypasses the scheduler; the Pod is then bound onto the node\nwhere you assigned it, even if there are NoSchedule taints on that node that you selected. If this happens and the node also has a\nNoExecute taint set, the kubelet will eject the Pod unless there is an appropriate tolerance set.\nHere's an example of a pod that has some tolerations defined:\npods/pod-with-toleration.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n526/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\nenv: test\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nimagePullPolicy: IfNotPresent\ntolerations:\n- key: \"example-key\"\noperator: \"Exists\"\neffect: \"NoSchedule\""}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0547", "text": "The default value for operator is Equal .\nA toleration \"matches\" a taint if the keys are the same and the effects are the same, and:\nthe operator is Exists (in which case no value should be specified), or\nthe operator is Equal and the values should be equal.\nNote:\nThere are two special cases:\nIf the key is empty, then the operator must be Exists , which matches all keys and values. Note that the effect still needs\nto be matched at the same time.\nAn empty effect matches all effects with key key1 .\n\nThe above example used the effect of NoSchedule . Alternatively, you can use the effect of PreferNoSchedule .\nThe allowed values for the effect field are:\nNoExecute\n\nThis affects pods that are already running on the node as follows:\nPods that do not tolerate the taint are evicted immediately\nPods that tolerate the taint without specifying tolerationSeconds in their toleration specification remain bound forever\nPods that tolerate the taint with a specified tolerationSeconds remain bound for the specified amount of time. After that\ntime elapses, the node lifecycle controller evicts the Pods from the node.\nNoSchedule\n\nNo new Pods will be scheduled on the tainted node unless they have a matching toleration. Pods currently running on the node\nare not evicted.\nPreferNoSchedule\nPreferNoSchedule is a \"preference\" or \"soft\" version of NoSchedule. The control plane will try to avoid placing a Pod that does not"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0548", "text": "tolerate the taint on the node, but it is not guaranteed.\nYou can put multiple taints on the same node and multiple tolerations on the same pod. The way Kubernetes processes multiple\ntaints and tolerations is like a filter: start with all of a node's taints, then ignore the ones for which the pod has a matching toleration;\nthe remaining un-ignored taints have the indicated effects on the pod. In particular,\nif there is at least one un-ignored taint with effect NoSchedule then Kubernetes will not schedule the pod onto that node\nif there is no un-ignored taint with effect NoSchedule but there is at least one un-ignored taint with effect PreferNoSchedule\nthen Kubernetes will try to not schedule the pod onto the node\nif there is at least one un-ignored taint with effect NoExecute then the pod will be evicted from the node (if it is already running\non the node), and will not be scheduled onto the node (if it is not yet running on the node).\nhttps://kubernetes.io/docs/concepts/_print/\n\n527/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, imagine you taint a node like this\n\nkubectl taint nodes node1 key1=value1:NoSchedule\nkubectl taint nodes node1 key1=value1:NoExecute\nkubectl taint nodes node1 key2=value2:NoSchedule\n\nAnd a pod has two tolerations:\n\ntolerations:\n- key: \"key1\"\noperator: \"Equal\"\nvalue: \"value1\"\neffect: \"NoSchedule\"\n- key: \"key1\"\noperator: \"Equal\"\nvalue: \"value1\"\neffect: \"NoExecute\"\n\nIn this case, the pod will not be able to schedule onto the node, because there is no toleration matching the third taint. But it will be\nable to continue running if it is already running on the node when the taint is added, because the third taint is the only one of the\nthree that is not tolerated by the pod.\nNormally, if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted\nimmediately, and pods that do tolerate the taint will never be evicted. However, a toleration with NoExecute effect can specify an\noptional tolerationSeconds field that dictates how long the pod will stay bound to the node after the taint is added. For example,\n\ntolerations:\n- key: \"key1\"\noperator: \"Equal\"\nvalue: \"value1\"\neffect: \"NoExecute\"\ntolerationSeconds: 3600"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0549", "text": "means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600\nseconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted.\n\nExample Use Cases\nTaints and tolerations are a flexible way to steer pods away from nodes or evict pods that shouldn't be running. A few of the use\ncases are\nDedicated Nodes: If you want to dedicate a set of nodes for exclusive use by a particular set of users, you can add a taint to\nthose nodes (say, kubectl taint nodes nodename dedicated=groupName:NoSchedule ) and then add a corresponding toleration\nto their pods (this would be done most easily by writing a custom admission controller). The pods with the tolerations will then\nbe allowed to use the tainted (dedicated) nodes as well as any other nodes in the cluster. If you want to dedicate the nodes to\nthem and ensure they only use the dedicated nodes, then you should additionally add a label similar to the taint to the same\nset of nodes (e.g. dedicated=groupName ), and the admission controller should additionally add a node affinity to require that\nthe pods can only schedule onto nodes labeled with dedicated=groupName .\nNodes with Special Hardware: In a cluster where a small subset of nodes have specialized hardware (for example GPUs), it is\ndesirable to keep pods that don't need the specialized hardware off of those nodes, thus leaving room for later-arriving pods\nthat do need the specialized hardware. This can be done by tainting the nodes that have the specialized hardware (e.g. kubectl\ntaint nodes nodename special=true:NoSchedule or kubectl taint nodes nodename special=true:PreferNoSchedule ) and\nadding a corresponding toleration to pods that use the special hardware. As in the dedicated nodes use case, it is probably\neasiest to apply the tolerations using a custom admission controller. For example, it is recommended to use Extended\nResources to represent the special hardware, taint your special hardware nodes with the extended resource name and run the\nExtendedResourceToleration admission controller. Now, because the nodes are tainted, no pods without the toleration will\nhttps://kubernetes.io/docs/concepts/_print/\n\n528/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0550", "text": "schedule on them. But when you submit a pod that requests the extended resource, the ExtendedResourceToleration\nadmission controller will automatically add the correct toleration to the pod and that pod will schedule on the special hardware\nnodes. This will make sure that these special hardware nodes are dedicated for pods requesting such hardware and you don't\nhave to manually add tolerations to your pods.\nTaint based Evictions: A per-pod-configurable eviction behavior when there are node problems, which is described in the next\nsection.\n\nTaint based Evictions\nâ“˜ FEATURE STATE: Kubernetes v1.18 [stable]\n\nThe node controller automatically taints a Node when certain conditions are true. The following taints are built in:\nnode.kubernetes.io/not-ready : Node is not ready. This corresponds to the NodeCondition Ready\n\nbeing \" False \".\n\nnode.kubernetes.io/unreachable : Node is unreachable from the node controller. This corresponds to the NodeCondition\nReady\n\nbeing \" Unknown \".\n\nnode.kubernetes.io/memory-pressure : Node has memory pressure.\nnode.kubernetes.io/disk-pressure : Node has disk pressure.\nnode.kubernetes.io/pid-pressure : Node has PID pressure.\nnode.kubernetes.io/network-unavailable : Node's network is unavailable.\nnode.kubernetes.io/unschedulable : Node is unschedulable.\nnode.cloudprovider.kubernetes.io/uninitialized : When the kubelet is started with an \"external\" cloud provider, this taint is"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0551", "text": "set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet\nremoves this taint.\nIn case a node is to be drained, the node controller or the kubelet adds relevant taints with NoExecute effect. This effect is added by\ndefault for the node.kubernetes.io/not-ready and node.kubernetes.io/unreachable taints. If the fault condition returns to\nnormal, the kubelet or node controller can remove the relevant taint(s).\nIn some cases when the node is unreachable, the API server is unable to communicate with the kubelet on the node. The decision to\ndelete the pods cannot be communicated to the kubelet until communication with the API server is re-established. In the meantime,\nthe pods that are scheduled for deletion may continue to run on the partitioned node.\nNote:\nThe control plane limits the rate of adding new taints to nodes. This rate limiting manages the number of evictions that are\ntriggered when many nodes become unreachable at once (for example: if there is a network disruption).\nYou can specify tolerationSeconds for a Pod to define how long that Pod stays bound to a failing or unresponsive Node.\nFor example, you might want to keep an application with a lot of local state bound to node for a long time in the event of network\npartition, hoping that the partition will recover and thus the pod eviction can be avoided. The toleration you set for that Pod might\nlook like:\n\ntolerations:\n- key: \"node.kubernetes.io/unreachable\"\noperator: \"Exists\"\neffect: \"NoExecute\"\ntolerationSeconds: 6000\n\nNote:\nKubernetes automatically adds a toleration for node.kubernetes.io/not-ready and node.kubernetes.io/unreachable with\ntolerationSeconds=300 , unless you, or a controller, set those tolerations explicitly.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n529/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThese automatically-added tolerations mean that Pods remain bound to Nodes for 5 minutes after one of these problems is\ndetected.\n\nDaemonSet pods are created with NoExecute tolerations for the following taints with no tolerationSeconds :\nnode.kubernetes.io/unreachable\nnode.kubernetes.io/not-ready\n\nThis ensures that DaemonSet pods are never evicted due to these problems.\nNote:\nThe node controller was responsible for adding taints to nodes and evicting pods. But after 1.29, the taint-based eviction\nimplementation has been moved out of node controller into a separate, and independent component called taint-evictioncontroller. Users can optionally disable taint-based eviction by setting --controllers=-taint-eviction-controller in kubecontroller-manager."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0552", "text": "Taint Nodes by Condition\nThe control plane, using the node controller, automatically creates taints with a NoSchedule effect for node conditions.\nThe scheduler checks taints, not node conditions, when it makes scheduling decisions. This ensures that node conditions don't\ndirectly affect scheduling. For example, if the DiskPressure node condition is active, the control plane adds the\nnode.kubernetes.io/disk-pressure taint and does not schedule new pods onto the affected node. If the MemoryPressure node\ncondition is active, the control plane adds the node.kubernetes.io/memory-pressure taint.\nYou can ignore node conditions for newly created pods by adding the corresponding Pod tolerations. The control plane also adds\nthe node.kubernetes.io/memory-pressure toleration on pods that have a QoS class other than BestEffort . This is because\nKubernetes treats pods in the Guaranteed or Burstable QoS classes (even pods with no memory request set) as if they are able to\ncope with memory pressure, while new BestEffort pods are not scheduled onto the affected node.\nThe DaemonSet controller automatically adds the following NoSchedule tolerations to all daemons, to prevent DaemonSets from\nbreaking.\nnode.kubernetes.io/memory-pressure\nnode.kubernetes.io/disk-pressure\nnode.kubernetes.io/pid-pressure\n\n(1.14 or later)\n\nnode.kubernetes.io/unschedulable\n\n(1.10 or later)\n\nnode.kubernetes.io/network-unavailable\n\n(host network only)\n\nAdding these tolerations ensures backward compatibility. You can also add arbitrary tolerations to DaemonSets.\n\nDevice taints and tolerations\nInstead of tainting entire nodes, administrators can also taint individual devices when the cluster uses dynamic resource allocation\nto manage special hardware. The advantage is that tainting can be targeted towards exactly the hardware that is faulty or needs\nmaintenance. Tolerations are also supported and can be specified when requesting devices. Like taints they apply to all pods which\nshare the same allocated device.\n\nWhat's next\nRead about Node-pressure Eviction and how you can configure it\nRead about Pod Priority\nRead about device taints and tolerations\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n530/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.7 - Scheduling Framework\nâ“˜ FEATURE STATE: Kubernetes v1.19 [stable]\n\nThe scheduling framework is a pluggable architecture for the Kubernetes scheduler. It consists of a set of \"plugin\" APIs that are\ncompiled directly into the scheduler. These APIs allow most scheduling features to be implemented as plugins, while keeping the\nscheduling \"core\" lightweight and maintainable. Refer to the design proposal of the scheduling framework for more technical\ninformation on the design of the framework."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0553", "text": "Framework workflow\nThe Scheduling Framework defines a few extension points. Scheduler plugins register to be invoked at one or more extension points.\nSome of these plugins can change the scheduling decisions and some are informational only.\nEach attempt to schedule one Pod is split into two phases, the scheduling cycle and the binding cycle.\n\nScheduling cycle & binding cycle\nThe scheduling cycle selects a node for the Pod, and the binding cycle applies that decision to the cluster. Together, a scheduling\ncycle and binding cycle are referred to as a \"scheduling context\".\nScheduling cycles are run serially, while binding cycles may run concurrently.\nA scheduling or binding cycle can be aborted if the Pod is determined to be unschedulable or if there is an internal error. The Pod\nwill be returned to the queue and retried.\n\nInterfaces\nThe following picture shows the scheduling context of a Pod and the interfaces that the scheduling framework exposes.\nOne plugin may implement multiple interfaces to perform more complex or stateful tasks.\nSome interfaces match the scheduler extension points which can be configured through Scheduler Configuration.\n\nScheduling framework extension points\n\nPreEnqueue\nThese plugins are called prior to adding Pods to the internal active queue, where Pods are marked as ready for scheduling.\nOnly when all PreEnqueue plugins return Success , the Pod is allowed to enter the active queue. Otherwise, it's placed in the\ninternal unschedulable Pods list, and doesn't get an Unschedulable condition.\nhttps://kubernetes.io/docs/concepts/_print/\n\n531/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor more details about how internal scheduler queues work, read Scheduling queue in kube-scheduler.\n\nEnqueueExtension\nEnqueueExtension is the interface where the plugin can control whether to retry scheduling of Pods rejected by this plugin, based on\nchanges in the cluster. Plugins that implement PreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.\n\nQueueingHint\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nQueueingHint is a callback function for deciding whether a Pod can be requeued to the active queue or backoff queue. It's executed\nevery time a certain kind of event or change happens in the cluster. When the QueueingHint finds that the event might make the\nPod schedulable, the Pod is put into the active queue or the backoff queue so that the scheduler will retry the scheduling of the Pod."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0554", "text": "QueueSort\nThese plugins are used to sort Pods in the scheduling queue. A queue sort plugin essentially provides a Less(Pod1, Pod2) function.\nOnly one queue sort plugin may be enabled at a time.\n\nPreFilter\nThese plugins are used to pre-process info about the Pod, or to check certain conditions that the cluster or the Pod must meet. If a\nPreFilter plugin returns an error, the scheduling cycle is aborted.\n\nFilter\nThese plugins are used to filter out nodes that cannot run the Pod. For each node, the scheduler will call filter plugins in their\nconfigured order. If any filter plugin marks the node as infeasible, the remaining plugins will not be called for that node. Nodes may\nbe evaluated concurrently.\n\nPostFilter\nThese plugins are called after the Filter phase, but only when no feasible nodes were found for the pod. Plugins are called in their\nconfigured order. If any postFilter plugin marks the node as Schedulable , the remaining plugins will not be called. A typical\nPostFilter implementation is preemption, which tries to make the pod schedulable by preempting other Pods.\n\nPreScore\nThese plugins are used to perform \"pre-scoring\" work, which generates a sharable state for Score plugins to use. If a PreScore plugin\nreturns an error, the scheduling cycle is aborted.\n\nScore\nThese plugins are used to rank nodes that have passed the filtering phase. The scheduler will call each scoring plugin for each node.\nThere will be a well defined range of integers representing the minimum and maximum scores. After the NormalizeScore phase, the\nscheduler will combine node scores from all plugins according to the configured plugin weights.\n\nCapacity scoring\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nThe feature gate VolumeCapacityPriority was used in v1.32 to support storage that are statically provisioned. Starting from v1.33,\nthe new feature gate StorageCapacityScoring replaces the old VolumeCapacityPriority gate with added support to dynamically\nprovisioned storage. When StorageCapacityScoring is enabled, the VolumeBinding plugin in the kube-scheduler is extended to\nscore Nodes based on the storage capacity on each of them. This feature is applicable to CSI volumes that supported Storage\nCapacity, including local storage backed by a CSI driver.\nhttps://kubernetes.io/docs/concepts/_print/\n\n532/684\n\n11/7/25, 4:37 PM\n\nNormalizeScore\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0555", "text": "These plugins are used to modify scores before the scheduler computes a final ranking of Nodes. A plugin that registers for this\nextension point will be called with the Score results from the same plugin. This is called once per plugin per scheduling cycle.\nFor example, suppose a plugin BlinkingLightScorer ranks Nodes based on how many blinking lights they have.\n\nfunc ScoreNode(_ *v1.pod, n *v1.Node) (int, error) {\nreturn getBlinkingLightCount(n)\n}\n\nHowever, the maximum count of blinking lights may be small compared to NodeScoreMax . To fix this, BlinkingLightScorer should\nalso register for this extension point.\n\nfunc NormalizeScores(scores map[string]int) {\nhighest := 0\nfor _, score := range scores {\nhighest = max(highest, score)\n}\nfor node, score := range scores {\nscores[node] = score*NodeScoreMax/highest\n}\n}\n\nIf any NormalizeScore plugin returns an error, the scheduling cycle is aborted.\nNote:\nPlugins wishing to perform \"pre-reserve\" work should use the NormalizeScore extension point.\n\nReserve\nA plugin that implements the Reserve interface has two methods, namely Reserve and Unreserve , that back two informational\nscheduling phases called Reserve and Unreserve, respectively. Plugins which maintain runtime state (aka \"stateful plugins\") should\nuse these phases to be notified by the scheduler when resources on a node are being reserved and unreserved for a given Pod.\nThe Reserve phase happens before the scheduler actually binds a Pod to its designated node. It exists to prevent race conditions\nwhile the scheduler waits for the bind to succeed. The Reserve method of each Reserve plugin may succeed or fail; if one Reserve\nmethod call fails, subsequent plugins are not executed and the Reserve phase is considered to have failed. If the Reserve method\nof all plugins succeed, the Reserve phase is considered to be successful and the rest of the scheduling cycle and the binding cycle are\nexecuted.\nThe Unreserve phase is triggered if the Reserve phase or a later phase fails. When this happens, the Unreserve method of all\nReserve plugins will be executed in the reverse order of Reserve method calls. This phase exists to clean up the state associated\nwith the reserved Pod.\nCaution:\nThe implementation of the Unreserve method in Reserve plugins must be idempotent and may not fail."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0556", "text": "Permit\nPermit plugins are invoked at the end of the scheduling cycle for each Pod, to prevent or delay the binding to the candidate node. A\npermit plugin can do one of the three things:\n1. approve\nOnce all Permit plugins approve a Pod, it is sent for binding.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n533/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n2. deny\nIf any Permit plugin denies a Pod, it is returned to the scheduling queue. This will trigger the Unreserve phase in Reserve\nplugins.\n3. wait (with a timeout)\nIf a Permit plugin returns \"wait\", then the Pod is kept in an internal \"waiting\" Pods list, and the binding cycle of this Pod starts\nbut directly blocks until it gets approved. If a timeout occurs, wait becomes deny and the Pod is returned to the scheduling\nqueue, triggering the Unreserve phase in Reserve plugins.\nNote:\nWhile any plugin can access the list of \"waiting\" Pods and approve them (see FrameworkHandle), we expect only the permit\nplugins to approve binding of reserved Pods that are in \"waiting\" state. Once a Pod is approved, it is sent to the PreBind phase.\n\nPreBind\nThese plugins are used to perform any work required before a Pod is bound. For example, a pre-bind plugin may provision a\nnetwork volume and mount it on the target node before allowing the Pod to run there.\nIf any PreBind plugin returns an error, the Pod is rejected and returned to the scheduling queue.\n\nBind\nThese plugins are used to bind a Pod to a Node. Bind plugins will not be called until all PreBind plugins have completed. Each bind\nplugin is called in the configured order. A bind plugin may choose whether or not to handle the given Pod. If a bind plugin chooses to\nhandle a Pod, the remaining bind plugins are skipped.\n\nPostBind\nThis is an informational interface. Post-bind plugins are called after a Pod is successfully bound. This is the end of a binding cycle,\nand can be used to clean up associated resources.\n\nPlugin API\nThere are two steps to the plugin API. First, plugins must register and get configured, then they use the extension point interfaces.\nExtension point interfaces have the following form.\n\ntype Plugin interface {\nName() string\n}\ntype QueueSortPlugin interface {\nPlugin\nLess(*v1.pod, *v1.pod) bool\n}\ntype PreFilterPlugin interface {\nPlugin\nPreFilter(context.Context, *framework.CycleState, *v1.pod) error\n}\n// ..."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0557", "text": "Plugin configuration\nYou can enable or disable plugins in the scheduler configuration. If you are using Kubernetes v1.18 or later, most scheduling plugins\nare in use and enabled by default.\nIn addition to default plugins, you can also implement your own scheduling plugins and get them configured along with default\nplugins. You can visit scheduler-plugins for more details.\nhttps://kubernetes.io/docs/concepts/_print/\n\n534/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf you are using Kubernetes v1.18 or later, you can configure a set of plugins as a scheduler profile and then define multiple profiles\nto fit various kinds of workload. Learn more at multiple profiles.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n535/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.8 - Dynamic Resource Allocation\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nThis page describes dynamic resource allocation (DRA) in Kubernetes.\n\nAbout DRA\nDRA is a Kubernetes feature that lets you request and share resources among Pods. These resources are often attached devices like\nhardware accelerators.\nWith DRA, device drivers and cluster admins define device classes that are available to claim in workloads. Kubernetes allocates\nmatching devices to specific claims and places the corresponding Pods on nodes that can access the allocated devices.\nAllocating resources with DRA is a similar experience to dynamic volume provisioning, in which you use PersistentVolumeClaims to\nclaim storage capacity from storage classes and request the claimed capacity in your Pods.\n\nBenefits of DRA\nDRA provides a flexible way to categorize, request, and use devices in your cluster. Using DRA provides benefits like the following:\nFlexible device filtering: use common expression language (CEL) to perform fine-grained filtering for specific device attributes.\nDevice sharing: share the same resource with multiple containers or Pods by referencing the corresponding resource claim.\nCentralized device categorization: device drivers and cluster admins can use device classes to provide app operators with\nhardware categories that are optimized for various use cases. For example, you can create a cost-optimized device class for\ngeneral-purpose workloads, and a high-performance device class for critical jobs.\nSimplified Pod requests: with DRA, app operators don't need to specify device quantities in Pod resource requests. Instead,\nthe Pod references a resource claim, and the device configuration in that claim applies to the Pod.\nThese benefits provide significant improvements in the device allocation workflow when compared to device plugins, which require\nper-container device requests, don't support device sharing, and don't support expression-based device filtering."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0558", "text": "Types of DRA users\nThe workflow of using DRA to allocate devices involves the following types of users:\nDevice owner: responsible for devices. Device owners might be commercial vendors, the cluster operator, or another entity.\nTo use DRA, devices must have DRA-compatible drivers that do the following:\nCreate ResourceSlices that provide Kubernetes with information about nodes and resources.\nUpdate ResourceSlices when resource capacity in the cluster changes.\nOptionally, create DeviceClasses that workload operators can use to claim devices.\nCluster admin: responsible for configuring clusters and nodes, attaching devices, installing drivers, and similar tasks. To use\nDRA, cluster admins do the following:\nAttach devices to nodes.\nInstall device drivers that support DRA.\nOptionally, create DeviceClasses that workload operators can use to claim devices.\nWorkload operator: responsible for deploying and managing workloads in the cluster. To use DRA to allocate devices to Pods,\nworkload operators do the following:\nCreate ResourceClaims or ResourceClaimTemplates to request specific configurations within DeviceClasses.\nDeploy workloads that use specific ResourceClaims or ResourceClaimTemplates.\n\nDRA terminology\nDRA uses the following Kubernetes API kinds to provide the core allocation functionality. All of these API kinds are included in the\nresource.k8s.io/v1 API group.\nhttps://kubernetes.io/docs/concepts/_print/\n\n536/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDeviceClass\nDefines a category of devices that can be claimed and how to select specific device attributes in claims. The DeviceClass\nparameters can match zero or more devices in ResourceSlices. To claim devices from a DeviceClass, ResourceClaims select\nspecific device attributes.\nResourceClaim\nDescribes a request for access to attached resources, such as devices, in the cluster. ResourceClaims provide Pods with access to\na specific resource. ResourceClaims can be created by workload operators or generated by Kubernetes based on a\nResourceClaimTemplate.\nResourceClaimTemplate\nDefines a template that Kubernetes uses to create per-Pod ResourceClaims for a workload. ResourceClaimTemplates provide\nPods with access to separate, similar resources. Each ResourceClaim that Kubernetes generates from the template is bound to a\nspecific Pod. When the Pod terminates, Kubernetes deletes the corresponding ResourceClaim.\nResourceSlice\nRepresents one or more resources that are attached to nodes, such as devices. Drivers create and manage ResourceSlices in the\ncluster. When a ResourceClaim is created and used in a Pod, Kubernetes uses ResourceSlices to find nodes that have access to\nthe claimed resources. Kubernetes allocates resources to the ResourceClaim and schedules the Pod onto a node that can access\nthe resources."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0559", "text": "DeviceClass\nA DeviceClass lets cluster admins or device drivers define categories of devices in the cluster. DeviceClasses tell operators what\ndevices they can request and how they can request those devices. You can use common expression language (CEL) to select devices\nbased on specific attributes. A ResourceClaim that references the DeviceClass can then request specific configurations within the\nDeviceClass.\nTo create a DeviceClass, see Set Up DRA in a Cluster.\n\nResourceClaims and ResourceClaimTemplates\nA ResourceClaim defines the resources that a workload needs. Every ResourceClaim has requests that reference a DeviceClass and\nselect devices from that DeviceClass. ResourceClaims can also use selectors to filter for devices that meet specific requirements, and\ncan use constraints to limit the devices that can satisfy a request. ResourceClaims can be created by workload operators or can be\ngenerated by Kubernetes based on a ResourceClaimTemplate. A ResourceClaimTemplate defines a template that Kubernetes can\nuse to auto-generate ResourceClaims for Pods.\n\nUse cases for ResourceClaims and ResourceClaimTemplates\nThe method that you use depends on your requirements, as follows:\nResourceClaim: you want multiple Pods to share access to specific devices. You manually manage the lifecycle of\nResourceClaims that you create.\nResourceClaimTemplate: you want Pods to have independent access to separate, similarly-configured devices. Kubernetes\ngenerates ResourceClaims from the specification in the ResourceClaimTemplate. The lifetime of each generated ResourceClaim\nis bound to the lifetime of the corresponding Pod.\nWhen you define a workload, you can use Common Expression Language (CEL) to filter for specific device attributes or capacity. The\navailable parameters for filtering depend on the device and the drivers.\nIf you directly reference a specific ResourceClaim in a Pod, that ResourceClaim must already exist in the same namespace as the\nPod. If the ResourceClaim doesn't exist in the namespace, the Pod won't schedule. This behavior is similar to how a\nPersistentVolumeClaim must exist in the same namespace as a Pod that references it.\nYou can reference an auto-generated ResourceClaim in a Pod, but this isn't recommended because auto-generated ResourceClaims\nare bound to the lifetime of the Pod that triggered the generation.\nTo learn how to claim resources using one of these methods, see Allocate Devices to Workloads with DRA.\n\nPrioritized list\nhttps://kubernetes.io/docs/concepts/_print/\n\n537/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0560", "text": "You can provide a prioritized list of subrequests for requests in a ResourceClaim or ResourceClaimTemplate. The scheduler will then\nselect the first subrequest that can be allocated. This allows users to specify alternative devices that can be used by the workload if\nthe primary choice is not available.\nIn the example below, the ResourceClaimTemplate requested a device with the color black and the size large. If a device with those\nattributes is not available, the pod cannot be scheduled. With the prioritized list feature, a second alternative can be specified, which\nrequests two devices with the color white and size small. The large black device will be allocated if it is available. If it is not, but two\nsmall white devices are available, the pod will still be able to run.\n\napiVersion: resource.k8s.io/v1\nkind: ResourceClaimTemplate\nmetadata:\nname: prioritized-list-claim-template\nspec:\nspec:\ndevices:\nrequests:\n- name: req-0\nfirstAvailable:\n- name: large-black\ndeviceClassName: resource.example.com\nselectors:\n- cel:\nexpression: |device.attributes[\"resource-driver.example.com\"].color == \"black\" &&\ndevice.attributes[\"resource-driver.example.com\"].size == \"large\"\n- name: small-white\ndeviceClassName: resource.example.com\nselectors:\n- cel:\nexpression: |device.attributes[\"resource-driver.example.com\"].color == \"white\" &&\ndevice.attributes[\"resource-driver.example.com\"].size == \"small\"\ncount: 2\n\nThe decision is made on a per-Pod basis, so if the Pod is a member of a ReplicaSet or similar grouping, you cannot rely on all the\nmembers of the group having the same subrequest chosen. Your workload must be able to accommodate this.\nPrioritized lists is a beta feature and is enabled by default with the DRAPrioritizedList feature gate in the kube-apiserver and kubescheduler."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0561", "text": "ResourceSlice\nEach ResourceSlice represents one or more devices in a pool. The pool is managed by a device driver, which creates and manages\nResourceSlices. The resources in a pool might be represented by a single ResourceSlice or span multiple ResourceSlices.\nResourceSlices provide useful information to device users and to the scheduler, and are crucial for dynamic resource allocation.\nEvery ResourceSlice must include the following information:\nResource pool: a group of one or more resources that the driver manages. The pool can span more than one ResourceSlice.\nChanges to the resources in a pool must be propagated across all of the ResourceSlices in that pool. The device driver that\nmanages the pool is responsible for ensuring that this propagation happens.\nDevices: devices in the managed pool. A ResourceSlice can list every device in a pool or a subset of the devices in a pool. The\nResourceSlice defines device information like attributes, versions, and capacity. Device users can select devices for allocation by\nfiltering for device information in ResourceClaims or in DeviceClasses.\nNodes: the nodes that can access the resources. Drivers can choose which nodes can access the resources, whether that's all\nof the nodes in the cluster, a single named node, or nodes that have specific node labels.\nDrivers use a controller to reconcile ResourceSlices in the cluster with the information that the driver has to publish. This controller\noverwrites any manual changes, such as cluster users creating or modifying ResourceSlices.\nhttps://kubernetes.io/docs/concepts/_print/\n\n538/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nConsider the following example ResourceSlice:\n\napiVersion: resource.k8s.io/v1\nkind: ResourceSlice\nmetadata:\nname: cat-slice\nspec:\ndriver: \"resource-driver.example.com\"\npool:\ngeneration: 1\nname: \"black-cat-pool\"\nresourceSliceCount: 1\n# The allNodes field defines whether any node in the cluster can access the device.\nallNodes: true\ndevices:\n- name: \"large-black-cat\"\nattributes:\ncolor:\nstring: \"black\"\nsize:\nstring: \"large\"\ncat:\nbool: true\n\nThis ResourceSlice is managed by the resource-driver.example.com driver in the black-cat-pool pool. The allNodes: true field\nindicates that any node in the cluster can access the devices. There's one device in the ResourceSlice, named large-black-cat , with\nthe following attributes:\ncolor : black\nsize : large\ncat : true\n\nA DeviceClass could select this ResourceSlice by using these attributes, and a ResourceClaim could filter for specific devices in that\nDeviceClass.\n\nHow resource allocation with DRA works\nThe following sections describe the workflow for the various types of DRA users and for the Kubernetes system during dynamic\nresource allocation."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0562", "text": "Workflow for users\n1. Driver creation: device owners or third-party entities create drivers that can create and manage ResourceSlices in the cluster.\nThese drivers optionally also create DeviceClasses that define a category of devices and how to request them.\n2. Cluster configuration: cluster admins create clusters, attach devices to nodes, and install the DRA device drivers. Cluster\nadmins optionally create DeviceClasses that define categories of devices and how to request them.\n3. Resource claims: workload operators create ResourceClaimTemplates or ResourceClaims that request specific device\nconfigurations within a DeviceClass. In the same step, workload operators modify their Kubernetes manifests to request those\nResourceClaimTemplates or ResourceClaims.\n\nWorkflow for Kubernetes\n1. ResourceSlice creation: drivers in the cluster create ResourceSlices that represent one or more devices in a managed pool of\nsimilar devices.\n2. Workload creation: the cluster control plane checks new workloads for references to ResourceClaimTemplates or to specific\nResourceClaims.\nIf the workload uses a ResourceClaimTemplate, a controller named the resourceclaim-controller generates\nResourceClaims for every Pod in the workload.\nhttps://kubernetes.io/docs/concepts/_print/\n\n539/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf the workload uses a specific ResourceClaim, Kubernetes checks whether that ResourceClaim exists in the cluster. If the\nResourceClaim doesn't exist, the Pods won't deploy.\n3. ResourceSlice filtering: for every Pod, Kubernetes checks the ResourceSlices in the cluster to find a device that satisfies all of\nthe following criteria:\nThe nodes that can access the resources are eligible to run the Pod.\nThe ResourceSlice has unallocated resources that match the requirements of the Pod's ResourceClaim.\n4. Resource allocation: after finding an eligible ResourceSlice for a Pod's ResourceClaim, the Kubernetes scheduler updates the\nResourceClaim with the allocation details.\n5. Pod scheduling: when resource allocation is complete, the scheduler places the Pod on a node that can access the allocated\nresource. The device driver and the kubelet on that node configure the device and the Pod's access to the device.\n\nObservability of dynamic resources\nYou can check the status of dynamically allocated resources by using any of the following methods:\nkubelet device metrics\nResourceClaim status\nDevice health monitoring\n\nkubelet device metrics\nThe PodResourcesLister kubelet gRPC service lets you monitor in-use devices. The DynamicResource message provides information\nthat's specific to dynamic resource allocation, such as the device name and the claim name. For details, see Monitoring device plugin\nresources."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0563", "text": "ResourceClaim device status\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)\n\nDRA drivers can report driver-specific device status data for each allocated device in the status.devices field of a ResourceClaim.\nFor example, the driver might list the IP addresses that are assigned to a network interface device.\nThe accuracy of the information that a driver adds to a ResourceClaim status.devices field depends on the driver. Evaluate drivers\nto decide whether you can rely on this field as the only source of device information.\nIf you disable the DRAResourceClaimDeviceStatus feature gate, the status.devices field automatically gets cleared when storing\nthe ResourceClaim. A ResourceClaim device status is supported when it is possible, from a DRA driver, to update an existing\nResourceClaim where the status.devices field is set.\nFor details about the status.devices field, see the ResourceClaim API reference.\n\nDevice Health Monitoring\nâ“˜ FEATURE STATE: Kubernetes v1.31 [alpha] (enabled by default: false)\n\nAs an alpha feature, Kubernetes provides a mechanism for monitoring and reporting the health of dynamically allocated\ninfrastructure resources. For stateful applications running on specialized hardware, it is critical to know when a device has failed or\nbecome unhealthy. It is also helpful to find out if the device recovers.\nTo enable this functionality, the ResourceHealthStatus feature gate must be enabled, and the DRA driver must implement the\nDRAResourceHealth gRPC service.\nWhen a DRA driver detects that an allocated device has become unhealthy, it reports this status back to the kubelet. This health\ninformation is then exposed directly in the Pod's status. The kubelet populates the allocatedResourcesStatus field in the status of\neach container, detailing the health of each device assigned to that container.\nThis provides crucial visibility for users and controllers to react to hardware failures. For a Pod that is failing, you can inspect this\nstatus to determine if the failure was related to an unhealthy device.\nhttps://kubernetes.io/docs/concepts/_print/\n\n540/684\n\n11/7/25, 4:37 PM\n\nPre-scheduled Pods\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0564", "text": "When you - or another API client - create a Pod with spec.nodeName already set, the scheduler gets bypassed. If some\nResourceClaim needed by that Pod does not exist yet, is not allocated or not reserved for the Pod, then the kubelet will fail to run\nthe Pod and re-check periodically because those requirements might still get fulfilled later.\nSuch a situation can also arise when support for dynamic resource allocation was not enabled in the scheduler at the time when the\nPod got scheduled (version skew, configuration, feature gate, etc.). kube-controller-manager detects this and tries to make the Pod\nrunnable by reserving the required ResourceClaims. However, this only works if those were allocated by the scheduler for some\nother pod.\nIt is better to avoid bypassing the scheduler because a Pod that is assigned to a node blocks normal resources (RAM, CPU) that then\ncannot be used for other Pods while the Pod is stuck. To make a Pod run on a specific node while still going through the normal\nscheduling flow, create the Pod with a node selector that exactly matches the desired node:\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: pod-with-cats\nspec:\nnodeSelector:\nkubernetes.io/hostname: name-of-the-intended-node\n...\n\nYou may also be able to mutate the incoming Pod, at admission time, to unset the .spec.nodeName field and to use a node selector\ninstead.\n\nDRA beta features\nThe following sections describe DRA features that are available in the Beta feature stage. For more information, see Set up DRA in\nthe cluster.\n\nAdmin access\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta] (enabled by default: true)\n\nYou can mark a request in a ResourceClaim or ResourceClaimTemplate as having privileged features for maintenance and\ntroubleshooting tasks. A request with admin access grants access to in-use devices and may enable additional permissions when\nmaking the device available in a container:\n\napiVersion: resource.k8s.io/v1\nkind: ResourceClaimTemplate\nmetadata:\nname: large-black-cat-claim-template\nspec:\nspec:\ndevices:\nrequests:\n- name: req-0\nexactly:\ndeviceClassName: resource.example.com\nallocationMode: All\nadminAccess: true\n\nIf this feature is disabled, the adminAccess field will be removed automatically when creating such a ResourceClaim.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n541/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0565", "text": "Admin access is a privileged mode and should not be granted to regular users in multi-tenant clusters. Starting with Kubernetes\nv1.33, only users authorized to create ResourceClaim or ResourceClaimTemplate objects in namespaces labeled with\nresource.k8s.io/admin-access: \"true\" (case-sensitive) can use the adminAccess field. This ensures that non-admin users cannot\nmisuse the feature. Starting with Kubernetes v1.34, this label has been updated to resource.kubernetes.io/admin-access: \"true\" .\n\nDRA alpha features\nThe following sections describe DRA features that are available in the Alpha feature stage. To use any of these features, you must\nalso set up DRA in your clusters by enabling the DynamicResourceAllocation feature gate and the DRA API groups. For more\ninformation, see Set up DRA in the cluster.\n\nExtended resource allocation by DRA\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)\n\nYou can provide an extended resource name for a DeviceClass. The scheduler will then select the devices matching the class for the\nextended resource requests. This allows users to continue using extended resource requests in a pod to request either extended\nresources provided by device plugin, or DRA devices. The same extended resource can be provided either by device plugin, or DRA\non one single cluster node. The same extended resource can be provided by device plugin on some nodes, and DRA on other nodes\nin the same cluster.\nIn the example below, the DeviceClass is given an extendedResourceName example.com/gpu . If a pod requested for the extended\nresource example.com/gpu: 2 , it can be scheduled to a node with two or more devices matching the DeviceClass.\n\napiVersion: resource.k8s.io/v1\nkind: DeviceClass\nmetadata:\nname: gpu.example.com\nspec:\nselectors:\n- cel:\nexpression: device.driver == 'gpu.example.com' && device.attributes['gpu.example.com'].type\n== 'gpu'\nextendedResourceName: example.com/gpu\n\nIn addition, users can use a special extended resource to allocate devices without having to explicitly create a ResourceClaim. Using\nthe extended resource name prefix deviceclass.resource.kubernetes.io/ and the DeviceClass name. This works for any\nDeviceClass, even if it does not specify the an extended resource name. The resulting ResourceClaim will contain a request for an\nExactCount of the specified number of devices of that DeviceClass.\nExtended resource allocation by DRA is an alpha feature and only enabled when the DRAExtendedResource feature gate is enabled in\nthe kube-apiserver, kube-scheduler, and kubelet.\n\nPartitionable devices\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0566", "text": "Devices represented in DRA don't necessarily have to be a single unit connected to a single machine, but can also be a logical device\ncomprised of multiple devices connected to multiple machines. These devices might consume overlapping resources of the\nunderlying phyical devices, meaning that when one logical device is allocated other devices will no longer be available.\nIn the ResourceSlice API, this is represented as a list of named CounterSets, each of which contains a set of named counters. The\ncounters represent the resources available on the physical device that are used by the logical devices advertised through DRA.\nLogical devices can specify the ConsumesCounters list. Each entry contains a reference to a CounterSet and a set of named counters\nwith the amounts they will consume. So for a device to be allocatable, the referenced counter sets must have sufficient quantity for\nthe counters referenced by the device.\nhttps://kubernetes.io/docs/concepts/_print/\n\n542/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nHere is an example of two devices, each consuming 6Gi of memory from the a shared counter with 8Gi of memory. Thus, only one of\nthe devices can be allocated at any point in time. The scheduler handles this and it is transparent to the consumer as the\nResourceClaim API is not affected.\n\nkind: ResourceSlice\napiVersion: resource.k8s.io/v1\nmetadata:\nname: resourceslice\nspec:\nnodeName: worker-1\npool:\nname: pool\ngeneration: 1\nresourceSliceCount: 1\ndriver: dra.example.com\nsharedCounters:\n- name: gpu-1-counters\ncounters:\nmemory:\nvalue: 8Gi\ndevices:\n- name: device-1\nconsumesCounters:\n- counterSet: gpu-1-counters\ncounters:\nmemory:\nvalue: 6Gi\n- name: device-2\nconsumesCounters:\n- counterSet: gpu-1-counters\ncounters:\nmemory:\nvalue: 6Gi\n\nPartitionable devices is an alpha feature and only enabled when the DRAPartitionableDevices feature gate is enabled in the kubeapiserver and kube-scheduler.\n\nConsumable capacity\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0567", "text": "The consumable capacity feature allows the same devices to be consumed by multiple independent ResourceClaims, with the\nKubernetes scheduler managing how much of the device's capacity is used up by each claim. This is analogous to how Pods can\nshare the resources on a Node; ResourceClaims can share the resources on a Device.\nThe device driver can set allowMultipleAllocations field added in .spec.devices of ResourceSlice to allow allocating that device\nto multiple independent ResourceClaims or to multiple requests within a ResourceClaim.\nUsers can set capacity field added in spec.devices.requests of ResourceClaim to specify the device resource requirements for\neach allocation.\nFor the device that allows multiple allocations, the requested capacity is drawn from â€” or consumed from â€” its total capacity, a\nconcept known as consumable capacity. Then, the scheduler ensures that the aggregate consumed capacity across all claims does\nnot exceed the deviceâ€™s overall capacity. Furthermore, driver authors can use the requestPolicy constraints on individual device\ncapacities to control how those capacities are consumed. For example, the driver author can specify that a given capacity is only\nconsumed in increments of 1Gi.\nHere is an example of a network device which allows multiple allocations and contains a consumable bandwidth capacity.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n543/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkind: ResourceSlice\napiVersion: resource.k8s.io/v1\nmetadata:\nname: resourceslice\nspec:\nnodeName: worker-1\npool:\nname: pool\ngeneration: 1\nresourceSliceCount: 1\ndriver: dra.example.com\ndevices:\n- name: eth1\nallowMultipleAllocations: true\nattributes:\nname:\nstring: \"eth1\"\ncapacity:\nbandwidth:\nrequestPolicy:\ndefault: \"1M\"\nvalidRange:\nmin: \"1M\"\nstep: \"8\"\nvalue: \"10G\"\n\nThe consumable capacity can be requested as shown in the below example.\n\napiVersion: resource.k8s.io/v1\nkind: ResourceClaimTemplate\nmetadata:\nname: bandwidth-claim-template\nspec:\nspec:\ndevices:\nrequests:\n- name: req-0\nexactly:\ndeviceClassName: resource.example.com\ncapacity:\nrequests:\nbandwidth: 1G\n\nThe allocation result will include the consumed capacity and the identifier of the share.\n\napiVersion: resource.k8s.io/v1\nkind: ResourceClaim\n...\nstatus:\nallocation:\ndevices:\nresults:\n- consumedCapacity:\nbandwidth: 1G\ndevice: eth1\nshareID: \"a671734a-e8e5-11e4-8fde-42010af09327\"\n\nIn this example, a multiply-allocatable device was chosen. However, any resource.example.com device with at least the requested\n1G bandwidth could have met the requirement. If a non-multiply-allocatable device were chosen, the allocation would have resulted\nin the entire device. To force the use of a only multiply-allocatable devices, you can use the CEL criteria\nhttps://kubernetes.io/docs/concepts/_print/\n\n544/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0568", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ndevice.allowMultipleAllocations == true .\n\nDevice taints and tolerations\nâ“˜ FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)\n\nDevice taints are similar to node taints: a taint has a string key, a string value, and an effect. The effect is applied to the\nResourceClaim which is using a tainted device and to all Pods referencing that ResourceClaim. The \"NoSchedule\" effect prevents\nscheduling those Pods. Tainted devices are ignored when trying to allocate a ResourceClaim because using them would prevent\nscheduling of Pods.\nThe \"NoExecute\" effect implies \"NoSchedule\" and in addition causes eviction of all Pods which have been scheduled already. This\neviction is implemented in the device taint eviction controller in kube-controller-manager by deleting affected Pods.\nResourceClaims can tolerate taints. If a taint is tolerated, its effect does not apply. An empty toleration matches all taints. A\ntoleration can be limited to certain effects and/or match certain key/value pairs. A toleration can check that a certain key exists,\nregardless which value it has, or it can check for specific values of a key. For more information on this matching see the node taint\nconcepts.\nEviction can be delayed by tolerating a taint for a certain duration. That delay starts at the time when a taint gets added to a device,\nwhich is recorded in a field of the taint.\nTaints apply as described above also to ResourceClaims allocating \"all\" devices on a node. All devices must be untainted or all of\ntheir taints must be tolerated. Allocating a device with admin access (described above) is not exempt either. An admin using that\nmode must explicitly tolerate all taints to access tainted devices.\nDevice taints and tolerations is an alpha feature and only enabled when the DRADeviceTaints feature gate is enabled in the kubeapiserver, kube-controller-manager and kube-scheduler. To use DeviceTaintRules, the resource.k8s.io/v1alpha3 API version must\nbe enabled.\nYou can add taints to devices in the following ways, by using the DeviceTaintRule API kind.\n\nTaints set by the driver\nA DRA driver can add taints to the device information that it publishes in ResourceSlices. Consult the documentation of a DRA driver\nto learn whether the driver uses taints and what their keys and values are."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0569", "text": "Taints set by an admin\nAn admin or a control plane component can taint devices without having to tell the DRA driver to include taints in its device\ninformation in ResourceSlices. They do that by creating DeviceTaintRules. Each DeviceTaintRule adds one taint to devices which\nmatch the device selector. Without such a selector, no devices are tainted. This makes it harder to accidentally evict all pods using\nResourceClaims when leaving out the selector by mistake.\nDevices can be selected by giving the name of a DeviceClass, driver, pool, and/or device. The DeviceClass selects all devices that are\nselected by the selectors in that DeviceClass. With just the driver name, an admin can taint all devices managed by that driver, for\nexample while doing some kind of maintenance of that driver across the entire cluster. Adding a pool name can limit the taint to a\nsingle node, if the driver manages node-local devices.\nFinally, adding the device name can select one specific device. The device name and pool name can also be used alone, if desired.\nFor example, drivers for node-local devices are encouraged to use the node name as their pool name. Then tainting with that pool\nname automatically taints all devices on a node.\nDrivers might use stable names like \"gpu-0\" that hide which specific device is currently assigned to that name. To support tainting a\nspecific hardware instance, CEL selectors can be used in a DeviceTaintRule to match a vendor-specific unique ID attribute, if the\ndriver supports one for its hardware.\nThe taint applies as long as the DeviceTaintRule exists. It can be modified and and removed at any time. Here is one example of a\nDeviceTaintRule for a fictional DRA driver:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n545/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: resource.k8s.io/v1alpha3\nkind: DeviceTaintRule\nmetadata:\nname: example\nspec:\n# The entire hardware installation for this\n# particular driver is broken.\n# Evict all pods and don't schedule new ones.\ndeviceSelector:\ndriver: dra.example.com\ntaint:\nkey: dra.example.com/unhealthy\nvalue: Broken\neffect: NoExecute\n\nDevice Binding Conditions\nâ“˜ FEATURE STATE: Kubernetes v1.34 [alpha] (enabled by default: false)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0570", "text": "Device Binding Conditions allow the Kubernetes scheduler to delay Pod binding until external resources, such as fabric-attached\nGPUs or reprogrammable FPGAs, are confirmed to be ready.\nThis waiting behavior is implemented in the PreBind phase of the scheduling framework. During this phase, the scheduler checks\nwhether all required device conditions are satisfied before proceeding with binding.\nThis improves scheduling reliability by avoiding premature binding and enables coordination with external device controllers.\nTo use this feature, device drivers (typically managed by driver owners) must publish the following fields in the Device section of a\nResourceSlice . Cluster administrators must enable the DRADeviceBindingConditions and DRAResourceClaimDeviceStatus feature\ngates for the scheduler to honor these fields.\nbindingConditions : A list of condition types that must be set to True in the status.conditions field of the associated\n\nResourceClaim before the Pod can be bound. These typically represent readiness signals such as \"DeviceAttached\" or\n\"DeviceInitialized\".\nbindingFailureConditions : A list of condition types that, if set to True in status.conditions field of the associated\nResourceClaim, indicate a failure state. If any of these conditions are True, the scheduler will abort binding and reschedule the\nPod.\nbindsToNode : if set to true , the scheduler records the selected node name in the status.allocation.nodeSelector field of\nthe ResourceClaim. This does not affect the Pod's spec.nodeSelector . Instead, it sets a node selector inside the\nResourceClaim, which external controllers can use to perform node-specific operations such as device attachment or\npreparation.\nAll condition types listed in bindingConditions and bindingFailureConditions are evaluated from the status.conditions field of the\nResourceClaim. External controllers are responsible for updating these conditions using standard Kubernetes condition semantics\n( type , status , reason , message , lastTransitionTime ).\nThe scheduler waits up to 600 seconds for all bindingConditions to become True . If the timeout is reached or any\nbindingFailureConditions are True , the scheduler clears the allocation and reschedules the Pod.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n546/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0571", "text": "apiVersion: resource.k8s.io/v1\nkind: ResourceSlice\nmetadata:\nname: gpu-slice\nspec:\ndriver: dra.example.com\nnodeSelector:\nnodeSelectorTerms:\n- matchExpressions:\n- key: accelerator-type\noperator: In\nvalues:\n- \"high-performance\"\npool:\nname: gpu-pool\ngeneration: 1\nresourceSliceCount: 1\ndevices:\n- name: gpu-1\nattributes:\nvendor:\nstring: \"example\"\nmodel:\nstring: \"example-gpu\"\nbindsToNode: true\nbindingConditions:\n- dra.example.com/is-prepared\nbindingFailureConditions:\n- dra.example.com/preparing-failed\n\nThis example ResourceSlice has the following properties:\nThe ResourceSlice targets nodes labeled with accelerator-type=high-performance , so that the scheduler uses only a specific\nset of eligible nodes.\nThe scheduler selects one node from the selected group (for example, node-3 ) and sets the status.allocation.nodeSelector\nfield in the ResourceClaim to that node name.\nThe dra.example.com/is-prepared binding condition indicates that the device gpu-1 must be prepared (the is-prepared\ncondition has a status of True ) before binding.\nIf the gpu-1 device preparation fails (the preparing-failed condition has a status of True ), the scheduler aborts binding.\nThe scheduler waits up to 600 seconds for the device to become ready.\nExternal controllers can use the node selector in the ResourceClaim to perform node-specific setup on the selected node.\n\nWhat's next\nSet Up DRA in a Cluster\nAllocate devices to workloads using DRA\nFor more information on the design, see the Dynamic Resource Allocation with Structured Parameters KEP.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n547/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.9 - Scheduler Performance Tuning\nâ“˜ FEATURE STATE: Kubernetes v1.14 [beta]\n\nkube-scheduler is the Kubernetes default scheduler. It is responsible for placement of Pods on Nodes in a cluster.\nNodes in a cluster that meet the scheduling requirements of a Pod are called feasible Nodes for the Pod. The scheduler finds feasible\nNodes for a Pod and then runs a set of functions to score the feasible Nodes, picking a Node with the highest score among the\nfeasible ones to run the Pod. The scheduler then notifies the API server about this decision in a process called Binding.\nThis page explains performance tuning optimizations that are relevant for large Kubernetes clusters.\nIn large clusters, you can tune the scheduler's behaviour balancing scheduling outcomes between latency (new Pods are placed\nquickly) and accuracy (the scheduler rarely makes poor placement decisions).\nYou configure this tuning setting via kube-scheduler setting percentageOfNodesToScore . This KubeSchedulerConfiguration setting\ndetermines a threshold for scheduling nodes in your cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0572", "text": "Setting the threshold\nThe percentageOfNodesToScore option accepts whole numeric values between 0 and 100. The value 0 is a special number which\nindicates that the kube-scheduler should use its compiled-in default. If you set percentageOfNodesToScore above 100, kubescheduler acts as if you had set a value of 100.\nTo change the value, edit the kube-scheduler configuration file and then restart the scheduler. In many cases, the configuration file\ncan be found at /etc/kubernetes/config/kube-scheduler.yaml .\nAfter you have made this change, you can run\n\nkubectl get pods -n kube-system | grep kube-scheduler\n\nto verify that the kube-scheduler component is healthy.\n\nNode scoring threshold\nTo improve scheduling performance, the kube-scheduler can stop looking for feasible nodes once it has found enough of them. In\nlarge clusters, this saves time compared to a naive approach that would consider every node.\nYou specify a threshold for how many nodes are enough, as a whole number percentage of all the nodes in your cluster. The kubescheduler converts this into an integer number of nodes. During scheduling, if the kube-scheduler has identified enough feasible\nnodes to exceed the configured percentage, the kube-scheduler stops searching for more feasible nodes and moves on to the\nscoring phase.\nHow the scheduler iterates over Nodes describes the process in detail.\n\nDefault threshold\nIf you don't specify a threshold, Kubernetes calculates a figure using a linear formula that yields 50% for a 100-node cluster and\nyields 10% for a 5000-node cluster. The lower bound for the automatic value is 5%.\nThis means that the kube-scheduler always scores at least 5% of your cluster no matter how large the cluster is, unless you have\nexplicitly set percentageOfNodesToScore to be smaller than 5.\nIf you want the scheduler to score all nodes in your cluster, set percentageOfNodesToScore to 100.\n\nExample\nBelow is an example configuration that sets percentageOfNodesToScore to 50%.\nhttps://kubernetes.io/docs/concepts/_print/\n\n548/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: kubescheduler.config.k8s.io/v1alpha1\nkind: KubeSchedulerConfiguration\nalgorithmSource:\nprovider: DefaultProvider\n...\npercentageOfNodesToScore: 50\n\nTuning percentageOfNodesToScore\nmust be a value between 1 and 100 with the default value being calculated based on the cluster size.\nThere is also a hardcoded minimum value of 100 nodes.\npercentageOfNodesToScore"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0573", "text": "Note:\nIn clusters with less than 100 feasible nodes, the scheduler still checks all the nodes because there are not enough feasible\nnodes to stop the scheduler's search early.\nIn a small cluster, if you set a low value for percentageOfNodesToScore , your change will have no or little effect, for a similar\nreason.\nIf your cluster has several hundred Nodes or fewer, leave this configuration option at its default value. Making changes is\nunlikely to improve the scheduler's performance significantly.\n\nAn important detail to consider when setting this value is that when a smaller number of nodes in a cluster are checked for\nfeasibility, some nodes are not sent to be scored for a given Pod. As a result, a Node which could possibly score a higher value for\nrunning the given Pod might not even be passed to the scoring phase. This would result in a less than ideal placement of the Pod.\nYou should avoid setting percentageOfNodesToScore very low so that kube-scheduler does not make frequent, poor Pod placement\ndecisions. Avoid setting the percentage to anything below 10%, unless the scheduler's throughput is critical for your application and\nthe score of nodes is not important. In other words, you prefer to run the Pod on any Node as long as it is feasible.\n\nHow the scheduler iterates over Nodes\nThis section is intended for those who want to understand the internal details of this feature.\nIn order to give all the Nodes in a cluster a fair chance of being considered for running Pods, the scheduler iterates over the nodes in\na round robin fashion. You can imagine that Nodes are in an array. The scheduler starts from the start of the array and checks\nfeasibility of the nodes until it finds enough Nodes as specified by percentageOfNodesToScore . For the next Pod, the scheduler\ncontinues from the point in the Node array that it stopped at when checking feasibility of Nodes for the previous Pod.\nIf Nodes are in multiple zones, the scheduler iterates over Nodes in various zones to ensure that Nodes from different zones are\nconsidered in the feasibility checks. As an example, consider six nodes in two zones:\nZone 1: Node 1, Node 2, Node 3, Node 4\nZone 2: Node 5, Node 6"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0574", "text": "The Scheduler evaluates feasibility of the nodes in this order:\nNode 1, Node 5, Node 2, Node 6, Node 3, Node 4\n\nAfter going over all the Nodes, it goes back to Node 1.\n\nWhat's next\nCheck the kube-scheduler configuration reference (v1)\nhttps://kubernetes.io/docs/concepts/_print/\n\n549/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.10 - Resource Bin Packing\n\nIn the scheduling-plugin NodeResourcesFit of kube-scheduler, there are two scoring strategies that support the bin packing of\nresources: MostAllocated and RequestedToCapacityRatio .\n\nEnabling bin packing using MostAllocated strategy\nThe MostAllocated strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation. For\neach resource type, you can set a weight to modify its influence in the node score.\nTo set the MostAllocated strategy for the NodeResourcesFit plugin, use a scheduler configuration similar to the following:\n\napiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n- args:\nscoringStrategy:\nresources:\n- name: cpu\nweight: 1\n- name: memory\nweight: 1\n- name: intel.com/foo\nweight: 3\n- name: intel.com/bar\nweight: 3\ntype: MostAllocated\nname: NodeResourcesFit\n\nTo learn more about other parameters and their default configuration, see the API documentation for NodeResourcesFitArgs .\n\nEnabling bin packing using RequestedToCapacityRatio\nThe RequestedToCapacityRatio strategy allows the users to specify the resources along with weights for each resource to score\nnodes based on the request to capacity ratio. This allows users to bin pack extended resources by using appropriate parameters to\nimprove the utilization of scarce resources in large clusters. It favors nodes according to a configured function of the allocated\nresources. The behavior of the RequestedToCapacityRatio in the NodeResourcesFit score function can be controlled by the\nscoringStrategy field. Within the scoringStrategy field, you can configure two parameters: requestedToCapacityRatio and\nresources . The shape in the requestedToCapacityRatio parameter allows the user to tune the function as least requested or\nmost requested based on utilization and score values. The resources parameter comprises both the name of the resource to\nbe considered during scoring and its corresponding weight , which specifies the weight of each resource.\nBelow is an example configuration that sets the bin packing behavior for extended resources intel.com/foo and intel.com/bar\nusing the requestedToCapacityRatio field.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n550/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0575", "text": "apiVersion: kubescheduler.config.k8s.io/v1\nkind: KubeSchedulerConfiguration\nprofiles:\n- pluginConfig:\n- args:\nscoringStrategy:\nresources:\n- name: intel.com/foo\nweight: 3\n- name: intel.com/bar\nweight: 3\nrequestedToCapacityRatio:\nshape:\n- utilization: 0\nscore: 0\n- utilization: 100\nscore: 10\ntype: RequestedToCapacityRatio\nname: NodeResourcesFit\n\nReferencing the KubeSchedulerConfiguration file with the kube-scheduler flag --config=/path/to/config/file will pass the\nconfiguration to the scheduler.\nTo learn more about other parameters and their default configuration, see the API documentation for NodeResourcesFitArgs .\n\nTuning the score function\nshape\n\nis used to specify the behavior of the RequestedToCapacityRatio function.\n\nshape:\n- utilization: 0\nscore: 0\n- utilization: 100\nscore: 10\n\nThe above arguments give the node a score of 0 if utilization is 0% and 10 for utilization 100%, thus enabling bin packing\nbehavior. To enable least requested the score value must be reversed as follows.\n\nshape:\n- utilization: 0\nscore: 10\n- utilization: 100\nscore: 0\n\nresources\n\nis an optional parameter which defaults to:\n\nresources:\n- name: cpu\nweight: 1\n- name: memory\nweight: 1\n\nIt can be used to add extended resources as follows:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n551/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nresources:\n- name: intel.com/foo\nweight: 5\n- name: cpu\nweight: 3\n- name: memory\nweight: 1\n\nThe weight parameter is optional and is set to 1 if not specified. Also, the weight cannot be set to a negative value.\n\nNode scoring for capacity allocation\nThis section is intended for those who want to understand the internal details of this feature. Below is an example of how the node\nscore is calculated for a given set of values.\nRequested resources:\nintel.com/foo : 2\nmemory: 256MB\ncpu: 2\n\nResource weights:\nintel.com/foo : 5\nmemory: 1\ncpu: 3\n\nFunctionShapePoint {{0, 0}, {100, 10}}\nNode 1 spec:\nAvailable:\nintel.com/foo: 4\nmemory: 1 GB\ncpu: 8\nUsed:\nintel.com/foo: 1\nmemory: 256MB\ncpu: 1\n\nNode score:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n552/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nintel.com/foo\n\n= resourceScoringFunction((2+1),4)\n= (100 - ((4-3)*100/4))\n= (100 - 25)\n= 75\n\n# requested + used = 75% * available\n\n= rawScoringFunction(75)\n= 7\nmemory\n\n# floor(75/10)\n\n= resourceScoringFunction((256+256),1024)\n= (100 -((1024-512)*100/1024))\n# requested + used = 50% * available\n\n= 5\n\n# floor(50/10)\n\ncpu\n\n= 50\n= rawScoringFunction(50)\n\n= resourceScoringFunction((2+1),8)\n= (100 -((8-3)*100/8))\n= 37.5\n\n# requested + used = 37.5% * available\n\n= rawScoringFunction(37.5)\n= 3\n# floor(37.5/10)\nNodeScore\n\n=\n\n((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3)\n\n="}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0576", "text": "5\n\nNode 2 spec:\nAvailable:\nintel.com/foo: 8\nmemory: 1GB\ncpu: 8\nUsed:\nintel.com/foo: 2\nmemory: 512MB\ncpu: 6\n\nNode score:\nintel.com/foo\n\n= resourceScoringFunction((2+2),8)\n=\n\n(100 - ((8-4)*100/8)\n\n=\n\n(100 - 50)\n\n=\n\n50\n\n=\n\nrawScoringFunction(50)\n\n= 5\nmemory\n\n= resourceScoringFunction((256+512),1024)\n= (100 -((1024-768)*100/1024))\n= 75\n= rawScoringFunction(75)\n= 7\n\ncpu\n\n= resourceScoringFunction((2+6),8)\n= (100 -((8-8)*100/8))\n= 100\n= rawScoringFunction(100)\n= 10\n\nNodeScore\n\n=\n=\n\n((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3)\n7\n\nWhat's next\nRead more about the scheduling framework\nRead more about scheduler configuration\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n553/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.11 - Pod Priority and Preemption\nâ“˜ FEATURE STATE: Kubernetes v1.14 [stable]\n\nPods can have priority. Priority indicates the importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the scheduler\ntries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible.\n\nWarning:\nIn a cluster where not all users are trusted, a malicious user could create Pods at the highest possible priorities, causing other\nPods to be evicted/not get scheduled. An administrator can use ResourceQuota to prevent users from creating pods at high\npriorities.\nSee limit Priority Class consumption by default for details.\n\nHow to use priority and preemption\nTo use priority and preemption:\n1. Add one or more PriorityClasses.\n2. Create Pods with priorityClassName set to one of the added PriorityClasses. Of course you do not need to create the Pods\ndirectly; normally you would add priorityClassName to the Pod template of a collection object like a Deployment.\nKeep reading for more information about these steps.\nNote:\nKubernetes already ships with two PriorityClasses: system-cluster-critical and system-node-critical. These are common\nclasses and are used to ensure that critical components are always scheduled first."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0577", "text": "PriorityClass\nA PriorityClass is a non-namespaced object that defines a mapping from a priority class name to the integer value of the priority. The\nname is specified in the name field of the PriorityClass object's metadata. The value is specified in the required value field. The\nhigher the value, the higher the priority. The name of a PriorityClass object must be a valid DNS subdomain name, and it cannot be\nprefixed with system- .\nA PriorityClass object can have any 32-bit integer value smaller than or equal to 1 billion. This means that the range of values for a\nPriorityClass object is from -2147483648 to 1000000000 inclusive. Larger numbers are reserved for built-in PriorityClasses that\nrepresent critical system Pods. A cluster admin should create one PriorityClass object for each such mapping that they want.\nPriorityClass also has two optional fields: globalDefault and description . The globalDefault field indicates that the value of this\nPriorityClass should be used for Pods without a priorityClassName . Only one PriorityClass with globalDefault set to true can exist\nin the system. If there is no PriorityClass with globalDefault set, the priority of Pods with no priorityClassName is zero.\nThe description field is an arbitrary string. It is meant to tell users of the cluster when they should use this PriorityClass.\n\nNotes about PodPriority and existing clusters\nIf you upgrade an existing cluster without this feature, the priority of your existing Pods is effectively zero.\nAddition of a PriorityClass with globalDefault set to true does not change the priorities of existing Pods. The value of such a\nPriorityClass is used only for Pods created after the PriorityClass is added.\nIf you delete a PriorityClass, existing Pods that use the name of the deleted PriorityClass remain unchanged, but you cannot\ncreate more Pods that use the name of the deleted PriorityClass.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n554/684\n\n11/7/25, 4:37 PM\n\nExample PriorityClass\n\nConcepts | Kubernetes\n\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\nname: high-priority\nvalue: 1000000\nglobalDefault: false\ndescription: \"This priority class should be used for XYZ service pods only.\"\n\nNon-preempting PriorityClass\nâ“˜ FEATURE STATE: Kubernetes v1.24 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0578", "text": "Pods with preemptionPolicy: Never will be placed in the scheduling queue ahead of lower-priority pods, but they cannot preempt\nother pods. A non-preempting pod waiting to be scheduled will stay in the scheduling queue, until sufficient resources are free, and\nit can be scheduled. Non-preempting pods, like other pods, are subject to scheduler back-off. This means that if the scheduler tries\nthese pods and they cannot be scheduled, they will be retried with lower frequency, allowing other pods with lower priority to be\nscheduled before them.\nNon-preempting pods may still be preempted by other, high-priority pods.\ndefaults to PreemptLowerPriority , which will allow pods of that PriorityClass to preempt lower-priority pods (as\nis existing default behavior). If preemptionPolicy is set to Never , pods in that PriorityClass will be non-preempting.\npreemptionPolicy\n\nAn example use case is for data science workloads. A user may submit a job that they want to be prioritized above other workloads,\nbut do not wish to discard existing work by preempting running pods. The high priority job with preemptionPolicy: Never will be\nscheduled ahead of other queued pods, as soon as sufficient cluster resources \"naturally\" become free.\n\nExample Non-preempting PriorityClass\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\nname: high-priority-nonpreempting\nvalue: 1000000\npreemptionPolicy: Never\nglobalDefault: false\ndescription: \"This priority class will not cause other pods to be preempted.\"\n\nPod priority\nAfter you have one or more PriorityClasses, you can create Pods that specify one of those PriorityClass names in their specifications.\nThe priority admission controller uses the priorityClassName field and populates the integer value of the priority. If the priority\nclass is not found, the Pod is rejected.\nThe following YAML is an example of a Pod configuration that uses the PriorityClass created in the preceding example. The priority\nadmission controller checks the specification and resolves the priority of the Pod to 1000000.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n555/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nlabels:\nenv: test\nspec:\ncontainers:\n- name: nginx\nimage: nginx\nimagePullPolicy: IfNotPresent\npriorityClassName: high-priority"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0579", "text": "Effect of Pod priority on scheduling order\nWhen Pod priority is enabled, the scheduler orders pending Pods by their priority and a pending Pod is placed ahead of other\npending Pods with lower priority in the scheduling queue. As a result, the higher priority Pod may be scheduled sooner than Pods\nwith lower priority if its scheduling requirements are met. If such Pod cannot be scheduled, the scheduler will continue and try to\nschedule other lower priority Pods.\n\nPreemption\nWhen Pods are created, they go to a queue and wait to be scheduled. The scheduler picks a Pod from the queue and tries to\nschedule it on a Node. If no Node is found that satisfies all the specified requirements of the Pod, preemption logic is triggered for\nthe pending Pod. Let's call the pending Pod P. Preemption logic tries to find a Node where removal of one or more Pods with lower\npriority than P would enable P to be scheduled on that Node. If such a Node is found, one or more lower priority Pods get evicted\nfrom the Node. After the Pods are gone, P can be scheduled on the Node.\n\nUser exposed information\nWhen Pod P preempts one or more Pods on Node N, nominatedNodeName field of Pod P's status is set to the name of Node N. This\nfield helps the scheduler track resources reserved for Pod P and also gives users information about preemptions in their clusters.\nPlease note that Pod P is not necessarily scheduled to the \"nominated Node\". The scheduler always tries the \"nominated Node\"\nbefore iterating over any other nodes. After victim Pods are preempted, they get their graceful termination period. If another node\nbecomes available while scheduler is waiting for the victim Pods to terminate, scheduler may use the other node to schedule Pod P.\nAs a result nominatedNodeName and nodeName of Pod spec are not always the same. Also, if the scheduler preempts Pods on Node\nN, but then a higher priority Pod than Pod P arrives, the scheduler may give Node N to the new higher priority Pod. In such a case,\nscheduler clears nominatedNodeName of Pod P. By doing this, scheduler makes Pod P eligible to preempt Pods on another Node."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0580", "text": "Limitations of preemption\nGraceful termination of preemption victims\nWhen Pods are preempted, the victims get their graceful termination period. They have that much time to finish their work and exit.\nIf they don't, they are killed. This graceful termination period creates a time gap between the point that the scheduler preempts\nPods and the time when the pending Pod (P) can be scheduled on the Node (N). In the meantime, the scheduler keeps scheduling\nother pending Pods. As victims exit or get terminated, the scheduler tries to schedule Pods in the pending queue. Therefore, there is\nusually a time gap between the point that scheduler preempts victims and the time that Pod P is scheduled. In order to minimize this\ngap, one can set graceful termination period of lower priority Pods to zero or a small number.\n\nPodDisruptionBudget is supported, but not guaranteed\nA PodDisruptionBudget (PDB) allows application owners to limit the number of Pods of a replicated application that are down\nsimultaneously from voluntary disruptions. Kubernetes supports PDB when preempting Pods, but respecting PDB is best effort. The\nscheduler tries to find victims whose PDB are not violated by preemption, but if no such victims are found, preemption will still\nhappen, and lower priority Pods will be removed despite their PDBs being violated.\n\nInter-Pod affinity on lower-priority Pods\nhttps://kubernetes.io/docs/concepts/_print/\n\n556/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0581", "text": "A Node is considered for preemption only when the answer to this question is yes: \"If all the Pods with lower priority than the\npending Pod are removed from the Node, can the pending Pod be scheduled on the Node?\"\nNote:\nPreemption does not necessarily remove all lower-priority Pods. If the pending Pod can be scheduled by removing fewer than\nall lower-priority Pods, then only a portion of the lower-priority Pods are removed. Even so, the answer to the preceding\nquestion must be yes. If the answer is no, the Node is not considered for preemption.\nIf a pending Pod has inter-pod affinity to one or more of the lower-priority Pods on the Node, the inter-Pod affinity rule cannot be\nsatisfied in the absence of those lower-priority Pods. In this case, the scheduler does not preempt any Pods on the Node. Instead, it\nlooks for another Node. The scheduler might find a suitable Node or it might not. There is no guarantee that the pending Pod can be\nscheduled.\nOur recommended solution for this problem is to create inter-Pod affinity only towards equal or higher priority Pods.\n\nCross node preemption\nSuppose a Node N is being considered for preemption so that a pending Pod P can be scheduled on N. P might become feasible on\nN only if a Pod on another Node is preempted. Here's an example:\nPod P is being considered for Node N.\nPod Q is running on another Node in the same Zone as Node N.\nPod P has Zone-wide anti-affinity with Pod Q ( topologyKey: topology.kubernetes.io/zone ).\nThere are no other cases of anti-affinity between Pod P and other Pods in the Zone.\nIn order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler does not perform cross-node preemption. So,\nPod P will be deemed unschedulable on Node N.\nIf Pod Q were removed from its Node, the Pod anti-affinity violation would be gone, and Pod P could possibly be scheduled on Node\nN.\nWe may consider adding cross Node preemption in future versions if there is enough demand and if we find an algorithm with\nreasonable performance.\n\nTroubleshooting\nPod priority and preemption can have unwanted side effects. Here are some examples of potential problems and ways to deal with\nthem."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0582", "text": "Pods are preempted unnecessarily\nPreemption removes existing Pods from a cluster under resource pressure to make room for higher priority pending Pods. If you\ngive high priorities to certain Pods by mistake, these unintentionally high priority Pods may cause preemption in your cluster. Pod\npriority is specified by setting the priorityClassName field in the Pod's specification. The integer value for priority is then resolved\nand populated to the priority field of podSpec .\nTo address the problem, you can change the priorityClassName for those Pods to use lower priority classes, or leave that field\nempty. An empty priorityClassName is resolved to zero by default.\nWhen a Pod is preempted, there will be events recorded for the preempted Pod. Preemption should happen only when a cluster\ndoes not have enough resources for a Pod. In such cases, preemption happens only when the priority of the pending Pod\n(preemptor) is higher than the victim Pods. Preemption must not happen when there is no pending Pod, or when the pending Pods\nhave equal or lower priority than the victims. If preemption happens in such scenarios, please file an issue.\n\nPods are preempted, but the preemptor is not scheduled\nWhen pods are preempted, they receive their requested graceful termination period, which is by default 30 seconds. If the victim\nPods do not terminate within this period, they are forcibly terminated. Once all the victims go away, the preemptor Pod can be\nscheduled.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n557/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhile the preemptor Pod is waiting for the victims to go away, a higher priority Pod may be created that fits on the same Node. In\nthis case, the scheduler will schedule the higher priority Pod instead of the preemptor.\nThis is expected behavior: the Pod with the higher priority should take the place of a Pod with a lower priority."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0583", "text": "Higher priority Pods are preempted before lower priority pods\nThe scheduler tries to find nodes that can run a pending Pod. If no node is found, the scheduler tries to remove Pods with lower\npriority from an arbitrary node in order to make room for the pending pod. If a node with low priority Pods is not feasible to run the\npending Pod, the scheduler may choose another node with higher priority Pods (compared to the Pods on the other node) for\npreemption. The victims must still have lower priority than the preemptor Pod.\nWhen there are multiple nodes available for preemption, the scheduler tries to choose the node with a set of Pods with lowest\npriority. However, if such Pods have PodDisruptionBudget that would be violated if they are preempted then the scheduler may\nchoose another node with higher priority Pods.\nWhen multiple nodes exist for preemption and none of the above scenarios apply, the scheduler chooses a node with the lowest\npriority.\n\nInteractions between Pod priority and quality of service\nPod priority and QoS class are two orthogonal features with few interactions and no default restrictions on setting the priority of a\nPod based on its QoS classes. The scheduler's preemption logic does not consider QoS when choosing preemption targets.\nPreemption considers Pod priority and attempts to choose a set of targets with the lowest priority. Higher-priority Pods are\nconsidered for preemption only if the removal of the lowest priority Pods is not sufficient to allow the scheduler to schedule the\npreemptor Pod, or if the lowest priority Pods are protected by PodDisruptionBudget .\nThe kubelet uses Priority to determine pod order for node-pressure eviction. You can use the QoS class to estimate the order in\nwhich pods are most likely to get evicted. The kubelet ranks pods for eviction based on the following factors:\n1. Whether the starved resource usage exceeds requests\n2. Pod Priority\n3. Amount of resource usage relative to requests\nSee Pod selection for kubelet eviction for more details.\nkubelet node-pressure eviction does not evict Pods when their usage does not exceed their requests. If a Pod with lower priority is\nnot exceeding its requests, it won't be evicted. Another Pod with higher priority that exceeds its requests may be evicted."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0584", "text": "What's next\nRead about using ResourceQuotas in connection with PriorityClasses: limit Priority Class consumption by default\nLearn about Pod Disruption\nLearn about API-initiated Eviction\nLearn about Node-pressure Eviction\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n558/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.12 - Node-pressure Eviction\n\nNode-pressure eviction is the process by which the kubelet proactively terminates pods to reclaim resource on nodes.\nThe kubelet monitors resources like memory, disk space, and filesystem inodes on your cluster's nodes. When one or more of these\nresources reach specific consumption levels, the kubelet can proactively fail one or more pods on the node to reclaim resources and\nprevent starvation.\nDuring a node-pressure eviction, the kubelet sets the phase for the selected pods to Failed , and terminates the Pod.\nNode-pressure eviction is not the same as API-initiated eviction.\nThe kubelet does not respect your configured PodDisruptionBudget or the pod's terminationGracePeriodSeconds . If you use soft\neviction thresholds, the kubelet respects your configured eviction-max-pod-grace-period . If you use hard eviction thresholds, the\nkubelet uses a 0s grace period (immediate shutdown) for termination.\n\nSelf healing behavior\nThe kubelet attempts to reclaim node-level resources before it terminates end-user pods. For example, it removes unused container\nimages when disk resources are starved.\nIf the pods are managed by a workload management object (such as StatefulSet or Deployment) that replaces failed pods, the\ncontrol plane ( kube-controller-manager ) creates new pods in place of the evicted pods.\n\nSelf healing for static pods\nIf you are running a static pod on a node that is under resource pressure, the kubelet may evict that static Pod. The kubelet then\ntries to create a replacement, because static Pods always represent an intent to run a Pod on that node.\nThe kubelet takes the priority of the static pod into account when creating a replacement. If the static pod manifest specifies a low\npriority, and there are higher-priority Pods defined within the cluster's control plane, and the node is under resource pressure, the\nkubelet may not be able to make room for that static pod. The kubelet continues to attempt to run all static pods even when there is\nresource pressure on a node.\n\nEviction signals and thresholds\nThe kubelet uses various parameters to make eviction decisions, like the following:\nEviction signals\nEviction thresholds\nMonitoring intervals"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0585", "text": "Eviction signals\nEviction signals are the current state of a particular resource at a specific point in time. The kubelet uses eviction signals to make\neviction decisions by comparing the signals to eviction thresholds, which are the minimum amount of the resource that should be\navailable on the node.\nThe kubelet uses the following eviction signals:\n\nEviction Signal\n\nDescription\n\nmemory.available\n\nmemory.available := node.status.capacity[memory] -\n\nLinux\nOnly\n\nnode.stats.memory.workingSet\nnodefs.available\n\nnodefs.available := node.stats.fs.available\n\nnodefs.inodesFree\n\nnodefs.inodesFree := node.stats.fs.inodesFree\n\nhttps://kubernetes.io/docs/concepts/_print/\n\nâ€¢\n559/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nEviction Signal\n\nDescription\n\nimagefs.available\n\nimagefs.available := node.stats.runtime.imagefs.available\n\nimagefs.inodesFree\n\nimagefs.inodesFree := node.stats.runtime.imagefs.inodesFree\n\ncontainerfs.available\n\ncontainerfs.available :=\n\nLinux\nOnly\n\nâ€¢\n\nnode.stats.runtime.containerfs.available\ncontainerfs.inodesFree\n\ncontainerfs.inodesFree :=\n\nâ€¢\n\nnode.stats.runtime.containerfs.inodesFree\npid.available\n\npid.available := node.stats.rlimit.maxpid -\n\nâ€¢\n\nnode.stats.rlimit.curproc\n\nIn this table, the Description column shows how kubelet gets the value of the signal. Each signal supports either a percentage or a\nliteral value. The kubelet calculates the percentage value relative to the total capacity associated with the signal.\n\nMemory signals\nOn Linux nodes, the value for memory.available is derived from the cgroupfs instead of tools like free -m . This is important\nbecause free -m does not work in a container, and if users use the node allocatable feature, out of resource decisions are made\nlocal to the end user Pod part of the cgroup hierarchy as well as the root node. This script or cgroupv2 script reproduces the same\nset of steps that the kubelet performs to calculate memory.available . The kubelet excludes inactive_file (the number of bytes of filebacked memory on the inactive LRU list) from its calculation, as it assumes that memory is reclaimable under pressure.\nOn Windows nodes, the value for memory.available is derived from the node's global memory commit levels (queried through the\nGetPerformanceInfo() system call) by subtracting the node's global CommitTotal from the node's CommitLimit . Please note that\nCommitLimit can change if the node's page-file size changes!"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0586", "text": "Filesystem signals\nThe kubelet recognizes three specific filesystem identifiers that can be used with eviction signals ( <identifier>.inodesFree or\n<identifier>.available ):\n1. nodefs : The node's main filesystem, used for local disk volumes, emptyDir volumes not backed by memory, log storage,\nephemeral storage, and more. For example, nodefs contains /var/lib/kubelet .\n2. imagefs : An optional filesystem that container runtimes can use to store container images (which are the read-only layers)\nand container writable layers.\n3. containerfs : An optional filesystem that container runtime can use to store the writeable layers. Similar to the main\nfilesystem (see nodefs ), it's used to store local disk volumes, emptyDir volumes not backed by memory, log storage, and\nephemeral storage, except for the container images. When containerfs is used, the imagefs filesystem can be split to only\nstore images (read-only layers) and nothing else.\nNote:\nâ“˜ FEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: true)\n\nThe split image filesystem feature, which enables support for the containerfs filesystem, adds several new eviction signals,\nthresholds and metrics. To use containerfs , the Kubernetes release v1.34 requires the KubeletSeparateDiskGC feature gate\nto be enabled. Currently, only CRI-O (v1.29 or higher) offers the containerfs filesystem support.\n\nAs such, kubelet generally allows three options for container filesystems:\nEverything is on the single nodefs , also referred to as \"rootfs\" or simply \"root\", and there is no dedicated image filesystem.\nhttps://kubernetes.io/docs/concepts/_print/\n\n560/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nContainer storage (see nodefs ) is on a dedicated disk, and imagefs (writable and read-only layers) is separate from the root\nfilesystem. This is often referred to as \"split disk\" (or \"separate disk\") filesystem.\nContainer filesystem containerfs (same as nodefs plus writable layers) is on root and the container images (read-only layers)\nare stored on separate imagefs . This is often referred to as \"split image\" filesystem.\nThe kubelet will attempt to auto-discover these filesystems with their current configuration directly from the underlying container\nruntime and will ignore other local node filesystems.\nThe kubelet does not support other container filesystems or storage configurations, and it does not currently support multiple\nfilesystems for images and containers.\n\nDeprecated kubelet garbage collection features\nSome kubelet garbage collection features are deprecated in favor of eviction:\nExisting Flag\n\nRationale\n\n--maximum-dead-containers\n\ndeprecated once old logs are stored outside of container's context\n\n--maximum-dead-containers-per-container\n\ndeprecated once old logs are stored outside of container's context"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0587", "text": "--minimum-container-ttl-duration\n\ndeprecated once old logs are stored outside of container's context\n\nEviction thresholds\nYou can specify custom eviction thresholds for the kubelet to use when it makes eviction decisions. You can configure soft and hard\neviction thresholds.\nEviction thresholds have the form [eviction-signal][operator][quantity] , where:\neviction-signal\noperator\n\nis the eviction signal to use.\n\nis the relational operator you want, such as < (less than).\n\nis the eviction threshold amount, such as 1Gi . The value of quantity must match the quantity representation used\nby Kubernetes. You can use either literal values or percentages ( % ).\nquantity\n\nFor example, if a node has 10GiB of total memory and you want trigger eviction if the available memory falls below 1GiB, you can\ndefine the eviction threshold as either memory.available<10% or memory.available<1Gi (you cannot use both).\n\nSoft eviction thresholds\nA soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period. The kubelet does not evict\npods until the grace period is exceeded. The kubelet returns an error on startup if you do not specify a grace period.\nYou can specify both a soft eviction threshold grace period and a maximum allowed pod termination grace period for kubelet to use\nduring evictions. If you specify a maximum allowed grace period and the soft eviction threshold is met, the kubelet uses the lesser of\nthe two grace periods. If you do not specify a maximum allowed grace period, the kubelet kills evicted pods immediately without\ngraceful termination.\nYou can use the following flags to configure soft eviction thresholds:\neviction-soft : A set of eviction thresholds like memory.available<1.5Gi\n\nthat can trigger pod eviction if held over the\n\nspecified grace period.\neviction-soft-grace-period : A set of eviction grace periods like memory.available=1m30s\n\nthat define how long a soft eviction\n\nthreshold must hold before triggering a Pod eviction.\neviction-max-pod-grace-period : The maximum allowed grace period (in seconds) to use when terminating pods in response\nto a soft eviction threshold being met.\n\nHard eviction thresholds\nA hard eviction threshold has no grace period. When a hard eviction threshold is met, the kubelet kills pods immediately without\ngraceful termination to reclaim the starved resource.\nhttps://kubernetes.io/docs/concepts/_print/\n\n561/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can use the eviction-hard flag to configure a set of hard eviction thresholds like memory.available<1Gi .\nThe kubelet has the following default hard eviction thresholds:\nmemory.available<100Mi\n\n(Linux nodes)\n\nmemory.available<500Mi\n\n(Windows nodes)"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0588", "text": "nodefs.available<10%\nimagefs.available<15%\nnodefs.inodesFree<5%\n\n(Linux nodes)\n\nimagefs.inodesFree<5%\n\n(Linux nodes)\n\nThese default values of hard eviction thresholds will only be set if none of the parameters is changed. If you change the value of any\nparameter, then the values of other parameters will not be inherited as the default values and will be set to zero. In order to provide\ncustom values, you should provide all the thresholds respectively. You can also set the kubelet config MergeDefaultEvictionSettings\nto true in the kubelet configuration file. If set to true and any parameter is changed, then the other parameters will inherit their\ndefault values instead of 0.\nThe containerfs.available and containerfs.inodesFree (Linux nodes) default eviction thresholds will be set as follows:\nIf a single filesystem is used for everything, then containerfs thresholds are set the same as nodefs .\nIf separate filesystems are configured for both images and containers, then containerfs thresholds are set the same as\nimagefs .\nSetting custom overrides for thresholds related to containersfs is currently not supported, and a warning will be issued if an\nattempt to do so is made; any provided custom values will, as such, be ignored.\n\nEviction monitoring interval\nThe kubelet evaluates eviction thresholds based on its configured housekeeping-interval , which defaults to 10s .\n\nNode conditions\nThe kubelet reports node conditions to reflect that the node is under pressure because hard or soft eviction threshold is met,\nindependent of configured grace periods.\nThe kubelet maps eviction signals to node conditions as follows:\nNode Condition\n\nEviction Signal\n\nDescription\n\nMemoryPressure\n\nmemory.available\n\nAvailable memory on the node has\nsatisfied an eviction threshold\n\nDiskPressure\n\nnodefs.available , nodefs.inodesFree ,\n\nAvailable disk space and inodes on either\nthe node's root filesystem, image\nfilesystem, or container filesystem has\nsatisfied an eviction threshold\n\nimagefs.available , imagefs.inodesFree ,\ncontainerfs.available , or\ncontainerfs.inodesFree\nPIDPressure\n\npid.available\n\nAvailable processes identifiers on the\n(Linux) node has fallen below an eviction\nthreshold\n\nThe control plane also maps these node conditions to taints.\nThe kubelet updates the node conditions based on the configured --node-status-update-frequency , which defaults to 10s .\n\nNode condition oscillation\nIn some cases, nodes oscillate above and below soft eviction thresholds without holding for the defined grace periods. This causes\nthe reported node condition to constantly switch between true and false , leading to bad eviction decisions.\nhttps://kubernetes.io/docs/concepts/_print/\n\n562/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0589", "text": "To protect against oscillation, you can use the eviction-pressure-transition-period flag, which controls how long the kubelet\nmust wait before transitioning a node condition to a different state. The transition period has a default value of 5m .\n\nReclaiming node level resources\nThe kubelet tries to reclaim node-level resources before it evicts end-user pods.\nWhen a DiskPressure node condition is reported, the kubelet reclaims node-level resources based on the filesystems on the node.\n\nWithout imagefs or containerfs\nIf the node only has a nodefs filesystem that meets eviction thresholds, the kubelet frees up disk space in the following order:\n1. Garbage collect dead pods and containers.\n2. Delete unused images.\n\nWith imagefs\nIf the node has a dedicated imagefs filesystem for container runtimes to use, the kubelet does the following:\nIf the nodefs filesystem meets the eviction thresholds, the kubelet garbage collects dead pods and containers.\nIf the imagefs filesystem meets the eviction thresholds, the kubelet deletes all unused images.\n\nWith imagefs and containerfs\nIf the node has a dedicated containerfs alongside the imagefs filesystem configured for the container runtimes to use, then\nkubelet will attempt to reclaim resources as follows:\nIf the containerfs filesystem meets the eviction thresholds, the kubelet garbage collects dead pods and containers.\nIf the imagefs filesystem meets the eviction thresholds, the kubelet deletes all unused images."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0590", "text": "Pod selection for kubelet eviction\nIf the kubelet's attempts to reclaim node-level resources don't bring the eviction signal below the threshold, the kubelet begins to\nevict end-user pods.\nThe kubelet uses the following parameters to determine the pod eviction order:\n1. Whether the pod's resource usage exceeds requests\n2. Pod Priority\n3. The pod's resource usage relative to requests\nAs a result, kubelet ranks and evicts pods in the following order:\n1. BestEffort or Burstable pods where the usage exceeds requests. These pods are evicted based on their Priority and then by\nhow much their usage level exceeds the request.\n2. Guaranteed pods and Burstable pods where the usage is less than requests are evicted last, based on their Priority.\nNote:\nThe kubelet does not use the pod's QoS class to determine the eviction order. You can use the QoS class to estimate the most\nlikely pod eviction order when reclaiming resources like memory. QoS classification does not apply to EphemeralStorage\nrequests, so the above scenario will not apply if the node is, for example, under DiskPressure.\npods are guaranteed only when requests and limits are specified for all the containers and they are equal. These pods\nwill never be evicted because of another pod's resource consumption. If a system daemon (such as kubelet and journald ) is\nconsuming more resources than were reserved via system-reserved or kube-reserved allocations, and the node only has\nGuaranteed or Burstable pods using less resources than requests left on it, then the kubelet must choose to evict one of these\npods to preserve node stability and to limit the impact of resource starvation on other pods. In this case, it will choose to evict pods\nof lowest Priority first.\nGuaranteed\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n563/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf you are running a static pod and want to avoid having it evicted under resource pressure, set the priority field for that Pod\ndirectly. Static pods do not support the priorityClassName field.\nWhen the kubelet evicts pods in response to inode or process ID starvation, it uses the Pods' relative priority to determine the\neviction order, because inodes and PIDs have no requests.\nThe kubelet sorts pods differently based on whether the node has a dedicated imagefs or containerfs filesystem:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0591", "text": "Without imagefs or containerfs (nodefs and imagefs use the same filesystem)\nIf nodefs triggers evictions, the kubelet sorts pods based on their total disk usage ( local volumes + logs and a writable\nlayer of all containers ).\n\nWith imagefs (nodefs and imagefs filesystems are separate)\nIf nodefs triggers evictions, the kubelet sorts pods based on nodefs usage ( local volumes + logs of all containers ).\nIf imagefs triggers evictions, the kubelet sorts pods based on the writable layer usage of all containers.\n\nWith imagesfs and containerfs (imagefs and containerfs have been split)\nIf containerfs triggers evictions, the kubelet sorts pods based on containerfs usage ( local volumes + logs and a writable\nlayer of all containers ).\nIf imagefs triggers evictions, the kubelet sorts pods based on the storage of images rank, which represents the disk usage of\na given image.\n\nMinimum eviction reclaim\nNote:\nAs of Kubernetes v1.34, you cannot set a custom value for the containerfs.available metric. The configuration for this\nspecific metric will be set automatically to reflect values set for either the nodefs or imagefs, depending on the configuration.\nIn some cases, pod eviction only reclaims a small amount of the starved resource. This can lead to the kubelet repeatedly hitting the\nconfigured eviction thresholds and triggering multiple evictions.\nYou can use the --eviction-minimum-reclaim flag or a kubelet config file to configure a minimum reclaim amount for each\nresource. When the kubelet notices that a resource is starved, it continues to reclaim that resource until it reclaims the quantity you\nspecify.\nFor example, the following configuration sets minimum reclaim amounts:\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nevictionHard:\nmemory.available: \"500Mi\"\nnodefs.available: \"1Gi\"\nimagefs.available: \"100Gi\"\nevictionMinimumReclaim:\nmemory.available: \"0Mi\"\nnodefs.available: \"500Mi\"\nimagefs.available: \"2Gi\"\n\nIn this example, if the nodefs.available signal meets the eviction threshold, the kubelet reclaims the resource until the signal\nreaches the threshold of 1GiB, and then continues to reclaim the minimum amount of 500MiB, until the available nodefs storage\nvalue reaches 1.5GiB.\nSimilarly, the kubelet tries to reclaim the imagefs resource until the imagefs.available value reaches 102Gi , representing 102\nGiB of available container image storage. If the amount of storage that the kubelet could reclaim is less than 2GiB, the kubelet\ndoesn't reclaim anything.\nhttps://kubernetes.io/docs/concepts/_print/\n\n564/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe default eviction-minimum-reclaim is 0 for all resources."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0592", "text": "Node out of memory behavior\nIf the node experiences an out of memory (OOM) event prior to the kubelet being able to reclaim memory, the node depends on the\noom_killer to respond.\nThe kubelet sets an oom_score_adj value for each container based on the QoS for the pod.\nQuality of Service\n\noom_score_adj\n\nGuaranteed\n\n-997\n\nBestEffort\n\n1000\n\nBurstable\n\nmin(max(2, 1000 - (1000 Ã— memoryRequestBytes) / machineMemoryCapacityBytes), 999)\n\nNote:\nThe kubelet also sets an oom_score_adj value of -997 for any containers in Pods that have system-node-critical Priority.\nIf the kubelet can't reclaim memory before a node experiences OOM, the oom_killer calculates an oom_score based on the\npercentage of memory it's using on the node, and then adds the oom_score_adj to get an effective oom_score for each container. It\nthen kills the container with the highest score.\nThis means that containers in low QoS pods that consume a large amount of memory relative to their scheduling requests are killed\nfirst.\nUnlike pod eviction, if a container is OOM killed, the kubelet can restart it based on its restartPolicy .\n\nGood practices\nThe following sections describe good practice for eviction configuration.\n\nSchedulable resources and eviction policies\nWhen you configure the kubelet with an eviction policy, you should make sure that the scheduler will not schedule pods if they will\ntrigger eviction because they immediately induce memory pressure.\nConsider the following scenario:\nNode memory capacity: 10GiB\nOperator wants to reserve 10% of memory capacity for system daemons (kernel, kubelet , etc.)\nOperator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.\nFor this to work, the kubelet is launched as follows:\n--eviction-hard=memory.available<500Mi\n--system-reserved=memory=1.5Gi\n\nIn this configuration, the --system-reserved flag reserves 1.5GiB of memory for the system, which is 10% of the total memory +\nthe eviction threshold amount .\nThe node can reach the eviction threshold if a pod is using more than its request, or if the system is using more than 1GiB of\nmemory, which makes the memory.available signal fall below 500MiB and triggers the threshold.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n565/684\n\n11/7/25, 4:37 PM\n\nDaemonSets and node-pressure eviction\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0593", "text": "Pod priority is a major factor in making eviction decisions. If you do not want the kubelet to evict pods that belong to a DaemonSet,\ngive those pods a high enough priority by specifying a suitable priorityClassName in the pod spec. You can also use a lower priority,\nor the default, to only allow pods from that DaemonSet to run when there are enough resources.\n\nKnown issues\nThe following sections describe known issues related to out of resource handling.\n\nkubelet may not observe memory pressure right away\nBy default, the kubelet polls cAdvisor to collect memory usage stats at a regular interval. If memory usage increases within that\nwindow rapidly, the kubelet may not observe MemoryPressure fast enough, and the OOM killer will still be invoked.\nYou can use the --kernel-memcg-notification flag to enable the memcg notification API on the kubelet to get notified immediately\nwhen a threshold is crossed.\nIf you are not trying to achieve extreme utilization, but a sensible measure of overcommit, a viable workaround for this issue is to\nuse the --kube-reserved and --system-reserved flags to allocate memory for the system.\n\nactive_file memory is not considered as available memory\nOn Linux, the kernel tracks the number of bytes of file-backed memory on active least recently used (LRU) list as the active_file\nstatistic. The kubelet treats active_file memory areas as not reclaimable. For workloads that make intensive use of block-backed\nlocal storage, including ephemeral local storage, kernel-level caches of file and block data means that many recently accessed cache\npages are likely to be counted as active_file . If enough of these kernel block buffers are on the active LRU list, the kubelet is liable\nto observe this as high resource use and taint the node as experiencing memory pressure - triggering pod eviction.\nFor more details, see https://github.com/kubernetes/kubernetes/issues/43916\nYou can work around that behavior by setting the memory limit and memory request the same for containers likely to perform\nintensive I/O activity. You will need to estimate or measure an optimal memory limit value for that container.\n\nWhat's next\nLearn about API-initiated Eviction\nLearn about Pod Priority and Preemption\nLearn about PodDisruptionBudgets\nLearn about Quality of Service (QoS)\nCheck out the Eviction API\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n566/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n10.13 - API-initiated Eviction"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0594", "text": "API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod\ntermination.\nYou can request eviction by calling the Eviction API directly, or programmatically using a client of the API server, like the kubectl\ndrain command. This creates an Eviction object, which causes the API server to terminate the Pod.\nAPI-initiated evictions respect your configured PodDisruptionBudgets and terminationGracePeriodSeconds .\nUsing the API to create an Eviction object for a Pod is like performing a policy-controlled DELETE operation on the Pod.\n\nCalling the Eviction API\nYou can use a Kubernetes language client to access the Kubernetes API and create an Eviction object. To do this, you POST the\nattempted operation, similar to the following example:\npolicy/v1\n\npolicy/v1beta1\n\nNote:\n\npolicy/v1 Eviction is available in v1.22+. Use policy/v1beta1 with prior releases.\n\n{\n\"apiVersion\": \"policy/v1\",\n\"kind\": \"Eviction\",\n\"metadata\": {\n\"name\": \"quux\",\n\"namespace\": \"default\"\n}\n}\n\nAlternatively, you can attempt an eviction operation by accessing the API using curl or wget , similar to the following example:\n\ncurl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods\n\nHow API-initiated eviction works\nWhen you request an eviction using the API, the API server performs admission checks and responds in one of the following ways:\n200 OK : the eviction is allowed, the Eviction\n\nsubresource is created, and the Pod is deleted, similar to sending a DELETE\n\nrequest to the Pod URL.\n429 Too Many Requests : the eviction is not currently allowed because of the configured PodDisruptionBudget. You may be\n\nable to attempt the eviction again later. You might also see this response because of API rate limiting.\n500 Internal Server Error : the eviction is not allowed because there is a misconfiguration, like if multiple\nPodDisruptionBudgets reference the same Pod.\nIf the Pod you want to evict isn't part of a workload that has a PodDisruptionBudget, the API server always returns 200 OK and\nallows the eviction.\nIf the API server allows the eviction, the Pod is deleted as follows:\n1. The Pod resource in the API server is updated with a deletion timestamp, after which the API server considers the Pod\nresource to be terminated. The Pod resource is also marked with the configured grace period.\n2. The kubelet on the node where the local Pod is running notices that the Pod resource is marked for termination and starts to\ngracefully shut down the local Pod.\nhttps://kubernetes.io/docs/concepts/_print/"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0595", "text": "567/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n3. While the kubelet is shutting the Pod down, the control plane removes the Pod from EndpointSlice objects. As a result,\ncontrollers no longer consider the Pod as a valid object.\n4. After the grace period for the Pod expires, the kubelet forcefully terminates the local Pod.\n5. The kubelet tells the API server to remove the Pod resource.\n6. The API server deletes the Pod resource.\n\nTroubleshooting stuck evictions\nIn some cases, your applications may enter a broken state, where the Eviction API will only return 429 or 500 responses until you\nintervene. This can happen if, for example, a ReplicaSet creates pods for your application but new pods do not enter a Ready state.\nYou may also notice this behavior in cases where the last evicted Pod had a long termination grace period.\nIf you notice stuck evictions, try one of the following solutions:\nAbort or pause the automated operation causing the issue. Investigate the stuck application before you restart the operation.\nWait a while, then directly delete the Pod from your cluster control plane instead of using the Eviction API.\n\nWhat's next\nLearn how to protect your applications with a Pod Disruption Budget.\nLearn about Node-pressure Eviction.\nLearn about Pod Priority and Preemption.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n568/684\n\n11/7/25, 4:37 PM\n\n11 - Cluster Administration\n\nConcepts | Kubernetes\n\nLower-level detail relevant to creating or administering a Kubernetes cluster.\nThe cluster administration overview is for anyone creating or administering a Kubernetes cluster. It assumes some familiarity with\ncore Kubernetes concepts."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0596", "text": "Planning a cluster\nSee the guides in Setup for examples of how to plan, set up, and configure Kubernetes clusters. The solutions listed in this article are\ncalled distros.\nNote:\nNot all distros are actively maintained. Choose distros which have been tested with a recent version of Kubernetes.\nBefore choosing a guide, here are some considerations:\nDo you want to try out Kubernetes on your computer, or do you want to build a high-availability, multi-node cluster? Choose\ndistros best suited for your needs.\nWill you be using a hosted Kubernetes cluster, such as Google Kubernetes Engine, or hosting your own cluster?\nWill your cluster be on-premises, or in the cloud (IaaS)? Kubernetes does not directly support hybrid clusters. Instead, you\ncan set up multiple clusters.\nIf you are configuring Kubernetes on-premises, consider which networking model fits best.\nWill you be running Kubernetes on \"bare metal\" hardware or on virtual machines (VMs)?\nDo you want to run a cluster, or do you expect to do active development of Kubernetes project code? If the latter, choose\nan actively-developed distro. Some distros only use binary releases, but offer a greater variety of choices.\nFamiliarize yourself with the components needed to run a cluster.\n\nManaging a cluster\nLearn how to manage nodes.\nRead about Node autoscaling.\nLearn how to set up and manage the resource quota for shared clusters.\n\nSecuring a cluster\nGenerate Certificates describes the steps to generate certificates using different tool chains.\nKubernetes Container Environment describes the environment for Kubelet managed containers on a Kubernetes node.\nControlling Access to the Kubernetes API describes how Kubernetes implements access control for its own API.\nAuthenticating explains authentication in Kubernetes, including the various authentication options.\nAuthorization is separate from authentication, and controls how HTTP calls are handled.\nUsing Admission Controllers explains plug-ins which intercepts requests to the Kubernetes API server after authentication and\nauthorization.\nAdmission Webhook Good Practices provides good practices and considerations when designing mutating admission\nwebhooks and validating admission webhooks.\nUsing Sysctls in a Kubernetes Cluster describes to an administrator how to use the sysctl command-line tool to set kernel\nparameters .\nAuditing describes how to interact with Kubernetes' audit logs.\nhttps://kubernetes.io/docs/concepts/_print/\n\n569/684\n\n11/7/25, 4:37 PM\n\nSecuring the kubelet\n\nConcepts | Kubernetes\n\nControl Plane-Node communication\nTLS bootstrapping\nKubelet authentication/authorization"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0597", "text": "Optional Cluster Services\nDNS Integration describes how to resolve a DNS name directly to a Kubernetes service.\nLogging and Monitoring Cluster Activity explains how logging in Kubernetes works and how to implement it.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n570/684\n\n11/7/25, 4:37 PM\n\n11.1 - Node Shutdowns\n\nConcepts | Kubernetes\n\nIn a Kubernetes cluster, a node can be shut down in a planned graceful way or unexpectedly because of reasons such as a power\noutage or something else external. A node shutdown could lead to workload failure if the node is not drained before the shutdown.\nA node shutdown can be either graceful or non-graceful.\n\nGraceful node shutdown\nThe kubelet attempts to detect node system shutdown and terminates pods running on the node.\nKubelet ensures that pods follow the normal pod termination process during the node shutdown. During node shutdown, the\nkubelet does not accept new Pods (even if those Pods are already bound to the node).\n\nEnabling graceful node shutdown\nLinux\n\nWindows\n\nâ“˜ FEATURE STATE: Kubernetes v1.21 [beta] (enabled by default:\n\ntrue)\n\nOn Linux, the graceful node shutdown feature is controlled with the GracefulNodeShutdown feature\ngate which is enabled by default in 1.21.\nNote:\nThe graceful node shutdown feature depends on systemd since it takes advantage of systemd\ninhibitor locks to delay the node shutdown with a given duration."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0598", "text": "Configuring graceful node shutdown\nNote that by default, both configuration options described below, shutdownGracePeriod and shutdownGracePeriodCriticalPods ,\nare set to zero, thus not activating the graceful node shutdown functionality. To activate the feature, both options should be\nconfigured appropriately and set to non-zero values.\nOnce the kubelet is notified of a node shutdown, it sets a NotReady condition on the Node, with the reason set to \"node is\nshutting down\" . The kube-scheduler honors this condition and does not schedule any Pods onto the affected node; other thirdparty schedulers are expected to follow the same logic. This means that new Pods won't be scheduled onto that node and therefore\nnone will start.\nThe kubelet also rejects Pods during the PodAdmission phase if an ongoing node shutdown has been detected, so that even Pods\nwith a toleration for node.kubernetes.io/not-ready:NoSchedule do not start there.\nWhen kubelet is setting that condition on its Node via the API, the kubelet also begins terminating any Pods that are running locally.\nDuring a graceful shutdown, kubelet terminates pods in two phases:\n1. Terminate regular pods running on the node.\n2. Terminate critical pods running on the node.\nThe graceful node shutdown feature is configured with two KubeletConfiguration options:\nshutdownGracePeriod :\n\nSpecifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for\nboth regular and critical pods.\nshutdownGracePeriodCriticalPods :\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n571/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0599", "text": "Specifies the duration used to terminate critical pods during a node shutdown. This value should be less than\nshutdownGracePeriod .\nNote:\nThere are cases when Node termination was cancelled by the system (or perhaps manually by an administrator). In either of\nthose situations the Node will return to the Ready state. However, Pods which already started the process of termination will\nnot be restored by kubelet and will need to be re-scheduled.\nFor example, if shutdownGracePeriod=30s , and shutdownGracePeriodCriticalPods=10s , kubelet will delay the node shutdown by 30\nseconds. During the shutdown, the first 20 (30-10) seconds would be reserved for gracefully terminating normal pods, and the last\n10 seconds would be reserved for terminating critical pods.\nNote:\nWhen pods were evicted during the graceful node shutdown, they are marked as shutdown. Running kubectl get pods\nshows the status of the evicted pods as Terminated . And kubectl describe pod indicates that the pod was evicted because\nof node shutdown:\nReason:\n\nTerminated\n\nMessage:\n\nPod was terminated in response to imminent node shutdown.\n\nPod Priority based graceful node shutdown\nâ“˜ FEATURE STATE: Kubernetes v1.24 [beta] (enabled by default: true)\n\nTo provide more flexibility during graceful node shutdown around the ordering of pods during shutdown, graceful node shutdown\nhonors the PriorityClass for Pods, provided that you enabled this feature in your cluster. The feature allows cluster administrators to\nexplicitly define the ordering of pods during graceful node shutdown based on priority classes.\nThe Graceful Node Shutdown feature, as described above, shuts down pods in two phases, non-critical pods, followed by critical\npods. If additional flexibility is needed to explicitly define the ordering of pods during shutdown in a more granular way, pod priority\nbased graceful shutdown can be used.\nWhen graceful node shutdown honors pod priorities, this makes it possible to do graceful node shutdown in multiple phases, each\nphase shutting down a particular priority class of pods. The kubelet can be configured with the exact phases and shutdown time per\nphase.\nAssuming the following custom pod priority classes in a cluster,\nPod priority class name\n\nPod priority class value\n\ncustom-class-a\n\n100000\n\ncustom-class-b\n\n10000\n\ncustom-class-c\n\n1000\n\nregular/unset\n\n0\n\nWithin the kubelet configuration the settings for shutdownGracePeriodByPodPriority could look like:\nPod priority class value\n\nShutdown period\n\n100000\n\n10 seconds\n\n10000\n\n180 seconds\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n572/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nPod priority class value\n\nShutdown period\n\n1000\n\n120 seconds\n\n0\n\n60 seconds"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0600", "text": "The corresponding kubelet config YAML configuration would be:\n\nshutdownGracePeriodByPodPriority:\n- priority: 100000\nshutdownGracePeriodSeconds: 10\n- priority: 10000\nshutdownGracePeriodSeconds: 180\n- priority: 1000\nshutdownGracePeriodSeconds: 120\n- priority: 0\nshutdownGracePeriodSeconds: 60\n\nThe above table implies that any pod with priority value >= 100000 will get just 10 seconds to shut down, any pod with value >=\n10000 and < 100000 will get 180 seconds to shut down, any pod with value >= 1000 and < 10000 will get 120 seconds to shut down.\nFinally, all other pods will get 60 seconds to shut down.\nOne doesn't have to specify values corresponding to all of the classes. For example, you could instead use these settings:\nPod priority class value\n\nShutdown period\n\n100000\n\n300 seconds\n\n1000\n\n120 seconds\n\n0\n\n60 seconds\n\nIn the above case, the pods with custom-class-b will go into the same bucket as custom-class-c for shutdown.\nIf there are no pods in a particular range, then the kubelet does not wait for pods in that priority range. Instead, the kubelet\nimmediately skips to the next priority class value range.\nIf this feature is enabled and no configuration is provided, then no ordering action will be taken.\nUsing this feature requires enabling the GracefulNodeShutdownBasedOnPodPriority feature gate, and setting\nShutdownGracePeriodByPodPriority in the kubelet config to the desired configuration containing the pod priority class values and\ntheir respective shutdown periods.\nNote:\nThe ability to take Pod priority into account during graceful node shutdown was introduced as an Alpha feature in Kubernetes\nv1.23. In Kubernetes 1.34 the feature is Beta and is enabled by default.\nMetrics graceful_shutdown_start_time_seconds and graceful_shutdown_end_time_seconds are emitted under the kubelet\nsubsystem to monitor node shutdowns.\n\nNon-graceful node shutdown handling\nâ“˜ FEATURE STATE: Kubernetes v1.28 [stable] (enabled by default: true)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n573/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0601", "text": "A node shutdown action may not be detected by kubelet's Node Shutdown Manager, either because the command does not trigger\nthe inhibitor locks mechanism used by kubelet or because of a user error, i.e., the ShutdownGracePeriod and\nShutdownGracePeriodCriticalPods are not configured properly. Please refer to above section Graceful Node Shutdown for more\ndetails.\nWhen a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods that are part of a StatefulSet will be\nstuck in terminating status on the shutdown node and cannot move to a new running node. This is because kubelet on the\nshutdown node is not available to delete the pods so the StatefulSet cannot create a new pod with the same name. If there are\nvolumes used by the pods, the VolumeAttachments will not be deleted from the original shutdown node so the volumes used by\nthese pods cannot be attached to a new running node. As a result, the application running on the StatefulSet cannot function\nproperly. If the original shutdown node comes up, the pods will be deleted by kubelet and new pods will be created on a different\nrunning node. If the original shutdown node does not come up, these pods will be stuck in terminating status on the shutdown node\nforever.\nTo mitigate the above situation, a user can manually add the taint node.kubernetes.io/out-of-service with either NoExecute or\nNoSchedule effect to a Node marking it out-of-service. If a Node is marked out-of-service with this taint, the pods on the node will be\nforcefully deleted if there are no matching tolerations on it and volume detach operations for the pods terminating on the node will\nhappen immediately. This allows the Pods on the out-of-service node to recover quickly on a different node.\nDuring a non-graceful shutdown, Pods are terminated in the two phases:\n1. Force delete the Pods that do not have matching out-of-service tolerations.\n2. Immediately perform detach volume operation for such pods.\nNote:\nBefore adding the taint node.kubernetes.io/out-of-service , it should be verified that the node is already in shutdown\nor power off state (not in the middle of restarting).\nThe user is required to manually remove the out-of-service taint after the pods are moved to a new node and the user\nhas checked that the shutdown node has been recovered since the user was the one who originally added the taint."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0602", "text": "Forced storage detach on timeout\nIn any situation where a pod deletion has not succeeded for 6 minutes, kubernetes will force detach volumes being unmounted if\nthe node is unhealthy at that instant. Any workload still running on the node that uses a force-detached volume will cause a violation\nof the CSI specification, which states that ControllerUnpublishVolume \"must be called after all NodeUnstageVolume and\nNodeUnpublishVolume on the volume are called and succeed\". In such circumstances, volumes on the node in question might\nencounter data corruption.\nThe forced storage detach behaviour is optional; users might opt to use the \"Non-graceful node shutdown\" feature instead.\nForce storage detach on timeout can be disabled by setting the disable-force-detach-on-timeout config field in kube-controllermanager . Disabling the force detach on timeout feature means that a volume that is hosted on a node that is unhealthy for more\nthan 6 minutes will not have its associated VolumeAttachment deleted.\nAfter this setting has been applied, unhealthy pods still attached to volumes must be recovered via the Non-Graceful Node\nShutdown procedure mentioned above.\nNote:\nCaution must be taken while using the Non-Graceful Node Shutdown procedure.\nDeviation from the steps documented above can result in data corruption.\n\nWhat's next\nLearn more about the following:\nBlog: Non-Graceful Node Shutdown.\nCluster Architecture: Nodes.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n574/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.2 - Swap memory management\n\nKubernetes can be configured to use swap memory on a node, allowing the kernel to free up physical memory by swapping out\npages to backing storage. This is useful for multiple use-cases. For example, nodes running workloads that can benefit from using\nswap, such as those that have large memory footprints but only access a portion of that memory at any given time. It also helps\nprevent Pods from being terminated during memory pressure spikes, shields nodes from system-level memory spikes that might\ncompromise its stability, allows for more flexible memory management on the node, and much more.\nTo learn about configuring swap in your cluster, read Configuring swap memory on Kubernetes nodes.\n\nOperating system support\nLinux nodes support swap; you need to configure each node to enable it. By default, the kubelet will not start on a Linux node\nthat has swap enabled.\nWindows nodes require swap space. By default, the kubelet does not start on a Windows node that has swap disabled."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0603", "text": "How does it work?\nThere are a number of possible ways that one could envision swap use on a node. If kubelet is already running on a node, it would\nneed to be restarted after swap is provisioned in order to identify it.\nWhen kubelet starts on a node in which swap is provisioned and available (with the failSwapOn: false configuration), kubelet will:\nBe able to start on this swap-enabled node.\nDirect the Container Runtime Interface (CRI) implementation, often referred to as the container runtime, to allocate zero swap\nmemory to Kubernetes workloads by default.\nSwap configuration on a node is exposed to a cluster admin via the memorySwap in the KubeletConfiguration. As a cluster\nadministrator, you can specify the node's behaviour in the presence of swap memory by setting memorySwap.swapBehavior .\n\nSwap behaviors\nYou need to pick a swap behavior to use. Different nodes in your cluster can use different swap behaviors.\nThe swap behaviors you can choose for Linux nodes are:\nNoSwap (default)\n\nWorkloads running as Pods on this node do not and cannot use swap.\nLimitedSwap\n\nKubernetes workloads can utilize swap memory.\nNote:\nIf you choose the NoSwap behavior, and you configure the kubelet to tolerate swap space ( failSwapOn: false ), then your\nworkloads don't use any swap.\nHowever, processes outside of Kubernetes-managed containers, such as systemi services (and even the kubelet itself!) can\nutilize swap.\n\nYou can read configuring swap memory on Kubernetes nodes to learn about enabling swap for your cluster.\n\nContainer runtime integration\nThe kubelet uses the container runtime API, and directs the container runtime to apply specific configuration (for example, in the\ncgroup v2 case, memory.swap.max ) in a manner that will enable the desired swap configuration for a container. For runtimes that use\ncontrol groups, or cgroups, the container runtime is then responsible for writing these settings to the container-level cgroup.\nhttps://kubernetes.io/docs/concepts/_print/\n\n575/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nObservability for swap use\nNode and container level metric statistics"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0604", "text": "Kubelet now collects node and container level metric statistics, which can be accessed at the /metrics/resource (which is used\nmainly by monitoring tools like Prometheus) and /stats/summary (which is used mainly by Autoscalers) kubelet HTTP endpoints.\nThis allows clients who can directly request the kubelet to monitor swap usage and remaining swap memory when using\nLimitedSwap . Additionally, a machine_swap_bytes metric has been added to cadvisor to show the total physical swap capacity of the\nmachine. See this page for more info.\nFor example, these /metrics/resource are supported:\nnode_swap_usage_bytes : Current swap usage of the node in bytes.\ncontainer_swap_usage_bytes : Current amount of the container swap usage in bytes.\ncontainer_swap_limit_bytes : Current amount of the container swap limit in bytes.\n\nUsing kubectl top --show-swap\nQuerying metrics is valuable, but somewhat cumbersome, as these metrics are designed to be used by software rather than\nhumans. In order to consume this data in a more user-friendly way, the kubectl top command has been extended to support swap\nmetrics, using the --show-swap flag.\nIn order to receive information about swap usage on nodes, kubectl top nodes --show-swap can be used:\n\nkubectl top nodes --show-swap\n\nThis will result in an output similar to:\nNAME\n\nCPU(cores)\n\nCPU(%)\n\nMEMORY(bytes)\n\nMEMORY(%)\n\nSWAP(bytes)\n\nSWAP(%)\n\nnode1\n\n1m\n\n10%\n\n2Mi\n\n10%\n\n1Mi\n\n0%\n\nnode2\n\n5m\n\n10%\n\n6Mi\n\n10%\n\n2Mi\n\n0%\n\nnode3\n\n3m\n\n10%\n\n4Mi\n\n10%\n\n<unknown>\n\n<unknown>\n\nIn order to receive information about swap usage by pods, kubectl top nodes --show-swap can be used:\n\nkubectl top pod -n kube-system --show-swap\n\nThis will result in an output similar to:\nNAME\n\nCPU(cores)\n\nMEMORY(bytes)\n\nSWAP(bytes)\n\ncoredns-58d5bc5cdb-5nbk4\n\n2m\n\n19Mi\n\n0Mi\n\ncoredns-58d5bc5cdb-jsh26\n\n3m\n\n37Mi\n\n0Mi\n\netcd-node01\n\n51m\n\n143Mi\n\n5Mi\n\nkube-apiserver-node01\nkube-controller-manager-node01\n\n98m\n20m\n\n824Mi\n135Mi\n\n16Mi\n9Mi\n\nkube-proxy-ffgs2\n\n1m\n\n24Mi\n\n0Mi\n\nkube-proxy-fhvwx\n\n1m\n\n39Mi\n\n0Mi\n\nkube-scheduler-node01\n\n13m\n\n69Mi\n\n0Mi\n\nmetrics-server-8598789fdb-d2kcj\n\n5m\n\n26Mi\n\n0Mi\n\nNodes to report swap capacity as part of node status\nA new node status field is now added, node.status.nodeInfo.swap.capacity , to report the swap capacity of a node.\nAs an example, the following command can be used to retrieve the swap capacity of the nodes in a cluster:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n576/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nkubectl get nodes -o go-template='{{range .items}}{{.metadata.name}}: {{if .status.nodeInfo.swap.capacity}}{{.status\n\nThis will result in an output similar to:\nnode1: 21474836480\nnode2: 42949664768\nnode3: <unknown>"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0605", "text": "Note:\nThe <unknown> value indicates that the .status.nodeInfo.swap.capacity field is not set for that Node. This probably means\nthat the node does not have swap provisioned, or less likely, that the kubelet is not able to determine the swap capacity of the\nnode.\n\nSwap discovery using Node Feature Discovery (NFD)\nNode Feature Discovery is a Kubernetes addon for detecting hardware features and configuration. It can be utilized to discover\nwhich nodes are provisioned with swap.\nAs an example, to figure out which nodes are provisioned with swap, use the following command:\n\nkubectl get nodes -o jsonpath='{range .items[?(@.metadata.labels.feature\\.node\\.kubernetes\\.io/memory-swap)]}{.metad\n\nThis will result in an output similar to:\nk8s-worker1: true\nk8s-worker2: true\nk8s-worker3: false\n\nIn this example, swap is provisioned on nodes k8s-worker1 and k8s-worker2 , but not on k8s-worker3 .\n\nRisks and caveats\nCaution:\nIt is deeply encouraged to encrypt the swap space. See Memory-backed volumes memory-backed volumes for more info.\nHaving swap available on a system reduces predictability. While swap can enhance performance by making more RAM available,\nswapping data back to memory is a heavy operation, sometimes slower by many orders of magnitude, which can cause unexpected\nperformance regressions. Furthermore, swap changes a system's behaviour under memory pressure. Enabling swap increases the\nrisk of noisy neighbors, where Pods that frequently use their RAM may cause other Pods to swap. In addition, since swap allows for\ngreater memory usage for workloads in Kubernetes that cannot be predictably accounted for, and due to unexpected packing\nconfigurations, the scheduler currently does not account for swap memory usage. This heightens the risk of noisy neighbors.\nThe performance of a node with swap memory enabled depends on the underlying physical storage. When swap memory is in use,\nperformance will be significantly worse in an I/O operations per second (IOPS) constrained environment, such as a cloud VM with I/O\nthrottling, when compared to faster storage mediums like solid-state drives or NVMe. As swap might cause IO pressure, it is\nrecommended to give a higher IO latency priority to system critical daemons. See the relevant section in the recommended practices\nsection below."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0606", "text": "Memory-backed volumes\nOn Linux nodes, memory-backed volumes (such as secret volume mounts, or emptyDir with medium: Memory ) are implemented\nwith a tmpfs filesystem. The contents of such volumes should remain in memory at all times, hence should not be swapped to disk.\nTo ensure the contents of such volumes remain in memory, the noswap tmpfs option is being used.\nhttps://kubernetes.io/docs/concepts/_print/\n\n577/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe Linux kernel officially supports the noswap option from version 6.3 (more info can be found in Linux Kernel Version\nRequirements). However, the different distributions often choose to backport this mount option to older Linux versions as well.\nIn order to verify whether the node supports the noswap option, the kubelet will do the following:\nIf the kernel's version is above 6.3 then the noswap option will be assumed to be supported.\nOtherwise, kubelet would try to mount a dummy tmpfs with the noswap option at startup. If kubelet fails with an error\nindicating of an unknown option, noswap will be assumed to not be supported, hence will not be used. A kubelet log entry will\nbe emitted to warn the user about memory-backed volumes might swap to disk. If kubelet succeeds, the dummy tmpfs will be\ndeleted and the noswap option will be used.\nIf the noswap option is not supported, kubelet will emit a warning log entry, then continue its execution.\nSee the section above with an example for setting unencrypted swap. However, handling encrypted swap is not within the scope of\nkubelet; rather, it is a general OS configuration concern and should be addressed at that level. It is the administrator's responsibility\nto provision encrypted swap to mitigate this risk."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0607", "text": "Evictions\nConfiguring memory eviction thresholds for swap-enabled nodes can be tricky.\nWith swap being disabled, it is reasonable to configure kubelet's eviction thresholds to be a bit lower than the node's memory\ncapacity. The rationale is that we want Kubernetes to start evicting Pods before the node runs out of memory and invokes the Out Of\nMemory (OOM) killer, since the OOM killer is not Kubernetes-aware, therefore does not consider things like QoS, pod priority, or\nother Kubernetes-specific factors.\nWith swap enabled, the situation is more complex. In Linux, the vm.min_free_kbytes parameter defines the memory threshold for\nthe kernel to start aggressively reclaiming memory, which includes swapping out pages. If the kubelet's eviction thresholds are set in\na way that eviction would take place before the kernel starts reclaiming memory, it could lead to workloads never being able to swap\nout during node memory pressure. However, setting the eviction thresholds too high could result in the node running out of\nmemory and invoking the OOM killer, which is not ideal either.\nTo address this, it is recommended to set the kubelet's eviction thresholds to be slightly lower than the vm.min_free_kbytes value.\nThis way, the node can start swapping before kubelet would start evicting Pods, allowing workloads to swap out unused data and\npreventing evictions from happening. On the other hand, since it is just slightly lower, kubelet is likely to start evicting Pods before\nthe node runs out of memory, thus avoiding the OOM killer.\nThe value of vm.min_free_kbytes can be determined by running the following command on the node:\n\ncat /proc/sys/vm/min_free_kbytes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0608", "text": "Unutilized swap space\nUnder the LimitedSwap behavior, the amount of swap available to a Pod is determined automatically, based on the proportion of\nthe memory requested relative to the node's total memory (For more details, see the section below).\nThis design means that usually there would be some portion of swap that will remain restricted for Kubernetes workloads. For\nexample, since Kubernetes 1.34 does not permit swap use for Pods in the Guaranteed QoS class, the amount of swap that's\nproportional to the memory request for Guaranteed pods would remain unused by Kubernetes workloads.\nThis behavior carries some risk in a situation where many pods are not eligible for swapping. On the other hand, it effectively keeps\nsome system-reserved amount of swap memory that can be used by processes outside of Kubernetes' scope, such as system\ndaemons and even kubelet itself.\n\nGood practice for using swap in a Kubernetes cluster\nDisable swap for system-critical daemons\nDuring the testing phase and based on user feedback, it was observed that the performance of system-critical daemons and services\nmight degrade. This implies that system daemons, including the kubelet, could operate slower than usual. If this issue is\nencountered, it is advisable to configure the cgroup of the system slice to prevent swapping (i.e., set memory.swap.max=0 ).\nhttps://kubernetes.io/docs/concepts/_print/\n\n578/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nProtect system-critical daemons for I/O latency\n\nSwap can increase the I/O load on a node. When memory pressure causes the kernel to rapidly swap pages in and out, systemcritical daemons and services that rely on I/O operations may experience performance degradation.\nTo mitigate this, it is recommended for systemd users to prioritize the system slice in terms of I/O latency. For non-systemd users,\nsetting up a dedicated cgroup for system daemons and processes and prioritizing I/O latency in the same way is advised. This can be\nachieved by setting io.latency for the system slice, thereby granting it higher I/O priority. See cgroup's documentation for more\ninfo.\n\nSwap and control plane nodes\nThe Kubernetes project recommends running control plane nodes without any swap space configured. The control plane primarily\nhosts Guaranteed QoS Pods, so swap can generally be disabled. The main concern is that swapping critical services on the control\nplane could negatively impact performance."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0609", "text": "Use of a dedicated disk for swap\nThe Kubernetes project recommends using encrypted swap, whenever you run nodes with swap enabled. If swap resides on a\npartition or the root filesystem, workloads may interfere with system processes that need to write to disk. When they share the\nsame disk, processes can overwhelm swap, disrupting the I/O of kubelet, container runtime, and systemd, which would impact other\nworkloads. Since swap space is located on a disk, it is crucial to ensure the disk is fast enough for the intended use cases.\nAlternatively, one can configure I/O priorities between different mapped areas of a single backing device.\n\nSwap-aware scheduling\nKubernetes 1.34 does not support allocating Pods to nodes in a way that accounts for swap memory usage. The scheduler typically\nuses requests for infrastructure resources to guide Pod placement, and Pods do not request swap space; they just request memory .\nThis means that the scheduler does not consider swap memory when making scheduling decisions. While this is something we are\nactively working on, it is not yet implemented.\nIn order for administrators to ensure that Pods are not scheduled on nodes with swap memory unless they are specifically intended\nto use it, Administrators can taint nodes with swap available to protect against this problem. Taints will ensure that workloads which\ntolerate swap will not spill onto nodes without swap under load.\n\nSelecting storage for optimal performance\nThe storage device designated for swap space is critical to maintaining system responsiveness during high memory usage. Rotational\nhard disk drives (HDDs) are ill-suited for this task as their mechanical nature introduces significant latency, leading to severe\nperformance degradation and system thrashing. For modern performance needs, a device such as a Solid State Drive (SSD) is\nprobably the appropriate choice for swap, as its low-latency electronic access minimizes the slowdown."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0610", "text": "Swap behavior details\nHow is the swap limit being determined with LimitedSwap?\nThe configuration of swap memory, including its limitations, presents a significant challenge. Not only is it prone to misconfiguration,\nbut as a system-level property, any misconfiguration could potentially compromise the entire node rather than just a specific\nworkload. To mitigate this risk and ensure the health of the node, we have implemented Swap with automatic configuration of\nlimitations.\nWith LimitedSwap , Pods that do not fall under the Burstable QoS classification (i.e. BestEffort / Guaranteed QoS Pods) are\nprohibited from utilizing swap memory. BestEffort QoS Pods exhibit unpredictable memory consumption patterns and lack\ninformation regarding their memory usage, making it difficult to determine a safe allocation of swap memory. Conversely,\nGuaranteed QoS Pods are typically employed for applications that rely on the precise allocation of resources specified by the\nworkload, with memory being immediately available. To maintain the aforementioned security and node health guarantees, these\nPods are not permitted to use swap memory when LimitedSwap is in effect. In addition, high-priority pods are not permitted to use\nswap in order to ensure the memory they consume always residents on disk, hence ready to use.\nPrior to detailing the calculation of the swap limit, it is necessary to define the following terms:\nnodeTotalMemory : The total amount of physical memory available on the node.\nhttps://kubernetes.io/docs/concepts/_print/\n\n579/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ntotalPodsSwapAvailable : The total amount of swap memory on the node that is available for use by Pods (some swap\n\nmemory may be reserved for system use).\ncontainerMemoryRequest : The container's memory request.\nSwap limitation is configured as:\n( containerMemoryRequest / nodeTotalMemory ) Ã— totalPodsSwapAvailable\nIn other words, the amount of swap that a container is able to use is proportionate to its memory request, the node's total physical\nmemory and the total amount of swap memory on the node that is available for use by Pods.\nIt is important to note that, for containers within Burstable QoS Pods, it is possible to opt-out of swap usage by specifying memory\nrequests that are equal to memory limits. Containers configured in this manner will not have access to swap memory."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0611", "text": "What's next\nTo learn about managing swap on Linux nodes, read configuring swap memory on Kubernetes nodes.\nYou can check out a blog post about Kubernetes and swap\nFor background information, please see the original KEP, KEP-2400, and its design.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n580/684\n\n11/7/25, 4:37 PM\n\n11.3 - Node Autoscaling\n\nConcepts | Kubernetes\n\nAutomatically provision and consolidate the Nodes in your cluster to adapt to demand and optimize cost.\nIn order to run workloads in your cluster, you need Nodes. Nodes in your cluster can be autoscaled - dynamically provisioned, or\nconsolidated to provide needed capacity while optimizing cost. Autoscaling is performed by Node autoscalers.\n\nNode provisioning\nIf there are Pods in a cluster that can't be scheduled on existing Nodes, new Nodes can be automatically added to the clusterâ€”\nprovisionedâ€”to accommodate the Pods. This is especially useful if the number of Pods changes over time, for example as a result of\ncombining horizontal workload with Node autoscaling.\nAutoscalers provision the Nodes by creating and deleting cloud provider resources backing them. Most commonly, the resources\nbacking the Nodes are Virtual Machines.\nThe main goal of provisioning is to make all Pods schedulable. This goal is not always attainable because of various limitations,\nincluding reaching configured provisioning limits, provisioning configuration not being compatible with a particular set of pods, or\nthe lack of cloud provider capacity. While provisioning, Node autoscalers often try to achieve additional goals (for example\nminimizing the cost of the provisioned Nodes or balancing the number of Nodes between failure domains).\nThere are two main inputs to a Node autoscaler when determining Nodes to provisionâ€”Pod scheduling constraints, and Node\nconstraints imposed by autoscaler configuration.\nAutoscaler configuration may also include other Node provisioning triggers (for example the number of Nodes falling below a\nconfigured minimum limit).\nNote:\nProvisioning was formerly known as scale-up in Cluster Autoscaler."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0612", "text": "Pod scheduling constraints\nPods can express scheduling constraints to impose limitations on the kind of Nodes they can be scheduled on. Node autoscalers\ntake these constraints into account to ensure that the pending Pods can be scheduled on the provisioned Nodes.\nThe most common kind of scheduling constraints are the resource requests specified by Pod containers. Autoscalers will make sure\nthat the provisioned Nodes have enough resources to satisfy the requests. However, they don't directly take into account the real\nresource usage of the Pods after they start running. In order to autoscale Nodes based on actual workload resource usage, you can\ncombine horizontal workload autoscaling with Node autoscaling.\nOther common Pod scheduling constraints include Node affinity, inter-Pod affinity, or a requirement for a particular storage volume.\n\nNode constraints imposed by autoscaler configuration\nThe specifics of the provisioned Nodes (for example the amount of resources, the presence of a given label) depend on autoscaler\nconfiguration. Autoscalers can either choose them from a pre-defined set of Node configurations, or use auto-provisioning.\n\nAuto-provisioning\nNode auto-provisioning is a mode of provisioning in which a user doesn't have to fully configure the specifics of the Nodes that can\nbe provisioned. Instead, the autoscaler dynamically chooses the Node configuration based on the pending Pods it's reacting to, as\nwell as pre-configured constraints (for example, the minimum amount of resources or the need for a given label).\n\nNode consolidation\nThe main consideration when running a cluster is ensuring that all schedulable pods are running, whilst keeping the cost of the\ncluster as low as possible. To achieve this, the Pods' resource requests should utilize as much of the Nodes' resources as possible.\nFrom this perspective, the overall Node utilization in a cluster can be used as a proxy for how cost-effective the cluster is.\nhttps://kubernetes.io/docs/concepts/_print/\n\n581/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0613", "text": "Note:\nCorrectly setting the resource requests of your Pods is as important to the overall cost-effectiveness of a cluster as optimizing\nNode utilization. Combining Node autoscaling with vertical workload autoscaling can help you achieve this.\nNodes in your cluster can be automatically consolidated in order to improve the overall Node utilization, and in turn the costeffectiveness of the cluster. Consolidation happens through removing a set of underutilized Nodes from the cluster. Optionally, a\ndifferent set of Nodes can be provisioned to replace them.\nConsolidation, like provisioning, only considers Pod resource requests and not real resource usage when making decisions.\nFor the purpose of consolidation, a Node is considered empty if it only has DaemonSet and static Pods running on it. Removing\nempty Nodes during consolidation is more straightforward than non-empty ones, and autoscalers often have optimizations\ndesigned specifically for consolidating empty Nodes.\nRemoving non-empty Nodes during consolidation is disruptiveâ€”the Pods running on them are terminated, and possibly have to be\nrecreated (for example by a Deployment). However, all such recreated Pods should be able to schedule on existing Nodes in the\ncluster, or the replacement Nodes provisioned as part of consolidation. No Pods should normally become pending as a result of\nconsolidation.\nNote:\nAutoscalers predict how a recreated Pod will likely be scheduled after a Node is provisioned or consolidated, but they don't\ncontrol the actual scheduling. Because of this, some Pods might become pending as a result of consolidation - if for example a\ncompletely new Pod appears while consolidation is being performed.\nAutoscaler configuration may also enable triggering consolidation by other conditions (for example, the time elapsed since a Node\nwas created), in order to optimize different properties (for example, the maximum lifespan of Nodes in a cluster).\nThe details of how consolidation is performed depend on the configuration of a given autoscaler.\nNote:\nConsolidation was formerly known as scale-down in Cluster Autoscaler."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0614", "text": "Autoscalers\nThe functionalities described in previous sections are provided by Node autoscalers. In addition to the Kubernetes API, autoscalers\nalso need to interact with cloud provider APIs to provision and consolidate Nodes. This means that they need to be explicitly\nintegrated with each supported cloud provider. The performance and feature set of a given autoscaler can differ between cloud\nprovider integrations.\ngraph TD na[Node autoscaler] k8s[Kubernetes] cp[Cloud Provider] k8s --> |get Pods/Nodes|na na -->\n|drain Nodes|k8s na --> |create/remove resources backing Nodes|cp cp --> |get resources backing\nNodes|na classDef white_on_blue fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef\nblue_on_white fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class na blue_on_white; class k8s,cp\nwhite_on_blue;\n\nAutoscaler implementations\nCluster Autoscaler and Karpenter are the two Node autoscalers currently sponsored by SIG Autoscaling.\nFrom the perspective of a cluster user, both autoscalers should provide a similar Node autoscaling experience. Both will provision\nnew Nodes for unschedulable Pods, and both will consolidate the Nodes that are no longer optimally utilized.\nDifferent autoscalers may also provide features outside the Node autoscaling scope described on this page, and those additional\nfeatures may differ between them.\nConsult the sections below, and the linked documentation for the individual autoscalers to decide which autoscaler fits your use\ncase better.\nhttps://kubernetes.io/docs/concepts/_print/\n\n582/684\n\n11/7/25, 4:37 PM\n\nCluster Autoscaler\n\nConcepts | Kubernetes\n\nCluster Autoscaler adds or removes Nodes to pre-configured Node groups. Node groups generally map to some sort of cloud\nprovider resource group (most commonly a Virtual Machine group). A single instance of Cluster Autoscaler can simultaneously\nmanage multiple Node groups. When provisioning, Cluster Autoscaler will add Nodes to the group that best fits the requests of\npending Pods. When consolidating, Cluster Autoscaler always selects specific Nodes to remove, as opposed to just resizing the\nunderlying cloud provider resource group.\nAdditional context:\nDocumentation overview\nCloud provider integrations\nCluster Autoscaler FAQ\nContact"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0615", "text": "Karpenter\nKarpenter auto-provisions Nodes based on NodePool configurations provided by the cluster operator. Karpenter handles all aspects\nof node lifecycle, not just autoscaling. This includes automatically refreshing Nodes once they reach a certain lifetime, and autoupgrading Nodes when new worker Node images are released. It works directly with individual cloud provider resources (most\ncommonly individual Virtual Machines), and doesn't rely on cloud provider resource groups.\nAdditional context:\nDocumentation\nCloud provider integrations\nKarpenter FAQ\nContact\n\nImplementation comparison\nMain differences between Cluster Autoscaler and Karpenter:\nCluster Autoscaler provides features related to just Node autoscaling. Karpenter has a wider scope, and also provides features\nintended for managing Node lifecycle altogether (for example, utilizing disruption to auto-recreate Nodes once they reach a\ncertain lifetime, or auto-upgrade them to new versions).\nCluster Autoscaler doesn't support auto-provisioning, the Node groups it can provision from have to be pre-configured.\nKarpenter supports auto-provisioning, so the user only has to configure a set of constraints for the provisioned Nodes, instead\nof fully configuring homogenous groups.\nCluster Autoscaler provides cloud provider integrations directly, which means that they're a part of the Kubernetes project. For\nKarpenter, the Kubernetes project publishes Karpenter as a library that cloud providers can integrate with to build a Node\nautoscaler.\nCluster Autoscaler provides integrations with numerous cloud providers, including smaller and less popular providers. There\nare fewer cloud providers that integrate with Karpenter, including AWS, and Azure.\n\nCombine workload and Node autoscaling\nHorizontal workload autoscaling\nNode autoscaling usually works in response to Podsâ€”it provisions new Nodes to accommodate unschedulable Pods, and then\nconsolidates the Nodes once they're no longer needed.\nHorizontal workload autoscaling automatically scales the number of workload replicas to maintain a desired average resource\nutilization across the replicas. In other words, it automatically creates new Pods in response to application load, and then removes\nthe Pods once the load decreases.\nYou can use Node autoscaling together with horizontal workload autoscaling to autoscale the Nodes in your cluster based on the\naverage real resource utilization of your Pods.\nIf the application load increases, the average utilization of its Pods should also increase, prompting workload autoscaling to create\nnew Pods. Node autoscaling should then provision new Nodes to accommodate the new Pods.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n583/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0616", "text": "Once the application load decreases, workload autoscaling should remove unnecessary Pods. Node autoscaling should, in turn,\nconsolidate the Nodes that are no longer needed.\nIf configured correctly, this pattern ensures that your application always has the Node capacity to handle load spikes if needed, but\nyou don't have to pay for the capacity when it's not needed.\n\nVertical workload autoscaling\nWhen using Node autoscaling, it's important to set Pod resource requests correctly. If the requests of a given Pod are too low,\nprovisioning a new Node for it might not help the Pod actually run. If the requests of a given Pod are too high, it might incorrectly\nprevent consolidating its Node.\nVertical workload autoscaling automatically adjusts the resource requests of your Pods based on their historical resource usage.\nYou can use Node autoscaling together with vertical workload autoscaling in order to adjust the resource requests of your Pods\nwhile preserving Node autoscaling capabilities in your cluster.\nCaution:\nWhen using Node autoscaling, it's not recommended to set up vertical workload autoscaling for DaemonSet Pods. Autoscalers\nhave to predict what DaemonSet Pods on a new Node will look like in order to predict available Node resources. Vertical\nworkload autoscaling might make these predictions unreliable, leading to incorrect scaling decisions.\n\nRelated components\nThis section describes components providing functionality related to Node autoscaling.\n\nDescheduler\nThe descheduler is a component providing Node consolidation functionality based on custom policies, as well as other features\nrelated to optimizing Nodes and Pods (for example deleting frequently restarting Pods).\n\nWorkload autoscalers based on cluster size\nCluster Proportional Autoscaler and Cluster Proportional Vertical Autoscaler provide horizontal, and vertical workload autoscaling\nbased on the number of Nodes in the cluster. You can read more in autoscaling based on cluster size.\n\nWhat's next\nRead about workload-level autoscaling\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n584/684\n\n11/7/25, 4:37 PM\n\n11.4 - Certificates\n\nConcepts | Kubernetes\n\nTo learn how to generate certificates for your cluster, see Certificates.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n585/684\n\n11/7/25, 4:37 PM\n\n11.5 - Cluster Networking\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0617", "text": "Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. There are 4\ndistinct networking problems to address:\n1. Highly-coupled container-to-container communications: this is solved by Pods and localhost communications.\n2. Pod-to-Pod communications: this is the primary focus of this document.\n3. Pod-to-Service communications: this is covered by Services.\n4. External-to-Service communications: this is also covered by Services.\nKubernetes is all about sharing machines among applications. Typically, sharing machines requires ensuring that two applications do\nnot try to use the same ports. Coordinating ports across multiple developers is very difficult to do at scale and exposes users to\ncluster-level issues outside of their control.\nDynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have\nto know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather\nthan deal with this, Kubernetes takes a different approach.\nTo learn about the Kubernetes networking model, see here.\n\nKubernetes IP address ranges\nKubernetes clusters require to allocate non-overlapping IP addresses for Pods, Services and Nodes, from a range of available\naddresses configured in the following components:\nThe network plugin is configured to assign IP addresses to Pods.\nThe kube-apiserver is configured to assign IP addresses to Services.\nThe kubelet or the cloud-controller-manager is configured to assign IP addresses to Nodes.\n\nCluster networking types\nKubernetes clusters, attending to the IP families configured, can be categorized into:\nIPv4 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv4\naddresses.\nIPv6 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv6\naddresses.\nIPv4/IPv6 or IPv6/IPv4 dual-stack:\nThe network plugin is configured to assign IPv4 and IPv6 addresses.\nThe kube-apiserver is configured to assign IPv4 and IPv6 addresses.\nThe kubelet or cloud-controller-manager is configured to assign IPv4 and IPv6 address.\nAll components must agree on the configured primary IP family.\nKubernetes clusters only consider the IP families present on the Pods, Services and Nodes objects, independently of the existing IPs\nof the represented objects. Per example, a server or a pod can have multiple IP addresses on its interfaces, but only the IP addresses\nin node.status.addresses or pod.status.ips are considered for implementing the Kubernetes network model and defining the\nhttps://kubernetes.io/docs/concepts/_print/\n\n586/684\n\n11/7/25, 4:37 PM"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0618", "text": "Concepts | Kubernetes\n\ntype of the cluster.\n\nHow to implement the Kubernetes network model\nThe network model is implemented by the container runtime on each node. The most common container runtimes use Container\nNetwork Interface (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many\ndifferent vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more\nsophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced\nIPAM features etc.\nSee this page for a non-exhaustive list of networking addons supported by Kubernetes.\n\nWhat's next\nThe early design of the networking model and its rationale are described in more detail in the networking design document. For\nfuture plans and some on-going efforts that aim to improve Kubernetes networking, please refer to the SIG-Network KEPs.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n587/684\n\n11/7/25, 4:37 PM\n\n11.6 - Observability\n\nConcepts | Kubernetes\n\nUnderstand how to gain end-to-end visibility of a Kubernetes cluster through the collection of metrics, logs,\nand traces.\nIn Kubernetes, observability is the process of collecting and analyzing metrics, logs, and tracesâ€”often referred to as the three pillars\nof observabilityâ€”in order to obtain a better understanding of the internal state, performance, and health of the cluster.\nKubernetes control plane components, as well as many add-ons, generate and emit these signals. By aggregating and correlating\nthem, you can gain a unified picture of the control plane, add-ons, and applications across the cluster.\nFigure 1 outlines how cluster components emit the three primary signal types.\nflowchart LR A[Cluster components] --> M[Metrics pipeline] A --> L[Log pipeline] A --> T[Trace pipeline] M -> S[(Storage and analysis)] L --> S T --> S S --> O[Operators and automation]\nFigure 1. High-level signals emitted by cluster components and their consumers."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0619", "text": "Metrics\nKubernetes components emit metrics in Prometheus format from their /metrics endpoints, including:\nkube-controller-manager\nkube-proxy\nkube-apiserver\nkube-scheduler\nkubelet\nThe kubelet also exposes metrics at /metrics/cadvisor , /metrics/resource , and /metrics/probes , and add-ons such as kubestate-metrics enrich those control plane signals with Kubernetes object status.\nA typical Kubernetes metrics pipeline periodically scrapes these endpoints and stores the samples in a time series database (for\nexample with Prometheus).\nSee the system metrics guide for details and configuration options.\nFigure 2 outlines a common Kubernetes metrics pipeline.\nflowchart LR C[Cluster components] --> P[Prometheus scraper] P --> TS[(Time series storage)] TS -->\nD[Dashboards and alerts] TS --> A[Automated actions]\nFigure 2. Components of a typical Kubernetes metrics pipeline.\nFor multi-cluster or multi-cloud visibility, distributed time series databases (for example Thanos or Cortex) can complement\nPrometheus.\nSee Common observability tools - metrics tools for metrics scrapers and time series databases.\n\nSee Also\nSystem metrics for Kubernetes components\nResource usage monitoring with metrics-server\nkube-state-metrics concept\nResource metrics pipeline overview\n\nLogs\nLogs provide a chronological record of events inside applications, Kubernetes system components, and security-related activities\nsuch as audit logging.\nhttps://kubernetes.io/docs/concepts/_print/\n\n588/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nContainer runtimes capture a containerized applicationâ€™s output from standard output ( stdout ) and standard error ( stderr )\nstreams. While runtimes implement this differently, the integration with the kubelet is standardized through the CRI logging format,\nand the kubelet makes these logs available through kubectl logs ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0620", "text": "Figure 3a. Node-level logging architecture.\nSystem component logs capture events from the cluster and are often useful for debugging and troubleshooting. These components\nare classified in two different ways: those that run in a container and those that do not. For example, the kube-scheduler and\nkube-proxy usually run in containers, whereas the kubelet and the container runtime run directly on the host.\nOn machines with systemd , the kubelet and container runtime write to journald. Otherwise, they write to .log files in the\n/var/log directory.\nSystem components that run inside containers always write to .log files in /var/log , bypassing the default container logging\nmechanism.\nSystem component and container logs stored under /var/log require log rotation to prevent uncontrolled growth. Some cluster\nprovisioning scripts install log rotation by default; verify your environment and adjust as needed. See the system logs reference for\ndetails on locations, formats, and configuration options.\nMost clusters run a node-level logging agent (for example, Fluent Bit or Fluentd) that tails these files and forwards entries to a central\nlog store. The logging architecture guidance explains how to design such pipelines, apply retention, and log flows to backends.\nFigure 3 outlines a common log aggregation pipeline.\nflowchart LR subgraph Sources A[Application stdout / stderr] B[Control plane logs] C[Audit records] end A\n--> N[Node log agent] B --> N C --> N N --> L[Central log store] L --> Q[Dashboards, alerting, SIEM]\nFigure 3. Components of a typical Kubernetes logs pipeline.\nSee Common observability tools - logging tools for logging agents and central log stores.\n\nSee Also\nLogging architecture\nSystem logs\nLogging tasks and tutorials\nConfigure audit logging\n\nTraces\nTraces capture how requests moves across Kubernetes components and applications, linking latency, timing and relationships\nbetween operations.By collecting traces, you can visualize end-to-end request flow, diagnose performance issues, and identify\nbottlenecks or unexpected interactions in the control plane, add-ons, or applications.\nKubernetes 1.34 can export spans over the OpenTelemetry Protocol (OTLP), either directly via built-in gRPC exporters or by\nforwarding them through an OpenTelemetry Collector.\nThe OpenTelemetry Collector receives spans from components and applications, processes them (for example by applying sampling\nor redaction), and forwards them to a tracing backend for storage and analysis.\nhttps://kubernetes.io/docs/concepts/_print/\n\n589/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0621", "text": "Figure 4 outlines a typical distributed tracing pipeline.\nflowchart LR subgraph Sources A[Control plane spans] B[Application spans] end A --> X[OTLP exporter] B -> X X --> COL[OpenTelemetry Collector] COL --> TS[(Tracing backend)] TS --> V[Visualization and analysis]\nFigure 4. Components of a typical Kubernetes traces pipeline.\nSee Common observability tools - tracing tools for tracing collectors and backends.\n\nSee Also\nSystem traces for Kubernetes components\nOpenTelemetry Collector getting started guide\nMonitoring and tracing tasks\n\nCommon observability tools\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nNote: This section links to third-party projects that provide observability capabilities required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide\nbefore submitting a change.\n\nMetrics tools\nCortex offers horizontally scalable, long-term Prometheus storage.\nGrafana Mimir is a Grafana Labs project that provides multi-tenant, horizontally scalable Prometheus-compatible storage.\nPrometheus is the monitoring system that scrapes and stores metrics from Kubernetes components.\nThanos extends Prometheus with global querying, downsampling, and object storage support.\n\nLogging tools\nElasticsearch delivers distributed log indexing and search.\nFluent Bit collects and forwards container and node logs with a low resource footprint.\nFluentd routes and transforms logs to multiple destinations.\nGrafana Loki stores logs in a Prometheus-inspired, label-based format.\nOpenSearch provides open source log indexing and search compatible with Elasticsearch APIs.\n\nTracing tools\nGrafana Tempo offers scalable, low-cost distributed tracing storage.\nJaeger captures and visualizes distributed traces for microservices.\nOpenTelemetry Collector receives, processes, and exports telemetry data including traces.\nZipkin provides distributed tracing collection and visualization.\n\nWhat's next\nLearn how to collect resource usage metrics with metrics-server\nExplore logging tasks and tutorials\nFollow the monitoring and tracing task guides\nReview the system metrics guide for component endpoints and stability\nReview the common observability tools section for vetted third-party options\nhttps://kubernetes.io/docs/concepts/_print/\n\n590/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0622", "text": "11.7 - Admission Webhook Good Practices\nRecommendations for designing and deploying admission webhooks in Kubernetes.\nThis page provides good practices and considerations when designing admission webhooks in Kubernetes. This information is\nintended for cluster operators who run admission webhook servers or third-party applications that modify or validate your API\nrequests.\nBefore reading this page, ensure that you're familiar with the following concepts:\nAdmission controllers\nAdmission webhooks\n\nImportance of good webhook design\nAdmission control occurs when any create, update, or delete request is sent to the Kubernetes API. Admission controllers intercept\nrequests that match specific criteria that you define. These requests are then sent to mutating admission webhooks or validating\nadmission webhooks. These webhooks are often written to ensure that specific fields in object specifications exist or have specific\nallowed values.\nWebhooks are a powerful mechanism to extend the Kubernetes API. Badly-designed webhooks often result in workload disruptions\nbecause of how much control the webhooks have over objects in the cluster. Like other API extension mechanisms, webhooks are\nchallenging to test at scale for compatibility with all of your workloads, other webhooks, add-ons, and plugins.\nAdditionally, with every release, Kubernetes adds or modifies the API with new features, feature promotions to beta or stable status,\nand deprecations. Even stable Kubernetes APIs are likely to change. For example, the Pod API changed in v1.29 to add the Sidecar\ncontainers feature. While it's rare for a Kubernetes object to enter a broken state because of a new Kubernetes API, webhooks that\nworked as expected with earlier versions of an API might not be able to reconcile more recent changes to that API. This can result in\nunexpected behavior after you upgrade your clusters to newer versions.\nThis page describes common webhook failure scenarios and how to avoid them by cautiously and thoughtfully designing and\nimplementing your webhooks.\n\nIdentify whether you use admission webhooks\nEven if you don't run your own admission webhooks, some third-party applications that you run in your clusters might use mutating\nor validating admission webhooks.\nTo check whether your cluster has any mutating admission webhooks, run the following command:\n\nkubectl get mutatingwebhookconfigurations\n\nThe output lists any mutating admission controllers in the cluster.\nTo check whether your cluster has any validating admission webhooks, run the following command:\n\nkubectl get validatingwebhookconfigurations\n\nThe output lists any validating admission controllers in the cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0623", "text": "Choose an admission control mechanism\nKubernetes includes multiple admission control and policy enforcement options. Knowing when to use a specific option can help you\nto improve latency and performance, reduce management overhead, and avoid issues during version upgrades. The following table\ndescribes the mechanisms that let you mutate or validate resources during admission:\nhttps://kubernetes.io/docs/concepts/_print/\n\n591/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nMechanism\n\nDescription\n\nUse cases\n\nMutating\nadmission\nwebhook\n\nIntercept API requests before admission and modify as\nneeded using custom logic.\n\nMake critical modifications that must\nhappen before resource admission.\nMake complex modifications that require\nadvanced logic, like calling external APIs.\n\nMutating\nadmission policy\n\nIntercept API requests before admission and modify as\nneeded using Common Expression Language (CEL)\nexpressions.\n\nMake critical modifications that must\nhappen before resource admission.\nMake simple modifications, such as\nadjusting labels or replica counts.\n\nValidating\nadmission\nwebhook\n\nIntercept API requests before admission and validate\nagainst complex policy declarations.\n\nValidate critical configurations before\nresource admission.\nEnforce complex policy logic before\nadmission.\n\nValidating\nadmission policy\n\nIntercept API requests before admission and validate\nagainst CEL expressions.\n\nValidate critical configurations before\nresource admission.\nEnforce policy logic using CEL\nexpressions.\n\nMutating and validating admission control in Kubernetes\nIn general, use webhook admission control when you want an extensible way to declare or configure the logic. Use built-in CEL-based\nadmission control when you want to declare simpler logic without the overhead of running a webhook server. The Kubernetes\nproject recommends that you use CEL-based admission control when possible.\n\nUse built-in validation and defaulting for CustomResourceDefinitions\nIf you use CustomResourceDefinitions, don't use admission webhooks to validate values in CustomResource specifications or to set\ndefault values for fields. Kubernetes lets you define validation rules and default field values when you create\nCustomResourceDefinitions.\nTo learn more, see the following resources:\nValidation rules\nDefaulting\n\nPerformance and latency\nThis section describes recommendations for improving performance and reducing latency. In summary, these are as follows:\nConsolidate webhooks and limit the number of API calls per webhook.\nUse audit logs to check for webhooks that repeatedly do the same action.\nUse load balancing for webhook availability.\nSet a small timeout value for each webhook.\nConsider cluster availability needs during webhook design."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0624", "text": "Design admission webhooks for low latency\nMutating admission webhooks are called in sequence. Depending on the mutating webhook setup, some webhooks might be called\nmultiple times. Every mutating webhook call adds latency to the admission process. This is unlike validating webhooks, which get\ncalled in parallel.\nWhen designing your mutating webhooks, consider your latency requirements and tolerance. The more mutating webhooks there\nare in your cluster, the greater the chance of latency increases.\nhttps://kubernetes.io/docs/concepts/_print/\n\n592/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nConsider the following to reduce latency:\nConsolidate webhooks that perform a similar mutation on different objects.\nReduce the number of API calls made in the mutating webhook server logic.\nLimit the match conditions of each mutating webhook to reduce how many webhooks are triggered by a specific API request.\nConsolidate small webhooks into one server and configuration to help with ordering and organization.\n\nPrevent loops caused by competing controllers\nConsider any other components that run in your cluster that might conflict with the mutations that your webhook makes. For\nexample, if your webhook adds a label that a different controller removes, your webhook gets called again. This leads to a loop.\nTo detect these loops, try the following:\n1. Update your cluster audit policy to log audit events. Use the following parameters:\nlevel : RequestResponse\nverbs : [\"patch\"]\nomitStages : RequestReceived\n\nSet the audit rule to create events for the specific resources that your webhook mutates.\n2. Check your audit events for webhooks being reinvoked multiple times with the same patch being applied to the same object, or\nfor an object having a field updated and reverted multiple times.\n\nSet a small timeout value\nAdmission webhooks should evaluate as quickly as possible (typically in milliseconds), since they add to API request latency. Use a\nsmall timeout for webhooks.\nFor details, see Timeouts.\n\nUse a load balancer to ensure webhook availability\nAdmission webhooks should leverage some form of load-balancing to provide high availability and performance benefits. If a\nwebhook is running within the cluster, you can run multiple webhook backends behind a Service of type ClusterIP ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0625", "text": "Use a high-availability deployment model\nConsider your cluster's availability requirements when designing your webhook. For example, during node downtime or zonal\noutages, Kubernetes marks Pods as NotReady to allow load balancers to reroute traffic to available zones and nodes. These updates\nto Pods might trigger your mutating webhooks. Depending on the number of affected Pods, the mutating webhook server has a risk\nof timing out or causing delays in Pod processing. As a result, traffic won't get rerouted as quickly as you need.\nConsider situations like the preceding example when writing your webhooks. Exclude operations that are a result of Kubernetes\nresponding to unavoidable incidents.\n\nRequest filtering\nThis section provides recommendations for filtering which requests trigger specific webhooks. In summary, these are as follows:\nLimit the webhook scope to avoid system components and read-only requests.\nLimit webhooks to specific namespaces.\nUse match conditions to perform fine-grained request filtering.\nMatch all versions of an object.\n\nLimit the scope of each webhook\nAdmission webhooks are only called when an API request matches the corresponding webhook configuration. Limit the scope of\neach webhook to reduce unnecessary calls to the webhook server. Consider the following scope limitations:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n593/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAvoid matching objects in the kube-system namespace. If you run your own Pods in the kube-system namespace, use an\nobjectSelector to avoid mutating a critical workload.\nDon't mutate node leases, which exist as Lease objects in the kube-node-lease system namespace. Mutating node leases\nmight result in failed node upgrades. Only apply validation controls to Lease objects in this namespace if you're confident that\nthe controls won't put your cluster at risk.\nDon't mutate TokenReview or SubjectAccessReview objects. These are always read-only requests. Modifying these objects\nmight break your cluster.\nLimit each webhook to a specific namespace by using a namespaceSelector.\n\nFilter for specific requests by using match conditions\nAdmission controllers support multiple fields that you can use to match requests that meet specific criteria. For example, you can\nuse a namespaceSelector to filter for requests that target a specific namespace.\nFor more fine-grained request filtering, use the matchConditions field in your webhook configuration. This field lets you write\nmultiple CEL expressions that must evaluate to true for a request to trigger your admission webhook. Using matchConditions\nmight significantly reduce the number of calls to your webhook server.\nFor details, see Matching requests: matchConditions ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0626", "text": "Match all versions of an API\nBy default, admission webhooks run on any API versions that affect a specified resource. The matchPolicy field in the webhook\nconfiguration controls this behavior. Specify a value of Equivalent in the matchPolicy field or omit the field to allow the webhook\nto run on any API version.\nFor details, see Matching requests: matchPolicy .\n\nMutation scope and field considerations\nThis section provides recommendations for the scope of mutations and any special considerations for object fields. In summary,\nthese are as follows:\nPatch only the fields that you need to patch.\nDon't overwrite array values.\nAvoid side effects in mutations when possible.\nAvoid self-mutations.\nFail open and validate the final state.\nPlan for future field updates in later versions.\nPrevent webhooks from self-triggering.\nDon't change immutable objects.\n\nPatch only required fields\nAdmission webhook servers send HTTP responses to indicate what to do with a specific Kubernetes API request. This response is an\nAdmissionReview object. A mutating webhook can add specific fields to mutate before allowing admission by using the patchType\nfield and the patch field in the response. Ensure that you only modify the fields that require a change.\nFor example, consider a mutating webhook that's configured to ensure that web-server Deployments have at least three replicas.\nWhen a request to create a Deployment object matches your webhook configuration, the webhook should only update the value in\nthe spec.replicas field.\n\nDon't overwrite array values\nFields in Kubernetes object specifications might include arrays. Some arrays contain key:value pairs (like the envVar field in a\ncontainer specification), while other arrays are unkeyed (like the readinessGates field in a Pod specification). The order of values in\nan array field might matter in some situations. For example, the order of arguments in the args field of a container specification\nmight affect the container.\nConsider the following when modifying arrays:\nhttps://kubernetes.io/docs/concepts/_print/\n\n594/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhenever possible, use the add JSONPatch operation instead of replace to avoid accidentally replacing a required value.\nTreat arrays that don't use key:value pairs as sets.\nEnsure that the values in the field that you modify aren't required to be in a specific order.\nDon't overwrite existing key:value pairs unless absolutely necessary.\nUse caution when modifying label fields. An accidental modification might cause label selectors to break, resulting in\nunintended behavior."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0627", "text": "Avoid side effects\nEnsure that your webhooks operate only on the content of the AdmissionReview that's sent to them, and do not make out-of-band\nchanges. These additional changes, called side effects, might cause conflicts during admission if they aren't reconciled properly. The\n.webhooks[].sideEffects field should be set to None if a webhook doesn't have any side effect.\nIf side effects are required during the admission evaluation, they must be suppressed when processing an AdmissionReview object\nwith dryRun set to true , and the .webhooks[].sideEffects field should be set to NoneOnDryRun .\nFor details, see Side effects.\n\nAvoid self-mutations\nA webhook running inside the cluster might cause deadlocks for its own deployment if it is configured to intercept resources\nrequired to start its own Pods.\nFor example, a mutating admission webhook is configured to admit create Pod requests only if a certain label is set in the Pod (such\nas env: prod ). The webhook server runs in a Deployment that doesn't set the env label.\nWhen a node that runs the webhook server Pods becomes unhealthy, the webhook Deployment tries to reschedule the Pods to\nanother node. However, the existing webhook server rejects the requests since the env label is unset. As a result, the migration\ncannot happen.\nExclude the namespace where your webhook is running with a namespaceSelector .\n\nAvoid dependency loops\nDependency loops can occur in scenarios like the following:\nTwo webhooks check each other's Pods. If both webhooks become unavailable at the same time, neither webhook can start.\nYour webhook intercepts cluster add-on components, such as networking plugins or storage plugins, that your webhook\ndepends on. If both the webhook and the dependent add-on become unavailable, neither component can function.\nTo avoid these dependency loops, try the following:\nUse ValidatingAdmissionPolicies to avoid introducing dependencies.\nPrevent webhooks from validating or mutating other webhooks. Consider excluding specific namespaces from triggering your\nwebhook.\nPrevent your webhooks from acting on dependent add-ons by using an objectSelector."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0628", "text": "Fail open and validate the final state\nMutating admission webhooks support the failurePolicy configuration field. This field indicates whether the API server should\nadmit or reject the request if the webhook fails. Webhook failures might occur because of timeouts or errors in the server logic.\nBy default, admission webhooks set the failurePolicy field to Fail. The API server rejects a request if the webhook fails. However,\nrejecting requests by default might result in compliant requests being rejected during webhook downtime.\nLet your mutating webhooks \"fail open\" by setting the failurePolicy field to Ignore. Use a validating controller to check the state of\nrequests to ensure that they comply with your policies.\nThis approach has the following benefits:\nMutating webhook downtime doesn't affect compliant resources from deploying.\nPolicy enforcement occurs during validating admission control.\nMutating webhooks don't interfere with other controllers in the cluster.\nhttps://kubernetes.io/docs/concepts/_print/\n\n595/684\n\n11/7/25, 4:37 PM\n\nPlan for future updates to fields\n\nConcepts | Kubernetes\n\nIn general, design your webhooks under the assumption that Kubernetes APIs might change in a later version. Don't write a server\nthat takes the stability of an API for granted. For example, the release of sidecar containers in Kubernetes added a restartPolicy\nfield to the Pod API.\n\nPrevent your webhook from triggering itself\nMutating webhooks that respond to a broad range of API requests might unintentionally trigger themselves. For example, consider a\nwebhook that responds to all requests in the cluster. If you configure the webhook to create Event objects for every mutation, it'll\nrespond to its own Event object creation requests.\nTo avoid this, consider setting a unique label in any resources that your webhook creates. Exclude this label from your webhook\nmatch conditions.\n\nDon't change immutable objects\nSome Kubernetes objects in the API server can't change. For example, when you deploy a static Pod, the kubelet on the node creates\na mirror Pod in the API server to track the static Pod. However, changes to the mirror Pod don't propagate to the static Pod.\nDon't attempt to mutate these objects during admission. All mirror Pods have the kubernetes.io/config.mirror annotation. To\nexclude mirror Pods while reducing the security risk of ignoring an annotation, allow static Pods to only run in specific namespaces."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0629", "text": "Mutating webhook ordering and idempotence\nThis section provides recommendations for webhook order and designing idempotent webhooks. In summary, these are as follows:\nDon't rely on a specific order of execution.\nValidate mutations before admission.\nCheck for mutations being overwritten by other controllers.\nEnsure that the set of mutating webhooks is idempotent, not just the individual webhooks.\n\nDon't rely on mutating webhook invocation order\nMutating admission webhooks don't run in a consistent order. Various factors might change when a specific webhook is called. Don't\nrely on your webhook running at a specific point in the admission process. Other webhooks could still mutate your modified object.\nThe following recommendations might help to minimize the risk of unintended changes:\nValidate mutations before admission\nUse a reinvocation policy to observe changes to an object by other plugins and re-run the webhook as needed. For details, see\nReinvocation policy.\n\nEnsure that the mutating webhooks in your cluster are idempotent\nEvery mutating admission webhook should be idempotent. The webhook should be able to run on an object that it already modified\nwithout making additional changes beyond the original change.\nAdditionally, all of the mutating webhooks in your cluster should, as a collection, be idempotent. After the mutation phase of\nadmission control ends, every individual mutating webhook should be able to run on an object without making additional changes to\nthe object.\nDepending on your environment, ensuring idempotence at scale might be challenging. The following recommendations might help:\nUse validating admission controllers to verify the final state of critical workloads.\nTest your deployments in a staging cluster to see if any objects get modified multiple times by the same webhook.\nEnsure that the scope of each mutating webhook is specific and limited.\nThe following examples show idempotent mutation logic:\n1. For a create Pod request, set the field .spec.securityContext.runAsNonRoot of the Pod to true.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n596/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0630", "text": "2. For a create Pod request, if the field .spec.containers[].resources.limits of a container is not set, set default resource\nlimits.\n3. For a create Pod request, inject a sidecar container with name foo-sidecar if no container with the name foo-sidecar\nalready exists.\nIn these cases, the webhook can be safely reinvoked, or admit an object that already has the fields set.\nThe following examples show non-idempotent mutation logic:\n1. For a create Pod request, inject a sidecar container with name foo-sidecar suffixed with the current timestamp (such as\nfoo-sidecar-19700101-000000 ).\nReinvoking the webhook can result in the same sidecar being injected multiple times to a Pod, each time with a different\ncontainer name. Similarly, the webhook can inject duplicated containers if the sidecar already exists in a user-provided pod.\n2. For a create/update Pod request, reject if the Pod has label env set, otherwise add an env: prod label to the Pod.\nReinvoking the webhook will result in the webhook failing on its own output.\n3. For a create Pod request, append a sidecar container named foo-sidecar without checking whether a foo-sidecar\ncontainer exists.\nReinvoking the webhook will result in duplicated containers in the Pod, which makes the request invalid and rejected by the API\nserver.\n\nMutation testing and validation\nThis section provides recommendations for testing your mutating webhooks and validating mutated objects. In summary, these are\nas follows:\nTest webhooks in staging environments.\nAvoid mutations that violate validations.\nTest minor version upgrades for regressions and conflicts.\nValidate mutated objects before admission.\n\nTest webhooks in staging environments\nRobust testing should be a core part of your release cycle for new or updated webhooks. If possible, test any changes to your cluster\nwebhooks in a staging environment that closely resembles your production clusters. At the very least, consider using a tool like\nminikube or kind to create a small test cluster for webhook changes.\n\nEnsure that mutations don't violate validations\nYour mutating webhooks shouldn't break any of the validations that apply to an object before admission. For example, consider a\nmutating webhook that sets the default CPU request of a Pod to a specific value. If the CPU limit of that Pod is set to a lower value\nthan the mutated request, the Pod fails admission.\nTest every mutating webhook against the validations that run in your cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0631", "text": "Test minor version upgrades to ensure consistent behavior\nBefore upgrading your production clusters to a new minor version, test your webhooks and workloads in a staging environment.\nCompare the results to ensure that your webhooks continue to function as expected after the upgrade.\nAdditionally, use the following resources to stay informed about API changes:\nKubernetes release notes\nKubernetes blog\n\nValidate mutations before admission\nMutating webhooks run to completion before any validating webhooks run. There is no stable order in which mutations are applied\nto objects. As a result, your mutations could get overwritten by a mutating webhook that runs at a later time.\nhttps://kubernetes.io/docs/concepts/_print/\n\n597/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nAdd a validating admission controller like a ValidatingAdmissionWebhook or a ValidatingAdmissionPolicy to your cluster to ensure\nthat your mutations are still present. For example, consider a mutating webhook that inserts the restartPolicy: Always field to\nspecific init containers to make them run as sidecar containers. You could run a validating webhook to ensure that those init\ncontainers retained the restartPolicy: Always configuration after all mutations were completed.\nFor details, see the following resources:\nValidating Admission Policy\nValidatingAdmissionWebhooks\n\nMutating webhook deployment\nThis section provides recommendations for deploying your mutating admission webhooks. In summary, these are as follows:\nGradually roll out the webhook configuration and monitor for issues by namespace.\nLimit access to edit the webhook configuration resources.\nLimit access to the namespace that runs the webhook server, if the server is in-cluster.\n\nInstall and enable a mutating webhook\nWhen you're ready to deploy your mutating webhook to a cluster, use the following order of operations:\n1. Install the webhook server and start it.\n2. Set the failurePolicy field in the MutatingWebhookConfiguration manifest to Ignore. This lets you avoid disruptions caused\nby misconfigured webhooks.\n3. Set the namespaceSelector field in the MutatingWebhookConfiguration manifest to a test namespace.\n4. Deploy the MutatingWebhookConfiguration to your cluster.\nMonitor the webhook in the test namespace to check for any issues, then roll the webhook out to other namespaces. If the webhook\nintercepts an API request that it wasn't meant to intercept, pause the rollout and adjust the scope of the webhook configuration."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0632", "text": "Limit edit access to mutating webhooks\nMutating webhooks are powerful Kubernetes controllers. Use RBAC or another authorization mechanism to limit access to your\nwebhook configurations and servers. For RBAC, ensure that the following access is only available to trusted entities:\nVerbs: create, update, patch, delete, deletecollection\nAPI group: admissionregistration.k8s.io/v1\nAPI kind: MutatingWebhookConfigurations\nIf your mutating webhook server runs in the cluster, limit access to create or modify any resources in that namespace.\n\nExamples of good implementations\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nThe following projects are examples of \"good\" custom webhook server implementations. You can use them as a starting point when\ndesigning your own webhooks. Don't use these examples as-is; use them as a starting point and design your webhooks to run well in\nyour specific environment.\ncert-manager\n\nGatekeeper Open Policy Agent (OPA)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n598/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nUse webhooks for authentication and authorization\nLearn about MutatingAdmissionPolicies\nLearn about ValidatingAdmissionPolicies\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n599/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.8 - Good practices for Dynamic Resource Allocation\nas a Cluster Admin\nThis page describes good practices when configuring a Kubernetes cluster utilizing Dynamic Resource Allocation (DRA). These\ninstructions are for cluster administrators.\n\nSeparate permissions to DRA related APIs\nDRA is orchestrated through a number of different APIs. Use authorization tools (like RBAC, or another solution) to control access to\nthe right APIs depending on the persona of your user.\nIn general, DeviceClasses and ResourceSlices should be restricted to admins and the DRA drivers. Cluster operators that will be\ndeploying Pods with claims will need access to ResourceClaim and ResourceClaimTemplate APIs; both of these APIs are namespace\nscoped.\n\nDRA driver deployment and maintenance\nDRA drivers are third-party applications that run on each node of your cluster to interface with the hardware of that node and\nKubernetes' native DRA components. The installation procedure depends on the driver you choose, but is likely deployed as a\nDaemonSet to all or a selection of the nodes (using node selectors or similar mechanisms) in your cluster."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0633", "text": "Use drivers with seamless upgrade if available\nDRA drivers implement the kubeletplugin package interface. Your driver may support seamless upgrades by implementing a\nproperty of this interface that allows two versions of the same DRA driver to coexist for a short time. This is only available for kubelet\nversions 1.33 and above and may not be supported by your driver for heterogeneous clusters with attached nodes running older\nversions of Kubernetes - check your driver's documentation to be sure.\nIf seamless upgrades are available for your situation, consider using it to minimize scheduling delays when your driver updates.\nIf you cannot use seamless upgrades, during driver downtime for upgrades you may observe that:\nPods cannot start unless the claims they depend on were already prepared for use.\nCleanup after the last pod which used a claim gets delayed until the driver is available again. The pod is not marked as\nterminated. This prevents reusing the resources used by the pod for other pods.\nRunning pods will continue to run.\n\nConfirm your DRA driver exposes a liveness probe and utilize it\nYour DRA driver likely implements a gRPC socket for healthchecks as part of DRA driver good practices. The easiest way to utilize this\ngrpc socket is to configure it as a liveness probe for the DaemonSet deploying your DRA driver. Your driver's documentation or\ndeployment tooling may already include this, but if you are building your configuration separately or not running your DRA driver as\na Kubernetes pod, be sure that your orchestration tooling restarts the DRA driver on failed healthchecks to this grpc socket. Doing so\nwill minimize any accidental downtime of the DRA driver and give it more opportunities to self heal, reducing scheduling delays or\ntroubleshooting time.\n\nWhen draining a node, drain the DRA driver as late as possible\nThe DRA driver is responsible for unpreparing any devices that were allocated to Pods, and if the DRA driver is drained before Pods\nwith claims have been deleted, it will not be able to finalize its cleanup. If you implement custom drain logic for nodes, consider\nchecking that there are no allocated/reserved ResourceClaim or ResourceClaimTemplates before terminating the DRA driver itself.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n600/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0634", "text": "Monitor and tune components for higher load, especially in high\nscale environments\nControl plane component kube-scheduler and the internal ResourceClaim controller orchestrated by the component\nkube-controller-manager do the heavy lifting during scheduling of Pods with claims based on metadata stored in the DRA APIs.\nCompared to non-DRA scheduled Pods, the number of API server calls, memory, and CPU utilization needed by these components is\nincreased for Pods using DRA claims. In addition, node local components like the DRA driver and kubelet utilize DRA APIs to allocated\nthe hardware request at Pod sandbox creation time. Especially in high scale environments where clusters have many nodes, and/or\ndeploy many workloads that heavily utilize DRA defined resource claims, the cluster administrator should configure the relevant\ncomponents to anticipate the increased load.\nThe effects of mistuned components can have direct or snowballing affects causing different symptoms during the Pod lifecycle. If\nthe kube-scheduler component's QPS and burst configurations are too low, the scheduler might quickly identify a suitable node for\na Pod but take longer to bind the Pod to that node. With DRA, during Pod scheduling, the QPS and Burst parameters in the client-go\nconfiguration within kube-controller-manager are critical.\nThe specific values to tune your cluster to depend on a variety of factors like number of nodes/pods, rate of pod creation, churn,\neven in non-DRA environments; see the SIG Scalability README on Kubernetes scalability thresholds for more information. In scale\ntests performed against a DRA enabled cluster with 100 nodes, involving 720 long-lived pods (90% saturation) and 80 churn pods\n(10% churn, 10 times), with a job creation QPS of 10, kube-controller-manager QPS could be set to as low as 75 and Burst to 150 to\nmeet equivalent metric targets for non-DRA deployments. At this lower bound, it was observed that the client side rate limiter was\ntriggered enough to protect the API server from explosive burst but was high enough that pod startup SLOs were not impacted.\nWhile this is a good starting point, you can get a better idea of how to tune the different components that have the biggest effect on\nDRA performance for your deployment by monitoring the following metrics. For more information on all the stable metrics in\nKubernetes, see the Kubernetes Metrics Reference."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0635", "text": "kube-controller-manager metrics\nThe following metrics look closely at the internal ResourceClaim controller managed by the kube-controller-manager component.\nWorkqueue Add Rate: Monitor sum(rate(workqueue_adds_total{name=\"resource_claim\"}[5m])) to gauge how quickly items\nare added to the ResourceClaim controller.\nWorkqueue Depth: Track sum(workqueue_depth{endpoint=\"kube-controller-manager\", name=\"resource_claim\"}) to identify\nany backlogs in the ResourceClaim controller.\nWorkqueue Work Duration: Observe histogram_quantile(0.99,\nsum(rate(workqueue_work_duration_seconds_bucket{name=\"resource_claim\"}[5m])) by (le)) to understand the speed at\nwhich the ResourceClaim controller processes work.\nIf you are experiencing low Workqueue Add Rate, high Workqueue Depth, and/or high Workqueue Work Duration, this suggests the\ncontroller isn't performing optimally. Consider tuning parameters like QPS, burst, and CPU/memory configurations.\nIf you are experiencing high Workequeue Add Rate, high Workqueue Depth, but reasonable Workqueue Work Duration, this\nindicates the controller is processing work, but concurrency might be insufficient. Concurrency is hardcoded in the controller, so as a\ncluster administrator, you can tune for this by reducing the pod creation QPS, so the add rate to the resource claim workqueue is\nmore manageable.\n\nkube-scheduler metrics\nThe following scheduler metrics are high level metrics aggregating performance across all Pods scheduled, not just those using DRA.\nIt is important to note that the end-to-end metrics are ultimately influenced by the kube-controller-manager 's performance in\ncreating ResourceClaims from ResourceClainTemplates in deployments that heavily use ResourceClainTemplates.\nScheduler End-to-End Duration: Monitor histogram_quantile(0.99,\nsum(increase(scheduler_pod_scheduling_sli_duration_seconds_bucket[5m])) by (le)) .\n\nScheduler Algorithm Latency: Track histogram_quantile(0.99,\nsum(increase(scheduler_scheduling_algorithm_duration_seconds_bucket[5m])) by (le)) .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n601/684\n\n11/7/25, 4:37 PM\n\nkubelet metrics\n\nConcepts | Kubernetes\n\nWhen a Pod bound to a node must have a ResourceClaim satisfied, kubelet calls the NodePrepareResources and\nNodeUnprepareResources methods of the DRA driver. You can observe this behavior from the kubelet's point of view with the\nfollowing metrics.\nKubelet NodePrepareResources: Monitor histogram_quantile(0.99,\nsum(rate(dra_operations_duration_seconds_bucket{operation_name=\"PrepareResources\"}[5m])) by (le)) .\n\nKubelet NodeUnprepareResources: Track histogram_quantile(0.99,\nsum(rate(dra_operations_duration_seconds_bucket{operation_name=\"UnprepareResources\"}[5m])) by (le)) ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0636", "text": "DRA kubeletplugin operations\nDRA drivers implement the kubeletplugin package interface which surfaces its own metric for the underlying gRPC operation\nNodePrepareResources and NodeUnprepareResources . You can observe this behavior from the point of view of the internal\nkubeletplugin with the following metrics.\nDRA kubeletplugin gRPC NodePrepareResources operation: Observe histogram_quantile(0.99,\nsum(rate(dra_grpc_operations_duration_seconds_bucket{method_name=~\".*NodePrepareResources\"}[5m])) by (le)) .\n\nDRA kubeletplugin gRPC NodeUnprepareResources operation: Observe histogram_quantile(0.99,\nsum(rate(dra_grpc_operations_duration_seconds_bucket{method_name=~\".*NodeUnprepareResources\"}[5m])) by (le)) .\n\nWhat's next\nLearn more about DRA\nRead the Kubernetes Metrics Reference\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n602/684\n\n11/7/25, 4:37 PM\n\n11.9 - Logging Architecture\n\nConcepts | Kubernetes\n\nApplication logs can help you understand what is happening inside your application. The logs are particularly useful for debugging\nproblems and monitoring cluster activity. Most modern applications have some kind of logging mechanism. Likewise, container\nengines are designed to support logging. The easiest and most adopted logging method for containerized applications is writing to\nstandard output and standard error streams.\nHowever, the native functionality provided by a container engine or runtime is usually not enough for a complete logging solution.\nFor example, you may want to access your application's logs if a container crashes, a pod gets evicted, or a node dies.\nIn a cluster, logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called\ncluster-level logging.\nCluster-level logging architectures require a separate backend to store, analyze, and query logs. Kubernetes does not provide a\nnative storage solution for log data. Instead, there are many logging solutions that integrate with Kubernetes. The following sections\ndescribe how to handle and store logs on nodes.\n\nPod and container logs\nKubernetes captures logs from each container in a running Pod.\nThis example uses a manifest for a Pod with a container that writes text to the standard output stream, once per second.\ndebug/counter-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: counter\nspec:\ncontainers:\n- name: count\nimage: busybox:1.28\nargs: [/bin/sh, -c,\n'i=0; while true; do echo \"$i: $(date)\"; i=$((i+1)); sleep 1; done']\n\nTo run this pod, use the following command:\n\nkubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml\n\nThe output is:\n\npod/counter created\n\nTo fetch the logs, use the kubectl logs command, as follows:\n\nkubectl logs counter\n\nThe output is similar to:\n\n0: Fri Apr\n\n1 11:42:23 UTC 2022\n\n1: Fri Apr\n\n1 11:42:24 UTC 2022\n\n2: Fri Apr\n\n1 11:42:25 UTC 2022"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0637", "text": "https://kubernetes.io/docs/concepts/_print/\n\n603/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nYou can use kubectl logs --previous to retrieve logs from a previous instantiation of a container. If your pod has multiple\ncontainers, specify which container's logs you want to access by appending a container name to the command, with a -c flag, like\nso:\n\nkubectl logs counter -c count\n\nContainer log streams\nâ“˜ FEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)\n\nAs an alpha feature, the kubelet can split out the logs from the two standard streams produced by a container: standard output and\nstandard error. To use this behavior, you must enable the PodLogsQuerySplitStreams feature gate. With that feature gate enabled,\nKubernetes 1.34 allows access to these log streams directly via the Pod API. You can fetch a specific stream by specifying the stream\nname (either Stdout or Stderr ), using the stream query string. You must have access to read the log subresource of that Pod.\nTo demonstrate this feature, you can create a Pod that periodically writes text to both the standard output and error stream.\ndebug/counter-pod-err.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: counter-err\nspec:\ncontainers:\n- name: count\nimage: busybox:1.28\nargs: [/bin/sh, -c,\n'i=0; while true; do echo \"$i: $(date)\"; echo \"$i: err\" >&2 ; i=$((i+1)); sleep 1; done']\n\nTo run this pod, use the following command:\n\nkubectl apply -f https://k8s.io/examples/debug/counter-pod-err.yaml\n\nTo fetch only the stderr log stream, you can run:\n\nkubectl get --raw \"/api/v1/namespaces/default/pods/counter-err/log?stream=Stderr\"\n\nSee the kubectl logs documentation for more details.\n\nHow nodes handle container logs\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n604/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA container runtime handles and redirects any output generated to a containerized application's stdout and stderr streams.\nDifferent container runtimes implement this in different ways; however, the integration with the kubelet is standardized as the CRI\nlogging format.\nBy default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node, all\ncorresponding containers are also evicted, along with their logs.\nThe kubelet makes logs available to clients via a special feature of the Kubernetes API. The usual way to access this is by running\nkubectl logs .\n\nLog rotation\nâ“˜ FEATURE STATE: Kubernetes v1.21 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0638", "text": "The kubelet is responsible for rotating container logs and managing the logging directory structure. The kubelet sends this\ninformation to the container runtime (using CRI), and the runtime writes the container logs to the given location.\nYou can configure two kubelet configuration settings, containerLogMaxSize (default 10Mi) and containerLogMaxFiles (default 5),\nusing the kubelet configuration file. These settings let you configure the maximum size for each log file and the maximum number of\nfiles allowed for each container respectively.\nIn order to perform an efficient log rotation in clusters where the volume of the logs generated by the workload is large, kubelet also\nprovides a mechanism to tune how the logs are rotated in terms of how many concurrent log rotations can be performed and the\ninterval at which the logs are monitored and rotated as required. You can configure two kubelet configuration settings,\ncontainerLogMaxWorkers and containerLogMonitorInterval using the kubelet configuration file.\nWhen you run kubectl logs as in the basic logging example, the kubelet on the node handles the request and reads directly from\nthe log file. The kubelet returns the content of the log file.\nNote:\nOnly the contents of the latest log file are available through kubectl logs .\nFor example, if a Pod writes 40 MiB of logs and the kubelet rotates logs after 10 MiB, running kubectl logs returns at most\n10MiB of data.\n\nSystem component logs\nThere are two types of system components: those that typically run in a container, and those components directly involved in\nrunning containers. For example:\nThe kubelet and container runtime do not run in containers. The kubelet runs your containers (grouped together in pods)\nThe Kubernetes scheduler, controller manager, and API server run within pods (usually static Pods). The etcd component runs\nin the control plane, and most commonly also as a static pod. If your cluster uses kube-proxy, you typically run this as a\nDaemonSet .\n\nLog locations\nThe way that the kubelet and container runtime write logs depends on the operating system that the node uses:\nLinux\n\nWindows"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0639", "text": "On Linux nodes that use systemd, the kubelet and container runtime write to journald by default. You\nuse journalctl to read the systemd journal; for example: journalctl -u kubelet .\nIf systemd is not present, the kubelet and container runtime write to .log files in the /var/log\ndirectory. If you want to have logs written elsewhere, you can indirectly run the kubelet via a helper\ntool, kube-log-runner , and use that tool to redirect kubelet logs to a directory that you choose.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n605/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nBy default, kubelet directs your container runtime to write logs into directories within\n/var/log/pods .\nFor more information on kube-log-runner , read System Logs.\n\nFor Kubernetes cluster components that run in pods, these write to files inside the /var/log directory, bypassing the default\nlogging mechanism (the components do not write to the systemd journal). You can use Kubernetes' storage mechanisms to map\npersistent storage into the container that runs the component.\nKubelet allows changing the pod logs directory from default /var/log/pods to a custom path. This adjustment can be made by\nconfiguring the podLogsDir parameter in the kubelet's configuration file.\nCaution:\nIt's important to note that the default location /var/log/pods has been in use for an extended period and certain processes\nmight implicitly assume this path. Therefore, altering this parameter must be approached with caution and at your own risk.\nAnother caveat to keep in mind is that the kubelet supports the location being on the same disk as /var . Otherwise, if the\nlogs are on a separate filesystem from /var , then the kubelet will not track that filesystem's usage, potentially leading to\nissues if it fills up."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0640", "text": "For details about etcd and its logs, view the etcd documentation. Again, you can use Kubernetes' storage mechanisms to map\npersistent storage into the container that runs the component.\nNote:\nIf you deploy Kubernetes cluster components (such as the scheduler) to log to a volume shared from the parent node, you\nneed to consider and ensure that those logs are rotated. Kubernetes does not manage that log rotation.\nYour operating system may automatically implement some log rotation - for example, if you share the directory /var/log into\na static Pod for a component, node-level log rotation treats a file in that directory the same as a file written by any component\noutside Kubernetes.\nSome deploy tools account for that log rotation and automate it; others leave this as your responsibility.\n\nCluster-level logging architectures\nWhile Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can\nconsider. Here are some options:\nUse a node-level logging agent that runs on every node.\nInclude a dedicated sidecar container for logging in an application pod.\nPush logs directly to a backend from within an application.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n606/684\n\n11/7/25, 4:37 PM\n\nUsing a node logging agent\n\nConcepts | Kubernetes\n\nYou can implement cluster-level logging by including a node-level logging agent on each node. The logging agent is a dedicated tool\nthat exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log\nfiles from all of the application containers on that node.\nBecause the logging agent must run on every node, it is recommended to run the agent as a DaemonSet .\nNode-level logging creates only one agent per node and doesn't require any changes to the applications running on the node.\nContainers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for\naggregation.\n\nUsing a sidecar container with the logging agent\nYou can use a sidecar container in one of the following ways:\nThe sidecar container streams application logs to its own stdout .\nThe sidecar container runs a logging agent, which is configured to pick up logs from an application container.\n\nStreaming sidecar container"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0641", "text": "By having your sidecar containers write to their own stdout and stderr streams, you can take advantage of the kubelet and the\nlogging agent that already run on each node. The sidecar containers read logs from a file, a socket, or journald. Each sidecar\ncontainer prints a log to its own stdout or stderr stream.\nThis approach allows you to separate several log streams from different parts of your application, some of which can lack support\nfor writing to stdout or stderr . The logic behind redirecting logs is minimal, so it's not a significant overhead. Additionally,\nbecause stdout and stderr are handled by the kubelet, you can use built-in tools like kubectl logs .\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n607/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFor example, a pod runs a single container, and the container writes to two different log files using two different formats. Here's a\nmanifest for the Pod:\nadmin/logging/two-files-counter-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\nname: counter\nspec:\ncontainers:\n- name: count\nimage: busybox:1.28\nargs:\n- /bin/sh\n- -c\n- >\ni=0;\nwhile true;\ndo\necho \"$i: $(date)\" >> /var/log/1.log;\necho \"$(date) INFO $i\" >> /var/log/2.log;\ni=$((i+1));\nsleep 1;\ndone\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\nvolumes:\n- name: varlog\nemptyDir: {}\n\nIt is not recommended to write log entries with different formats to the same log stream, even if you managed to redirect both\ncomponents to the stdout stream of the container. Instead, you can create two sidecar containers. Each sidecar container could tail\na particular log file from a shared volume and then redirect the logs to its own stdout stream.\nHere's a manifest for a pod that has two sidecar containers:\nadmin/logging/two-files-counter-pod-streaming-sidecar.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n608/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: counter\nspec:\ncontainers:\n- name: count\nimage: busybox:1.28\nargs:\n- /bin/sh\n- -c\n- >\ni=0;\nwhile true;\ndo\necho \"$i: $(date)\" >> /var/log/1.log;\necho \"$(date) INFO $i\" >> /var/log/2.log;\ni=$((i+1));\nsleep 1;\ndone\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\n- name: count-log-1\nimage: busybox:1.28\nargs: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log']\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\n- name: count-log-2\nimage: busybox:1.28\nargs: [/bin/sh, -c, 'tail -n+1 -F /var/log/2.log']\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\nvolumes:\n- name: varlog\nemptyDir: {}\n\nNow when you run this pod, you can access each log stream separately by running the following commands:\n\nkubectl logs counter count-log-1\n\nThe output is similar to:"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0642", "text": "0: Fri Apr\n\n1 11:42:26 UTC 2022\n\n1: Fri Apr\n\n1 11:42:27 UTC 2022\n\n2: Fri Apr\n...\n\n1 11:42:28 UTC 2022\n\nkubectl logs counter count-log-2\n\nThe output is similar to:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n609/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFri Apr\nFri Apr\n\n1 11:42:29 UTC 2022 INFO 0\n1 11:42:30 UTC 2022 INFO 0\n\nFri Apr\n\n1 11:42:31 UTC 2022 INFO 0\n\n...\n\nIf you installed a node-level agent in your cluster, that agent picks up those log streams automatically without any further\nconfiguration. If you like, you can configure the agent to parse log lines depending on the source container.\nEven for Pods that only have low CPU and memory usage (order of a couple of millicores for cpu and order of several megabytes for\nmemory), writing logs to a file and then streaming them to stdout can double how much storage you need on the node. If you have\nan application that writes to a single file, it's recommended to set /dev/stdout as the destination rather than implement the\nstreaming sidecar container approach.\nSidecar containers can also be used to rotate log files that cannot be rotated by the application itself. An example of this approach is\na small container running logrotate periodically. However, it's more straightforward to use stdout and stderr directly, and leave\nrotation and retention policies to the kubelet.\n\nSidecar container with a logging agent\n\nIf the node-level logging agent is not flexible enough for your situation, you can create a sidecar container with a separate logging\nagent that you have configured specifically to run with your application.\nNote:\nUsing a logging agent in a sidecar container can lead to significant resource consumption. Moreover, you won't be able to\naccess those logs using kubectl logs because they are not controlled by the kubelet.\nHere are two example manifests that you can use to implement a sidecar container with a logging agent. The first manifest contains\na ConfigMap to configure fluentd.\nadmin/logging/fluentd-sidecar-config.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n610/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: fluentd-config\ndata:\nfluentd.conf: |\n<source>\ntype tail\nformat none\npath /var/log/1.log\npos_file /var/log/1.log.pos\ntag count.format1\n</source>\n<source>\ntype tail\nformat none\npath /var/log/2.log\npos_file /var/log/2.log.pos\ntag count.format2\n</source>\n<match **>\ntype google_cloud\n</match>"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0643", "text": "Note:\nIn the sample configurations, you can replace fluentd with any logging agent, reading from any source inside an application\ncontainer.\nThe second manifest describes a pod that has a sidecar container running fluentd. The pod mounts a volume where fluentd can pick\nup its configuration data.\nadmin/logging/two-files-counter-pod-agent-sidecar.yaml\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n611/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: counter\nspec:\ncontainers:\n- name: count\nimage: busybox:1.28\nargs:\n- /bin/sh\n- -c\n- >\ni=0;\nwhile true;\ndo\necho \"$i: $(date)\" >> /var/log/1.log;\necho \"$(date) INFO $i\" >> /var/log/2.log;\ni=$((i+1));\nsleep 1;\ndone\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\n- name: count-agent\nimage: registry.k8s.io/fluentd-gcp:1.30\nenv:\n- name: FLUENTD_ARGS\nvalue: -c /etc/fluentd-config/fluentd.conf\nvolumeMounts:\n- name: varlog\nmountPath: /var/log\n- name: config-volume\nmountPath: /etc/fluentd-config\nvolumes:\n- name: varlog\nemptyDir: {}\n- name: config-volume\nconfigMap:\nname: fluentd-config\n\nExposing logs directly from the application\n\nCluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes.\n\nWhat's next\nRead about Kubernetes system logs\nLearn about Traces For Kubernetes System Components\nLearn how to customise the termination message that Kubernetes records when a Pod fails\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n612/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.10 - Compatibility Version For Kubernetes Control\nPlane Components\nSince release v1.32, we introduced configurable version compatibility and emulation options to Kubernetes control plane\ncomponents to make upgrades safer by providing more control and increasing the granularity of steps available to cluster\nadministrators.\n\nEmulated Version\nThe emulation option is set by the --emulated-version flag of control plane components. It allows the component to emulate the\nbehavior (APIs, features, ...) of an earlier version of Kubernetes.\nWhen used, the capabilities available will match the emulated version:\nAny capabilities present in the binary version that were introduced after the emulation version will be unavailable.\nAny capabilities removed after the emulation version will be available.\nThis enables a binary from a particular Kubernetes release to emulate the behavior of a previous version with sufficient fidelity that\ninteroperability with other system components can be defined in terms of the emulated version.\nThe --emulated-version must be <= binaryVersion . See the help message of the --emulated-version flag for supported range of\nemulated versions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n613/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0644", "text": "11.11 - Metrics For Kubernetes System Components\nSystem component metrics can give a better look into what is happening inside them. Metrics are particularly useful for building\ndashboards and alerts.\nKubernetes components emit metrics in Prometheus format. This format is structured plain text, designed so that people and\nmachines can both read it.\n\nMetrics in Kubernetes\nIn most cases metrics are available on /metrics endpoint of the HTTP server. For components that don't expose endpoint by\ndefault, it can be enabled using --bind-address flag.\nExamples of those components:\nkube-controller-manager\nkube-proxy\nkube-apiserver\nkube-scheduler\nkubelet\nIn a production environment you may want to configure Prometheus Server or some other metrics scraper to periodically gather\nthese metrics and make them available in some kind of time series database.\nNote that kubelet also exposes metrics in /metrics/cadvisor , /metrics/resource and /metrics/probes endpoints. Those metrics\ndo not have the same lifecycle.\nIf your cluster uses RBAC, reading metrics requires authorization via a user, group or ServiceAccount with a ClusterRole that allows\naccessing /metrics . For example:\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: prometheus\nrules:\n- nonResourceURLs:\n- \"/metrics\"\nverbs:\n- get\n\nMetric lifecycle\nAlpha metric â†’ Beta metric â†’ Stable metric â†’ Deprecated metric â†’ Hidden metric â†’ Deleted metric\nAlpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.\nBeta metrics observe a looser API contract than its stable counterparts. No labels can be removed from beta metrics during their\nlifetime, however, labels can be added while the metric is in the beta stage.\nStable metrics are guaranteed to not change. This means:\nA stable metric without a deprecated signature will not be deleted or renamed\nA stable metric's type will not be modified\nDeprecated metrics are slated for deletion, but are still available for use. These metrics include an annotation about the version in\nwhich they became deprecated.\nFor example:\nBefore deprecation\nhttps://kubernetes.io/docs/concepts/_print/\n\n614/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n# HELP some_counter this counts things\n# TYPE some_counter counter\nsome_counter 0\n\nAfter deprecation\n# HELP some_counter (Deprecated since 1.15.0) this counts things\n# TYPE some_counter counter\nsome_counter 0"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0645", "text": "Hidden metrics are no longer published for scraping, but are still available for use. A deprecated metric becomes a hidden metric\nafter a period of time, based on its stability level:\nSTABLE metrics become hidden after a minimum of 3 releases or 9 months, whichever is longer.\nBETA metrics become hidden after a minimum of 1 release or 4 months, whichever is longer.\nALPHA metrics can be hidden or removed in the same release in which they are deprecated.\nTo use a hidden metric, you must enable it. For more details, refer to the Show hidden metrics section.\nDeleted metrics are no longer published and cannot be used.\n\nShow hidden metrics\nAs described above, admins can enable hidden metrics through a command-line flag on a specific binary. This intends to be used as\nan escape hatch for admins if they missed the migration of the metrics deprecated in the last release.\nThe flag show-hidden-metrics-for-version takes a version for which you want to show metrics deprecated in that release. The\nversion is expressed as x.y, where x is the major version, y is the minor version. The patch version is not needed even though a\nmetrics can be deprecated in a patch release, the reason for that is the metrics deprecation policy runs against the minor release.\nThe flag can only take the previous minor version as its value. If you want to show all metrics hidden in the previous release, you can\nset the show-hidden-metrics-for-version flag to the previous version. Using a version that is too old is not allowed because it\nviolates the metrics deprecation policy.\nFor example, let's assume metric A is deprecated in 1.29 . The version in which metric A becomes hidden depends on its stability\nlevel:\nIf metric A is ALPHA, it could be hidden in 1.29 .\nIf metric A is BETA, it will be hidden in 1.30 at the earliest. If you are upgrading to 1.30 and still need A , you must use the\ncommand-line flag --show-hidden-metrics-for-version=1.29 .\nIf metric A is STABLE, it will be hidden in 1.32 at the earliest. If you are upgrading to 1.32 and still need A , you must use\nthe command-line flag --show-hidden-metrics-for-version=1.31 ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0646", "text": "Component metrics\nkube-controller-manager metrics\nController manager metrics provide important insight into the performance and health of the controller manager. These metrics\ninclude common Go language runtime metrics such as go_routine count and controller specific metrics such as etcd request\nlatencies or Cloudprovider (AWS, GCE, OpenStack) API latencies that can be used to gauge the health of a cluster.\nStarting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations for GCE, AWS, Vsphere and\nOpenStack. These metrics can be used to monitor health of persistent volume operations.\nFor example, for GCE these metrics are called:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n615/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ncloudprovider_gce_api_request_duration_seconds { request = \"instance_list\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_insert\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"disk_delete\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"attach_disk\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"detach_disk\"}\ncloudprovider_gce_api_request_duration_seconds { request = \"list_disk\"}\n\nkube-scheduler metrics\nâ“˜ FEATURE STATE: Kubernetes v1.21 [beta]\n\nThe scheduler exposes optional metrics that reports the requested resources and the desired limits of all running pods. These\nmetrics can be used to build capacity planning dashboards, assess current or historical scheduling limits, quickly identify workloads\nthat cannot schedule due to lack of resources, and compare actual usage to the pod's request.\nThe kube-scheduler identifies the resource requests and limits configured for each Pod; when either a request or limit is non-zero,\nthe kube-scheduler reports a metrics timeseries. The time series is labelled by:\nnamespace\npod name\nthe node where the pod is scheduled or an empty string if not yet scheduled\npriority\nthe assigned scheduler for that pod\nthe name of the resource (for example, cpu )\nthe unit of the resource if known (for example, cores )\nOnce a pod reaches completion (has a restartPolicy of Never or OnFailure and is in the Succeeded or Failed pod phase, or\nhas been deleted and all containers have a terminated state) the series is no longer reported since the scheduler is now free to\nschedule other pods to run. The two metrics are called kube_pod_resource_request and kube_pod_resource_limit .\nThe metrics are exposed at the HTTP endpoint /metrics/resources . They require authorization for the /metrics/resources\nendpoint, usually granted by a ClusterRole with the get verb for the /metrics/resources non-resource URL.\nOn Kubernetes 1.21 you must use the --show-hidden-metrics-for-version=1.20 flag to expose these alpha stability metrics."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0647", "text": "kubelet Pressure Stall Information (PSI) metrics\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta]\n\nAs a beta feature, Kubernetes lets you configure kubelet to collect Linux kernel Pressure Stall Information (PSI) for CPU, memory and\nI/O usage. The information is collected at node, pod and container level. The metrics are exposed at the /metrics/cadvisor\nendpoint with the following names:\ncontainer_pressure_cpu_stalled_seconds_total\ncontainer_pressure_cpu_waiting_seconds_total\ncontainer_pressure_memory_stalled_seconds_total\ncontainer_pressure_memory_waiting_seconds_total\ncontainer_pressure_io_stalled_seconds_total\ncontainer_pressure_io_waiting_seconds_total\n\nThis feature is enabled by default, by setting the KubeletPSI feature gate. The information is also exposed in the Summary API.\nYou can learn how to interpret the PSI metrics in Understand PSI Metrics.\n\nRequirements\nPressure Stall Information requires:\nLinux kernel versions 4.20 or later.\nhttps://kubernetes.io/docs/concepts/_print/\n\n616/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\ncgroup v2\n\nDisabling metrics\nYou can explicitly turn off metrics via command line flag --disabled-metrics . This may be desired if, for example, a metric is\ncausing a performance problem. The input is a list of disabled metrics (i.e. --disabled-metrics=metric1,metric2 ).\n\nMetric cardinality enforcement\nMetrics with unbounded dimensions could cause memory issues in the components they instrument. To limit resource use, you can\nuse the --allow-metric-labels command line option to dynamically configure an allow-list of label values for a metric.\nIn alpha stage, the flag can only take in a series of mappings as metric label allow-list. Each mapping is of the format <metric_name>,\n<label_name>=<allowed_labels> where <allowed_labels> is a comma-separated list of acceptable label names.\nThe overall format looks like:\n--allow-metric-labels <metric_name>,<label_name>='<allow_value1>, <allow_value2>...', <metric_name2>,<label_name>='<\n\nHere is an example:\n--allow-metric-labels number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_me\n\nIn addition to specifying this from the CLI, this can also be done within a configuration file. You can specify the path to that\nconfiguration file using the --allow-metric-labels-manifest command line argument to a component. Here's an example of the\ncontents of that configuration file:\n\n\"metric1,label2\": \"v1,v2,v3\"\n\"metric2,label1\": \"v1,v2,v3\"\n\nAdditionally, the cardinality_enforcement_unexpected_categorizations_total meta-metric records the count of unexpected\ncategorizations during cardinality enforcement, that is, whenever a label value is encountered that is not allowed with respect to the\nallow-list constraints."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0648", "text": "What's next\nRead about the Prometheus text format for metrics\nSee the list of stable Kubernetes metrics\nRead about the Kubernetes deprecation policy\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n617/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.12 - Metrics for Kubernetes Object States\nkube-state-metrics, an add-on agent to generate and expose cluster-level metrics.\nThe state of Kubernetes objects in the Kubernetes API can be exposed as metrics. An add-on agent called kube-state-metrics can\nconnect to the Kubernetes API server and expose a HTTP endpoint with metrics generated from the state of individual objects in the\ncluster. It exposes various information about the state of objects like labels and annotations, startup and termination times, status\nor the phase the object currently is in. For example, containers running in pods create a kube_pod_container_info metric. This\nincludes the name of the container, the name of the pod it is part of, the namespace the pod is running in, the name of the\ncontainer image, the ID of the image, the image name from the spec of the container, the ID of the running container and the ID of\nthe pod as labels.\nðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. More information\n\nAn external component that is able and capable to scrape the endpoint of kube-state-metrics (for example via Prometheus) can now\nbe used to enable the following use cases.\n\nExample: using metrics from kube-state-metrics to query the\ncluster state\nMetric series generated by kube-state-metrics are helpful to gather further insights into the cluster, as they can be used for\nquerying.\nIf you use Prometheus or another tool that uses the same query language, the following PromQL query returns the number of pods\nthat are not ready:\ncount(kube_pod_status_ready{condition=\"false\"}) by (namespace, pod)\n\nExample: alerting based on from kube-state-metrics\nMetrics generated from kube-state-metrics also allow for alerting on issues in the cluster.\nIf you use Prometheus or a similar tool that uses the same alert rule language, the following alert will fire if there are pods that have\nbeen in a Terminating state for more than 5 minutes:\n\ngroups:\n- name: Pod state\nrules:\n- alert: PodsBlockedInTerminatingState\nexpr: count(kube_pod_deletion_timestamp) by (namespace, pod) * count(kube_pod_status_reason{reason=\"NodeLost\"} =\nfor: 5m\nlabels:\nseverity: page\nannotations:\nsummary: Pod {{$labels.namespace}}/{{$labels.pod}} blocked in Terminating state.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n618/684\n\n11/7/25, 4:37 PM\n\n11.13 - System Logs\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0649", "text": "System component logs record events happening in cluster, which can be very useful for debugging. You can configure log verbosity\nto see more or less detail. Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing stepby-step traces of events (like HTTP access logs, pod state changes, controller actions, or scheduler decisions).\n\nWarning:\nIn contrast to the command line flags described here, the log output itself does not fall under the Kubernetes API stability\nguarantees: individual log entries and their formatting may change from one release to the next!\n\nKlog\nklog is the Kubernetes logging library. klog generates log messages for the Kubernetes system components.\nKubernetes is in the process of simplifying logging in its components. The following klog command line flags are deprecated starting\nwith Kubernetes v1.23 and removed in Kubernetes v1.26:\n--add-dir-header\n--alsologtostderr\n--log-backtrace-at\n--log-dir\n--log-file\n--log-file-max-size\n--logtostderr\n--one-output\n--skip-headers\n--skip-log-headers\n--stderrthreshold\n\nOutput will always be written to stderr, regardless of the output format. Output redirection is expected to be handled by the\ncomponent which invokes a Kubernetes component. This can be a POSIX shell or a tool like systemd.\nIn some cases, for example a distroless container or a Windows system service, those options are not available. Then the kube-logrunner binary can be used as wrapper around a Kubernetes component to redirect output. A prebuilt binary is included in several\nKubernetes base images under its traditional name as /go-runner and as kube-log-runner in server and node release archives.\nThis table shows how kube-log-runner invocations correspond to shell redirection:\nPOSIX shell (such as\nbash)\n\nkube-log-runner <options> <cmd>\n\nMerge stderr and stdout, write to\nstdout\n\n2>&1\n\nkube-log-runner (default behavior)\n\nRedirect both into log file\n\n1>>/tmp/log 2>&1\n\nkube-log-runner -log-file=/tmp/log\n\nCopy into log file and to stdout\n\n2>&1 | tee -a\n\nkube-log-runner -log-file=/tmp/log -also-stdout\n\nUsage\n\n/tmp/log\n\nRedirect only stdout into log file\n\n>/tmp/log\n\nkube-log-runner -log-file=/tmp/log -redirectstderr=false\n\nKlog output\nAn example of the traditional klog native format:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n619/684\n\n11/7/25, 4:37 PM\n\nI1025 00:15:15.525108\n\nConcepts | Kubernetes\n\n1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f\n\nThe message string may contain line breaks:\nI1025 00:15:15.525108\n\n1 example.go:79] This is a message\n\nwhich has a line break.\n\nStructured Logging\nâ“˜ FEATURE STATE: Kubernetes v1.23 [beta]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0650", "text": "Warning:\nMigration to structured log messages is an ongoing process. Not all log messages are structured in this version. When parsing\nlog files, you must also handle unstructured log messages.\nLog formatting and value serialization are subject to change.\n\nStructured logging introduces a uniform structure in log messages allowing for programmatic extraction of information. You can\nstore and process structured logs with less effort and cost. The code which generates a log message determines whether it uses the\ntraditional unstructured klog output or structured logging.\nThe default formatting of structured log messages is as text, with a format that is backward compatible with traditional klog:\n<klog header> \"<message>\" <key1>=\"<value1>\" <key2>=\"<value2>\" ...\n\nExample:\nI1025 00:15:15.525108\n\n1 controller_utils.go:116] \"Pod status updated\" pod=\"kube-system/kubedns\" status=\"ready\"\n\nStrings are quoted. Other values are formatted with %+v , which may cause log messages to continue on the next line depending on\nthe data.\nI1025 00:15:15.525108\nsecond line.}\n\n1 example.go:116] \"Example\" data=\"This is text with a line break\\nand \\\"quotation marks\\\n\nContextual Logging\nâ“˜ FEATURE STATE: Kubernetes v1.30 [beta]\n\nContextual logging builds on top of structured logging. It is primarily about how developers use logging calls: code based on that\nconcept is more flexible and supports additional use cases as described in the Contextual Logging KEP.\nIf developers use additional functions like WithValues or WithName in their components, then log entries contain additional\ninformation that gets passed into functions by their caller.\nFor Kubernetes 1.34, this is gated behind the ContextualLogging feature gate and is enabled by default. The infrastructure for this\nwas added in 1.24 without modifying components. The component-base/logs/example command demonstrates how to use the new\nlogging calls and how a component behaves that supports contextual logging.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n620/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n$ cd $GOPATH/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/\n$ go run . --help\n...\n--feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental fea\nAllAlpha=true|false (ALPHA - default=false)\nAllBeta=true|false (BETA - default=false)\nContextualLogging=true|false (BETA - default=true)\n$ go run . --feature-gates ContextualLogging=true\n...\nI0222 15:13:31.645988 197901 example.go:54] \"runtime\" logger=\"example.myname\" foo=\"bar\" duration=\"1m0s\"\nI0222 15:13:31.646007 197901 example.go:55] \"another runtime\" logger=\"example\" foo=\"bar\" duration=\"1h0m0s\" duration"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0651", "text": "The logger key and foo=\"bar\" were added by the caller of the function which logs the runtime message and duration=\"1m0s\"\nvalue, without having to modify that function.\nWith contextual logging disable, WithValues and WithName do nothing and log calls go through the global klog logger. Therefore\nthis additional information is not in the log output anymore:\n\n$ go run . --feature-gates ContextualLogging=false\n...\nI0222 15:14:40.497333\n\n198174 example.go:54] \"runtime\" duration=\"1m0s\"\n\nI0222 15:14:40.497346\n\n198174 example.go:55] \"another runtime\" duration=\"1h0m0s\" duration=\"1m0s\"\n\nJSON log format\nâ“˜ FEATURE STATE: Kubernetes v1.19 [alpha]\n\nWarning:\nJSON output does not support many standard klog flags. For list of unsupported klog flags, see the Command line tool\nreference.\nNot all logs are guaranteed to be written in JSON format (for example, during process start). If you intend to parse logs, make\nsure you can handle log lines that are not JSON as well.\nField names and JSON serialization are subject to change.\n\nThe --logging-format=json flag changes the format of logs from klog native format to JSON format. Example of JSON log format\n(pretty printed):\n\n{\n\"ts\": 1580306777.04728,\n\"v\": 4,\n\"msg\": \"Pod status updated\",\n\"pod\":{\n\"name\": \"nginx-1\",\n\"namespace\": \"default\"\n},\n\"status\": \"ready\"\n}\n\nKeys with special meaning:\nts\n\n- timestamp as Unix time (required, float)\n\nv\n\n- verbosity (only for info and not for error messages, int)\n\nerr\n\n- error string (optional, string)\n\nmsg\n\n- message (required, string)\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n621/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nList of components currently supporting JSON format:\nkube-controller-manager\nkube-apiserver\nkube-scheduler\nkubelet\n\nLog verbosity level\nThe -v flag controls log verbosity. Increasing the value increases the number of logged events. Decreasing the value decreases the\nnumber of logged events. Increasing verbosity settings logs increasingly less severe events. A verbosity setting of 0 logs only critical\nevents."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0652", "text": "Log location\nThere are two types of system components: those that run in a container and those that do not run in a container. For example:\nThe Kubernetes scheduler and kube-proxy run in a container.\nThe kubelet and container runtime do not run in containers.\nOn machines with systemd, the kubelet and container runtime write to journald. Otherwise, they write to .log files in the\n/var/log directory. System components inside containers always write to .log files in the /var/log directory, bypassing the\ndefault logging mechanism. Similar to the container logs, you should rotate system component logs in the /var/log directory. In\nKubernetes clusters created by the kube-up.sh script, log rotation is configured by the logrotate tool. The logrotate tool rotates\nlogs daily, or once the log size is greater than 100MB.\n\nLog query\nâ“˜ FEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: false)\n\nTo help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows viewing logs of services running on the\nnode. To use the feature, ensure that the NodeLogQuery feature gate is enabled for that node, and that the kubelet configuration\noptions enableSystemLogHandler and enableSystemLogQuery are both set to true. On Linux the assumption is that service logs are\navailable via journald. On Windows the assumption is that service logs are available in the application log provider. On both\noperating systems, logs are also available by reading files within /var/log/ .\nProvided you are authorized to interact with node objects, you can try out this feature on all your nodes or just a subset. Here is an\nexample to retrieve the kubelet service logs from a node:\n\n# Fetch kubelet logs from a node named node-1.example\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet\"\n\nYou can also fetch files, provided that the files are in a directory that the kubelet allows for log fetches. For example, you can fetch a\nlog from /var/log on a Linux node:\n\nkubectl get --raw \"/api/v1/nodes/<insert-node-name-here>/proxy/logs/?query=/<insert-log-file-name-here>\"\n\nThe kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given system service is writing logs to the\noperating system's native logger like journald or to a log file in /var/log/ . The heuristics first checks the native logger and if that is\nnot available attempts to retrieve the first logs from /var/log/<servicename> or /var/log/<servicename>.log or\n/var/log/<servicename>/<servicename>.log .\nThe complete list of options that can be used are:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n622/684"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0653", "text": "11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOption\n\nDescription\n\nboot\n\nboot show messages from a specific system boot\n\npattern\n\npattern filters log entries by the provided PERL-compatible regular expression\n\nquery\n\nquery specifies services(s) or files from which to return logs (required)\n\nsinceTime\n\nan RFC3339 timestamp from which to show logs (inclusive)\n\nuntilTime\n\nan RFC3339 timestamp until which to show logs (inclusive)\n\ntailLines\n\nspecify how many lines from the end of the log to retrieve; the default is to fetch the whole log\n\nExample of a more complex query:\n\n# Fetch kubelet logs from a node named node-1.example that have the word \"error\"\nkubectl get --raw \"/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&pattern=error\"\n\nWhat's next\nRead about the Kubernetes Logging Architecture\nRead about Structured Logging\nRead about Contextual Logging\nRead about deprecation of klog flags\nRead about the Conventions for logging severity\nRead about Log Query\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n623/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.14 - Traces For Kubernetes System Components\nâ“˜ FEATURE STATE: Kubernetes v1.27 [beta]\n\nSystem component traces record the latency of and relationships between operations in the cluster.\nKubernetes components emit traces using the OpenTelemetry Protocol with the gRPC exporter and can be collected and routed to\ntracing backends using an OpenTelemetry Collector.\n\nTrace Collection\nKubernetes components have built-in gRPC exporters for OTLP to export traces, either with an OpenTelemetry Collector, or without\nan OpenTelemetry Collector.\nFor a complete guide to collecting traces and using the collector, see Getting Started with the OpenTelemetry Collector. However,\nthere are a few things to note that are specific to Kubernetes components.\nBy default, Kubernetes components export traces using the grpc exporter for OTLP on the IANA OpenTelemetry port, 4317. As an\nexample, if the collector is running as a sidecar to a Kubernetes component, the following receiver configuration will collect spans\nand log them to standard output:\n\nreceivers:\notlp:\nprotocols:\ngrpc:\nexporters:\n# Replace this exporter with the exporter for your backend\nexporters:\ndebug:\nverbosity: detailed\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [debug]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0654", "text": "To directly emit traces to a backend without utilizing a collector, specify the endpoint field in the Kubernetes tracing configuration\nfile with the desired trace backend address. This method negates the need for a collector and simplifies the overall structure.\nFor trace backend header configuration, including authentication details, environment variables can be used with\nOTEL_EXPORTER_OTLP_HEADERS , see OTLP Exporter Configuration.\nAdditionally, for trace resource attribute configuration such as Kubernetes cluster name, namespace, Pod name, etc., environment\nvariables can also be used with OTEL_RESOURCE_ATTRIBUTES , see OTLP Kubernetes Resource.\n\nComponent traces\nkube-apiserver traces\nThe kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests to webhooks, etcd, and re-entrant\nrequests. It propagates the W3C Trace Context with outgoing requests but does not make use of the trace context attached to\nincoming requests, as the kube-apiserver is often a public endpoint.\n\nEnabling tracing in the kube-apiserver\nTo enable tracing, provide the kube-apiserver with a tracing configuration file with --tracing-config-file=<path-to-config> . This\nis an example config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:\nhttps://kubernetes.io/docs/concepts/_print/\n\n624/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\napiVersion: apiserver.config.k8s.io/v1\nkind: TracingConfiguration\n# default value\n#endpoint: localhost:4317\nsamplingRatePerMillion: 100\n\nFor more information about the TracingConfiguration struct, see API server config API (v1).\n\nkubelet traces\nâ“˜ FEATURE STATE: Kubernetes v1.34 [stable] (enabled by default: true)\n\nThe kubelet CRI interface and authenticated http servers are instrumented to generate trace spans. As with the apiserver, the\nendpoint and sampling rate are configurable. Trace context propagation is also configured. A parent span's sampling decision is\nalways respected. A provided tracing configuration sampling rate will apply to spans without a parent. Enabled without a configured\nendpoint, the default OpenTelemetry Collector receiver address of \"localhost:4317\" is set.\n\nEnabling tracing in the kubelet\nTo enable tracing, apply the tracing configuration. This is an example snippet of a kubelet config that records spans for 1 in 10000\nrequests, and uses the default OpenTelemetry endpoint:\n\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\ntracing:\n# default value\n#endpoint: localhost:4317\nsamplingRatePerMillion: 100"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0655", "text": "If the samplingRatePerMillion is set to one million ( 1000000 ), then every span will be sent to the exporter.\nThe kubelet in Kubernetes v1.34 collects spans from the garbage collection, pod synchronization routine as well as every gRPC\nmethod. The kubelet propagates trace context with gRPC requests so that container runtimes with trace instrumentation, such as\nCRI-O and containerd, can associate their exported spans with the trace context from the kubelet. The resulting traces will have\nparent-child links between kubelet and container runtime spans, providing helpful context when debugging node issues.\nPlease note that exporting spans always comes with a small performance overhead on the networking and CPU side, depending on\nthe overall configuration of the system. If there is any issue like that in a cluster which is running with tracing enabled, then mitigate\nthe problem by either reducing the samplingRatePerMillion or disabling tracing completely by removing the configuration.\n\nStability\nTracing instrumentation is still under active development, and may change in a variety of ways. This includes span names, attached\nattributes, instrumented endpoints, etc. Until this feature graduates to stable, there are no guarantees of backwards compatibility\nfor tracing instrumentation.\n\nWhat's next\nRead about Getting Started with the OpenTelemetry Collector\nRead about OTLP Exporter Configuration\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n625/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.15 - Proxies in Kubernetes\nThis page explains proxies used with Kubernetes."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0656", "text": "Proxies\nThere are several different proxies you may encounter when using Kubernetes:\n1. The kubectl proxy:\nruns on a user's desktop or in a pod\nproxies from a localhost address to the Kubernetes apiserver\nclient to proxy uses HTTP\nproxy to apiserver uses HTTPS\nlocates apiserver\nadds authentication headers\n2. The apiserver proxy:\nis a bastion built into the apiserver\nconnects a user outside of the cluster to cluster IPs which otherwise might not be reachable\nruns in the apiserver processes\nclient to proxy uses HTTPS (or http if apiserver so configured)\nproxy to target may use HTTP or HTTPS as chosen by proxy using available information\ncan be used to reach a Node, Pod, or Service\ndoes load balancing when used to reach a Service\n3. The kube proxy:\nruns on each node\nproxies UDP, TCP and SCTP\ndoes not understand HTTP\nprovides load balancing\nis only used to reach services\n4. A Proxy/Load-balancer in front of apiserver(s):\nexistence and implementation varies from cluster to cluster (e.g. nginx)\nsits between all clients and one or more apiservers\nacts as load balancer if there are several apiservers.\n5. Cloud Load Balancers on external services:\nare provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)\nare created automatically when the Kubernetes service has type LoadBalancer\nusually supports UDP/TCP only\nSCTP support is up to the load balancer implementation of the cloud provider\nimplementation varies by cloud provider.\nKubernetes users will typically not need to worry about anything other than the first two types. The cluster admin will typically\nensure that the latter types are set up correctly.\n\nRequesting redirects\nProxies have replaced redirect capabilities. Redirects have been deprecated.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n626/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.16 - API Priority and Fairness\nâ“˜ FEATURE STATE: Kubernetes v1.29 [stable]"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0657", "text": "Controlling the behavior of the Kubernetes API server in an overload situation is a key task for cluster administrators. The\nkube-apiserver has some controls available (i.e. the --max-requests-inflight and --max-mutating-requests-inflight commandline flags) to limit the amount of outstanding work that will be accepted, preventing a flood of inbound requests from overloading\nand potentially crashing the API server, but these flags are not enough to ensure that the most important requests get through in a\nperiod of high traffic.\nThe API Priority and Fairness feature (APF) is an alternative that improves upon aforementioned max-inflight limitations. APF\nclassifies and isolates requests in a more fine-grained way. It also introduces a limited amount of queuing, so that no requests are\nrejected in cases of very brief bursts. Requests are dispatched from queues using a fair queuing technique so that, for example, a\npoorly-behaved controller need not starve others (even at the same priority level).\nThis feature is designed to work well with standard controllers, which use informers and react to failures of API requests with\nexponential back-off, and other clients that also work this way.\nCaution:\nSome requests classified as \"long-running\"â€”such as remote command execution or log tailingâ€”are not subject to the API\nPriority and Fairness filter. This is also true for the --max-requests-inflight flag without the API Priority and Fairness feature\nenabled. API Priority and Fairness does apply to watch requests. When API Priority and Fairness is disabled, watch requests\nare not subject to the --max-requests-inflight limit.\n\nEnabling/Disabling API Priority and Fairness\nThe API Priority and Fairness feature is controlled by a command-line flag and is enabled by default. See Options for a general\nexplanation of the available kube-apiserver command-line options and how to enable and disable them. The name of the commandline option for APF is \"--enable-priority-and-fairness\". This feature also involves an API Group with: (a) a stable v1 version,\nintroduced in 1.29, and enabled by default (b) a v1beta3 version, enabled by default, and deprecated in v1.29. You can disable the\nAPI group beta version v1beta3 by adding the following command-line flags to your kube-apiserver invocation:\n\nkube-apiserver \\\n--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \\\n# â€¦and other flags as usual\n\nThe command-line flag --enable-priority-and-fairness=false will disable the API Priority and Fairness feature."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0658", "text": "Recursive server scenarios\nAPI Priority and Fairness must be used carefully in recursive server scenarios. These are scenarios in which some server A, while\nserving a request, issues a subsidiary request to some server B. Perhaps server B might even make a further subsidiary call back to\nserver A. In situations where Priority and Fairness control is applied to both the original request and some subsidiary ones(s), no\nmatter how deep in the recursion, there is a danger of priority inversions and/or deadlocks.\nOne example of recursion is when the kube-apiserver issues an admission webhook call to server B, and while serving that call,\nserver B makes a further subsidiary request back to the kube-apiserver . Another example of recursion is when an APIService\nobject directs the kube-apiserver to delegate requests about a certain API group to a custom external server B (this is one of the\nthings called \"aggregation\").\nWhen the original request is known to belong to a certain priority level, and the subsidiary controlled requests are classified to\nhigher priority levels, this is one possible solution. When the original requests can belong to any priority level, the subsidiary\ncontrolled requests have to be exempt from Priority and Fairness limitation. One way to do that is with the objects that configure\nclassification and handling, discussed below. Another way is to disable Priority and Fairness on server B entirely, using the\ntechniques discussed above. A third way, which is the simplest to use when server B is not kube-apiserver , is to build server B with\nPriority and Fairness disabled in the code.\nhttps://kubernetes.io/docs/concepts/_print/\n\n627/684\n\n11/7/25, 4:37 PM\n\nConcepts\n\nConcepts | Kubernetes\n\nThere are several distinct features involved in the API Priority and Fairness feature. Incoming requests are classified by attributes of\nthe request using FlowSchemas, and assigned to priority levels. Priority levels add a degree of isolation by maintaining separate\nconcurrency limits, so that requests assigned to different priority levels cannot starve each other. Within a priority level, a fairqueuing algorithm prevents requests from different flows from starving each other, and allows for requests to be queued to prevent\nbursty traffic from causing failed requests when the average load is acceptably low."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0659", "text": "Priority Levels\nWithout APF enabled, overall concurrency in the API server is limited by the kube-apiserver flags --max-requests-inflight and -max-mutating-requests-inflight . With APF enabled, the concurrency limits defined by these flags are summed and then the sum is\ndivided up among a configurable set of priority levels. Each incoming request is assigned to a single priority level, and each priority\nlevel will only dispatch as many concurrent requests as its particular limit allows.\nThe default configuration, for example, includes separate priority levels for leader-election requests, requests from built-in\ncontrollers, and requests from Pods. This means that an ill-behaved Pod that floods the API server with requests cannot prevent\nleader election or actions by the built-in controllers from succeeding.\nThe concurrency limits of the priority levels are periodically adjusted, allowing under-utilized priority levels to temporarily lend\nconcurrency to heavily-utilized levels. These limits are based on nominal limits and bounds on how much concurrency a priority level\nmay lend and how much it may borrow, all derived from the configuration objects mentioned below.\n\nSeats Occupied by a Request\nThe above description of concurrency management is the baseline story. Requests have different durations but are counted equally\nat any given moment when comparing against a priority level's concurrency limit. In the baseline story, each request occupies one\nunit of concurrency. The word \"seat\" is used to mean one unit of concurrency, inspired by the way each passenger on a train or\naircraft takes up one of the fixed supply of seats.\nBut some requests take up more than one seat. Some of these are list requests that the server estimates will return a large number\nof objects. These have been found to put an exceptionally heavy burden on the server. For this reason, the server estimates the\nnumber of objects that will be returned and considers the request to take a number of seats that is proportional to that estimated\nnumber."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0660", "text": "Execution time tweaks for watch requests\nAPI Priority and Fairness manages watch requests, but this involves a couple more excursions from the baseline behavior. The first\nconcerns how long a watch request is considered to occupy its seat. Depending on request parameters, the response to a watch\nrequest may or may not begin with create notifications for all the relevant pre-existing objects. API Priority and Fairness considers a\nwatch request to be done with its seat once that initial burst of notifications, if any, is over.\nThe normal notifications are sent in a concurrent burst to all relevant watch response streams whenever the server is notified of an\nobject create/update/delete. To account for this work, API Priority and Fairness considers every write request to spend some\nadditional time occupying seats after the actual writing is done. The server estimates the number of notifications to be sent and\nadjusts the write request's number of seats and seat occupancy time to include this extra work.\n\nQueuing\nEven within a priority level there may be a large number of distinct sources of traffic. In an overload situation, it is valuable to\nprevent one stream of requests from starving others (in particular, in the relatively common case of a single buggy client flooding the\nkube-apiserver with requests, that buggy client would ideally not have much measurable impact on other clients at all). This is\nhandled by use of a fair-queuing algorithm to process requests that are assigned the same priority level. Each request is assigned to\na flow, identified by the name of the matching FlowSchema plus a flow distinguisher â€” which is either the requesting user, the target\nresource's namespace, or nothing â€” and the system attempts to give approximately equal weight to requests in different flows of\nthe same priority level. To enable distinct handling of distinct instances, controllers that have many instances should authenticate\nwith distinct usernames\nAfter classifying a request into a flow, the API Priority and Fairness feature then may assign the request to a queue. This assignment\nuses a technique known as shuffle sharding, which makes relatively efficient use of queues to insulate low-intensity flows from highintensity flows.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n628/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0661", "text": "The details of the queuing algorithm are tunable for each priority level, and allow administrators to trade off memory use, fairness\n(the property that independent flows will all make progress when total traffic exceeds capacity), tolerance for bursty traffic, and the\nadded latency induced by queuing.\n\nExempt requests\nSome requests are considered sufficiently important that they are not subject to any of the limitations imposed by this feature.\nThese exemptions prevent an improperly-configured flow control configuration from totally disabling an API server.\n\nResources\nThe flow control API involves two kinds of resources. PriorityLevelConfigurations define the available priority levels, the share of the\navailable concurrency budget that each can handle, and allow for fine-tuning queuing behavior. FlowSchemas are used to classify\nindividual inbound requests, matching each to a single PriorityLevelConfiguration."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0662", "text": "PriorityLevelConfiguration\nA PriorityLevelConfiguration represents a single priority level. Each PriorityLevelConfiguration has an independent limit on the\nnumber of outstanding requests, and limitations on the number of queued requests.\nThe nominal concurrency limit for a PriorityLevelConfiguration is not specified in an absolute number of seats, but rather in\n\"nominal concurrency shares.\" The total concurrency limit for the API Server is distributed among the existing\nPriorityLevelConfigurations in proportion to these shares, to give each level its nominal limit in terms of seats. This allows a cluster\nadministrator to scale up or down the total amount of traffic to a server by restarting kube-apiserver with a different value for -max-requests-inflight (or --max-mutating-requests-inflight ), and all PriorityLevelConfigurations will see their maximum allowed\nconcurrency go up (or down) by the same fraction.\nCaution:\nIn the versions before v1beta3 the relevant PriorityLevelConfiguration field is named \"assured concurrency shares\" rather than\n\"nominal concurrency shares\". Also, in Kubernetes release 1.25 and earlier there were no periodic adjustments: the\nnominal/assured limits were always applied without adjustment.\nThe bounds on how much concurrency a priority level may lend and how much it may borrow are expressed in the\nPriorityLevelConfiguration as percentages of the level's nominal limit. These are resolved to absolute numbers of seats by\nmultiplying with the nominal limit / 100.0 and rounding. The dynamically adjusted concurrency limit of a priority level is constrained\nto lie between (a) a lower bound of its nominal limit minus its lendable seats and (b) an upper bound of its nominal limit plus the\nseats it may borrow. At each adjustment the dynamic limits are derived by each priority level reclaiming any lent seats for which\ndemand recently appeared and then jointly fairly responding to the recent seat demand on the priority levels, within the bounds just\ndescribed.\nCaution:\nWith the Priority and Fairness feature enabled, the total concurrency limit for the server is set to the sum of --max-requestsinflight and --max-mutating-requests-inflight. There is no longer any distinction made between mutating and nonmutating requests; if you want to treat them separately for a given resource, make separate FlowSchemas that match the\nmutating and non-mutating verbs respectively.\nWhen the volume of inbound requests assigned to a single PriorityLevelConfiguration is more than its permitted concurrency level,\nthe type field of its specification determines what will happen to extra requests. A type of Reject means that excess traffic will\nimmediately be rejected with an HTTP 429 (Too Many Requests) error. A type of Queue means that requests above the threshold will\nbe queued, with the shuffle sharding and fair queuing techniques used to balance progress between request flows.\nThe queuing configuration allows tuning the fair queuing algorithm for a priority level. Details of the algorithm can be read in the\nenhancement proposal, but in short:\nIncreasing queues reduces the rate of collisions between different flows, at the cost of increased memory usage. A value of 1\nhere effectively disables the fair-queuing logic, but still allows requests to be queued."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0663", "text": "https://kubernetes.io/docs/concepts/_print/\n\n629/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIncreasing queueLengthLimit allows larger bursts of traffic to be sustained without dropping any requests, at the cost of\nincreased latency and memory usage.\nChanging handSize allows you to adjust the probability of collisions between different flows and the overall concurrency\navailable to a single flow in an overload situation.\nNote:\nA larger handSize makes it less likely for two individual flows to collide (and therefore for one to be able to starve the\nother), but more likely that a small number of flows can dominate the apiserver. A larger handSize also potentially\nincreases the amount of latency that a single high-traffic flow can cause. The maximum number of queued requests\npossible from a single flow is handSize * queueLengthLimit.\nFollowing is a table showing an interesting collection of shuffle sharding configurations, showing for each the probability that a given\nmouse (low-intensity flow) is squished by the elephants (high-intensity flows) for an illustrative collection of numbers of elephants.\nSee https://play.golang.org/p/Gi0PLgVHiUg , which computes this table.\nHandSize\n\nQueues\n\n1 elephant\n\n4 elephants\n\n16 elephants\n\n12\n\n32\n\n4.428838398950118e-09\n\n0.11431348830099144\n\n0.9935089607656024\n\n10\n\n32\n\n1.550093439632541e-08\n\n0.0626479840223545\n\n0.9753101519027554\n\n10\n\n64\n\n6.601827268370426e-12\n\n0.00045571320990370776\n\n0.49999929150089345\n\n9\n\n64\n\n3.6310049976037345e-11\n\n0.00045501212304112273\n\n0.4282314876454858\n\n8\n\n64\n\n2.25929199850899e-10\n\n0.0004886697053040446\n\n0.35935114681123076\n\n8\n\n128\n\n6.994461389026097e-13\n\n3.4055790161620863e-06\n\n0.02746173137155063\n\n7\n\n128\n\n1.0579122850901972e-11\n\n6.960839379258192e-06\n\n0.02406157386340147\n\n7\n\n256\n\n7.597695465552631e-14\n\n6.728547142019406e-08\n\n0.0006709661542533682\n\n6\n\n256\n\n2.7134626662687968e-12\n\n2.9516464018476436e-07\n\n0.0008895654642000348\n\n6\n\n512\n\n4.116062922897309e-14\n\n4.982983350480894e-09\n\n2.26025764343413e-05\n\n6\n\n1024\n\n6.337324016514285e-16\n\n8.09060164312957e-11\n\n4.517408062903668e-07"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0664", "text": "FlowSchema\nA FlowSchema matches some inbound requests and assigns them to a priority level. Every inbound request is tested against\nFlowSchemas, starting with those with the numerically lowest matchingPrecedence and working upward. The first match wins.\nCaution:\nOnly the first matching FlowSchema for a given request matters. If multiple FlowSchemas match a single inbound request, it\nwill be assigned based on the one with the highest matchingPrecedence. If multiple FlowSchemas with equal\nmatchingPrecedence match the same request, the one with lexicographically smaller name will win, but it's better not to rely on\nthis, and instead to ensure that no two FlowSchemas have the same matchingPrecedence.\nA FlowSchema matches a given request if at least one of its rules matches. A rule matches if at least one of its subjects and at\nleast one of its resourceRules or nonResourceRules (depending on whether the incoming request is for a resource or non-resource\nURL) match the request.\nFor the name field in subjects, and the verbs , apiGroups , resources , namespaces , and nonResourceURLs fields of resource and\nnon-resource rules, the wildcard * may be specified to match all values for the given field, effectively removing it from\nconsideration.\nhttps://kubernetes.io/docs/concepts/_print/\n\n630/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nA FlowSchema's distinguisherMethod.type determines how requests matching that schema will be separated into flows. It may be\nByUser , in which one requesting user will not be able to starve other users of capacity; ByNamespace , in which requests for\nresources in one namespace will not be able to starve requests for resources in other namespaces of capacity; or blank (or\ndistinguisherMethod may be omitted entirely), in which all requests matched by this FlowSchema will be considered part of a single\nflow. The correct choice for a given FlowSchema depends on the resource and your particular environment.\n\nDefaults\nEach kube-apiserver maintains two sorts of APF configuration objects: mandatory and suggested."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0665", "text": "Mandatory Configuration Objects\nThe four mandatory configuration objects reflect fixed built-in guardrail behavior. This is behavior that the servers have before those\nobjects exist, and when those objects exist their specs reflect this behavior. The four mandatory objects are as follows.\nThe mandatory exempt priority level is used for requests that are not subject to flow control at all: they will always be\ndispatched immediately. The mandatory exempt FlowSchema classifies all requests from the system:masters group into this\npriority level. You may define other FlowSchemas that direct other requests to this priority level, if appropriate.\nThe mandatory catch-all priority level is used in combination with the mandatory catch-all FlowSchema to make sure that\nevery request gets some kind of classification. Typically you should not rely on this catch-all configuration, and should create\nyour own catch-all FlowSchema and PriorityLevelConfiguration (or use the suggested global-default priority level that is\ninstalled by default) as appropriate. Because it is not expected to be used normally, the mandatory catch-all priority level\nhas a very small concurrency share and does not queue requests."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0666", "text": "Suggested Configuration Objects\nThe suggested FlowSchemas and PriorityLevelConfigurations constitute a reasonable default configuration. You can modify these\nand/or create additional configuration objects if you want. If your cluster is likely to experience heavy load then you should consider\nwhat configuration will work best.\nThe suggested configuration groups requests into six priority levels:\nThe node-high priority level is for health updates from nodes.\nThe system priority level is for non-health requests from the system:nodes group, i.e. Kubelets, which must be able to contact\nthe API server in order for workloads to be able to schedule on them.\nThe leader-election priority level is for leader election requests from built-in controllers (in particular, requests for\nendpoints , configmaps , or leases coming from the system:kube-controller-manager or system:kube-scheduler users and\nservice accounts in the kube-system namespace). These are important to isolate from other traffic because failures in leader\nelection cause their controllers to fail and restart, which in turn causes more expensive traffic as the new controllers sync their\ninformers.\nThe workload-high priority level is for other requests from built-in controllers.\nThe workload-low priority level is for requests from any other service account, which will typically include all requests from\ncontrollers running in Pods.\nThe global-default priority level handles all other traffic, e.g. interactive kubectl commands run by nonprivileged users.\nThe suggested FlowSchemas serve to steer requests into the above priority levels, and are not enumerated here.\n\nMaintenance of the Mandatory and Suggested Configuration Objects\nEach kube-apiserver independently maintains the mandatory and suggested configuration objects, using initial and periodic\nbehavior. Thus, in a situation with a mixture of servers of different versions there may be thrashing as long as different servers have\ndifferent opinions of the proper content of these objects.\nEach kube-apiserver makes an initial maintenance pass over the mandatory and suggested configuration objects, and after that\ndoes periodic maintenance (once per minute) of those objects.\nFor the mandatory configuration objects, maintenance consists of ensuring that the object exists and, if it does, has the proper spec.\nThe server refuses to allow a creation or update with a spec that is inconsistent with the server's guardrail behavior.\nhttps://kubernetes.io/docs/concepts/_print/\n\n631/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0667", "text": "Maintenance of suggested configuration objects is designed to allow their specs to be overridden. Deletion, on the other hand, is not\nrespected: maintenance will restore the object. If you do not want a suggested configuration object then you need to keep it around\nbut set its spec to have minimal consequences. Maintenance of suggested objects is also designed to support automatic migration\nwhen a new version of the kube-apiserver is rolled out, albeit potentially with thrashing while there is a mixed population of\nservers.\nMaintenance of a suggested configuration object consists of creating it --- with the server's suggested spec --- if the object does not\nexist. OTOH, if the object already exists, maintenance behavior depends on whether the kube-apiservers or the users control the\nobject. In the former case, the server ensures that the object's spec is what the server suggests; in the latter case, the spec is left\nalone.\nThe question of who controls the object is answered by first looking for an annotation with key apf.kubernetes.io/autoupdatespec . If there is such an annotation and its value is true then the kube-apiservers control the object. If there is such an annotation\nand its value is false then the users control the object. If neither of those conditions holds then the metadata.generation of the\nobject is consulted. If that is 1 then the kube-apiservers control the object. Otherwise the users control the object. These rules were\nintroduced in release 1.22 and their consideration of metadata.generation is for the sake of migration from the simpler earlier\nbehavior. Users who wish to control a suggested configuration object should set its apf.kubernetes.io/autoupdate-spec annotation\nto false .\nMaintenance of a mandatory or suggested configuration object also includes ensuring that it has an apf.kubernetes.io/autoupdatespec annotation that accurately reflects whether the kube-apiservers control the object.\nMaintenance also includes deleting objects that are neither mandatory nor suggested but are annotated\napf.kubernetes.io/autoupdate-spec=true ."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0668", "text": "Health check concurrency exemption\nThe suggested configuration gives no special treatment to the health check requests on kube-apiservers from their local kubelets --which tend to use the secured port but supply no credentials. With the suggested config, these requests get assigned to the globaldefault FlowSchema and the corresponding global-default priority level, where other traffic can crowd them out.\nIf you add the following additional FlowSchema, this exempts those requests from rate limiting.\nCaution:\nMaking this change also allows any hostile party to then send health-check requests that match this FlowSchema, at any\nvolume they like. If you have a web traffic filter or similar external security mechanism to protect your cluster's API server from\ngeneral internet traffic, you can configure rules to block any health check requests that originate from outside your cluster.\n\npriority-and-fairness/health-for-strangers.yaml\napiVersion: flowcontrol.apiserver.k8s.io/v1\nkind: FlowSchema\nmetadata:\nname: health-for-strangers\nspec:\nmatchingPrecedence: 1000\npriorityLevelConfiguration:\nname: exempt\nrules:\n- nonResourceRules:\n- nonResourceURLs:\n- \"/healthz\"\n- \"/livez\"\n- \"/readyz\"\nverbs:\n- \"*\"\nsubjects:\n- kind: Group\ngroup:\nname: \"system:unauthenticated\"\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n632/684\n\n11/7/25, 4:37 PM\n\nObservability\n\nConcepts | Kubernetes\n\nMetrics\nNote:\nIn versions of Kubernetes before v1.20, the labels flow_schema and priority_level were inconsistently named flowSchema and\npriorityLevel, respectively. If you're running Kubernetes versions v1.19 and earlier, you should refer to the documentation\nfor your version.\nWhen you enable the API Priority and Fairness feature, the kube-apiserver exports additional metrics. Monitoring these can help you\ndetermine whether your configuration is inappropriately throttling important traffic, or find poorly-behaved workloads that may be\nharming system health.\n\nMaturity level BETA\nis a counter vector (cumulative since server start) of requests that were\nrejected, broken down by the labels flow_schema (indicating the one that matched the request), priority_level (indicating\nthe one to which the request was assigned), and reason . The reason label will be one of the following values:\napiserver_flowcontrol_rejected_requests_total\n\nqueue-full , indicating that too many requests were already queued.\nconcurrency-limit , indicating that the PriorityLevelConfiguration is configured to reject rather than queue excess\n\nrequests.\ntime-out , indicating that the request was still in the queue when its queuing time limit expired.\ncancelled , indicating that the request is not purge locked and has been ejected from the queue."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0669", "text": "is a counter vector (cumulative since server start) of requests that began\nexecuting, broken down by flow_schema and priority_level .\napiserver_flowcontrol_dispatched_requests_total\n\nis a gauge vector holding the instantaneous number of queued (not\nexecuting) requests, broken down by priority_level and flow_schema .\napiserver_flowcontrol_current_inqueue_requests\n\nis a gauge vector holding the instantaneous number of executing (not\nwaiting in a queue) requests, broken down by priority_level and flow_schema .\napiserver_flowcontrol_current_executing_requests\n\nis a gauge vector holding the instantaneous number of occupied seats,\nbroken down by priority_level and flow_schema .\napiserver_flowcontrol_current_executing_seats\n\nis a histogram vector of how long requests spent queued, broken\ndown by the labels flow_schema , priority_level , and execute . The execute label indicates whether the request has\nstarted executing.\napiserver_flowcontrol_request_wait_duration_seconds\n\nNote:\nSince each FlowSchema always assigns requests to a single PriorityLevelConfiguration, you can add the histograms for all\nthe FlowSchemas for one priority level to get the effective histogram for requests assigned to that priority level.\nis a gauge vector holding each priority level's nominal concurrency limit,\ncomputed from the API server's total concurrency limit and the priority level's configured nominal concurrency shares.\napiserver_flowcontrol_nominal_limit_seats\n\nMaturity level ALPHA\nis a gauge vector of recent high water marks of the number of queued requests,\ngrouped by a label named request_kind whose value is mutating or readOnly . These high water marks describe the largest\nnumber seen in the one second window most recently completed. These complement the older\napiserver_current_inflight_requests gauge vector that holds the last window's high water mark of number of requests\nactively being served.\napiserver_current_inqueue_requests\n\nis a gauge vector of the sum over queued requests of the largest number of seats each will\noccupy, grouped by labels named flow_schema and priority_level .\napiserver_current_inqueue_seats\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n633/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nis a histogram vector of observations, made at the end of every\nnanosecond, of the number of requests broken down by the labels phase (which takes on the values waiting and\nexecuting ) and request_kind (which takes on the values mutating and readOnly ). Each observed value is a ratio, between 0\nand 1, of the number of requests divided by the corresponding limit on the number of requests (queue volume limit for waiting\nand concurrency limit for executing).\napiserver_flowcontrol_read_vs_write_current_requests\n\napiserver_flowcontrol_request_concurrency_in_use"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0670", "text": "is a gauge vector holding the instantaneous number of occupied seats,\n\nbroken down by priority_level and flow_schema .\nis a histogram vector of observations, made at the end of each\nnanosecond, of the number of requests broken down by the labels phase (which takes on the values waiting and\nexecuting ) and priority_level . Each observed value is a ratio, between 0 and 1, of a number of requests divided by the\ncorresponding limit on the number of requests (queue volume limit for waiting and concurrency limit for executing).\napiserver_flowcontrol_priority_level_request_utilization\n\nis a histogram vector of observations, made at the end of each\nnanosecond, of the utilization of a priority level's concurrency limit, broken down by priority_level . This utilization is the\nfraction (number of seats occupied) / (concurrency limit). This metric considers all stages of execution (both normal and the\nextra delay at the end of a write to cover for the corresponding notification work) of all requests except WATCHes; for those it\nconsiders only the initial stage that delivers notifications of pre-existing objects. Each histogram in the vector is also labeled\nwith phase: executing (there is no seat limit for the waiting phase).\napiserver_flowcontrol_priority_level_seat_utilization\n\nis a histogram vector of queue lengths for the queues, broken\ndown by priority_level and flow_schema , as sampled by the enqueued requests. Each request that gets queued\ncontributes one sample to its histogram, reporting the length of the queue immediately after the request was added. Note that\nthis produces different statistics than an unbiased survey would.\napiserver_flowcontrol_request_queue_length_after_enqueue\n\nNote:\nAn outlier value in a histogram here means it is likely that a single flow (i.e., requests by one user or for one namespace,\ndepending on configuration) is flooding the API server, and being throttled. By contrast, if one priority level's histogram\nshows that all queues for that priority level are longer than those for other priority levels, it may be appropriate to\nincrease that PriorityLevelConfiguration's concurrency shares.\nis the same as apiserver_flowcontrol_nominal_limit_seats . Before the\nintroduction of concurrency borrowing between priority levels, this was always equal to\napiserver_flowcontrol_current_limit_seats (which did not exist as a distinct metric).\napiserver_flowcontrol_request_concurrency_limit\n\napiserver_flowcontrol_lower_limit_seats\n\nis a gauge vector holding the lower bound on each priority level's dynamic\n\nconcurrency limit.\napiserver_flowcontrol_upper_limit_seats\n\nis a gauge vector holding the upper bound on each priority level's dynamic"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0671", "text": "concurrency limit.\nis a histogram vector counting observations, at the end of every nanosecond, of each\npriority level's ratio of (seat demand) / (nominal concurrency limit). A priority level's seat demand is the sum, over both queued\nrequests and those in the initial phase of execution, of the maximum of the number of seats occupied in the request's initial\nand final execution phases.\napiserver_flowcontrol_demand_seats\n\nis a gauge vector holding, for each priority level, the maximum seat\ndemand seen during the last concurrency borrowing adjustment period.\napiserver_flowcontrol_demand_seats_high_watermark\n\nis a gauge vector holding, for each priority level, the time-weighted average\nseat demand seen during the last concurrency borrowing adjustment period.\napiserver_flowcontrol_demand_seats_average\n\nis a gauge vector holding, for each priority level, the time-weighted population\nstandard deviation of seat demand seen during the last concurrency borrowing adjustment period.\napiserver_flowcontrol_demand_seats_stdev\n\nis a gauge vector holding, for each priority level, the smoothed enveloped seat\ndemand determined at the last concurrency adjustment.\napiserver_flowcontrol_demand_seats_smoothed\n\napiserver_flowcontrol_target_seats\n\nis a gauge vector holding, for each priority level, the concurrency target going into the\n\nborrowing allocation problem.\napiserver_flowcontrol_seat_fair_frac\n\nis a gauge holding the fair allocation fraction determined in the last borrowing\n\nadjustment.\nhttps://kubernetes.io/docs/concepts/_print/\n\n634/684\n\n11/7/25, 4:37 PM\n\napiserver_flowcontrol_current_limit_seats\n\nConcepts | Kubernetes\n\nis a gauge vector holding, for each priority level, the dynamic concurrency limit\n\nderived in the last adjustment.\napiserver_flowcontrol_request_execution_seconds\n\nis a histogram vector of how long requests took to actually execute,\n\nbroken down by flow_schema and priority_level .\nis a histogram vector of the number of active WATCH requests relevant to a\ngiven write, broken down by flow_schema and priority_level .\napiserver_flowcontrol_watch_count_samples\n\nis a histogram vector of the number of estimated seats (maximum of initial and\nfinal stage of execution) associated with requests, broken down by flow_schema and priority_level .\napiserver_flowcontrol_work_estimated_seats\n\nis a counter vector of the number of events that in\nprinciple could have led to a request being dispatched but did not, due to lack of available concurrency, broken down by\nflow_schema and priority_level .\napiserver_flowcontrol_request_dispatch_no_accommodation_total\n\nis a counter vector of the number of attempts to jump a priority level's progress\nmeter backward to avoid numeric overflow, grouped by priority_level and success .\napiserver_flowcontrol_epoch_advance_total"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0672", "text": "Good practices for using API Priority and Fairness\nWhen a given priority level exceeds its permitted concurrency, requests can experience increased latency or be dropped with an\nHTTP 429 (Too Many Requests) error. To prevent these side effects of APF, you can modify your workload or tweak your APF settings\nto ensure there are sufficient seats available to serve your requests.\nTo detect whether requests are being rejected due to APF, check the following metrics:\napiserver_flowcontrol_rejected_requests_total: the total number of requests rejected per FlowSchema and\nPriorityLevelConfiguration.\napiserver_flowcontrol_current_inqueue_requests: the current number of requests queued per FlowSchema and\nPriorityLevelConfiguration.\napiserver_flowcontrol_request_wait_duration_seconds: the latency added to requests waiting in queues.\napiserver_flowcontrol_priority_level_seat_utilization: the seat utilization per PriorityLevelConfiguration.\n\nWorkload modifications\nTo prevent requests from queuing and adding latency or being dropped due to APF, you can optimize your requests by:\nReducing the rate at which requests are executed. A fewer number of requests over a fixed period will result in a fewer number\nof seats being needed at a given time.\nAvoid issuing a large number of expensive requests concurrently. Requests can be optimized to use fewer seats or have lower\nlatency so that these requests hold those seats for a shorter duration. List requests can occupy more than 1 seat depending on\nthe number of objects fetched during the request. Restricting the number of objects retrieved in a list request, for example by\nusing pagination, will use less total seats over a shorter period. Furthermore, replacing list requests with watch requests will\nrequire lower total concurrency shares as watch requests only occupy 1 seat during its initial burst of notifications. If using\nstreaming lists in versions 1.27 and later, watch requests will occupy the same number of seats as a list request for its initial\nburst of notifications because the entire state of the collection has to be streamed. Note that in both cases, a watch request will\nnot hold any seats after this initial phase.\nKeep in mind that queuing or rejected requests from APF could be induced by either an increase in the number of requests or an\nincrease in latency for existing requests. For example, if requests that normally take 1s to execute start taking 60s, it is possible that\nAPF will start rejecting requests because requests are occupying seats for a longer duration than normal due to this increase in\nlatency. If APF starts rejecting requests across multiple priority levels without a significant change in workload, it is possible there is\nan underlying issue with control plane performance rather than the workload or APF settings."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0673", "text": "Priority and fairness settings\nYou can also modify the default FlowSchema and PriorityLevelConfiguration objects or create new objects of these types to better\naccommodate your workload.\nAPF settings can be modified to:\nGive more seats to high priority requests.\nhttps://kubernetes.io/docs/concepts/_print/\n\n635/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIsolate non-essential or expensive requests that would starve a concurrency level if it was shared with other flows.\n\nGive more seats to high priority requests\n1. If possible, the number of seats available across all priority levels for a particular kube-apiserver can be increased by\nincreasing the values for the max-requests-inflight and max-mutating-requests-inflight flags. Alternatively, horizontally\nscaling the number of kube-apiserver instances will increase the total concurrency per priority level across the cluster\nassuming there is sufficient load balancing of requests.\n2. You can create a new FlowSchema which references a PriorityLevelConfiguration with a larger concurrency level. This new\nPriorityLevelConfiguration could be an existing level or a new level with its own set of nominal concurrency shares. For\nexample, a new FlowSchema could be introduced to change the PriorityLevelConfiguration for your requests from globaldefault to workload-low to increase the number of seats available to your user. Creating a new PriorityLevelConfiguration will\nreduce the number of seats designated for existing levels. Recall that editing a default FlowSchema or\nPriorityLevelConfiguration will require setting the apf.kubernetes.io/autoupdate-spec annotation to false.\n3. You can also increase the NominalConcurrencyShares for the PriorityLevelConfiguration which is serving your high priority\nrequests. Alternatively, for versions 1.26 and later, you can increase the LendablePercent for competing priority levels so that\nthe given priority level has a higher pool of seats it can borrow."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0674", "text": "Isolate non-essential requests from starving other flows\nFor request isolation, you can create a FlowSchema whose subject matches the user making these requests or create a FlowSchema\nthat matches what the request is (corresponding to the resourceRules). Next, you can map this FlowSchema to a\nPriorityLevelConfiguration with a low share of seats.\nFor example, suppose list event requests from Pods running in the default namespace are using 10 seats each and execute for 1\nminute. To prevent these expensive requests from impacting requests from other Pods using the existing service-accounts\nFlowSchema, you can apply the following FlowSchema to isolate these list calls from other requests.\nExample FlowSchema object to isolate list event requests:\npriority-and-fairness/list-events-default-service-account.yaml\napiVersion: flowcontrol.apiserver.k8s.io/v1\nkind: FlowSchema\nmetadata:\nname: list-events-default-service-account\nspec:\ndistinguisherMethod:\ntype: ByUser\nmatchingPrecedence: 8000\npriorityLevelConfiguration:\nname: catch-all\nrules:\n- resourceRules:\n- apiGroups:\n- '*'\nnamespaces:\n- default\nresources:\n- events\nverbs:\n- list\nsubjects:\n- kind: ServiceAccount\nserviceAccount:\nname: default\nnamespace: default\n\nThis FlowSchema captures all list event calls made by the default service account in the default namespace. The matching\nprecedence 8000 is lower than the value of 9000 used by the existing service-accounts FlowSchema so these list event calls will\nmatch list-events-default-service-account rather than service-accounts.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n636/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nThe catch-all PriorityLevelConfiguration is used to isolate these requests. The catch-all priority level has a very small\nconcurrency share and does not queue requests.\n\nWhat's next\nYou can visit flow control reference doc to learn more about troubleshooting.\nFor background information on design details for API priority and fairness, see the enhancement proposal.\nYou can make suggestions and feature requests via SIG API Machinery or the feature's slack channel.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n637/684\n\n11/7/25, 4:37 PM\n\n11.17 - Installing Addons\n\nConcepts | Kubernetes\n\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nAdd-ons extend the functionality of Kubernetes.\nThis page lists some of the available add-ons and links to their respective installation instructions. The list does not try to be\nexhaustive."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0675", "text": "Networking and Network Policy\nACI provides integrated container networking and network security with Cisco ACI.\nAntrea operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the\nnetworking data plane. Antrea is a CNCF project at the Sandbox level.\nCalico is a networking and network policy provider. Calico supports a flexible set of networking options so you can choose the\nmost efficient option for your situation, including non-overlay and overlay networks, with or without BGP. Calico uses the same\nengine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications at the service mesh layer.\nCanal unites Flannel and Calico, providing networking and network policy.\nCilium is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3\nnetwork with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce\nnetwork policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as\na replacement for kube-proxy; it also offers additional, opt-in observability and security features. Cilium is a CNCF project at the\nGraduated level.\nCNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave. CNIGenie is a CNCF project at the Sandbox level.\nContiv provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various\nuse cases and a rich policy framework. Contiv project is fully open sourced. The installer provides both kubeadm and nonkubeadm based installation options.\nContrail, based on Tungsten Fabric, is an open source, multi-cloud network virtualization and policy management platform.\nContrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos,\nand provide isolation modes for virtual machines, containers/pods and bare metal workloads.\nFlannel is an overlay network provider that can be used with Kubernetes.\nGateway API is an open source project managed by the SIG Network community and provides an expressive, extensible, and\nrole-oriented API for modeling service networking.\nKnitter is a plugin to support multiple network interfaces in a Kubernetes pod.\nMultus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv,\nFlannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.\nOVN-Kubernetes is a networking provider for Kubernetes based on OVN (Open Virtual Network), a virtual networking\nimplementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking\nimplementation for Kubernetes, including an OVS based implementation of load balancing and network policy.\nNodus is an OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC).\nNSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as\nwell as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and\nOpenShift.\nNuage is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments\nwith visibility and security monitoring.\nRomana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API.\nSpiderpool is an underlay and RDMA networking solution for Kubernetes. Spiderpool is supported on bare metal, virtual\nmachines, and public cloud environments.\nTerway is a suite of CNI plugins based on AlibabaCloud's VPC and ECS network products. It provides native VPC networking and\nnetwork policies in AlibabaCloud environments.\nWeave Net provides networking and network policy, will carry on working on both sides of a network partition, and does not\nrequire an external database.\nhttps://kubernetes.io/docs/concepts/_print/"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0676", "text": "638/684\n\n11/7/25, 4:37 PM\n\nService Discovery\n\nConcepts | Kubernetes\n\nCoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods.\n\nVisualization & Control\nDashboard is a dashboard web interface for Kubernetes.\n\nInfrastructure\nKubeVirt is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.\nThe node problem detector runs on Linux nodes and reports system issues as either Events or Node conditions.\n\nInstrumentation\nkube-state-metrics\n\nLegacy Add-ons\nThere are several other add-ons documented in the deprecated cluster/addons directory.\nWell-maintained ones should be linked to here. PRs welcome!\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n639/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n11.18 - Coordinated Leader Election\nâ“˜ FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)\n\nKubernetes 1.34 includes a beta feature that allows control plane components to deterministically select a leader via coordinated\nleader election. This is useful to satisfy Kubernetes version skew constraints during cluster upgrades. Currently, the only builtin\nselection strategy is OldestEmulationVersion , preferring the leader with the lowest emulation version, followed by binary version,\nfollowed by creation timestamp.\n\nEnabling coordinated leader election\nEnsure that CoordinatedLeaderElection feature gate is enabled when you start the API Server: and that the\ncoordination.k8s.io/v1beta1 API group is enabled.\nThis can be done by setting flags --feature-gates=\"CoordinatedLeaderElection=true\" and --runtimeconfig=\"coordination.k8s.io/v1beta1=true\" .\n\nComponent configuration\nProvided that you have enabled the CoordinatedLeaderElection feature gate and\nhave the coordination.k8s.io/v1beta1 API group enabled, compatible control plane\ncomponents automatically use the LeaseCandidate and Lease APIs to elect a leader\nas needed.\nFor Kubernetes 1.34, two control plane components\n(kube-controller-manager and kube-scheduler) automatically use coordinated\nleader election when the feature gate and API group are enabled.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n640/684\n\n11/7/25, 4:37 PM\n\n12 - Windows in Kubernetes\n\nConcepts | Kubernetes\n\nKubernetes supports nodes that run Microsoft Windows.\nKubernetes supports worker nodes running either Linux or Microsoft Windows.\nðŸ›‡ This item links to a third party project or product that is not part of Kubernetes itself. More information"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0677", "text": "The CNCF and its parent the Linux Foundation take a vendor-neutral approach towards compatibility. It is possible to join your\nWindows server as a worker node to a Kubernetes cluster.\nYou can install and set up kubectl on Windows no matter what operating system you use within your cluster.\nIf you are using Windows nodes, you can read:\nNetworking On Windows\nWindows Storage In Kubernetes\nResource Management for Windows Nodes\nConfigure RunAsUserName for Windows Pods and Containers\nCreate A Windows HostProcess Pod\nConfigure Group Managed Service Accounts for Windows Pods and Containers\nSecurity For Windows Nodes\nWindows Debugging Tips\nGuide for Scheduling Windows Containers in Kubernetes\nor, for an overview, read:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n641/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n12.1 - Windows containers in Kubernetes\nWindows applications constitute a large portion of the services and applications that run in many organizations. Windows containers\nprovide a way to encapsulate processes and package dependencies, making it easier to use DevOps practices and follow cloud\nnative patterns for Windows applications.\nOrganizations with investments in Windows-based applications and Linux-based applications don't have to look for separate\norchestrators to manage their workloads, leading to increased operational efficiencies across their deployments, regardless of\noperating system.\n\nWindows nodes in Kubernetes\nTo enable the orchestration of Windows containers in Kubernetes, include Windows nodes in your existing Linux cluster. Scheduling\nWindows containers in Pods on Kubernetes is similar to scheduling Linux-based containers.\nIn order to run Windows containers, your Kubernetes cluster must include multiple operating systems. While you can only run the\ncontrol plane on Linux, you can deploy worker nodes running either Windows or Linux.\nWindows nodes are supported provided that the operating system is Windows Server 2019 or Windows Server 2022.\nThis document uses the term Windows containers to mean Windows containers with process isolation. Kubernetes does not support\nrunning Windows containers with Hyper-V isolation."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0678", "text": "Compatibility and limitations\nSome node features are only available if you use a specific container runtime; others are not available on Windows nodes, including:\nHugePages: not supported for Windows containers\nPrivileged containers: not supported for Windows containers. HostProcess Containers offer similar functionality.\nTerminationGracePeriod: requires containerD\nNot all features of shared namespaces are supported. See API compatibility for more details.\nSee Windows OS version compatibility for details on the Windows versions that Kubernetes is tested against.\nFrom an API and kubectl perspective, Windows containers behave in much the same way as Linux-based containers. However, there\nare some notable differences in key functionality which are outlined in this section.\n\nComparison with Linux\nKey Kubernetes elements work the same way in Windows as they do in Linux. This section refers to several key workload\nabstractions and how they map to Windows.\nPods\nA Pod is the basic building block of Kubernetesâ€“the smallest and simplest unit in the Kubernetes object model that you create\nor deploy. You may not deploy Windows and Linux containers in the same Pod. All containers in a Pod are scheduled onto a\nsingle Node where each Node represents a specific platform and architecture. The following Pod capabilities, properties and\nevents are supported with Windows containers:\nSingle or multiple containers per Pod with process isolation and volume sharing\nPod status fields\nReadiness, liveness, and startup probes\npostStart & preStop container lifecycle hooks\nConfigMap, Secrets: as environment variables or volumes\nemptyDir\n\nvolumes\n\nNamed pipe host mounts\nResource limits\nhttps://kubernetes.io/docs/concepts/_print/\n\n642/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nOS field:\nThe .spec.os.name field should be set to windows to indicate that the current Pod uses Windows containers.\nIf you set the .spec.os.name field to windows , you must not set the following fields in the .spec of that Pod:\nspec.hostPID\nspec.hostIPC\nspec.securityContext.seLinuxOptions\nspec.securityContext.seccompProfile\nspec.securityContext.fsGroup\nspec.securityContext.fsGroupChangePolicy\nspec.securityContext.sysctls\nspec.shareProcessNamespace\nspec.securityContext.runAsUser\nspec.securityContext.runAsGroup\nspec.securityContext.supplementalGroups\nspec.containers[*].securityContext.seLinuxOptions\nspec.containers[*].securityContext.seccompProfile\nspec.containers[*].securityContext.capabilities\nspec.containers[*].securityContext.readOnlyRootFilesystem\nspec.containers[*].securityContext.privileged\nspec.containers[*].securityContext.allowPrivilegeEscalation\nspec.containers[*].securityContext.procMount\nspec.containers[*].securityContext.runAsUser\nspec.containers[*].securityContext.runAsGroup"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0679", "text": "In the above list, wildcards ( * ) indicate all elements in a list. For example, spec.containers[*].securityContext refers\nto the SecurityContext object for all containers. If any of these fields is specified, the Pod will not be admitted by the API\nserver.\nWorkload resources including:\nReplicaSet\nDeployment\nStatefulSet\nDaemonSet\nJob\nCronJob\nReplicationController\nServices See Load balancing and Services for more details.\nPods, workload resources, and Services are critical elements to managing Windows workloads on Kubernetes. However, on their\nown they are not enough to enable the proper lifecycle management of Windows workloads in a dynamic cloud native environment.\nkubectl exec\n\nPod and container metrics\nHorizontal pod autoscaling\nResource quotas\nScheduler preemption\n\nCommand line options for the kubelet\nSome kubelet command line options behave differently on Windows, as described below:\nThe --windows-priorityclass lets you set the scheduling priority of the kubelet process (see CPU resource management)\nThe --kube-reserved , --system-reserved , and --eviction-hard flags update NodeAllocatable\nEviction by using --enforce-node-allocable is not implemented\nhttps://kubernetes.io/docs/concepts/_print/\n\n643/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nWhen running on a Windows node the kubelet does not have memory or CPU restrictions. --kube-reserved and --systemreserved only subtract from NodeAllocatable and do not guarantee resource provided for workloads. See Resource\nManagement for Windows nodes for more information.\nThe PIDPressure Condition is not implemented\nThe kubelet does not take OOM eviction actions"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0680", "text": "API compatibility\nThere are subtle differences in the way the Kubernetes APIs work for Windows due to the OS and container runtime. Some workload\nproperties were designed for Linux, and fail to run on Windows.\nAt a high level, these OS concepts are different:\nIdentity - Linux uses userID (UID) and groupID (GID) which are represented as integer types. User and group names are not\ncanonical - they are just an alias in /etc/groups or /etc/passwd back to UID+GID. Windows uses a larger binary security\nidentifier (SID) which is stored in the Windows Security Access Manager (SAM) database. This database is not shared between\nthe host and containers, or between containers.\nFile permissions - Windows uses an access control list based on (SIDs), whereas POSIX systems such as Linux use a bitmask\nbased on object permissions and UID+GID, plus optional access control lists.\nFile paths - the convention on Windows is to use \\ instead of / . The Go IO libraries typically accept both and just make it\nwork, but when you're setting a path or command line that's interpreted inside a container, \\ may be needed.\nSignals - Windows interactive apps handle termination differently, and can implement one or more of these:\nA UI thread handles well-defined messages including WM_CLOSE .\nConsole apps handle Ctrl-C or Ctrl-break using a Control Handler.\nServices register a Service Control Handler function that can accept SERVICE_CONTROL_STOP control codes.\nContainer exit codes follow the same convention where 0 is success, and nonzero is failure. The specific error codes may differ\nacross Windows and Linux. However, exit codes passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.\n\nField compatibility for container specifications\nThe following list documents differences between how Pod container specifications work between Windows and Linux:\nHuge pages are not implemented in the Windows container runtime, and are not available. They require asserting a user\nprivilege that's not configurable for containers.\nrequests.cpu and requests.memory - requests are subtracted from node available resources, so they can be used to avoid\noverprovisioning a node. However, they cannot be used to guarantee resources in an overprovisioned node. They should be\napplied to all containers as a best practice if the operator wants to avoid overprovisioning entirely.\nsecurityContext.allowPrivilegeEscalation - not possible on Windows; none of the capabilities are hooked up\nsecurityContext.capabilities\n\n- POSIX capabilities are not implemented on Windows\n\nsecurityContext.privileged"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0681", "text": "- Windows doesn't support privileged containers, use HostProcess Containers instead\n\nsecurityContext.procMount\n\n- Windows doesn't have a /proc filesystem\n\nsecurityContext.readOnlyRootFilesystem\n\n- not possible on Windows; write access is required for registry & system processes\n\nto run inside the container\nsecurityContext.runAsGroup\n\n- not possible on Windows as there is no GID support\n\n- this setting will prevent containers from running as ContainerAdministrator which is the\nclosest equivalent to a root user on Windows.\nsecurityContext.runAsUser - use runAsUserName instead\nsecurityContext.runAsNonRoot\n\nsecurityContext.seLinuxOptions\n\n- not possible on Windows as SELinux is Linux-specific\n\n- this has some limitations in that Windows doesn't support mapping single files. The default value is\n/dev/termination-log , which does work because it does not exist on Windows by default.\nterminationMessagePath\n\nField compatibility for Pod specifications\nThe following list documents differences between how Pod specifications work between Windows and Linux:\nhostIPC\n\nand hostpid - host namespace sharing is not possible on Windows\n\nhostNetwork\n\n- host networking is not possible on Windows\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n644/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n- setting the Pod dnsPolicy to ClusterFirstWithHostNet is not supported on Windows because host networking\nis not provided. Pods always run with a container network.\npodSecurityContext see below\ndnsPolicy\n\n- this is a beta feature, and depends on Linux namespaces which are not implemented on Windows.\nWindows cannot share process namespaces or the container's root filesystem. Only the network can be shared.\nterminationGracePeriodSeconds - this is not fully implemented in Docker on Windows, see the GitHub issue. The behavior\ntoday is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT, then Windows waits 5 seconds by default, and finally\nshuts down all processes using the normal Windows shutdown behavior. The 5 second default is actually in the Windows\nregistry inside the container, so it can be overridden when the container is built.\nvolumeDevices - this is a beta feature, and is not implemented on Windows. Windows cannot attach raw block devices to pods.\nshareProcessNamespace\n\nvolumes\n\nIf you define an emptyDir volume, you cannot set its volume source to memory .\nYou cannot enable mountPropagation for volume mounts as this is not supported on Windows.\n\nHost network access\nKubernetes v1.26 to v1.32 included alpha support for running Windows Pods in the host's network namespace.\nKubernetes v1.34 does not include the WindowsHostNetwork feature gate or support for running Windows Pods in the host's\nnetwork namespace."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0682", "text": "Field compatibility for Pod security context\nOnly the securityContext.runAsNonRoot and securityContext.windowsOptions from the Pod securityContext fields work on\nWindows.\n\nNode problem detector\nThe node problem detector (see Monitor Node Health) has preliminary support for Windows. For more information, visit the\nproject's GitHub page.\n\nPause container\nIn a Kubernetes Pod, an infrastructure or â€œpauseâ€ container is first created to host the container. In Linux, the cgroups and\nnamespaces that make up a pod need a process to maintain their continued existence; the pause process provides this. Containers\nthat belong to the same pod, including infrastructure and worker containers, share a common network endpoint (same IPv4 and / or\nIPv6 address, same network port spaces). Kubernetes uses pause containers to allow for worker containers crashing or restarting\nwithout losing any of the networking configuration.\nKubernetes maintains a multi-architecture image that includes support for Windows. For Kubernetes v1.34.0 the recommended\npause image is registry.k8s.io/pause:3.6 . The source code is available on GitHub.\nMicrosoft maintains a different multi-architecture image, with Linux and Windows amd64 support, that you can find as\nmcr.microsoft.com/oss/kubernetes/pause:3.6 . This image is built from the same source as the Kubernetes maintained image but\nall of the Windows binaries are authenticode signed by Microsoft. The Kubernetes project recommends using the Microsoft\nmaintained image if you are deploying to a production or production-like environment that requires signed binaries.\n\nContainer runtimes\nYou need to install a container runtime into each node in the cluster so that Pods can run there.\nThe following container runtimes work with Windows:\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nhttps://kubernetes.io/docs/concepts/_print/\n\n645/684\n\n11/7/25, 4:37 PM\n\nContainerD\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.20 [stable]\n\nYou can use ContainerD 1.4.0+ as the container runtime for Kubernetes nodes that run Windows.\nLearn how to install ContainerD on a Windows node.\nNote:\nThere is a known limitation when using GMSA with containerd to access Windows network shares, which requires a kernel\npatch.\n\nMirantis Container Runtime\nMirantis Container Runtime (MCR) is available as a container runtime for all Windows Server 2019 and later versions.\nSee Install MCR on Windows Servers for more information."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0683", "text": "Windows OS version compatibility\nOn Windows nodes, strict compatibility rules apply where the host OS version must match the container base image OS version.\nOnly Windows containers with a container operating system of Windows Server 2019 are fully supported.\nFor Kubernetes v1.34, operating system compatibility for Windows nodes (and Pods) is as follows:\nWindows Server LTSC release\nWindows Server 2019\nWindows Server 2022\nWindows Server SAC release\nWindows Server version 20H2\nThe Kubernetes version-skew policy also applies.\n\nHardware recommendations and considerations\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\n\nNote:\nThe following hardware specifications outlined here should be regarded as sensible default values. They are not intended to\nrepresent minimum requirements or specific recommendations for production environments. Depending on the requirements\nfor your workload these values may need to be adjusted.\n64-bit processor 4 CPU cores or more, capable of supporting virtualization\n8GB or more of RAM\n50GB or more of free disk space\nRefer to Hardware requirements for Windows Server Microsoft documentation for the most up-to-date information on minimum\nhardware requirements. For guidance on deciding on resources for production worker nodes refer to Production worker nodes\nKubernetes documentation.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n646/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nTo optimize system resources, if a graphical user interface is not required, it may be preferable to use a Windows Server OS\ninstallation that excludes the Windows Desktop Experience installation option, as this configuration typically frees up more system\nresources.\nIn assessing disk space for Windows worker nodes, take note that Windows container images are typically larger than Linux\ncontainer images, with container image sizes ranging from 300MB to over 10GB for a single image. Additionally, take note that the\nC: drive in Windows containers represents a virtual free size of 20GB by default, which is not the actual consumed space, but rather\nthe disk size for which a single container can grow to occupy when using local storage on the host. See Containers on Windows Container Storage Documentation for more detail."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0684", "text": "Getting help and troubleshooting\nYour main source of help for troubleshooting your Kubernetes cluster should start with the Troubleshooting page.\nSome additional, Windows-specific troubleshooting help is included in this section. Logs are an important element of\ntroubleshooting issues in Kubernetes. Make sure to include them any time you seek troubleshooting assistance from other\ncontributors. Follow the instructions in the SIG Windows contributing guide on gathering logs.\n\nReporting issues and feature requests\nIf you have what looks like a bug, or you would like to make a feature request, please follow the SIG Windows contributing guide to\ncreate a new issue. You should first search the list of issues in case it was reported previously and comment with your experience on\nthe issue and add additional logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support\nand troubleshooting ideas prior to creating a ticket.\n\nValidating the Windows cluster operability\nThe Kubernetes project provides a Windows Operational Readiness specification, accompanied by a structured test suite. This suite is\nsplit into two sets of tests, core and extended, each containing categories aimed at testing specific areas. It can be used to validate\nall the functionalities of a Windows and hybrid system (mixed with Linux nodes) with full coverage.\nTo set up the project on a newly created cluster, refer to the instructions in the project guide.\n\nDeployment tools\nThe kubeadm tool helps you to deploy a Kubernetes cluster, providing the control plane to manage the cluster it, and nodes to run\nyour workloads.\nThe Kubernetes cluster API project also provides means to automate deployment of Windows nodes.\n\nWindows distribution channels\nFor a detailed explanation of Windows distribution channels see the Microsoft documentation.\nInformation on the different Windows Server servicing channels including their support models can be found at Windows Server\nservicing channels.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n647/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0685", "text": "12.2 - Guide for Running Windows Containers in\nKubernetes\nThis page provides a walkthrough for some steps you can follow to run Windows containers using Kubernetes. The page also\nhighlights some Windows specific functionality within Kubernetes.\nIt is important to note that creating and deploying services and workloads on Kubernetes behaves in much the same way for Linux\nand Windows containers. The kubectl commands to interface with the cluster are identical. The examples in this page are provided\nto jumpstart your experience with Windows containers.\n\nObjectives\nConfigure an example deployment to run Windows containers on a Windows node.\n\nBefore you begin\nYou should already have access to a Kubernetes cluster that includes a worker node running Windows Server.\n\nGetting Started: Deploying a Windows workload\nThe example YAML file below deploys a simple webserver application running inside a Windows container.\nCreate a manifest named win-webserver.yaml with the contents below:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n648/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n--apiVersion: v1\nkind: Service\nmetadata:\nname: win-webserver\nlabels:\napp: win-webserver\nspec:\nports:\n# the port that this service should serve on\n- port: 80\ntargetPort: 80\nselector:\napp: win-webserver\ntype: NodePort\n--apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp: win-webserver\nname: win-webserver\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp: win-webserver\ntemplate:\nmetadata:\nlabels:\napp: win-webserver\nname: win-webserver\nspec:\ncontainers:\n- name: windowswebserver\nimage: mcr.microsoft.com/windows/servercore:ltsc2019\ncommand:\n- powershell.exe\n- -command\n- \"<#code used from https://gist.github.com/19WAS85/5424431#> ; $$listener = New-Object System.Net.HttpListe\nnodeSelector:\nkubernetes.io/os: windows\n\nNote:\nPort mapping is also supported, but for simplicity this example exposes port 80 of the container directly to the Service.\n1. Check that all nodes are healthy:\n\nkubectl get nodes\n\n2. Deploy the service and watch for pod updates:\n\nkubectl apply -f win-webserver.yaml\nkubectl get pods -o wide -w\n\nWhen the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.\n3. Check that the deployment succeeded. To verify:\nhttps://kubernetes.io/docs/concepts/_print/\n\n649/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nSeveral pods listed from the Linux control plane node, use kubectl get pods\nNode-to-pod communication across the network, curl port 80 of your pod IPs from the Linux control plane node to\ncheck for a web server response\nPod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node) using\nkubectl exec"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0686", "text": "Service-to-pod communication, curl the virtual service IP (seen under kubectl get services ) from the Linux control\nplane node and from individual pods\nService discovery, curl the service name with the Kubernetes default DNS suffix\nInbound connectivity, curl the NodePort from the Linux control plane node or machines outside of the cluster\nOutbound connectivity, curl external IPs from inside the pod using kubectl exec\nNote:\nWindows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the\nWindows networking stack. Only Windows pods are able to access service IPs.\n\nObservability\nCapturing logs from workloads\nLogs are an important element of observability; they enable users to gain insights into the operational aspect of workloads and are a\nkey ingredient to troubleshooting issues. Because Windows containers and workloads inside Windows containers behave differently\nfrom Linux containers, users had a hard time collecting logs, limiting operational visibility. Windows workloads for example are\nusually configured to log to ETW (Event Tracing for Windows) or push entries to the application event log. LogMonitor, an open\nsource tool by Microsoft, is the recommended way to monitor configured log sources inside a Windows container. LogMonitor\nsupports monitoring event logs, ETW providers, and custom application logs, piping them to STDOUT for consumption by kubectl\nlogs <pod> .\nFollow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files to all your containers and add the\nnecessary entrypoints for LogMonitor to push your logs to STDOUT.\n\nConfiguring container user\nUsing configurable Container usernames\nWindows containers can be configured to run their entrypoints and processes with different usernames than the image defaults.\nLearn more about it here.\n\nManaging Workload Identity with Group Managed Service Accounts\nWindows container workloads can be configured to use Group Managed Service Accounts (GMSA). Group Managed Service Accounts\nare a specific type of Active Directory account that provide automatic password management, simplified service principal name\n(SPN) management, and the ability to delegate the management to other administrators across multiple servers. Containers\nconfigured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the\nGMSA. Learn more about configuring and using GMSA for Windows containers here."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0687", "text": "Taints and tolerations\nUsers need to use some combination of taint and node selectors in order to schedule Linux and Windows workloads to their\nrespective OS-specific nodes. The recommended approach is outlined below, with one of its main goals being that this approach\nshould not break compatibility for existing Linux workloads.\nYou can (and should) set .spec.os.name for each Pod, to indicate the operating system that the containers in that Pod are designed\nfor. For Pods that run Linux containers, set .spec.os.name to linux . For Pods that run Windows containers, set .spec.os.name to\nwindows .\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n650/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nIf you are running a version of Kubernetes older than 1.24, you may need to enable the IdentifyPodOS feature gate to be able\nto set a value for .spec.pod.os.\nThe scheduler does not use the value of .spec.os.name when assigning Pods to nodes. You should use normal Kubernetes\nmechanisms for assigning pods to nodes to ensure that the control plane for your cluster places pods onto nodes that are running\nthe appropriate operating system.\nThe .spec.os.name value has no effect on the scheduling of the Windows pods, so taints and tolerations (or node selectors) are still\nrequired to ensure that the Windows pods land onto appropriate Windows nodes."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0688", "text": "Ensuring OS-specific workloads land on the appropriate container host\nUsers can ensure Windows containers can be scheduled on the appropriate host using taints and tolerations. All Kubernetes nodes\nrunning Kubernetes 1.34 have the following default labels:\nkubernetes.io/os = [windows|linux]\nkubernetes.io/arch = [amd64|arm64|...]\nIf a Pod specification does not specify a nodeSelector such as \"kubernetes.io/os\": windows , it is possible the Pod can be\nscheduled on any host, Windows or Linux. This can be problematic since a Windows container can only run on Windows and a Linux\ncontainer can only run on Linux. The best practice for Kubernetes 1.34 is to use a nodeSelector .\nHowever, in many cases users have a pre-existing large number of deployments for Linux containers, as well as an ecosystem of offthe-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with operators. In those\nsituations, you may be hesitant to make the configuration change to add nodeSelector fields to all Pods and Pod templates. The\nalternative is to use taints. Because the kubelet can set taints during registration, it could easily be modified to automatically add a\ntaint when running on Windows only.\nFor example: --register-with-taints='os=windows:NoSchedule'\nBy adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods). In order for a\nWindows Pod to be scheduled on a Windows node, it would need both the nodeSelector and the appropriate matching toleration\nto choose Windows.\n\nnodeSelector:\nkubernetes.io/os: windows\nnode.kubernetes.io/windows-build: '10.0.17763'\ntolerations:\n- key: \"os\"\noperator: \"Equal\"\nvalue: \"windows\"\neffect: \"NoSchedule\"\n\nHandling multiple Windows versions in the same cluster\nThe Windows Server version used by each pod must match that of the node. If you want to use multiple Windows Server versions in\nthe same cluster, then you should set additional node labels and nodeSelector fields.\nKubernetes automatically adds a label, node.kubernetes.io/windows-build to simplify this.\nThis label reflects the Windows major, minor, and build number that need to match for compatibility. Here are values used for each\nWindows Server version:\nProduct Name\n\nVersion\n\nWindows Server 2019\n\n10.0.17763\n\nWindows Server 2022\n\n10.0.20348\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n651/684\n\n11/7/25, 4:37 PM\n\nSimplifying with RuntimeClass\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0689", "text": "RuntimeClass can be used to simplify the process of using taints and tolerations. A cluster administrator can create a RuntimeClass\nobject which is used to encapsulate these taints and tolerations.\n1. Save this file to runtimeClasses.yml . It includes the appropriate nodeSelector for the Windows OS, architecture, and version.\n\n--apiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\nname: windows-2019\nhandler: example-container-runtime-handler\nscheduling:\nnodeSelector:\nkubernetes.io/os: 'windows'\nkubernetes.io/arch: 'amd64'\nnode.kubernetes.io/windows-build: '10.0.17763'\ntolerations:\n- effect: NoSchedule\nkey: os\noperator: Equal\nvalue: \"windows\"\n\n2. Run kubectl create -f runtimeClasses.yml using as a cluster administrator\n3. Add runtimeClassName: windows-2019 as appropriate to Pod specs\nFor example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n652/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n--apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: iis-2019\nlabels:\napp: iis-2019\nspec:\nreplicas: 1\ntemplate:\nmetadata:\nname: iis-2019\nlabels:\napp: iis-2019\nspec:\nruntimeClassName: windows-2019\ncontainers:\n- name: iis\nimage: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\nresources:\nlimits:\ncpu: 1\nmemory: 800Mi\nrequests:\ncpu: .1\nmemory: 300Mi\nports:\n- containerPort: 80\nselector:\nmatchLabels:\napp: iis-2019\n--apiVersion: v1\nkind: Service\nmetadata:\nname: iis\nspec:\ntype: LoadBalancer\nports:\n- protocol: TCP\nport: 80\nselector:\napp: iis-2019\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n653/684\n\n11/7/25, 4:37 PM\n\n13 - Extending Kubernetes\n\nConcepts | Kubernetes\n\nDifferent ways to change the behavior of your Kubernetes cluster.\nKubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or submit patches to the Kubernetes\nproject code.\nThis guide describes the options for customizing a Kubernetes cluster. It is aimed at cluster operators who want to understand how\nto adapt their Kubernetes cluster to the needs of their work environment. Developers who are prospective Platform Developers or\nKubernetes Project Contributors will also find it useful as an introduction to what extension points and patterns exist, and their\ntrade-offs and limitations.\nCustomization approaches can be broadly divided into configuration, which only involves changing command line arguments, local\nconfiguration files, or API resources; and extensions, which involve running additional programs, additional network services, or\nboth. This document is primarily about extensions.\n\nConfiguration\nConfiguration files and command arguments are documented in the Reference section of the online documentation, with a page for\neach binary:\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nkubelet\nkube-proxy"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0690", "text": "Command arguments and configuration files may not always be changeable in a hosted Kubernetes service or a distribution with\nmanaged installation. When they are changeable, they are usually only changeable by the cluster operator. Also, they are subject to\nchange in future Kubernetes versions, and setting them may require restarting processes. For those reasons, they should be used\nonly when there are no other options.\nBuilt-in policy APIs, such as ResourceQuota, NetworkPolicy and Role-based Access Control (RBAC), are built-in Kubernetes APIs that\nprovide declaratively configured policy settings. APIs are typically usable even with hosted Kubernetes services and with managed\nKubernetes installations. The built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods. When\nyou use a policy APIs that is stable, you benefit from a defined support policy like other Kubernetes APIs. For these reasons, policy\nAPIs are recommended over configuration files and command arguments where suitable.\n\nExtensions\nExtensions are software components that extend and deeply integrate with Kubernetes. They adapt it to support new types and new\nkinds of hardware.\nMany cluster administrators use a hosted or distribution instance of Kubernetes. These clusters come with extensions pre-installed.\nAs a result, most Kubernetes users will not need to install extensions and even fewer users will need to author new ones.\n\nExtension patterns\nKubernetes is designed to be automated by writing client programs. Any program that reads and/or writes to the Kubernetes API can\nprovide useful automation. Automation can run on the cluster or off it. By following the guidance in this doc you can write highly\navailable and robust automation. Automation generally works with any Kubernetes cluster, including hosted clusters and managed\ninstallations.\nThere is a specific pattern for writing client programs that work well with Kubernetes called the controller pattern. Controllers\ntypically read an object's .spec , possibly do things, and then update the object's .status .\nA controller is a client of the Kubernetes API. When Kubernetes is the client and calls out to a remote service, Kubernetes calls this a\nwebhook. The remote service is called a webhook backend. As with custom controllers, webhooks do add a point of failure.\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n654/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0691", "text": "Outside of Kubernetes, the term â€œwebhookâ€ typically refers to a mechanism for asynchronous notifications, where the\nwebhook call serves as a one-way notification to another system or component. In the Kubernetes ecosystem, even\nsynchronous HTTP callouts are often described as â€œwebhooksâ€.\nIn the webhook model, Kubernetes makes a network request to a remote service. With the alternative binary Plugin model,\nKubernetes executes a binary (program). Binary plugins are used by the kubelet (for example, CSI storage plugins and CNI network\nplugins), and by kubectl (see Extend kubectl with plugins).\n\nExtension points\nThis diagram shows the extension points in a Kubernetes cluster and the clients that access it.\n\nKubernetes extension points\n\nKey to the figure\n1. Users often interact with the Kubernetes API using kubectl . Plugins customise the behaviour of clients. There are generic\nextensions that can apply to different clients, as well as specific ways to extend kubectl .\n2. The API server handles all requests. Several types of extension points in the API server allow authenticating requests, or\nblocking them based on their content, editing content, and handling deletion. These are described in the API Access Extensions\nsection.\n3. The API server serves various kinds of resources. Built-in resource kinds, such as pods , are defined by the Kubernetes project\nand can't be changed. Read API extensions to learn about extending the Kubernetes API.\n4. The Kubernetes scheduler decides which nodes to place pods on. There are several ways to extend scheduling, which are\ndescribed in the Scheduling extensions section.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n655/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n5. Much of the behavior of Kubernetes is implemented by programs called controllers, that are clients of the API server.\nControllers are often used in conjunction with custom resources. Read combining new APIs with automation and Changing\nbuilt-in resources to learn more.\n6. The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs on the cluster network.\nNetwork Plugins allow for different implementations of pod networking.\n7. You can use Device Plugins to integrate custom hardware or other special node-local facilities, and make these available to\nPods running in your cluster. The kubelet includes support for working with device plugins.\nThe kubelet also mounts and unmounts volume for pods and their containers. You can use Storage Plugins to add support for\nnew kinds of storage and other volume types."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0692", "text": "Extension point choice flowchart\nIf you are unsure where to start, this flowchart can help. Note that some solutions may involve several types of extensions.\n\nYES\n\nDo you want to\nadd entirely new\ntypes to the\nKubernetes API?\n\nNO\n\nGo to \"API Extensions\"\nDo you want to restrict or\nautomatically edit fields in\nsome or all API types?\n\nYES\n\nGo to \"API Access\nExtensions\"\n\nNO\n\nDo you want to change the\nunderlying implementation of\nthe built-in API types?\n\nYES\n\nYES\n\nDo you want to change\nVolumes, Services, Ingresses,\nPersistentVolumes?\n\nGo to \"Infrastructure\"\n\nNO\n\nNO\n\nGo to \"Automation\"\n\nFlowchart guide to select an extension approach\n\nClient extensions\nPlugins for kubectl are separate binaries that add or replace the behavior of specific subcommands. The kubectl tool can also\nintegrate with credential plugins These extensions only affect a individual user's local environment, and so cannot enforce site-wide\npolicies.\nIf you want to extend the kubectl tool, read Extend kubectl with plugins.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n656/684\n\n11/7/25, 4:37 PM\n\nAPI extensions\n\nConcepts | Kubernetes\n\nCustom resource definitions\nConsider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other\ndeclarative APIs, and to manage them using Kubernetes tools, such as kubectl .\nFor more about Custom Resources, see the Custom Resources concept guide.\n\nAPI aggregation layer\nYou can use Kubernetes' API Aggregation Layer to integrate the Kubernetes API with additional services such as for metrics.\n\nCombining new APIs with automation\nA combination of a custom resource API and a control loop is called the controllers pattern. If your controller takes the place of a\nhuman operator deploying infrastructure based on a desired state, then the controller may also be following the operator pattern.\nThe Operator pattern is used to manage specific applications; usually, these are applications that maintain state and require care in\nhow they are managed.\nYou can also make your own custom APIs and control loops that manage other resources, such as storage, or to define policies (such\nas an access control restriction).\n\nChanging built-in resources\nWhen you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You\ncannot replace or change existing API groups. Adding an API does not directly let you affect the behavior of existing APIs (such as\nPods), whereas API Access Extensions do."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0693", "text": "API access extensions\nWhen a request reaches the Kubernetes API Server, it is first authenticated, then authorized, and is then subject to various types of\nadmission control (some requests are in fact not authenticated, and get special treatment). See Controlling Access to the Kubernetes\nAPI for more on this flow.\nEach of the steps in the Kubernetes authentication / authorization flow offers extension points.\n\nAuthentication\nAuthentication maps headers or certificates in all requests to a username for the client making the request.\nKubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can\nsend a token from an Authorization: header to a remote service for verification (an authentication webhook) if those don't meet\nyour needs.\n\nAuthorization\nAuthorization determines whether specific users can read, write, and do other operations on API resources. It works at the level of\nwhole resources -- it doesn't discriminate based on arbitrary object fields.\nIf the built-in authorization options don't meet your needs, an authorization webhook allows calling out to custom code that makes\nan authorization decision.\n\nDynamic admission control\nAfter a request is authorized, if it is a write operation, it also goes through Admission Control steps. In addition to the built-in steps,\nthere are several extensions:\nThe Image Policy webhook restricts what images can be run in containers.\nTo make arbitrary admission control decisions, a general Admission webhook can be used. Admission webhooks can reject\ncreations or updates. Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.\nhttps://kubernetes.io/docs/concepts/_print/\n\n657/684\n\n11/7/25, 4:37 PM\n\nInfrastructure extensions\n\nConcepts | Kubernetes\n\nDevice plugins\nDevice plugins allow a node to discover new Node resources (in addition to the builtin ones like cpu and memory) via a Device Plugin."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0694", "text": "Storage plugins\nContainer Storage Interface (CSI) plugins provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes\ncan be backed by durable external storage, or provide ephemeral storage, or they might offer a read-only interface to information\nusing a filesystem paradigm.\nKubernetes also includes support for FlexVolume plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).\nFlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When you run a Pod that relies\non FlexVolume storage, the kubelet calls a binary plugin to mount the volume. The archived FlexVolume design proposal has more\ndetail on this approach.\nThe Kubernetes Volume Plugin FAQ for Storage Vendors includes general information on storage plugins.\n\nNetwork plugins\nYour Kubernetes cluster needs a network plugin in order to have a working Pod network and to support other aspects of the\nKubernetes network model.\nNetwork Plugins allow Kubernetes to work with different networking topologies and technologies.\n\nKubelet image credential provider plugins\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]\n\nKubelet image credential providers are plugins for the kubelet to dynamically retrieve image registry credentials. The credentials are\nthen used when pulling images from container image registries that match the configuration.\nThe plugins can communicate with external services or use local files to obtain credentials. This way, the kubelet does not need to\nhave static credentials for each registry, and can support various authentication methods and protocols.\nFor plugin configuration details, see Configure a kubelet image credential provider."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0695", "text": "Scheduling extensions\nThe scheduler is a special type of controller that watches pods, and assigns pods to nodes. The default scheduler can be replaced\nentirely, while continuing to use other Kubernetes components, or multiple schedulers can run at the same time.\nThis is a significant undertaking, and almost all Kubernetes users find they do not need to modify the scheduler.\nYou can control which scheduling plugins are active, or associate sets of plugins with different named scheduler profiles. You can\nalso write your own plugin that integrates with one or more of the kube-scheduler's extension points.\nFinally, the built in kube-scheduler component supports a webhook that permits a remote HTTP backend (scheduler extension) to\nfilter and / or prioritize the nodes that the kube-scheduler chooses for a pod.\nNote:\nYou can only affect node filtering and node prioritization with a scheduler extender webhook; other extension points are not\navailable through the webhook integration.\n\nWhat's next\nLearn more about infrastructure extensions\nhttps://kubernetes.io/docs/concepts/_print/\n\n658/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nDevice Plugins\nNetwork Plugins\nCSI storage plugins\nLearn about kubectl plugins\nLearn more about Custom Resources\nLearn more about Extension API Servers\nLearn about Dynamic admission control\nLearn about the Operator pattern\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n659/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0696", "text": "13.1 - Compute, Storage, and Networking Extensions\nThis section covers extensions to your cluster that do not come as part as Kubernetes itself. You can use these extensions to\nenhance the nodes in your cluster, or to provide the network fabric that links Pods together.\nCSI and FlexVolume storage plugins\nContainer Storage Interface (CSI) plugins provide a way to extend Kubernetes with supports for new kinds of volumes. The\nvolumes can be backed by durable external storage, or provide ephemeral storage, or they might offer a read-only interface to\ninformation using a filesystem paradigm.\nKubernetes also includes support for FlexVolume plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).\nFlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When you run a Pod that\nrelies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume. The archived FlexVolume design proposal\nhas more detail on this approach.\nThe Kubernetes Volume Plugin FAQ for Storage Vendors includes general information on storage plugins.\nDevice plugins\nDevice plugins allow a node to discover new Node facilities (in addition to the built-in node resources such as cpu and\nmemory ), and provide these custom node-local facilities to Pods that request them.\nNetwork plugins\nNetwork plugins allow Kubernetes to work with different networking topologies and technologies. Your Kubernetes cluster\nneeds a network plugin in order to have a working Pod network and to support other aspects of the Kubernetes network model.\nKubernetes 1.34 is compatible with CNI network plugins.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n660/684\n\n11/7/25, 4:37 PM\n\n13.1.1 - Network Plugins\n\nConcepts | Kubernetes\n\nKubernetes (version 1.3 through to the latest 1.34, and likely onwards) lets you use Container Network Interface (CNI) plugins for\ncluster networking. You must use a CNI plugin that is compatible with your cluster and that suits your needs. Different plugins are\navailable (both open- and closed- source) in the wider Kubernetes ecosystem.\nA CNI plugin is required to implement the Kubernetes network model.\nYou must use a CNI plugin that is compatible with the v0.4.0 or later releases of the CNI specification. The Kubernetes project\nrecommends using a plugin that is compatible with the v1.0.0 CNI specification (plugins can be compatible with multiple spec\nversions)."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0697", "text": "Installation\nA Container Runtime, in the networking context, is a daemon on a node configured to provide CRI Services for kubelet. In particular,\nthe Container Runtime must be configured to load the CNI plugins required to implement the Kubernetes network model.\nNote:\nPrior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the cni-bin-dir and network-plugin\ncommand-line parameters. These command-line parameters were removed in Kubernetes 1.24, with management of the CNI\nno longer in scope for kubelet.\nSee Troubleshooting CNI plugin-related errors if you are facing issues following the removal of dockershim.\n\nFor specific information about how a Container Runtime manages the CNI plugins, see the documentation for that Container\nRuntime, for example:\ncontainerd\nCRI-O\nFor specific information about how to install and manage a CNI plugin, see the documentation for that plugin or networking\nprovider.\n\nNetwork Plugin Requirements\nLoopback CNI\nIn addition to the CNI plugin installed on the nodes for implementing the Kubernetes network model, Kubernetes also requires the\ncontainer runtimes to provide a loopback interface lo , which is used for each sandbox (pod sandboxes, vm sandboxes, ...).\nImplementing the loopback interface can be accomplished by re-using the CNI loopback plugin. or by developing your own code to\nachieve this (see this example from CRI-O).\n\nSupport hostPort\nThe CNI networking plugin supports hostPort . You can use the official portmap plugin offered by the CNI plugin team or use your\nown plugin with portMapping functionality.\nIf you want to enable hostPort support, you must specify portMappings capability in your cni-conf-dir . For example:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n661/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n{\n\"name\": \"k8s-pod-network\",\n\"cniVersion\": \"0.4.0\",\n\"plugins\": [\n{\n\"type\": \"calico\",\n\"log_level\": \"info\",\n\"datastore_type\": \"kubernetes\",\n\"nodename\": \"127.0.0.1\",\n\"ipam\": {\n\"type\": \"host-local\",\n\"subnet\": \"usePodCidr\"\n},\n\"policy\": {\n\"type\": \"k8s\"\n},\n\"kubernetes\": {\n\"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n}\n},\n{\n\"type\": \"portmap\",\n\"capabilities\": {\"portMappings\": true},\n\"externalSetMarkChain\": \"KUBE-MARK-MASQ\"\n}\n]\n}\n\nSupport traffic shaping\nExperimental Feature\nThe CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the official bandwidth plugin offered by\nthe CNI plugin team or use your own plugin with bandwidth control functionality.\nIf you want to enable traffic shaping support, you must add the bandwidth plugin to your CNI configuration file (default\n/etc/cni/net.d ) and ensure that the binary is included in your CNI bin dir (default /opt/cni/bin )."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0698", "text": "{\n\"name\": \"k8s-pod-network\",\n\"cniVersion\": \"0.4.0\",\n\"plugins\": [\n{\n\"type\": \"calico\",\n\"log_level\": \"info\",\n\"datastore_type\": \"kubernetes\",\n\"nodename\": \"127.0.0.1\",\n\"ipam\": {\n\"type\": \"host-local\",\n\"subnet\": \"usePodCidr\"\n},\n\"policy\": {\n\"type\": \"k8s\"\n},\n\"kubernetes\": {\n\"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\"\n}\n},\n{\n\"type\": \"bandwidth\",\n\"capabilities\": {\"bandwidth\": true}\n}\n]\n}\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n662/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nNow you can add the kubernetes.io/ingress-bandwidth and kubernetes.io/egress-bandwidth annotations to your Pod. For\nexample:\n\napiVersion: v1\nkind: Pod\nmetadata:\nannotations:\nkubernetes.io/ingress-bandwidth: 1M\nkubernetes.io/egress-bandwidth: 1M\n...\n\nWhat's next\nLearn more about Cluster Networking\nLearn more about Network Policies\nLearn about the Troubleshooting CNI plugin-related errors\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n663/684\n\n11/7/25, 4:37 PM\n\n13.1.2 - Device Plugins\n\nConcepts | Kubernetes\n\nDevice plugins let you configure your cluster with support for devices or resources that require vendorspecific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory.\nâ“˜ FEATURE STATE: Kubernetes v1.26 [stable]\n\nKubernetes provides a device plugin framework that you can use to advertise system hardware resources to the Kubelet.\nInstead of customizing the code for Kubernetes itself, vendors can implement a device plugin that you deploy either manually or as a\nDaemonSet. The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters, and other similar computing\nresources that may require vendor specific initialization and setup.\n\nDevice plugin registration\nThe kubelet exports a Registration gRPC service:\nservice Registration {\nrpc Register(RegisterRequest) returns (Empty) {}\n}"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0699", "text": "A device plugin can register itself with the kubelet through this gRPC service. During the registration, the device plugin needs to\nsend:\nThe name of its Unix socket.\nThe Device Plugin API version against which it was built.\nThe ResourceName it wants to advertise. Here ResourceName needs to follow the extended resource naming scheme as\nvendor-domain/resourcetype . (For example, an NVIDIA GPU is advertised as nvidia.com/gpu .)\nFollowing a successful registration, the device plugin sends the kubelet the list of devices it manages, and the kubelet is then in\ncharge of advertising those resources to the API server as part of the kubelet node status update. For example, after a device plugin\nregisters hardware-vendor.example/foo with the kubelet and reports two healthy devices on a node, the node status is updated to\nadvertise that the node has 2 \"Foo\" devices installed and available.\nThen, users can request devices as part of a Pod specification (see container ). Requesting extended resources is similar to how you\nmanage requests and limits for other resources, with the following differences:\nExtended resources are only supported as integer resources and cannot be overcommitted.\nDevices cannot be shared between containers.\n\nExample\nSuppose a Kubernetes cluster is running a device plugin that advertises resource hardware-vendor.example/foo on certain nodes.\nHere is an example of a pod requesting this resource to run a demo workload:\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n664/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n--apiVersion: v1\nkind: Pod\nmetadata:\nname: demo-pod\nspec:\ncontainers:\n- name: demo-container-1\nimage: registry.k8s.io/pause:3.8\nresources:\nlimits:\nhardware-vendor.example/foo: 2\n#\n# This Pod needs 2 of the hardware-vendor.example/foo devices\n# and can only schedule onto a Node that's able to satisfy\n# that need.\n#\n# If the Node has more than 2 of those devices available, the\n# remainder would be available for other Pods to use."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0700", "text": "Device plugin implementation\nThe general workflow of a device plugin includes the following steps:\n1. Initialization. During this phase, the device plugin performs vendor-specific initialization and setup to make sure the devices are\nin a ready state.\n2. The plugin starts a gRPC service, with a Unix socket under the host path /var/lib/kubelet/device-plugins/ , that implements\nthe following interfaces:\nservice DevicePlugin {\n// GetDevicePluginOptions returns options to be communicated with Device Manager.\nrpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}\n// ListAndWatch returns a stream of List of Devices\n// Whenever a Device state change or a Device disappears, ListAndWatch\n// returns the new list\nrpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\n// Allocate is called during container creation so that the Device\n// Plugin can run device specific operations and instruct Kubelet\n// of the steps to make the Device available in the container\nrpc Allocate(AllocateRequest) returns (AllocateResponse) {}\n// GetPreferredAllocation returns a preferred set of devices to allocate\n// from a list of available ones. The resulting preferred allocation is not\n// guaranteed to be the allocation ultimately performed by the\n// devicemanager. It is only designed to help the devicemanager make a more\n// informed allocation decision when possible.\nrpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}\n// PreStartContainer is called, if indicated by Device Plugin during registration phase,\n// before each container start. Device plugin can run device specific operations\n// such as resetting the device before making devices available to the container.\nrpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}\n}\n\nNote:\nPlugins are not required to provide useful implementations for GetPreferredAllocation() or PreStartContainer(). Flags\nindicating the availability of these calls, if any, should be set in the DevicePluginOptions message sent back by a call to\nGetDevicePluginOptions(). The kubelet will always call GetDevicePluginOptions() to see which optional functions are\navailable, before calling any of them directly.\nhttps://kubernetes.io/docs/concepts/_print/\n\n665/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0701", "text": "3. The plugin registers itself with the kubelet through the Unix socket at host path /var/lib/kubelet/deviceplugins/kubelet.sock .\nNote:\nThe ordering of the workflow is important. A plugin MUST start serving gRPC service before registering itself with kubelet\nfor successful registration.\n4. After successfully registering itself, the device plugin runs in serving mode, during which it keeps monitoring device health and\nreports back to the kubelet upon any device state changes. It is also responsible for serving Allocate gRPC requests. During\nAllocate , the device plugin may do device-specific preparation; for example, GPU cleanup or QRNG initialization. If the\noperations succeed, the device plugin returns an AllocateResponse that contains container runtime configurations for\naccessing the allocated devices. The kubelet passes this information to the container runtime.\nAn AllocateResponse contains zero or more ContainerAllocateResponse objects. In these, the device plugin defines\nmodifications that must be made to a container's definition to provide access to the device. These modifications include:\nAnnotations\ndevice nodes\nenvironment variables\nmounts\nfully-qualified CDI device names\nNote:\nThe processing of the fully-qualified CDI device names by the Device Manager requires that the DevicePluginCDIDevices\nfeature gate is enabled for both the kubelet and the kube-apiserver. This was added as an alpha feature in Kubernetes\nv1.28, graduated to beta in v1.29 and to GA in v1.31.\n\nHandling kubelet restarts\nA device plugin is expected to detect kubelet restarts and re-register itself with the new kubelet instance. A new kubelet instance\ndeletes all the existing Unix sockets under /var/lib/kubelet/device-plugins when it starts. A device plugin can monitor the\ndeletion of its Unix socket and re-register itself upon such an event."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0702", "text": "Device plugin and unhealthy devices\nThere are cases when devices fail or are shut down. The responsibility of the Device Plugin in this case is to notify the kubelet about\nthe situation using the ListAndWatchResponse API.\nOnce a device is marked as unhealthy, the kubelet will decrease the allocatable count for this resource on the Node to reflect how\nmany devices can be used for scheduling new pods. Capacity count for the resource will not change.\nPods that were assigned to the failed devices will continue be assigned to this device. It is typical that code relying on the device will\nstart failing and Pod may get into Failed phase if restartPolicy for the Pod was not Always or enter the crash loop otherwise.\nBefore Kubernetes v1.31, the way to know whether or not a Pod is associated with the failed device is to use the PodResources API.\nâ“˜ FEATURE STATE: Kubernetes v1.31 [alpha] (enabled by default: false)\n\nBy enabling the feature gate ResourceHealthStatus , the field allocatedResourcesStatus will be added to each container status,\nwithin the .status for each Pod. The allocatedResourcesStatus field reports health information for each device assigned to the\ncontainer.\nFor a failed Pod, or where you suspect a fault, you can use this status to understand whether the Pod behavior may be associated\nwith device failure. For example, if an accelerator is reporting an over-temperature event, the allocatedResourcesStatus field may\nbe able to report this.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n666/684\n\n11/7/25, 4:37 PM\n\nDevice plugin deployment\n\nConcepts | Kubernetes\n\nYou can deploy a device plugin as a DaemonSet, as a package for your node's operating system, or manually.\nThe canonical directory /var/lib/kubelet/device-plugins requires privileged access, so a device plugin must run in a privileged\nsecurity context. If you're deploying a device plugin as a DaemonSet, /var/lib/kubelet/device-plugins must be mounted as a\nVolume in the plugin's PodSpec.\nIf you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's Pod onto Nodes, to restart the\ndaemon Pod after failure, and to help automate upgrades."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0703", "text": "API compatibility\nPreviously, the versioning scheme required the Device Plugin's API version to match exactly the Kubelet's version. Since the\ngraduation of this feature to Beta in v1.12 this is no longer a hard requirement. The API is versioned and has been stable since Beta\ngraduation of this feature. Because of this, kubelet upgrades should be seamless but there still may be changes in the API before\nstabilization making upgrades not guaranteed to be non-breaking.\nNote:\nAlthough the Device Manager component of Kubernetes is a generally available feature, the device plugin API is not stable. For\ninformation on the device plugin API and version compatibility, read Device Plugin API versions.\nAs a project, Kubernetes recommends that device plugin developers:\nWatch for Device Plugin API changes in the future releases.\nSupport multiple versions of the device plugin API for backward/forward compatibility.\nTo run device plugins on nodes that need to be upgraded to a Kubernetes release with a newer device plugin API version, upgrade\nyour device plugins to support both versions before upgrading these nodes. Taking that approach will ensure the continuous\nfunctioning of the device allocations during the upgrade.\n\nMonitoring device plugin resources\nâ“˜ FEATURE STATE: Kubernetes v1.28 [stable]\n\nIn order to monitor resources provided by device plugins, monitoring agents need to be able to discover the set of devices that are\nin-use on the node and obtain metadata to describe which container the metric should be associated with. Prometheus metrics\nexposed by device monitoring agents should follow the Kubernetes Instrumentation Guidelines, identifying containers using pod ,\nnamespace , and container prometheus labels.\nThe kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata for these devices:\n// PodResourcesLister is a service provided by the kubelet that provides information about the\n// node resources consumed by pods and containers on the node\nservice PodResourcesLister {\nrpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}\nrpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}\nrpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {}\n}\n\nList gRPC endpoint\nThe List endpoint provides information on resources of running pods, with details such as the id of exclusively allocated CPUs,\ndevice id as it was reported by device plugins and id of the NUMA node where these devices are allocated. Also, for NUMA-based\nmachines, it contains the information about memory and hugepages reserved for a container.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n667/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0704", "text": "Starting from Kubernetes v1.27, the List endpoint can provide information on resources of running pods allocated in\nResourceClaims by the DynamicResourceAllocation API. Starting from Kubernetes v1.34, this feature is enabled by default. To\ndisable, kubelet must be started with the following flags:\n--feature-gates=KubeletPodResourcesDynamicResources=false\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n668/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n// ListPodResourcesResponse is the response returned by List function\nmessage ListPodResourcesResponse {\nrepeated PodResources pod_resources = 1;\n}\n// PodResources contains information about the node resources assigned to a pod\nmessage PodResources {\nstring name = 1;\nstring namespace = 2;\nrepeated ContainerResources containers = 3;\n}\n// ContainerResources contains information about the resources assigned to a container\nmessage ContainerResources {\nstring name = 1;\nrepeated ContainerDevices devices = 2;\nrepeated int64 cpu_ids = 3;\nrepeated ContainerMemory memory = 4;\nrepeated DynamicResource dynamic_resources = 5;\n}\n// ContainerMemory contains information about memory and hugepages assigned to a container\nmessage ContainerMemory {\nstring memory_type = 1;\nuint64 size = 2;\nTopologyInfo topology = 3;\n}\n// Topology describes hardware topology of the resource\nmessage TopologyInfo {\nrepeated NUMANode nodes = 1;\n}\n// NUMA representation of NUMA node\nmessage NUMANode {\nint64 ID = 1;\n}\n// ContainerDevices contains information about the devices assigned to a container\nmessage ContainerDevices {\nstring resource_name = 1;\nrepeated string device_ids = 2;\nTopologyInfo topology = 3;\n}\n// DynamicResource contains information about the devices assigned to a container by Dynamic Resource Allocation\nmessage DynamicResource {\nstring class_name = 1;\nstring claim_name = 2;\nstring claim_namespace = 3;\nrepeated ClaimResource claim_resources = 4;\n}\n// ClaimResource contains per-plugin resource information\nmessage ClaimResource {\nrepeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = \"CDIDevices\"];\n}\n// CDIDevice specifies a CDI device information\nmessage CDIDevice {\n// Fully qualified CDI device name\n// for example: vendor.com/gpu=gpudevice1\n// see more details in the CDI specification:\n// https://github.com/container-orchestrated-devices/container-device-interface/blob/main/SPEC.md\nstring name = 1;\n}\n\nNote:\nhttps://kubernetes.io/docs/concepts/_print/\n\n669/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0705", "text": "cpu_ids in the ContainerResources in the List endpoint correspond to exclusive CPUs allocated to a particular container. If\nthe goal is to evaluate CPUs that belong to the shared pool, the List endpoint needs to be used in conjunction with the\nGetAllocatableResources endpoint as explained below:\n1. Call GetAllocatableResources to get a list of all the allocatable CPUs\n2. Call GetCpuIds on all ContainerResources in the system\n3. Subtract out all of the CPUs from the GetCpuIds calls from the GetAllocatableResources call\n\nGetAllocatableResources gRPC endpoint\nâ“˜ FEATURE STATE: Kubernetes v1.28 [stable]\n\nGetAllocatableResources provides information on resources initially available on the worker node. It provides more information than\nkubelet exports to APIServer.\nNote:\n\nshould only be used to evaluate allocatable resources on a node. If the goal is to evaluate\nfree/unallocated resources it should be used in conjunction with the List() endpoint. The result obtained by\nGetAllocatableResources would remain the same unless the underlying resources exposed to kubelet change. This happens\nrarely but when it does (for example: hotplug/hotunplug, device health changes), client is expected to call\nGetAlloctableResources endpoint.\nGetAllocatableResources\n\nHowever, calling GetAllocatableResources endpoint is not sufficient in case of cpu and/or memory update and Kubelet needs\nto be restarted to reflect the correct resource capacity and allocatable.\n\n// AllocatableResourcesResponses contains information about all the devices known by the kubelet\nmessage AllocatableResourcesResponse {\nrepeated ContainerDevices devices = 1;\nrepeated int64 cpu_ids = 2;\nrepeated ContainerMemory memory = 3;\n}\n\ndo expose the topology information declaring to which NUMA cells the device is affine. The NUMA cells are\nidentified using a opaque integer ID, which value is consistent to what device plugins report when they register themselves to the\nkubelet.\nContainerDevices"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0706", "text": "The gRPC service is served over a unix socket at /var/lib/kubelet/pod-resources/kubelet.sock . Monitoring agents for device\nplugin resources can be deployed as a daemon, or as a DaemonSet. The canonical directory /var/lib/kubelet/pod-resources\nrequires privileged access, so monitoring agents must run in a privileged security context. If a device monitoring agent is running as\na DaemonSet, /var/lib/kubelet/pod-resources must be mounted as a Volume in the device monitoring agent's PodSpec.\nNote:\nWhen accessing the /var/lib/kubelet/pod-resources/kubelet.sock from DaemonSet or any other app deployed as a\ncontainer on the host, which is mounting socket as a volume, it is a good practice to mount directory /var/lib/kubelet/podresources/ instead of the /var/lib/kubelet/pod-resources/kubelet.sock . This will ensure that after kubelet restart,\ncontainer will be able to re-connect to this socket.\nContainer mounts are managed by inode referencing the socket or directory, depending on what was mounted. When kubelet\nrestarts, socket is deleted and a new socket is created, while directory stays untouched. So the original inode for the socket\nbecome unusable. Inode to directory will continue working.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n670/684\n\n11/7/25, 4:37 PM\n\nGet gRPC endpoint\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.34 [beta]\n\nThe Get endpoint provides information on resources of a running Pod. It exposes information similar to those described in the\nList endpoint. The Get endpoint requires PodName and PodNamespace of the running Pod.\n// GetPodResourcesRequest contains information about the pod\nmessage GetPodResourcesRequest {\nstring pod_name = 1;\nstring pod_namespace = 2;\n}\n\nTo disable this feature, you must start your kubelet services with the following flag:\n--feature-gates=KubeletPodResourcesGet=false\n\nThe Get endpoint can provide Pod information related to dynamic resources allocated by the dynamic resource allocation API.\nStarting from Kubernetes v1.34, this feature is enabled by default. To disable, kubelet must be started with the following flags:\n--feature-gates=KubeletPodResourcesDynamicResources=false\n\nDevice plugin integration with the Topology Manager\nâ“˜ FEATURE STATE: Kubernetes v1.27 [stable]\n\nThe Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology aligned manner. In order to\ndo this, the Device Plugin API was extended to include a TopologyInfo struct.\nmessage TopologyInfo {\nrepeated NUMANode nodes = 1;\n}\nmessage NUMANode {\nint64 ID = 1;\n}"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0707", "text": "Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo struct as part of the device\nregistration, along with the device IDs and the health of the device. The device manager will then use this information to consult with\nthe Topology Manager and make resource assignment decisions.\nsupports setting a nodes field to either nil or a list of NUMA nodes. This allows the Device Plugin to advertise a\ndevice that spans multiple NUMA nodes.\nTopologyInfo\n\nSetting TopologyInfo to nil or providing an empty list of NUMA nodes for a given device indicates that the Device Plugin does not\nhave a NUMA affinity preference for that device.\nAn example TopologyInfo struct populated for a device by a Device Plugin:\npluginapi.Device{ID: \"25102017\", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo{Nodes: []*pluginapi.NUM\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n671/684\n\n11/7/25, 4:37 PM\n\nDevice plugin examples\n\nConcepts | Kubernetes\n\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nHere are some examples of device plugin implementations:\nAkri, which lets you easily expose heterogeneous leaf devices (such as IP cameras and USB devices).\nThe AMD GPU device plugin\nThe generic device plugin for generic Linux devices and USB devices\nThe HAMi for heterogeneous AI computing virtualization middleware (for example, NVIDIA, Cambricon, Hygon, Iluvatar,\nMThreads, Ascend, Metax)\nThe Intel device plugins for Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devices\nThe KubeVirt device plugins for hardware-assisted virtualization\nThe NVIDIA GPU device plugin, NVIDIA's official device plugin to expose NVIDIA GPUs and monitor GPU health\nThe NVIDIA GPU device plugin for Container-Optimized OS\nThe RDMA device plugin\nThe SocketCAN device plugin\nThe Solarflare device plugin\nThe SR-IOV Network device plugin\nThe Xilinx FPGA device plugins for Xilinx FPGA devices\n\nWhat's next\nLearn about scheduling GPU resources using device plugins\nLearn about advertising extended resources on a node\nLearn about the Topology Manager\nRead about using hardware acceleration for TLS ingress with Kubernetes\nRead more about Extended Resource allocation by DRA\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n672/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0708", "text": "13.2 - Extending the Kubernetes API\nCustom resources are extensions of the Kubernetes API. Kubernetes provides two ways to add custom resources to your cluster:\nThe CustomResourceDefinition (CRD) mechanism allows you to declaratively define a new custom API with an API group, kind,\nand schema that you specify. The Kubernetes control plane serves and handles the storage of your custom resource. CRDs\nallow you to create new types of resources for your cluster without writing and running a custom API server.\nThe aggregation layer sits behind the primary API server, which acts as a proxy. This arrangement is called API Aggregation (AA),\nwhich allows you to provide specialized implementations for your custom resources by writing and deploying your own API\nserver. The main API server delegates requests to your API server for the custom APIs that you specify, making them available\nto all of its clients.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n673/684\n\n11/7/25, 4:37 PM\n\n13.2.1 - Custom Resources\n\nConcepts | Kubernetes\n\nCustom resources are extensions of the Kubernetes API. This page discusses when to add a custom resource to your Kubernetes\ncluster and when to use a standalone service. It describes the two methods for adding custom resources and how to choose\nbetween them.\n\nCustom resources\nA resource is an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind; for example, the built-in pods\nresource contains a collection of Pod objects.\nA custom resource is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation. It\nrepresents a customization of a particular Kubernetes installation. However, many core Kubernetes functions are now built using\ncustom resources, making Kubernetes more modular.\nCustom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update\ncustom resources independently of the cluster itself. Once a custom resource is installed, users can create and access its objects\nusing kubectl, just as they do for built-in resources like Pods."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0709", "text": "Custom controllers\nOn their own, custom resources let you store and retrieve structured data. When you combine a custom resource with a custom\ncontroller, custom resources provide a true declarative API.\nThe Kubernetes declarative API enforces a separation of responsibilities. You declare the desired state of your resource. The\nKubernetes controller keeps the current state of Kubernetes objects in sync with your declared desired state. This is in contrast to an\nimperative API, where you instruct a server what to do.\nYou can deploy and update a custom controller on a running cluster, independently of the cluster's lifecycle. Custom controllers can\nwork with any kind of resource, but they are especially effective when combined with custom resources. The Operator pattern\ncombines custom resources and custom controllers. You can use custom controllers to encode domain knowledge for specific\napplications into an extension of the Kubernetes API.\n\nShould I add a custom resource to my Kubernetes cluster?\nWhen creating a new API, consider whether to aggregate your API with the Kubernetes cluster APIs or let your API stand alone.\nConsider API aggregation if:\n\nPrefer a stand-alone API if:\n\nYour API is Declarative.\n\nYour API does not fit the Declarative model.\n\nYou want your new types to be readable and writable using kubectl .\n\nkubectl support is not required\n\nYou want to view your new types in a Kubernetes UI, such as dashboard,\nalongside built-in types.\n\nKubernetes UI support is not required.\n\nYou are developing a new API.\n\nYou already have a program that serves your API\nand works well.\n\nYou are willing to accept the format restriction that Kubernetes puts on\nREST resource paths, such as API Groups and Namespaces. (See the API\nOverview.)\n\nYou need to have specific REST paths to be\ncompatible with an already defined REST API.\n\nYour resources are naturally scoped to a cluster or namespaces of a\ncluster.\n\nCluster or namespace scoped resources are a poor\nfit; you need control over the specifics of resource\npaths.\n\nYou want to reuse Kubernetes API support features.\n\nYou don't need those features.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n674/684\n\n11/7/25, 4:37 PM\n\nDeclarative APIs\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0710", "text": "In a Declarative API, typically:\nYour API consists of a relatively small number of relatively small objects (resources).\nThe objects define configuration of applications or infrastructure.\nThe objects are updated relatively infrequently.\nHumans often need to read and write the objects.\nThe main operations on the objects are CRUD-y (creating, reading, updating and deleting).\nTransactions across objects are not required: the API represents a desired state, not an exact state.\nImperative APIs are not declarative. Signs that your API might not be declarative include:\nThe client says \"do this\", and then gets a synchronous response back when it is done.\nThe client says \"do this\", and then gets an operation ID back, and has to check a separate Operation object to determine\ncompletion of the request.\nYou talk about Remote Procedure Calls (RPCs).\nDirectly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.\nHigh bandwidth access (10s of requests per second sustained) needed.\nStore end-user data (such as images, PII, etc.) or other large-scale data processed by applications.\nThe natural operations on the objects are not CRUD-y.\nThe API is not easily modeled as objects.\nYou chose to represent pending operations with an operation ID or an operation object."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0711", "text": "Should I use a ConfigMap or a custom resource?\nUse a ConfigMap if any of the following apply:\nThere is an existing, well-documented configuration file format, such as a mysql.cnf or pom.xml .\nYou want to put the entire configuration into one key of a ConfigMap.\nThe main use of the configuration file is for a program running in a Pod on your cluster to consume the file to configure itself.\nConsumers of the file prefer to consume via file in a Pod or environment variable in a pod, rather than the Kubernetes API.\nYou want to perform rolling updates via Deployment, etc., when the file is updated.\nNote:\nUse a Secret for sensitive data, which is similar to a ConfigMap but more secure.\nUse a custom resource (CRD or Aggregated API) if most of the following apply:\nYou want to use Kubernetes client libraries and CLIs to create and update the new resource.\nYou want top-level support from kubectl ; for example, kubectl get my-object object-name .\nYou want to build new automation that watches for updates on the new object, and then CRUD other objects, or vice versa.\nYou want to write automation that handles updates to the object.\nYou want to use Kubernetes API conventions like .spec , .status , and .metadata .\nYou want the object to be an abstraction over a collection of controlled resources, or a summarization of other resources.\n\nAdding custom resources\nKubernetes provides two ways to add custom resources to your cluster:\nCRDs are simple and can be created without any programming.\nAPI Aggregation requires programming, but allows more control over API behaviors like how data is stored and conversion\nbetween API versions.\nKubernetes provides these two options to meet the needs of different users, so that neither ease of use nor flexibility is\ncompromised.\nhttps://kubernetes.io/docs/concepts/_print/\n\n675/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0712", "text": "Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts as a proxy. This arrangement is called\nAPI Aggregation(AA). To users, the Kubernetes API appears extended.\nCRDs allow users to create new types of resources without adding another API server. You do not need to understand API\nAggregation to use CRDs.\nRegardless of how they are installed, the new resources are referred to as Custom Resources to distinguish them from built-in\nKubernetes resources (like pods).\nNote:\nAvoid using a Custom Resource as data storage for application, end user, or monitoring data: architecture designs that store\napplication data within the Kubernetes API typically represent a design that is too closely coupled.\nArchitecturally, cloud native application architectures favor loose coupling between components. If part of your workload\nrequires a backing service for its routine operation, run that backing service as a component or consume it as an external\nservice. This way, your workload does not rely on the Kubernetes API for its normal operation.\n\nCustomResourceDefinitions\nThe CustomResourceDefinition API resource allows you to define custom resources. Defining a CRD object creates a new custom\nresource with a name and schema that you specify. The Kubernetes API serves and handles the storage of your custom resource.\nThe name of the CRD object itself must be a valid DNS subdomain name derived from the defined resource name and its API group;\nsee how to create a CRD for more details. Further, the name of an object whose kind/resource is defined by a CRD must also be a\nvalid DNS subdomain name.\nThis frees you from writing your own API server to handle the custom resource, but the generic nature of the implementation means\nyou have less flexibility than with API server aggregation.\nRefer to the custom controller example for an example of how to register a new custom resource, work with instances of your new\nresource type, and use a controller to handle events."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0713", "text": "API server aggregation\nUsually, each resource in the Kubernetes API requires code that handles REST requests and manages persistent storage of objects.\nThe main Kubernetes API server handles built-in resources like pods and services, and can also generically handle custom resources\nthrough CRDs.\nThe aggregation layer allows you to provide specialized implementations for your custom resources by writing and deploying your\nown API server. The main API server delegates requests to your API server for the custom resources that you handle, making them\navailable to all of its clients.\n\nChoosing a method for adding custom resources\nCRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.\nTypically, CRDs are a good fit if:\nYou have a handful of fields\nYou are using the resource within your company, or as part of a small open-source project (as opposed to a commercial\nproduct)\n\nComparing ease of use\nCRDs are easier to create than Aggregated APIs.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n676/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nCRDs\n\nAggregated API\n\nDo not require programming. Users can choose any language for a\nCRD controller.\n\nRequires programming and building binary and image.\n\nNo additional service to run; CRDs are handled by API server.\n\nAn additional service to create and that could fail.\n\nNo ongoing support once the CRD is created. Any bug fixes are\npicked up as part of normal Kubernetes Master upgrades.\n\nMay need to periodically pickup bug fixes from upstream\nand rebuild and update the Aggregated API server.\n\nNo need to handle multiple versions of your API; for example, when\nyou control the client for this resource, you can upgrade it in sync\nwith the API.\n\nYou need to handle multiple versions of your API; for\nexample, when developing an extension to share with\nthe world.\n\nAdvanced features and flexibility\nAggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.\nAggregated\nAPI\n\nFeature\n\nDescription\n\nCRDs\n\nValidation\n\nHelp users prevent errors and allow you to\nevolve your API independently of your clients.\nThese features are most useful when there\nare many clients who can't all update at the\nsame time."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0714", "text": "Yes. Most validation can be specified in the\nCRD using OpenAPI v3.0 validation.\nCRDValidationRatcheting feature gate allows\nfailing validations specified using OpenAPI\nalso can be ignored if the failing part of the\nresource was unchanged. Any other\nvalidations supported by addition of a\nValidating Webhook.\n\nYes, arbitrary\nvalidation\nchecks\n\nDefaulting\n\nSee above\n\nYes, either via OpenAPI v3.0 validation\ndefault keyword (GA in 1.17), or via a\nMutating Webhook (though this will not be\nrun when reading from etcd for old objects).\n\nYes\n\nMultiversioning\n\nAllows serving the same object through two\nAPI versions. Can help ease API changes like\nrenaming fields. Less important if you control\nyour client versions.\n\nYes\n\nYes\n\nCustom\nStorage\n\nIf you need storage with a different\nperformance mode (for example, a timeseries database instead of key-value store) or\nisolation for security (for example, encryption\nof sensitive information, etc.)\n\nNo\n\nYes\n\nCustom\nBusiness\nLogic\n\nPerform arbitrary checks or actions when\ncreating, reading, updating or deleting an\nobject\n\nYes, using Webhooks.\n\nYes\n\nScale\nSubresource\n\nAllows systems like HorizontalPodAutoscaler\nand PodDisruptionBudget interact with your\nnew resource\n\nYes\n\nYes\n\nStatus\nSubresource\n\nAllows fine-grained access control where user\nwrites the spec section and the controller\nwrites the status section. Allows incrementing\nobject Generation on custom resource data\nmutation (requires separate spec and status\nsections in the resource)\n\nYes\n\nYes\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n677/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFeature\n\nDescription\n\nCRDs\n\nAggregated\nAPI\n\nOther\nSubresources\n\nAdd operations other than CRUD, such as\n\"logs\" or \"exec\".\n\nNo\n\nYes\n\nstrategicmerge-patch\n\nThe new endpoints support PATCH with\n\nNo\n\nYes\n\nContent-Type:\napplication/strategic-mergepatch+json . Useful for updating objects\n\nthat may be modified both locally, and by the\nserver. For more information, see \"Update\nAPI Objects in Place Using kubectl patch\"\nProtocol\nBuffers\n\nThe new resource supports clients that want\nto use Protocol Buffers\n\nNo\n\nYes\n\nOpenAPI\nSchema\n\nIs there an OpenAPI (swagger) schema for the\ntypes that can be dynamically fetched from\nthe server? Is the user protected from\nmisspelling field names by ensuring only\nallowed fields are set? Are types enforced (in\nother words, don't put an int in a string\nfield?)\n\nYes, based on the OpenAPI v3.0 validation\nschema (GA in 1.16).\n\nYes\n\nInstance\nName\n\nDoes this extension mechanism impose any\nconstraints on the names of objects whose\nkind/resource is defined this way?\n\nYes, such an object's name must be a valid\nDNS subdomain name.\n\nNo"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0715", "text": "Common Features\nWhen you create a custom resource, either via a CRD or an AA, you get many features for your API, compared to implementing it\noutside the Kubernetes platform:\nFeature\n\nWhat it does\n\nCRUD\n\nThe new endpoints support CRUD basic operations via HTTP and kubectl\n\nWatch\n\nThe new endpoints support Kubernetes Watch operations via HTTP\n\nDiscovery\n\nClients like kubectl and dashboard automatically offer list, display, and field edit operations on\nyour resources\n\njson-patch\n\nThe new endpoints support PATCH with Content-Type: application/json-patch+json\n\nmerge-patch\n\nThe new endpoints support PATCH with Content-Type: application/merge-patch+json\n\nHTTPS\n\nThe new endpoints uses HTTPS\n\nBuilt-in Authentication\n\nAccess to the extension uses the core API server (aggregation layer) for authentication\n\nBuilt-in Authorization\n\nAccess to the extension can reuse the authorization used by the core API server; for example, RBAC.\n\nFinalizers\n\nBlock deletion of extension resources until external cleanup happens.\n\nAdmission Webhooks\n\nSet default values and validate extension resources during any create/update/delete operation.\n\nUI/CLI Display\n\nKubectl, dashboard can display extension resources.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n678/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nFeature\n\nWhat it does\n\nUnset versus Empty\n\nClients can distinguish unset fields from zero-valued fields.\n\nClient Libraries\nGeneration\n\nKubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.\n\nLabels and annotations\n\nCommon metadata across objects that tools know how to edit for core and custom resources.\n\nPreparing to install a custom resource\nThere are several points to be aware of before adding a custom resource to your cluster.\n\nThird party code and new points of failure\nWhile creating a CRD does not automatically add any new points of failure (for example, by causing third party code to run on your\nAPI server), packages (for example, Charts) or other installation bundles often include CRDs as well as a Deployment of third-party\ncode that implements the business logic for a new custom resource.\nInstalling an Aggregated API server always involves running a new Deployment.\n\nStorage\nCustom resources consume storage space in the same way that ConfigMaps do. Creating too many custom resources may overload\nyour API server's storage space.\nAggregated API servers may use the same storage as the main API server, in which case the same warning applies."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0716", "text": "Authentication, authorization, and auditing\nCRDs always use the same authentication, authorization, and audit logging as the built-in resources of your API server.\nIf you use RBAC for authorization, most RBAC roles will not grant access to the new resources (except the cluster-admin role or any\nrole created with wildcard rules). You'll need to explicitly grant access to the new resources. CRDs and Aggregated APIs often come\nbundled with new role definitions for the types they add.\nAggregated API servers may or may not use the same authentication, authorization, and auditing as the primary API server.\n\nAccessing a custom resource\nKubernetes client libraries can be used to access custom resources. Not all client libraries support custom resources. The Go and\nPython client libraries do.\nWhen you add a custom resource, you can access it using:\nkubectl\n\nThe Kubernetes dynamic client.\nA REST client that you write.\nA client generated using Kubernetes client generation tools (generating one is an advanced undertaking, but some projects\nmay provide a client along with the CRD or AA).\n\nCustom resource field selectors\nField Selectors let clients select custom resources based on the value of one or more resource fields.\nAll custom resources support the metadata.name and metadata.namespace field selectors.\nFields declared in a CustomResourceDefinition may also be used with field selectors when included in the\nspec.versions[*].selectableFields field of the CustomResourceDefinition.\nhttps://kubernetes.io/docs/concepts/_print/\n\n679/684\n\n11/7/25, 4:37 PM\n\nSelectable fields for custom resources\n\nConcepts | Kubernetes\n\nâ“˜ FEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)\n\nThe spec.versions[*].selectableFields field of a CustomResourceDefinition may be used to declare which other fields in a\ncustom resource may be used in field selectors.\nThe following example adds the .spec.color and .spec.size fields as selectable fields.\ncustomresourcedefinition/shirt-resource-definition.yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\nname: shirts.stable.example.com\nspec:\ngroup: stable.example.com\nscope: Namespaced\nnames:\nplural: shirts\nsingular: shirt\nkind: Shirt\nversions:\n- name: v1\nserved: true\nstorage: true\nschema:\nopenAPIV3Schema:\ntype: object\nproperties:\nspec:\ntype: object\nproperties:\ncolor:\ntype: string\nsize:\ntype: string\nselectableFields:\n- jsonPath: .spec.color\n- jsonPath: .spec.size\nadditionalPrinterColumns:\n- jsonPath: .spec.color\nname: Color\ntype: string\n- jsonPath: .spec.size\nname: Size\ntype: string\n\nField selectors can then be used to get only resources with a color of blue :\n\nkubectl get shirts.stable.example.com --field-selector spec.color=blue\n\nThe output should be:\nNAME\n\nCOLOR\n\nSIZE\n\nexample1\n\nblue\n\nS\n\nexample2\n\nblue\n\nM"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0717", "text": "https://kubernetes.io/docs/concepts/_print/\n\n680/684\n\n11/7/25, 4:37 PM\n\nWhat's next\n\nConcepts | Kubernetes\n\nLearn how to Extend the Kubernetes API with the aggregation layer.\nLearn how to Extend the Kubernetes API with CustomResourceDefinition.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n681/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\n13.2.2 - Kubernetes API Aggregation Layer\nThe aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is offered by the core Kubernetes APIs.\nThe additional APIs can either be ready-made solutions such as a metrics server, or APIs that you develop yourself.\nThe aggregation layer is different from Custom Resource Definitions, which are a way to make the kube-apiserver recognise new\nkinds of object.\n\nAggregation layer\nThe aggregation layer runs in-process with the kube-apiserver. Until an extension resource is registered, the aggregation layer will\ndo nothing. To register an API, you add an APIService object, which \"claims\" the URL path in the Kubernetes API. At that point, the\naggregation layer will proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/â€¦ ) to the registered APIService.\nThe most common way to implement the APIService is to run an extension API server in Pod(s) that run in your cluster. If you're using\nthe extension API server to manage resources in your cluster, the extension API server (also written as \"extension-apiserver\") is\ntypically paired with one or more controllers. The apiserver-builder library provides a skeleton for both extension API servers and\nthe associated controller(s).\n\nResponse latency\nExtension API servers should have low latency networking to and from the kube-apiserver. Discovery requests are required to\nround-trip from the kube-apiserver in five seconds or less.\nIf your extension API server cannot achieve that latency requirement, consider making changes that let you meet it.\n\nWhat's next\nTo get the aggregator working in your environment, configure the aggregation layer.\nThen, setup an extension api-server to work with the aggregation layer.\nRead about APIService in the API reference\nLearn about Declarative Validation Concepts, an internal mechanism for defining validation rules that in the future will help\nsupport validation for extension API server development.\nAlternatively: learn how to extend the Kubernetes API using Custom Resource Definitions.\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n682/684\n\n11/7/25, 4:37 PM\n\n13.3 - Operator pattern\n\nConcepts | Kubernetes\n\nOperators are software extensions to Kubernetes that make use of custom resources to manage applications and their components.\nOperators follow Kubernetes principles, notably the control loop."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0718", "text": "Motivation\nThe operator pattern aims to capture the key aim of a human operator who is managing a service or set of services. Human\noperators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy\nit, and how to react if there are problems.\nPeople who run workloads on Kubernetes often like to use automation to take care of repeatable tasks. The operator pattern\ncaptures how you can write code to automate a task beyond what Kubernetes itself provides.\n\nOperators in Kubernetes\nKubernetes is designed for automation. Out of the box, you get lots of built-in automation from the core of Kubernetes. You can use\nKubernetes to automate deploying and running workloads, and you can automate how Kubernetes does that.\nKubernetes' operator pattern concept lets you extend the cluster's behaviour without modifying the code of Kubernetes itself by\nlinking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a Custom\nResource."}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0719", "text": "An example operator\nSome of the things that you can use an operator to automate include:\ndeploying an application on demand\ntaking and restoring backups of that application's state\nhandling upgrades of the application code alongside related changes such as database schemas or extra configuration settings\npublishing a Service to applications that don't support Kubernetes APIs to discover them\nsimulating failure in all or part of your cluster to test its resilience\nchoosing a leader for a distributed application without an internal member election process\nWhat might an operator look like in more detail? Here's an example:\n1. A custom resource named SampleDB, that you can configure into the cluster.\n2. A Deployment that makes sure a Pod is running that contains the controller part of the operator.\n3. A container image of the operator code.\n4. Controller code that queries the control plane to find out what SampleDB resources are configured.\n5. The core of the operator is code to tell the API server how to make reality match the configured resources.\nIf you add a new SampleDB, the operator sets up PersistentVolumeClaims to provide durable database storage, a\nStatefulSet to run SampleDB and a Job to handle initial configuration.\nIf you delete it, the operator takes a snapshot, then makes sure that the StatefulSet and Volumes are also removed.\n6. The operator also manages regular database backups. For each SampleDB resource, the operator determines when to create a\nPod that can connect to the database and take backups. These Pods would rely on a ConfigMap and / or a Secret that has\ndatabase connection details and credentials.\n7. Because the operator aims to provide robust automation for the resource it manages, there would be additional supporting\ncode. For this example, code checks to see if the database is running an old version and, if so, creates Job objects that upgrade\nit for you.\n\nDeploying operators\nThe most common way to deploy an operator is to add the Custom Resource Definition and its associated Controller to your cluster.\nThe Controller will normally run outside of the control plane, much as you would run any containerized application. For example,\nyou can run the controller in your cluster as a Deployment.\nhttps://kubernetes.io/docs/concepts/_print/\n\n683/684\n\n11/7/25, 4:37 PM\n\nConcepts | Kubernetes\n\nUsing an operator"}
{"doc_id": "Concepts _ Kubernetes.txt", "chunk_id": "Concepts _ Kubernetes.txt__0720", "text": "Once you have an operator deployed, you'd use it by adding, modifying or deleting the kind of resource that the operator uses.\nFollowing the above example, you would set up a Deployment for the operator itself, and then:\n\nkubectl get SampleDB\n\n# find configured databases\n\nkubectl edit SampleDB/example-database # manually change some settings\n\nâ€¦and that's it! The operator will take care of applying the changes as well as keeping the existing service in good shape.\n\nWriting your own operator\nIf there isn't an operator in the ecosystem that implements the behavior you want, you can code your own.\nYou also implement an operator (that is, a Controller) using any language / runtime that can act as a client for the Kubernetes API.\nFollowing are a few libraries and tools you can use to write your own cloud native operator.\nNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project\nauthors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content\nguide before submitting a change. More information.\nCharmed Operator Framework\nJava Operator SDK\nKopf (Kubernetes Operator Pythonic Framework)\nkube-rs (Rust)\nkubebuilder\nKubeOps (.NET operator SDK)\nMast\nMetacontroller along with WebHooks that you implement yourself\nOperator Framework\nshell-operator\n\nWhat's next\nRead the CNCF Operator White Paper.\nLearn more about Custom Resources\nFind ready-made operators on OperatorHub.io to suit your use case\nPublish your operator for other people to use\nRead CoreOS' original article that introduced the operator pattern (this is an archived version of the original article).\nRead an article from Google Cloud about best practices for building operators\n\nhttps://kubernetes.io/docs/concepts/_print/\n\n684/684"}
